[ns_server:info,2025-05-15T21:09:32.147Z,nonode@nohost:<0.154.0>:ns_server:init_logging:180]Started & configured logging
[ns_server:warn,2025-05-15T21:09:32.153Z,nonode@nohost:<0.154.0>:config_profile:load:123]Could not load profile file ("/etc/couchbase.d/config_profile") because it does not exist
[ns_server:debug,2025-05-15T21:09:32.154Z,nonode@nohost:<0.154.0>:ns_server:setup_server_profile:108]Using profile 'default': [{name,"default"},
                          {{indexer,disable_shard_affinity},true}]
[ns_server:warn,2025-05-15T21:09:32.155Z,nonode@nohost:<0.154.0>:ns_server:config_profile_continuity_checker:129]Writing config_profile '"default"' to disk.
[ns_server:info,2025-05-15T21:09:32.156Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]Static config terms:
[{error_logger_mf_dir,"/opt/couchbase/var/lib/couchbase/logs"},
 {path_config_bindir,"/opt/couchbase/bin"},
 {path_config_etcdir,"/opt/couchbase/etc/couchbase"},
 {path_config_libdir,"/opt/couchbase/lib"},
 {path_config_datadir,"/opt/couchbase/var/lib/couchbase"},
 {path_config_tmpdir,"/opt/couchbase/var/lib/couchbase/tmp"},
 {path_config_secdir,"/opt/couchbase/etc/security"},
 {nodefile,"/opt/couchbase/var/lib/couchbase/couchbase-server.node"},
 {loglevel_default,debug},
 {loglevel_couchdb,info},
 {loglevel_ns_server,debug},
 {loglevel_error_logger,debug},
 {loglevel_user,debug},
 {loglevel_menelaus,debug},
 {loglevel_ns_doctor,debug},
 {loglevel_stats,debug},
 {loglevel_rebalance,debug},
 {loglevel_cluster,debug},
 {loglevel_views,debug},
 {loglevel_mapreduce_errors,debug},
 {loglevel_xdcr,debug},
 {loglevel_access,info},
 {loglevel_cbas,debug},
 {disk_sink_opts,
     [{rotation,
          [{compress,true},
           {size,41943040},
           {num_files,10},
           {buffer_size_max,52428800}]}]},
 {disk_sink_opts_disk_json_rpc,
     [{rotation,
          [{compress,true},
           {size,41943040},
           {num_files,2},
           {buffer_size_max,52428800}]}]},
 {disk_sink_opts_disk_tls_key_log,
     [{rotation,
          [{compress,true},
           {size,10485760},
           {num_files,1},
           {buffer_size_max,13107200}]}]},
 {net_kernel_verbosity,10}]
[ns_server:warn,2025-05-15T21:09:32.157Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter error_logger_mf_dir, which is given from command line
[ns_server:warn,2025-05-15T21:09:32.157Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter path_config_bindir, which is given from command line
[ns_server:warn,2025-05-15T21:09:32.157Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter path_config_etcdir, which is given from command line
[ns_server:warn,2025-05-15T21:09:32.157Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter path_config_libdir, which is given from command line
[ns_server:warn,2025-05-15T21:09:32.157Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter path_config_datadir, which is given from command line
[ns_server:warn,2025-05-15T21:09:32.157Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter path_config_tmpdir, which is given from command line
[ns_server:warn,2025-05-15T21:09:32.157Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter path_config_secdir, which is given from command line
[ns_server:warn,2025-05-15T21:09:32.157Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter nodefile, which is given from command line
[ns_server:warn,2025-05-15T21:09:32.157Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_default, which is given from command line
[ns_server:warn,2025-05-15T21:09:32.157Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_couchdb, which is given from command line
[ns_server:warn,2025-05-15T21:09:32.157Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_ns_server, which is given from command line
[ns_server:warn,2025-05-15T21:09:32.157Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_error_logger, which is given from command line
[ns_server:warn,2025-05-15T21:09:32.157Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_user, which is given from command line
[ns_server:warn,2025-05-15T21:09:32.157Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_menelaus, which is given from command line
[ns_server:warn,2025-05-15T21:09:32.157Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_ns_doctor, which is given from command line
[ns_server:warn,2025-05-15T21:09:32.157Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_stats, which is given from command line
[ns_server:warn,2025-05-15T21:09:32.157Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_rebalance, which is given from command line
[ns_server:warn,2025-05-15T21:09:32.157Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_cluster, which is given from command line
[ns_server:warn,2025-05-15T21:09:32.157Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_views, which is given from command line
[ns_server:warn,2025-05-15T21:09:32.157Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_mapreduce_errors, which is given from command line
[ns_server:warn,2025-05-15T21:09:32.157Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_xdcr, which is given from command line
[ns_server:warn,2025-05-15T21:09:32.157Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_access, which is given from command line
[ns_server:warn,2025-05-15T21:09:32.157Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_cbas, which is given from command line
[ns_server:warn,2025-05-15T21:09:32.157Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter disk_sink_opts, which is given from command line
[ns_server:warn,2025-05-15T21:09:32.157Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter disk_sink_opts_disk_json_rpc, which is given from command line
[ns_server:warn,2025-05-15T21:09:32.157Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter disk_sink_opts_disk_tls_key_log, which is given from command line
[ns_server:warn,2025-05-15T21:09:32.157Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter net_kernel_verbosity, which is given from command line
[ns_server:info,2025-05-15T21:09:32.161Z,nonode@nohost:dist_manager<0.215.0>:dist_manager:read_address_config_from_path:83]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip_start"
[ns_server:info,2025-05-15T21:09:32.161Z,nonode@nohost:dist_manager<0.215.0>:dist_manager:read_address_config_from_path:83]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2025-05-15T21:09:32.162Z,nonode@nohost:dist_manager<0.215.0>:dist_manager:init:180]ip config not found. Looks like we're brand new node
[ns_server:info,2025-05-15T21:09:32.163Z,nonode@nohost:dist_manager<0.215.0>:dist_manager:bringup:246]Attempting to bring up net_kernel with name 'ns_1@cb.local'
[error_logger:info,2025-05-15T21:09:32.171Z,nonode@nohost:ssl_dist_admin_sup<0.218.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_admin_sup}
    started: [{pid,<0.219.0>},
              {id,ssl_pem_cache_dist},
              {mfargs,{ssl_pem_cache,start_link_dist,[[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:32.171Z,nonode@nohost:ssl_dist_admin_sup<0.218.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_admin_sup}
    started: [{pid,<0.220.0>},
              {id,ssl_dist_manager},
              {mfargs,{ssl_manager,start_link_dist,[[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:32.171Z,nonode@nohost:ssl_dist_sup<0.217.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_sup}
    started: [{pid,<0.218.0>},
              {id,ssl_dist_admin_sup},
              {mfargs,{ssl_dist_admin_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T21:09:32.173Z,nonode@nohost:tls_dist_sup<0.221.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_sup}
    started: [{pid,<0.222.0>},
              {id,dist_tls_connection_sup},
              {mfargs,{tls_connection_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T21:09:32.174Z,nonode@nohost:tls_dist_server_sup<0.223.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_server_sup}
    started: [{pid,<0.224.0>},
              {id,dist_ssl_listen_tracker_sup},
              {mfargs,{ssl_listen_tracker_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T21:09:32.174Z,nonode@nohost:tls_dist_server_sup<0.223.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_server_sup}
    started: [{pid,<0.225.0>},
              {id,dist_tls_server_session_ticket},
              {mfargs,{tls_server_session_ticket_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T21:09:32.174Z,nonode@nohost:tls_dist_server_sup<0.223.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_server_sup}
    started: [{pid,<0.226.0>},
              {id,dist_ssl_upgrade_server_session_cache_sup},
              {mfargs,
                  {ssl_upgrade_server_session_cache_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T21:09:32.174Z,nonode@nohost:tls_dist_sup<0.221.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_sup}
    started: [{pid,<0.223.0>},
              {id,tls_dist_server_sup},
              {mfargs,{tls_dist_server_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T21:09:32.174Z,nonode@nohost:ssl_dist_sup<0.217.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_sup}
    started: [{pid,<0.221.0>},
              {id,tls_dist_sup},
              {mfargs,{tls_dist_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T21:09:32.174Z,nonode@nohost:net_sup<0.216.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.217.0>},
              {id,ssl_dist_sup},
              {mfargs,{ssl_dist_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T21:09:32.174Z,nonode@nohost:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Starting cb_dist with config [{config_vsn,2}]
[error_logger:info,2025-05-15T21:09:32.175Z,nonode@nohost:net_sup<0.216.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.227.0>},
              {id,cb_dist},
              {mfargs,{cb_dist,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:32.176Z,nonode@nohost:net_sup<0.216.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.228.0>},
              {id,auth},
              {mfargs,{auth,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,2000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T21:09:32.177Z,nonode@nohost:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Initial protos: [{external,inet_tcp_dist}], required protos: [{external,
                                                                        inet_tcp_dist}]
[ns_server:debug,2025-05-15T21:09:32.177Z,nonode@nohost:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Starting {external,inet_tcp_dist} listener on 21100...
[ns_server:debug,2025-05-15T21:09:32.179Z,nonode@nohost:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Started listener: inet_tcp_dist
[ns_server:debug,2025-05-15T21:09:32.219Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated server ssl_dist_opts (keep_secrets) - false
[ns_server:debug,2025-05-15T21:09:32.219Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Started acceptor inet_tcp_dist: <0.231.0>
[ns_server:debug,2025-05-15T21:09:32.219Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Ensure config is going to change listeners. Will be stopped: [], will be started: [], will be restarted: []
[error_logger:info,2025-05-15T21:09:32.219Z,ns_1@cb.local:net_sup<0.216.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.229.0>},
              {id,net_kernel},
              {mfargs,{net_kernel,start_link,
                                  [#{clean_halt => false,
                                     name => 'ns_1@cb.local',
                                     name_domain => longnames,
                                     net_tickintensity => 4,
                                     net_ticktime => 60,
                                     supervisor => net_sup_dynamic}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,2000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:32.219Z,ns_1@cb.local:kernel_sup<0.49.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,kernel_sup}
    started: [{pid,<0.216.0>},
              {id,net_sup_dynamic},
              {mfargs,
                  {erl_distribution,start_link,
                      [#{clean_halt => false,name => 'ns_1@cb.local',
                         name_domain => longnames,net_tickintensity => 4,
                         net_ticktime => 60,supervisor => net_sup_dynamic}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,60000},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T21:09:32.219Z,ns_1@cb.local:dist_manager<0.215.0>:dist_manager:configure_net_kernel:302]Set net_kernel vebosity to 10 -> 0
[ns_server:info,2025-05-15T21:09:32.221Z,ns_1@cb.local:dist_manager<0.215.0>:dist_manager:save_node:160]saving node name '"ns_1@cb.local"' to "/opt/couchbase/var/lib/couchbase/couchbase-server.node"
[ns_server:debug,2025-05-15T21:09:32.222Z,ns_1@cb.local:dist_manager<0.215.0>:dist_manager:bringup:269]Attempted to save node name to disk: ok
[ns_server:debug,2025-05-15T21:09:32.223Z,ns_1@cb.local:dist_manager<0.215.0>:dist_manager:wait_for_node:276]Waiting for connection to node 'babysitter_of_ns_1@cb.local' to be established
[error_logger:info,2025-05-15T21:09:32.223Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{connect,normal,'babysitter_of_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T21:09:32.224Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'babysitter_of_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T21:09:32.224Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.730592461.1793589253.72312>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T21:09:32.224Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.730592461.1793589253.72312>,
                                  inet_tcp_dist,<0.233.0>,
                                  #Ref<0.730592461.1793589253.72315>}
[ns_server:debug,2025-05-15T21:09:32.227Z,ns_1@cb.local:dist_manager<0.215.0>:dist_manager:wait_for_node:288]Observed node 'babysitter_of_ns_1@cb.local' to come up
[ns_server:info,2025-05-15T21:09:32.228Z,ns_1@cb.local:dist_manager<0.215.0>:dist_manager:save_address_config:147]Deleting irrelevant ip file "/opt/couchbase/var/lib/couchbase/ip_start": {error,
                                                                          enoent}
[ns_server:info,2025-05-15T21:09:32.228Z,ns_1@cb.local:dist_manager<0.215.0>:dist_manager:save_address_config:148]saving ip config to "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2025-05-15T21:09:32.229Z,ns_1@cb.local:dist_manager<0.215.0>:dist_manager:save_address_config:151]Persisted the address successfully
[error_logger:info,2025-05-15T21:09:32.232Z,ns_1@cb.local:root_sup<0.214.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,root_sup}
    started: [{pid,<0.215.0>},
              {id,dist_manager},
              {mfargs,{dist_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:32.236Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.236.0>},
              {id,local_tasks},
              {mfargs,{local_tasks,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[ns_server:info,2025-05-15T21:09:32.237Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:log_os_info:start_link:19]OS type: {unix,linux} Version: {6,10,14}
Runtime info: [{otp_release,"25"},
               {erl_version,"13.2.2.3"},
               {erl_version_long,
                   "Erlang/OTP 25 [erts-13.2.2.3] [source-15104f9619] [64-bit] [smp:10:10] [ds:10:10:10] [async-threads:16] [jit]\n"},
               {system_arch_raw,"aarch64-unknown-linux-gnu"},
               {system_arch,"aarch64-unknown-linux-gnu"},
               {localtime,{{2025,5,15},{21,9,32}}},
               {memory,
                   [{total,44142592},
                    {processes,9984928},
                    {processes_used,9975728},
                    {system,34157664},
                    {atom,540873},
                    {atom_used,522143},
                    {binary,130448},
                    {code,9676182},
                    {ets,2639560}]},
               {loaded,
                   [ns_info,log_os_info,local_tasks,restartable,
                    ns_server_cluster_sup,ns_cluster,dist_util,ns_node_disco,
                    crypto,inet_tcp,re,auth,tls_dist_server_sup,tls_dist_sup,
                    ssl_dist_admin_sup,ssl_dist_sup,inet_tls_dist,
                    inet_tcp_dist,cb_epmd,gen_udp,inet_hosts,dist_manager,
                    root_sup,cb_dist,path_config,config_profile,
                    ns_server_stats,calendar,ale_default_formatter,
                    'ale_logger-tls_key','ale_logger-metakv',
                    'ale_logger-rebalance','ale_logger-chronicle',
                    'ale_logger-ns_server_trace','ale_logger-menelaus',
                    'ale_logger-stats','ale_logger-json_rpc',
                    'ale_logger-access','ale_logger-ns_server',
                    'ale_logger-user','ale_logger-ns_doctor',
                    'ale_logger-cluster','ale_logger-xdcr',erl_bits,
                    otp_internal,cb_log_counter_sink,ns_log_sink,
                    ale_disk_sink,misc,couch_util,ns_server,esaml_util,esaml,
                    ale_error_logger_handler,timer,cpu_sup,filelib,memsup,
                    disksup,os_mon,unicode_util,string,io,release_handler,
                    alarm_handler,sasl,httpd_sup,httpc_handler_sup,
                    httpc_cookie,inets_trace,httpc_manager,httpc,
                    httpc_profile_sup,httpc_sup,inets_sup,inets_app,ssl,
                    lhttpc_manager,lhttpc_sup,lhttpc,
                    dtls_server_session_cache_sup,dtls_listener_sup,
                    dtls_server_sup,dtls_connection_sup,dtls_sup,
                    ssl_upgrade_server_session_cache_sup,
                    ssl_server_session_cache_sup,
                    tls_server_session_ticket_sup,ssl_listen_tracker_sup,
                    tls_server_sup,tls_connection_sup,tls_sup,
                    ssl_connection_sup,tls_client_ticket_store,
                    ssl_client_session_cache_db,ssl_config,ssl_manager,
                    ssl_pkix_db,ssl_pem_cache,ssl_admin_sup,ssl_sup,
                    logger_h_common,logger_std_h,ssl_logger,ssl_app,
                    'ale_logger-trace_logger','ale_logger-ale_logger',
                    'ale_logger-error_logger',beam_opcodes,beam_dict,beam_asm,
                    beam_z,beam_flatten,beam_trim,beam_clean,beam_block,
                    beam_utils,beam_jump,beam_a,beam_validator,
                    beam_ssa_codegen,beam_ssa_pre_codegen,beam_ssa_throw,
                    beam_ssa_dead,beam_call_types,beam_types,beam_ssa_type,
                    beam_ssa_bc_size,beam_ssa_opt,beam_ssa_bsm,beam_ssa_recv,
                    beam_ssa_share,beam_ssa_bool,beam_ssa,beam_kernel_to_ssa,
                    v3_kernel,sys_core_bsm,sys_core_alias,erl_bifs,
                    cerl_clauses,sys_core_fold,sys_core_inline,cerl_trees,
                    core_lib,cerl,sets,v3_core,erl_expand_records,sofs,
                    erl_internal,ordsets,compile,dynamic_compile,ale_utils,
                    io_lib_pretty,io_lib_format,io_lib,ale_codegen,dict,ale,
                    ale_dynamic_sup,ale_sup,ale_app,ns_bootstrap,child_erlang,
                    raw_file_io,orddict,c,erl_signal_handler,
                    logger_handler_watcher,logger_sup,kernel_refc,
                    kernel_config,user_io,user_sup,supervisor_bridge,
                    standard_error,erpc,global_group,erl_distribution,maps,
                    rand,net_kernel,global,rpc,epp,inet_parse,inet,inet_udp,
                    inet_config,inet_db,unicode,os,gb_trees,gb_sets,binary,
                    beam_lib,peer,erl_anno,erl_features,proplists,erl_scan,
                    queue,logger_olp,logger_proxy,error_handler,application,
                    application_master,application_controller,error_logger,
                    code,code_server,file_server,file,heart,file_io_server,
                    kernel,logger_filters,logger_backend,erl_eval,
                    logger_server,logger_config,logger_simple_h,logger,
                    proc_lib,gen,supervisor,gen_event,gen_server,lists,
                    filename,ets,erl_lint,erl_parse,persistent_term,counters,
                    atomics,erts_dirty_process_signal_handler,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,prim_net,prim_socket,
                    socket_registry,zlib,prim_file,prim_inet,prim_eval,
                    prim_buffer,init,erl_init,erts_code_purger]},
               {applications,
                   [{stdlib,"ERTS  CXC 138 10","4.3.1.2"},
                    {sasl,"SASL  CXC 138 11","4.2"},
                    {public_key,"Public key infrastructure","1.13.3.1"},
                    {crypto,"CRYPTO","5.1.4.1"},
                    {lhttpc,"Lightweight HTTP Client","1.3.0"},
                    {asn1,"The Erlang ASN1 compiler version 5.0.21","5.0.21"},
                    {ssl,"Erlang/OTP SSL application","10.9.1.2"},
                    {esaml,"SAML Server Provider library for erlang","4.4.0"},
                    {ale,"Another Logger for Erlang","0.0.0"},
                    {xmerl,"XML parser","1.3.31.1"},
                    {inets,"INETS  CXC 138 49","8.3.1.2"},
                    {os_mon,"CPO  CXC 138 46","2.8.2"},
                    {ns_server,"Couchbase server","7.6.2-3721-enterprise"},
                    {kernel,"ERTS  CXC 138 10","8.5.4.2"}]},
               {pre_loaded,
                   [persistent_term,counters,atomics,
                    erts_dirty_process_signal_handler,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,prim_net,prim_socket,
                    socket_registry,zlib,prim_file,prim_inet,prim_eval,
                    prim_buffer,init,erl_init,erts_code_purger]},
               {process_count,159},
               {node,'ns_1@cb.local'},
               {nodes,[]},
               {registered,
                   [ssl_connection_sup,standard_error,net_kernel,
                    ssl_upgrade_server_session_cache_sup,kernel_refc,
                    'sink-disk_debug',ale_sup,httpd_sup,
                    ssl_upgrade_server_session_cache_sup_dist,
                    tls_client_ticket_store,dtls_server_session_cache_sup,
                    'sink-disk_trace',inets_sup,tls_sup,erts_code_purger,
                    'sink-disk_tls_key_log',dist_manager,ssl_pem_cache_dist,
                    cb_dist,ale,erl_prim_loader,global_group,cpu_sup,init,
                    application_controller,ns_server_cluster_sup,
                    httpc_manager,erl_signal_server,ssl_dist_sup,
                    tls_dist_connection_sup,ssl_sup,esaml,logger_sup,
                    kernel_safe_sup,'sink-disk_error',
                    tls_server_session_ticket_sup_dist,ssl_listen_tracker_sup,
                    os_mon_sup,'sink-disk_reports',dtls_sup,rex,tls_dist_sup,
                    tls_server_session_ticket_sup,'sink-disk_metakv',disksup,
                    ssl_manager,'sink-ns_log',logger_proxy,
                    'sink-disk_default',ale_dynamic_sup,
                    ssl_listen_tracker_sup_dist,auth,logger_std_h_ssl_handler,
                    standard_error_sup,'sink-disk_stats',httpc_profile_sup,
                    kernel_sup,ssl_admin_sup,inet_db,user,sasl_safe_sup,
                    release_handler,'sink-disk_access_int',
                    ssl_server_session_cache_sup,root_sup,logger,
                    alarm_handler,lhttpc_manager,dtls_server_sup,
                    logger_handler_watcher,httpc_sup,tls_dist_server_sup,
                    global_group_check,'sink-disk_json_rpc',local_tasks,
                    memsup,ssl_pem_cache,'sink-disk_xdcr',dtls_connection_sup,
                    global_name_server,file_server_2,httpc_handler_sup,
                    sasl_sup,tls_connection_sup,ssl_manager_dist,lhttpc_sup,
                    'sink-disk_access',dtls_listener_sup,ale_stats_events,
                    socket_registry,code_server,net_sup,esaml_ets_table_owner,
                    'sink-cb_log_counter',tls_server_sup,ssl_dist_admin_sup]},
               {cookie,nocookie},
               {wordsize,8},
               {wall_clock,2}]
[ns_server:info,2025-05-15T21:09:32.241Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:log_os_info:start_link:21]Manifest:
["<?xml version=\"1.0\" encoding=\"UTF-8\"?>","<manifest>",
 "  <remote name=\"blevesearch\" fetch=\"https://github.com/blevesearch/\"/>",
 "  <remote name=\"couchbase\" fetch=\"https://github.com/couchbase/\" review=\"review.couchbase.org\"/>",
 "  <remote name=\"couchbase-priv\" fetch=\"ssh://git@github.com/couchbase/\" review=\"review.couchbase.org\"/>",
 "  <remote name=\"couchbasedeps\" fetch=\"https://github.com/couchbasedeps/\" review=\"review.couchbase.org\"/>",
 "  <remote name=\"couchbaselabs\" fetch=\"https://github.com/couchbaselabs/\" review=\"review.couchbase.org\"/>",
 "  ","  <default remote=\"couchbase\" revision=\"master\"/>","  ",
 "  <project name=\"HdrHistogram_c\" path=\"third_party/HdrHistogram_c\" remote=\"couchbasedeps\" revision=\"9ead6b88adbf8d6131e5ae7a3a699c477a3b4195\" groups=\"kv\"/>",
 "  <project name=\"analytics-dcp-client\" path=\"analytics/java-dcp-client\" revision=\"081d9934d4a28b4abdadcd13891792ea423416c0\" upstream=\"trinity\" dest-branch=\"trinity\" groups=\"notdefault,enterprise,analytics\"/>",
 "  <project name=\"asterixdb\" path=\"analytics/asterixdb\" revision=\"6a10f3f81db977c706447ece476b487cbe56414c\" groups=\"notdefault,enterprise,analytics\"/>",
 "  <project name=\"backup\" remote=\"couchbase-priv\" revision=\"192d7500ba2a7b5281d2c61af126c8027bbb858d\" groups=\"backup,notdefault,enterprise\"/>",
 "  <project name=\"build\" path=\"cbbuild\" revision=\"cedf9d4ec929eac7e61f8e86488aeac5402c8563\" groups=\"notdefault,build\">",
 "    <annotation name=\"RELEASE\" value=\"trinity\"/>",
 "    <annotation name=\"PRODUCT\" value=\"couchbase-server\"/>",
 "    <annotation name=\"BLD_NUM\" value=\"3721\"/>",
 "    <annotation name=\"VERSION\" value=\"7.6.2\"/>","  </project>",
 "  <project name=\"cbas\" path=\"goproj/src/github.com/couchbase/cbas\" remote=\"couchbase-priv\" revision=\"2db6eb59fd5af47a1ce81d53c8f4e58c7a14df3a\" groups=\"notdefault,enterprise,analytics\"/>",
 "  <project name=\"cbas-core\" path=\"analytics\" remote=\"couchbase-priv\" revision=\"2c1d23ee3aba4c80196d9d94ceaca3917b8ea8a7\" upstream=\"7.6.2\" dest-branch=\"7.6.2\" groups=\"notdefault,enterprise,analytics\"/>",
 "  <project name=\"cbas-ui\" revision=\"704db180d01de15f70cacc9fc11c5d8d8d4ff965\" groups=\"notdefault,enterprise,analytics\"/>",
 "  <project name=\"cbauth\" path=\"goproj/src/github.com/couchbase/cbauth\" revision=\"a9992170165a1d330cb5a9918a29d5bd417c5e46\" groups=\"backup\"/>",
 "  <project name=\"cbbs\" remote=\"couchbase-priv\" revision=\"f1d0272decc7f1b445b08e56e1f75e99f743aa90\" groups=\"backup,notdefault,enterprise\"/>",
 "  <project name=\"cbft\" revision=\"69d32cca4a8eca6e5aad5dad689795ab72ecdd6e\"/>",
 "  <project name=\"cbftx\" remote=\"couchbase-priv\" revision=\"258e3829db59f06a202ea2435c776a351a590eba\" groups=\"notdefault,enterprise\"/>",
 "  <project name=\"cbgt\" revision=\"b7dd01a11c5c56fbca88b9b950d8eca4dacce36f\"/>",
 "  <project name=\"cbsummary\" path=\"goproj/src/github.com/couchbase/cbsummary\" revision=\"fb656c91554a97318c44f58e3cc7f166f1eef4fc\"/>",
 "  <project name=\"chronicle\" path=\"ns_server/deps/chronicle\" revision=\"8d1feeb0d8b15e2b6a4c1a417addfd159b422a71\"/>",
 "  <project name=\"client_golang\" path=\"godeps/src/github.com/prometheus/client_golang\" remote=\"couchbasedeps\" revision=\"2e1c4818ccfdcf953ce399cadad615ff2bed968c\" upstream=\"refs/tags/v1.12.1\" dest-branch=\"refs/tags/v1.12.1\"/>",
 "  <project name=\"client_model\" path=\"godeps/src/github.com/prometheus/client_model\" remote=\"couchbasedeps\" revision=\"6dc836ede0b5b08c61893c3ffeb474498b18bb83\"/>",
 "  <project name=\"clog\" path=\"godeps/src/github.com/couchbase/clog\" revision=\"f935d1fdfc36541b505cf86fea4822e4067f9c39\"/>",
 "  <project name=\"common\" path=\"godeps/src/github.com/prometheus/common\" remote=\"couchbasedeps\" revision=\"902cb39e6c079571d32c2db8da220da13c11b562\" upstream=\"refs/tags/v0.33.0\" dest-branch=\"refs/tags/v0.33.0\"/>",
 "  <project name=\"couchbase-cli\" revision=\"941f6d7bbac8f8a42870c3f5459376b9f19ef1fd\" groups=\"kv\"/>",
 "  <project name=\"couchdb\" revision=\"3e5b8f248d77dd9317b36b50eed2567bcfb5f4cf\" dest-branch=\"unstable\"/>",
 "  <project name=\"couchdbx-app\" revision=\"702647dd015e7443de9cdb789806351774e85463\" groups=\"notdefault,packaging\"/>",
 "  <project name=\"couchstore\" revision=\"ce7305bab3feb64bd2504f34d24a1419008e8bda\" groups=\"kv\"/>",
 "  <project name=\"crypto\" path=\"godeps/src/golang.org/x/crypto\" remote=\"couchbasedeps\" revision=\"eb61739cd99fb244c7cd188d3c5bae54824e781d\" upstream=\"refs/tags/v0.15.0\" dest-branch=\"refs/tags/v0.15.0\"/>",
 "  <project name=\"docloader\" path=\"goproj/src/github.com/couchbase/docloader\" revision=\"cf3254d7dfb042192c9a23bd2e64a281c32a29d8\"/>",
 "  <project name=\"errors\" path=\"godeps/src/github.com/pkg/errors\" remote=\"couchbasedeps\" revision=\"30136e27e2ac8d167177e8a583aa4c3fea5be833\"/>",
 "  <project name=\"eventing\" path=\"goproj/src/github.com/couchbase/eventing\" revision=\"047b756132464b8f756cc35e02a15b5f498f80d5\" dest-branch=\"unstable\" groups=\"notdefault,enterprise\"/>",
 "  <project name=\"eventing-ee\" path=\"goproj/src/github.com/couchbase/eventing-ee\" remote=\"couchbase-priv\" revision=\"5425f180a0756868524081f889ab224cfc10b70d\" dest-branch=\"unstable\" groups=\"notdefault,enterprise\"/>",
 "  <project name=\"flatbuffers\" path=\"godeps/src/github.com/google/flatbuffers\" remote=\"couchbasedeps\" revision=\"1a8968225130caeddd16e227678e6f8af1926303\"/>",
 "  <project name=\"forestdb\" revision=\"9efe6d75d7d61e742af70fb47fe97ad1d04ba86f\" groups=\"backup\"/>",
 "  <project name=\"geocouch\" revision=\"68f3b9d36630682d17ca5232770f1693b9b8fa18\"/>",
 "  <project name=\"go-couchbase\" path=\"goproj/src/github.com/couchbase/go-couchbase\" revision=\"959eaf944140a6c660990f38b1db310ddd6d8e42\" groups=\"backup\"/>",
 "  <project name=\"go-metrics\" path=\"godeps/src/github.com/rcrowley/go-metrics\" remote=\"couchbasedeps\" revision=\"cf1acfcdf4751e0554ffa765d03e479ec491cad6\"/>",
 "  <project name=\"go_json\" path=\"goproj/src/github.com/couchbase/go_json\" revision=\"d6e17ad2b9a218e82569e09b761c226fa8df726a\"/>",
 "  <project name=\"gocb\" path=\"godeps/src/github.com/couchbase/gocb/v2\" revision=\"40020eef484873e507745107edbf99c421927a93\" upstream=\"refs/tags/v2.2.5\" dest-branch=\"refs/tags/v2.2.5\"/>",
 "  <project name=\"gocbcore\" path=\"godeps/src/github.com/couchbase/gocbcore/v9\" revision=\"0ece206041d8cf5f5fcd919767446603691bdb69\" upstream=\"refs/tags/v9.1.6\" dest-branch=\"refs/tags/v9.1.6\"/>",
 "  <project name=\"godbc\" path=\"goproj/src/github.com/couchbase/godbc\" revision=\"2d3ecc3de903a5e4d0bc9181adedb5e637f83435\"/>",
 "  <project name=\"gojsonsm\" path=\"godeps/src/github.com/couchbaselabs/gojsonsm\" remote=\"couchbaselabs\" revision=\"8db06ae62940835d35db4de075bd68f0e00ea6b7\" groups=\"bsl\"/>",
 "  <project name=\"golang\" remote=\"couchbaselabs\" revision=\"4dd1b189981c94835b61c1607ca765e88604ce5a\" groups=\"kv\"/>",
 "  <project name=\"golang-pkg-pcre\" path=\"godeps/src/github.com/glenn-brown/golang-pkg-pcre\" remote=\"couchbasedeps\" revision=\"48bb82a8b8ceea98f4e97825b43870f6ba1970d6\"/>",
 "  <project name=\"golang-snappy\" path=\"godeps/src/github.com/golang/snappy\" remote=\"couchbasedeps\" revision=\"723cc1e459b8eea2dea4583200fd60757d40097a\"/>",
 "  <project name=\"golang-tools\" path=\"godeps/src/golang.org/x/tools\" remote=\"couchbasedeps\" revision=\"a28dfb48e06b2296b66678872c2cb638f0304f20\"/>",
 "  <project name=\"golang_protobuf_extensions\" path=\"godeps/src/github.com/matttproud/golang_protobuf_extensions\" remote=\"couchbasedeps\" revision=\"c182affec369e30f25d3eb8cd8a478dee585ae7d\"/>",
 "  <project name=\"gomemcached\" path=\"goproj/src/github.com/couchbase/gomemcached\" revision=\"689b8f03386ba2e7bac304bfd3a525b1e1427675\" groups=\"backup\"/>",
 "  <project name=\"gometa\" path=\"goproj/src/github.com/couchbase/gometa\" revision=\"816f7d6346c9fc5473c4a11e3efe9ed29a2f7f72\"/>",
 "  <project name=\"goutils\" path=\"goproj/src/github.com/couchbase/goutils\" revision=\"30adfca73d8113b5b217097414d7c3adeeef849a\" groups=\"bsl\"/>",
 "  <project name=\"goxdcr\" path=\"goproj/src/github.com/couchbase/goxdcr\" revision=\"4c570a31e5a6f3e087e147edf781022352497f64\" groups=\"bsl\"/>",
 "  <project name=\"gsl-lite\" path=\"third_party/gsl-lite\" remote=\"couchbasedeps\" revision=\"e1c381746c2625a76227255f999ae9f14a062208\" upstream=\"refs/tags/v0.38.1\" dest-branch=\"refs/tags/v0.38.1\" groups=\"kv\"/>",
 "  <project name=\"hebrew\" remote=\"couchbase-priv\" revision=\"c57616b187889a5318688f49817ccaceb9c098b9\" groups=\"notdefault,enterprise\"/>",
 "  <project name=\"indexing\" path=\"goproj/src/github.com/couchbase/indexing\" revision=\"7e924978fef8498113ae2d6e2be8ccd27da70d2d\" groups=\"bsl\"/>",
 "  <project name=\"jsonschema\" path=\"godeps/src/github.com/santhosh-tekuri/jsonschema\" remote=\"couchbasedeps\" revision=\"137f44a49015e5060a447c331aa37de6e0f50267\"/>",
 "  <project name=\"kv_engine\" revision=\"cfff435cf5fe6efcf1a18aa5580993fb8a4b2010\" groups=\"kv,bsl\"/>",
 "  <project name=\"libcouchbase\" revision=\"684931e59cd87e0c6292e8142c2b18897be5b10c\"/>",
 "  <project name=\"magma\" remote=\"couchbase-priv\" revision=\"86c2233ab8780e7aa71e0199bb957dcda2cf6cd1\" groups=\"notdefault,enterprise,kv_ee\"/>",
 "  <project name=\"mux\" path=\"godeps/src/github.com/gorilla/mux\" remote=\"couchbasedeps\" revision=\"043ee6597c29786140136a5747b6a886364f5282\"/>",
 "  <project name=\"n1fty\" path=\"goproj/src/github.com/couchbase/n1fty\" revision=\"a1fc533c18e5094ce75262c9e711d7189d256cd2\" groups=\"bsl\"/>",
 "  <project name=\"nitro\" path=\"goproj/src/github.com/couchbase/nitro\" revision=\"b70d849f0207f7cfe7ebf32b2db35b534929e041\" groups=\"bsl\"/>",
 "  <project name=\"ns_server\" revision=\"6954b533143cf0a8f906ef9086a8337ee50004a6\" groups=\"bsl\"/>",
 "  <project name=\"participle\" path=\"godeps/src/github.com/alecthomas/participle\" remote=\"couchbasedeps\" revision=\"d638c6e1953ed899e05a34da3935146790c60e46\"/>",
 "  <project name=\"perks\" path=\"godeps/src/github.com/beorn7/perks\" remote=\"couchbasedeps\" revision=\"37c8de3658fcb183f997c4e13e8337516ab753e6\" upstream=\"refs/tags/v1.0.1\" dest-branch=\"refs/tags/v1.0.1\"/>",
 "  <project name=\"phosphor\" revision=\"c0a034fe407eec4723f2e01db2d72762efdbc276\" groups=\"bsl,kv\"/>",
 "  <project name=\"pkcs8\" path=\"godeps/src/github.com/youmark/pkcs8\" remote=\"couchbasedeps\" revision=\"1be2e3e5546da8a58903ff4adcfab015022538ea\" upstream=\"refs/tags/v1.1\" dest-branch=\"refs/tags/v1.1\"/>",
 "  <project name=\"plasma\" path=\"goproj/src/github.com/couchbase/plasma\" remote=\"couchbase-priv\" revision=\"34f14cf1d8cc543932e60d9780cc13d48fa7ea5c\" groups=\"bsl,notdefault,enterprise\"/>",
 "  <project name=\"platform\" revision=\"a158d359293665b6251973868fdc42c3b642474c\" groups=\"bsl,kv\"/>",
 "  <project name=\"procfs\" path=\"godeps/src/github.com/prometheus/procfs\" remote=\"couchbasedeps\" revision=\"76fc8b844e3a18c31bf689e4fe7efdd5a2f41298\"/>",
 "  <project name=\"product-metadata\" revision=\"1bd027c34f33919f7005ddae0ba032a3120fe776\" groups=\"notdefault,packaging\"/>",
 "  <project name=\"product-texts\" revision=\"ec39f811376df6d18e56c81873fd565093666505\" upstream=\"master\" dest-branch=\"master\"/>",
 "  <project name=\"protobuf\" path=\"godeps/src/github.com/golang/protobuf\" remote=\"couchbasedeps\" revision=\"d04d7b157bb510b1e0c10132224b616ac0e26b17\" upstream=\"refs/tags/v1.4.2\" dest-branch=\"refs/tags/v1.4.2\"/>",
 "  <project name=\"protobuf-go\" path=\"godeps/src/google.golang.org/protobuf\" remote=\"couchbasedeps\" revision=\"32051b4f86e54c2142c7c05362c6e96ae3454a1c\" upstream=\"refs/tags/v1.28.0\" dest-branch=\"refs/tags/v1.28.0\"/>",
 "  <project name=\"query\" path=\"goproj/src/github.com/couchbase/query\" revision=\"5da4cb71df5aa00aad1e01e9a3b9d5be0c4f2769\" groups=\"bsl\"/>",
 "  <project name=\"query-ee\" path=\"goproj/src/github.com/couchbase/query-ee\" remote=\"couchbase-priv\" revision=\"6924a352019351c746fe08a2cf9a1993b54093e8\" groups=\"notdefault,enterprise\"/>",
 "  <project name=\"query-ui\" revision=\"abcc90e091c46ad74a59bb2fe768b6f09864ddbf\" groups=\"bsl\"/>",
 "  <project name=\"regulator\" path=\"goproj/src/github.com/couchbase/regulator\" remote=\"couchbase-priv\" revision=\"4ef404748ecc34fd87bdebc56074ebe99d240464\" groups=\"notdefault,enterprise\"/>",
 "  <project name=\"sigar\" revision=\"2da0c123cfb45ae39e76e730bd960db8812e3f20\" groups=\"kv\"/>",
 "  <project name=\"simdutf\" path=\"third_party/simdutf\" remote=\"couchbasedeps\" revision=\"4a212616ba23c65c7048f9604faccbff5353300f\" upstream=\"refs/tags/v3.2.14\" dest-branch=\"refs/tags/v3.2.14\" groups=\"kv\"/>",
 "  <project name=\"subjson\" revision=\"a619faccb30e43a4bc0708ee11b1b24abb349f18\" groups=\"bsl,kv\"/>",
 "  <project name=\"sys\" path=\"godeps/src/golang.org/x/sys\" remote=\"couchbasedeps\" revision=\"d36c6a25d886e7c9975d5bf247ac24887ba6da37\"/>",
 "  <project name=\"testrunner\" revision=\"b2d76b82c7d75a75ae78f787a76dc8300c427adf\" upstream=\"trinity\" dest-branch=\"trinity\"/>",
 "  <project name=\"tlm\" revision=\"0c610d8e4738567440ffb1f557dfa15bff81b99d\" upstream=\"7.6.2\" dest-branch=\"7.6.2\" groups=\"bsl,kv\">",
 "    <copyfile src=\"Build.sh\" dest=\"Build.sh\"/>",
 "    <copyfile src=\"GNUmakefile\" dest=\"GNUmakefile\"/>",
 "    <copyfile src=\"Makefile\" dest=\"Makefile\"/>",
 "    <copyfile src=\"CMakeLists.txt\" dest=\"CMakeLists.txt\"/>",
 "    <copyfile src=\"dot-clang-format\" dest=\".clang-format\"/>",
 "    <copyfile src=\"dot-clang-tidy\" dest=\".clang-tidy\"/>",
 "    <copyfile src=\"third-party-CMakeLists.txt\" dest=\"third_party/CMakeLists.txt\"/>",
 "  </project>",
 "  <project name=\"udf-api\" path=\"goproj/src/github.com/couchbase/udf-api\" revision=\"b2788ae3d412356a330b36d7f38ad2c66edb5879\"/>",
 "  <project name=\"uuid\" path=\"godeps/src/github.com/google/uuid\" remote=\"couchbasedeps\" revision=\"dec09d789f3dba190787f8b4454c7d3c936fed9e\"/>",
 "  <project name=\"vbmap\" revision=\"6cce93c4af4497d8108c3ed31b84d7139321cc82\"/>",
 "  <project name=\"voltron\" remote=\"couchbase-priv\" revision=\"19881dacfffb6d834a7aaa4a6d1925a904ea387f\" groups=\"notdefault,packaging\"/>",
 "  <project name=\"xxhash\" path=\"goproj/src/github.com/cespare/xxhash\" remote=\"couchbasedeps\" revision=\"e7a6b52374f7e2abfb8abb27249d53a1997b09a7\" upstream=\"refs/tags/v2.1.2\" dest-branch=\"refs/tags/v2.1.2\"/>",
 "</manifest>"]

[error_logger:info,2025-05-15T21:09:32.244Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.237.0>},
              {id,timeout_diag_logger},
              {mfargs,{timeout_diag_logger,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:32.245Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.238.0>},
              {id,ns_cookie_manager},
              {mfargs,{ns_cookie_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T21:09:32.247Z,ns_1@cb.local:chronicle_local<0.239.0>:chronicle_local:init:54]Ensure chronicle is started
[error_logger:info,2025-05-15T21:09:32.260Z,ns_1@cb.local:chronicle_sup<0.244.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.245.0>},
              {id,chronicle_events},
              {mfargs,{chronicle_events,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:32.260Z,ns_1@cb.local:chronicle_sup<0.244.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.246.0>},
              {id,chronicle_external_events},
              {mfargs,{gen_event,start_link,
                                 [{local,chronicle_external_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:32.261Z,ns_1@cb.local:chronicle_sup<0.244.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.247.0>},
              {id,chronicle_ets},
              {mfargs,{chronicle_ets,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:32.262Z,ns_1@cb.local:chronicle_sup<0.244.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.248.0>},
              {id,chronicle_settings},
              {mfargs,{chronicle_settings,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:32.264Z,ns_1@cb.local:chronicle_agent_sup<0.249.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_agent_sup}
    started: [{pid,<0.250.0>},
              {id,chronicle_snapshot_mgr},
              {mfargs,{chronicle_snapshot_mgr,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:32.264Z,ns_1@cb.local:chronicle_agent_sup<0.249.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_agent_sup}
    started: [{pid,<0.251.0>},
              {id,chronicle_rsm_events},
              {mfargs,{chronicle_events,start_link,[chronicle_rsm_events]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[chronicle:info,2025-05-15T21:09:32.275Z,ns_1@cb.local:chronicle_agent<0.252.0>:chronicle_storage:open_logs:180]Reading log files [{0,
                    "/opt/couchbase/var/lib/couchbase/config/chronicle/logs/0.log"}]
[chronicle:info,2025-05-15T21:09:32.277Z,ns_1@cb.local:chronicle_agent<0.252.0>:chronicle_storage:open_current_log:232]Error while opening log file "/opt/couchbase/var/lib/couchbase/config/chronicle/logs/0.log": no_header
[chronicle:info,2025-05-15T21:09:32.277Z,ns_1@cb.local:chronicle_agent<0.252.0>:chronicle_storage:create_log:280]Creating log file /opt/couchbase/var/lib/couchbase/config/chronicle/logs/0.log (high seqno = 0)
[chronicle:info,2025-05-15T21:09:32.278Z,ns_1@cb.local:chronicle_agent<0.252.0>:chronicle_agent:maybe_seed_storage:2444]Found empty storage. Seeding it with default metadata:
#{committed_seqno => 0,history_id => <<"no-history">>,peer => nonode@nohost,
  peer_id => <<>>,pending_branch => undefined,state => not_provisioned,
  term => {0,nonode@nohost}}
[error_logger:info,2025-05-15T21:09:32.278Z,ns_1@cb.local:chronicle_agent_sup<0.249.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_agent_sup}
    started: [{pid,<0.252.0>},
              {id,chronicle_agent},
              {mfargs,{chronicle_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:32.278Z,ns_1@cb.local:chronicle_sup<0.244.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.249.0>},
              {id,chronicle_agent_sup},
              {mfargs,{chronicle_agent_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T21:09:32.280Z,ns_1@cb.local:chronicle_sup<0.244.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.254.0>},
              {id,chronicle_secondary_sup},
              {mfargs,{chronicle_secondary_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T21:09:32.280Z,ns_1@cb.local:application_controller<0.44.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    application: chronicle
    started_at: 'ns_1@cb.local'

[ns_server:debug,2025-05-15T21:09:32.281Z,ns_1@cb.local:chronicle_local<0.239.0>:chronicle_local:init:58]Chronicle state is: not_provisioned
[ns_server:debug,2025-05-15T21:09:32.281Z,ns_1@cb.local:chronicle_local<0.239.0>:chronicle_local:provision:139]Provision chronicle on this node
[chronicle:debug,2025-05-15T21:09:32.282Z,ns_1@cb.local:chronicle_agent<0.252.0>:chronicle_agent:handle_provision:1199]Provisioning with history <<"29b17497cea731943091527ebf7f7991">>. Config:
{config,undefined,0,undefined,
        #{'ns_1@cb.local' =>
              #{id => <<"7dc8077bc3da6cde870e5aa3464eff05">>,role => voter}},
        undefined,
        #{chronicle_config_rsm => {rsm_config,chronicle_config_rsm,[]},
          kv => {rsm_config,chronicle_kv,[]}},
        #{},undefined,
        [{<<"29b17497cea731943091527ebf7f7991">>,0}]}
[error_logger:info,2025-05-15T21:09:32.284Z,ns_1@cb.local:<0.255.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.255.0>,dynamic_supervisor}
    started: [{pid,<0.256.0>},
              {id,chronicle_leader},
              {mfargs,{chronicle_leader,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:32.286Z,ns_1@cb.local:chronicle_secondary_restartable_sup<0.257.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_secondary_restartable_sup}
    started: [{pid,<0.258.0>},
              {id,chronicle_status},
              {mfargs,{chronicle_status,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:32.287Z,ns_1@cb.local:chronicle_secondary_restartable_sup<0.257.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_secondary_restartable_sup}
    started: [{pid,<0.259.0>},
              {id,chronicle_failover},
              {mfargs,{chronicle_failover,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:32.287Z,ns_1@cb.local:<0.255.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.255.0>,dynamic_supervisor}
    started: [{pid,<0.257.0>},
              {id,chronicle_secondary_restartable_sup},
              {mfargs,{chronicle_secondary_restartable_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T21:09:32.288Z,ns_1@cb.local:<0.255.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.255.0>,dynamic_supervisor}
    started: [{pid,<0.260.0>},
              {id,chronicle_server},
              {mfargs,{chronicle_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[chronicle:info,2025-05-15T21:09:32.293Z,ns_1@cb.local:chronicle_config_rsm<0.264.0>:chronicle_rsm:get_incarnation:1594]No incarnation file found at '/opt/couchbase/var/lib/couchbase/config/chronicle/rsms/chronicle_config_rsm/incarnation'
[chronicle:debug,2025-05-15T21:09:32.294Z,ns_1@cb.local:chronicle_server<0.260.0>:chronicle_server:handle_register_rsm:361]Registering RSM chronicle_config_rsm with pid <0.264.0>
[error_logger:info,2025-05-15T21:09:32.294Z,ns_1@cb.local:<0.263.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.263.0>,chronicle_single_rsm_sup}
    started: [{pid,<0.264.0>},
              {id,chronicle_config_rsm},
              {mfargs,{chronicle_rsm,start_link,
                                     [chronicle_config_rsm,
                                      <<"7dc8077bc3da6cde870e5aa3464eff05">>,
                                      chronicle_config_rsm,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:32.294Z,ns_1@cb.local:<0.262.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.262.0>,dynamic_supervisor}
    started: [{pid,<0.263.0>},
              {id,chronicle_config_rsm},
              {mfargs,
                  {chronicle_single_rsm_sup,start_link,
                      [chronicle_config_rsm,
                       <<"7dc8077bc3da6cde870e5aa3464eff05">>,
                       chronicle_config_rsm,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T21:09:32.295Z,ns_1@cb.local:<0.266.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.266.0>,chronicle_single_rsm_sup}
    started: [{pid,<0.267.0>},
              {id,'kv-events'},
              {mfargs,{gen_event,start_link,[{local,'kv-events'}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[chronicle:info,2025-05-15T21:09:32.296Z,ns_1@cb.local:kv<0.268.0>:chronicle_rsm:get_incarnation:1594]No incarnation file found at '/opt/couchbase/var/lib/couchbase/config/chronicle/rsms/kv/incarnation'
[chronicle:debug,2025-05-15T21:09:32.297Z,ns_1@cb.local:chronicle_server<0.260.0>:chronicle_server:handle_register_rsm:361]Registering RSM kv with pid <0.268.0>
[error_logger:info,2025-05-15T21:09:32.297Z,ns_1@cb.local:<0.266.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.266.0>,chronicle_single_rsm_sup}
    started: [{pid,<0.268.0>},
              {id,kv},
              {mfargs,{chronicle_rsm,start_link,
                                     [kv,
                                      <<"7dc8077bc3da6cde870e5aa3464eff05">>,
                                      chronicle_kv,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:32.297Z,ns_1@cb.local:<0.262.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.262.0>,dynamic_supervisor}
    started: [{pid,<0.266.0>},
              {id,kv},
              {mfargs,
                  {chronicle_single_rsm_sup,start_link,
                      [kv,<<"7dc8077bc3da6cde870e5aa3464eff05">>,chronicle_kv,
                       []]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T21:09:32.297Z,ns_1@cb.local:<0.255.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.255.0>,dynamic_supervisor}
    started: [{pid,<0.261.0>},
              {id,chronicle_rsm_sup},
              {mfargs,{chronicle_rsm_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:info,2025-05-15T21:09:32.298Z,ns_1@cb.local:chronicle_local<0.239.0>:chronicle_upgrade:initialize:70]Setup initial chronicle content [{set,counters,[]},
                                 {set,auto_reprovision_cfg,
                                  [{enabled,true},{max_nodes,1},{count,0}]},
                                 {set,bucket_names,[]},
                                 {set,nodes_wanted,['ns_1@cb.local']},
                                 {set,server_groups,
                                  [[{uuid,<<"0">>},
                                    {name,<<"Group 1">>},
                                    {nodes,['ns_1@cb.local']}]]},
                                 {set,
                                  {node,'ns_1@cb.local',membership},
                                  active},
                                 {set,autocompaction,
                                  [{database_fragmentation_threshold,
                                    {30,undefined}},
                                   {view_fragmentation_threshold,
                                    {30,undefined}},
                                   {magma_fragmentation_percentage,50}]}]
[chronicle:debug,2025-05-15T21:09:32.395Z,ns_1@cb.local:chronicle_leader<0.256.0>:chronicle_leader:handle_state_timeout:608]State timeout when state is: {observer,true,false}
[chronicle:info,2025-05-15T21:09:32.395Z,ns_1@cb.local:<0.270.0>:chronicle_leader:do_election_worker:892]Starting election.
History ID: <<"29b17497cea731943091527ebf7f7991">>
Log position: {{1,'ns_1@cb.local'},1}
Peers: ['ns_1@cb.local']
Required quorum: {majority,{set,1,16,16,8,80,48,
                                {[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],
                                 []},
                                {{[],[],[],[],[],[],[],[],[],[],[],[],
                                  ['ns_1@cb.local'],
                                  [],[],[]}}}}
[chronicle:info,2025-05-15T21:09:32.395Z,ns_1@cb.local:<0.270.0>:chronicle_leader:do_election_worker:901]I'm the only peer, so I'm the leader.
[chronicle:info,2025-05-15T21:09:32.395Z,ns_1@cb.local:chronicle_leader<0.256.0>:chronicle_leader:handle_election_result:691]Going to become a leader in term {2,'ns_1@cb.local'} (history id <<"29b17497cea731943091527ebf7f7991">>)
[chronicle:debug,2025-05-15T21:09:32.399Z,ns_1@cb.local:chronicle_agent<0.252.0>:chronicle_agent:handle_establish_term:1529]Accepted term {2,'ns_1@cb.local'} in history <<"29b17497cea731943091527ebf7f7991">>
[chronicle:debug,2025-05-15T21:09:32.399Z,ns_1@cb.local:chronicle_proposer<0.271.0>:chronicle_proposer:establish_term_init:367]Going to establish term {2,'ns_1@cb.local'} (history id <<"29b17497cea731943091527ebf7f7991">>).
Quorum peers: ['ns_1@cb.local']
Metadata:
{metadata,'ns_1@cb.local',<<"7dc8077bc3da6cde870e5aa3464eff05">>,
          <<"29b17497cea731943091527ebf7f7991">>,
          {1,'ns_1@cb.local'},
          {1,'ns_1@cb.local'},
          1,1,
          {log_entry,<<"29b17497cea731943091527ebf7f7991">>,
                     {1,'ns_1@cb.local'},
                     1,
                     {config,undefined,0,undefined,
                             #{'ns_1@cb.local' =>
                                   #{id =>
                                         <<"7dc8077bc3da6cde870e5aa3464eff05">>,
                                     role => voter}},
                             undefined,
                             #{chronicle_config_rsm =>
                                   {rsm_config,chronicle_config_rsm,[]},
                               kv => {rsm_config,chronicle_kv,[]}},
                             #{},undefined,
                             [{<<"29b17497cea731943091527ebf7f7991">>,0}]}},
          {log_entry,<<"29b17497cea731943091527ebf7f7991">>,
                     {1,'ns_1@cb.local'},
                     1,
                     {config,undefined,0,undefined,
                             #{'ns_1@cb.local' =>
                                   #{id =>
                                         <<"7dc8077bc3da6cde870e5aa3464eff05">>,
                                     role => voter}},
                             undefined,
                             #{chronicle_config_rsm =>
                                   {rsm_config,chronicle_config_rsm,[]},
                               kv => {rsm_config,chronicle_kv,[]}},
                             #{},undefined,
                             [{<<"29b17497cea731943091527ebf7f7991">>,0}]}},
          undefined}
[chronicle:debug,2025-05-15T21:09:32.399Z,ns_1@cb.local:chronicle_proposer<0.271.0>:chronicle_proposer:establish_term_maybe_transition:686]Established term {2,'ns_1@cb.local'} (history id <<"29b17497cea731943091527ebf7f7991">>) successfully.
Votes: ['ns_1@cb.local']
[chronicle:debug,2025-05-15T21:09:32.399Z,ns_1@cb.local:chronicle_proposer<0.271.0>:chronicle_proposer:handle_state_enter:284]Starting recovery for term {2,'ns_1@cb.local'} in history <<"29b17497cea731943091527ebf7f7991">>
[chronicle:debug,2025-05-15T21:09:32.401Z,ns_1@cb.local:chronicle_proposer<0.271.0>:chronicle_proposer:handle_state_enter:296]Proposer for term {2,'ns_1@cb.local'} in history <<"29b17497cea731943091527ebf7f7991">> is ready. Committed seqno: 2
[chronicle:info,2025-05-15T21:09:32.401Z,ns_1@cb.local:chronicle_leader<0.256.0>:chronicle_leader:handle_note_term_status:596]Term {2,'ns_1@cb.local'} established.
[ns_server:info,2025-05-15T21:09:32.423Z,ns_1@cb.local:chronicle_local<0.239.0>:chronicle_upgrade:initialize:72]Chronicle content was initialized. Rev = {<<"29b17497cea731943091527ebf7f7991">>,
                                          3}.
[error_logger:info,2025-05-15T21:09:32.424Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.239.0>},
              {id,chronicle_local},
              {mfargs,{chronicle_local,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2025-05-15T21:09:32.425Z,ns_1@cb.local:ns_cluster<0.273.0>:ns_cluster:handle_info:514]Chronicle state is: provisioned
[error_logger:info,2025-05-15T21:09:32.425Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.273.0>},
              {id,ns_cluster},
              {mfargs,{ns_cluster,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2025-05-15T21:09:32.426Z,ns_1@cb.local:sigar<0.275.0>:sigar:spawn_sigar:134]Spawning sigar process 'portsigar for ns_1@cb.local'("/opt/couchbase/bin/sigar_port") with babysitter pid: 42 and log file "/opt/couchbase/var/lib/couchbase/logs/sigar_port.log"
[error_logger:info,2025-05-15T21:09:32.427Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.275.0>},
              {id,sigar},
              {mfargs,{sigar,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2025-05-15T21:09:32.428Z,ns_1@cb.local:ns_config_sup<0.276.0>:ns_config_sup:init:26]loading static ns_config from "/opt/couchbase/etc/couchbase/config"
[error_logger:info,2025-05-15T21:09:32.429Z,ns_1@cb.local:ns_config_sup<0.276.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.277.0>},
              {id,tombstone_keeper},
              {mfargs,{tombstone_keeper,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:32.429Z,ns_1@cb.local:ns_config_sup<0.276.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.278.0>},
              {id,ns_config_events},
              {mfargs,{gen_event,start_link,[{local,ns_config_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:32.429Z,ns_1@cb.local:ns_config_sup<0.276.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.279.0>},
              {id,ns_config_events_local},
              {mfargs,{gen_event,start_link,[{local,ns_config_events_local}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[ns_server:info,2025-05-15T21:09:32.467Z,ns_1@cb.local:ns_config<0.280.0>:ns_config:load_config:1113]Loading static config from "/opt/couchbase/etc/couchbase/config"
[ns_server:info,2025-05-15T21:09:32.468Z,ns_1@cb.local:ns_config<0.280.0>:ns_config:load_config:1127]Loading dynamic config from "/opt/couchbase/var/lib/couchbase/config/config.dat"
[ns_server:info,2025-05-15T21:09:32.469Z,ns_1@cb.local:ns_config<0.280.0>:ns_config:load_config:1132]No dynamic config file found. Assuming we're brand new node
[ns_server:debug,2025-05-15T21:09:32.471Z,ns_1@cb.local:ns_config<0.280.0>:ns_config:load_config:1135]Here's full dynamic config we loaded:
[[]]
[ns_server:info,2025-05-15T21:09:32.474Z,ns_1@cb.local:ns_config<0.280.0>:ns_config:load_config:1157]Here's full dynamic config we loaded + static & default config:
[{resource_management,
  [{bucket,
    [{resident_ratio,
      [{enabled,false},{couchstore_minimum,1},{magma_minimum,0.2}]},
     {data_size,[{enabled,false},{couchstore_maximum,2},{magma_maximum,16}]}]},
   {index,[]},
   {cores_per_bucket,[{enabled,false},{minimum,0.4}]},
   {disk_usage,[{enabled,false},{maximum,96}]},
   {collections_per_quota,[{enabled,false},{maximum,1}]}]},
 {{node,'ns_1@cb.local',index_dir},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]},
   47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
   98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
 {{node,'ns_1@cb.local',database_dir},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]},
   47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
   98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
 {auto_failover_cfg,
  [{enabled,true},
   {timeout,120},
   {count,0},
   {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
   {failover_server_group,false},
   {max_count,1},
   {failed_over_server_groups,[]},
   {can_abort_rebalance,true}]},
 {retry_rebalance,[{enabled,false},{after_time_period,300},{max_attempts,1}]},
 {{node,'ns_1@cb.local',{project_intact,is_vulnerable}},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   false]},
 {{node,'ns_1@cb.local',ssl_capi_port},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   18092]},
 {{node,'ns_1@cb.local',capi_port},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   8092]},
 {{node,'ns_1@cb.local',backup_grpc_port},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   9124]},
 {{node,'ns_1@cb.local',backup_https_port},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   18097]},
 {{node,'ns_1@cb.local',backup_http_port},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   8097]},
 {{node,'ns_1@cb.local',prometheus_http_port},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   9123]},
 {{node,'ns_1@cb.local',cbas_debug_port},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|-1]},
 {{node,'ns_1@cb.local',cbas_parent_port},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   9122]},
 {{node,'ns_1@cb.local',cbas_metadata_port},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   9121]},
 {{node,'ns_1@cb.local',cbas_replication_port},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   9120]},
 {{node,'ns_1@cb.local',cbas_metadata_callback_port},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   9119]},
 {{node,'ns_1@cb.local',cbas_messaging_port},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   9118]},
 {{node,'ns_1@cb.local',cbas_result_port},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   9117]},
 {{node,'ns_1@cb.local',cbas_data_port},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   9116]},
 {{node,'ns_1@cb.local',cbas_cluster_port},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   9115]},
 {{node,'ns_1@cb.local',cbas_console_port},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   9114]},
 {{node,'ns_1@cb.local',cbas_cc_client_port},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   9113]},
 {{node,'ns_1@cb.local',cbas_cc_cluster_port},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   9112]},
 {{node,'ns_1@cb.local',cbas_cc_http_port},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   9111]},
 {{node,'ns_1@cb.local',cbas_admin_port},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   9110]},
 {{node,'ns_1@cb.local',cbas_ssl_port},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   18095]},
 {{node,'ns_1@cb.local',cbas_http_port},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   8095]},
 {{node,'ns_1@cb.local',eventing_https_port},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   18096]},
 {{node,'ns_1@cb.local',eventing_debug_port},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   9140]},
 {{node,'ns_1@cb.local',eventing_http_port},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   8096]},
 {{node,'ns_1@cb.local',fts_grpc_ssl_port},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   19130]},
 {{node,'ns_1@cb.local',fts_grpc_port},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   9130]},
 {{node,'ns_1@cb.local',fts_ssl_port},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   18094]},
 {{node,'ns_1@cb.local',fts_http_port},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   8094]},
 {{node,'ns_1@cb.local',indexer_https_port},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   19102]},
 {{node,'ns_1@cb.local',indexer_stmaint_port},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   9105]},
 {{node,'ns_1@cb.local',indexer_stcatchup_port},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   9104]},
 {{node,'ns_1@cb.local',indexer_stinit_port},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   9103]},
 {{node,'ns_1@cb.local',indexer_http_port},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   9102]},
 {{node,'ns_1@cb.local',indexer_scan_port},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   9101]},
 {{node,'ns_1@cb.local',indexer_admin_port},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   9100]},
 {{node,'ns_1@cb.local',ssl_query_port},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   18093]},
 {{node,'ns_1@cb.local',query_port},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   8093]},
 {{node,'ns_1@cb.local',projector_ssl_port},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   9999]},
 {{node,'ns_1@cb.local',projector_port},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   9999]},
 {{node,'ns_1@cb.local',memcached_prometheus},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   11280]},
 {{node,'ns_1@cb.local',memcached_dedicated_ssl_port},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   11206]},
 {{node,'ns_1@cb.local',xdcr_rest_port},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   9998]},
 {{node,'ns_1@cb.local',ssl_rest_port},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   18091]},
 {{node,'ns_1@cb.local',rest},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]},
   {port,8091},
   {port_meta,global}]},
 {rest,[{port,8091}]},
 {password_policy,[{min_length,6},{must_present,[]}]},
 {health_monitor_refresh_interval,[]},
 {service_orchestrator_weight,
  [{kv,10000},
   {index,1000},
   {fts,1000},
   {cbas,1000},
   {n1ql,100},
   {eventing,100},
   {backup,10}]},
 {log_redaction_default_cfg,[{redact_level,none}]},
 {replication,[{enabled,true}]},
 {alert_limits,
  [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
 {email_alerts,
  [{recipients,["root@localhost"]},
   {sender,"couchbase@localhost"},
   {enabled,false},
   {email_server,
    [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
   {alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     indexer_low_resident_percentage,ep_clock_cas_drift_threshold_exceeded,
     communication_issue,time_out_of_sync,disk_usage_analyzer_stuck,
     cert_expires_soon,cert_expired,memory_threshold,history_size_warning,
     stuck_rebalance,memcached_connections]},
   {pop_up_alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     indexer_low_resident_percentage,ep_clock_cas_drift_threshold_exceeded,
     communication_issue,time_out_of_sync,disk_usage_analyzer_stuck,
     cert_expires_soon,cert_expired,memory_threshold,history_size_warning,
     stuck_rebalance,memcached_connections]}]},
 {{node,'ns_1@cb.local',event_log},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/event_log"}]},
 {{node,'ns_1@cb.local',ns_log},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
 {{node,'ns_1@cb.local',port_servers},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}]},
 {secure_headers,[]},
 {buckets,[{configs,[]}]},
 {cbas_memory_quota,1549},
 {fts_memory_quota,512},
 {memory_quota,3406},
 {{node,'ns_1@cb.local',memcached_config},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   {[{interfaces,{memcached_config_mgr,get_interfaces,[]}},
     {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
     {connection_idle_time,connection_idle_time},
     {privilege_debug,privilege_debug},
     {breakpad,
      {[{enabled,breakpad_enabled},
        {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
     {deployment_model,{memcached_config_mgr,get_config_profile,[]}},
     {verbosity,verbosity},
     {audit_file,{"~s",[audit_file]}},
     {rbac_file,{"~s",[rbac_file]}},
     {dedupe_nmvb_maps,dedupe_nmvb_maps},
     {tracing_enabled,tracing_enabled},
     {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
     {xattr_enabled,true},
     {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
     {collections_enabled,true},
     {max_connections,max_connections},
     {system_connections,system_connections},
     {num_reader_threads,num_reader_threads},
     {num_writer_threads,num_writer_threads},
     {num_auxio_threads,num_auxio_threads},
     {num_nonio_threads,num_nonio_threads},
     {num_storage_threads,num_storage_threads},
     {logger,
      {[{filename,{"~s/~s",[log_path,log_prefix]}},
        {cyclesize,log_cyclesize}]}},
     {external_auth_service,
      {memcached_config_mgr,get_external_auth_service,[]}},
     {active_external_users_push_interval,
      {memcached_config_mgr,get_external_users_push_interval,[]}},
     {prometheus,{memcached_config_mgr,prometheus_cfg,[]}},
     {sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
     {ssl_sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
     {tcp_keepalive_idle,tcp_keepalive_idle},
     {tcp_keepalive_interval,tcp_keepalive_interval},
     {tcp_keepalive_probes,tcp_keepalive_probes},
     {tcp_user_timeout,tcp_user_timeout},
     {always_collect_trace_info,always_collect_trace_info},
     {connection_limit_mode,connection_limit_mode},
     {free_connection_pool_size,free_connection_pool_size},
     {max_client_connection_details,max_client_connection_details}]}]},
 {{node,'ns_1@cb.local',memcached},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]},
   {port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,11206},
   {ssl_port,11207},
   {admin_user,"@ns_server"},
   {other_users,
    ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing","@cbas",
     "@backup"]},
   {admin_pass,"*****"},
   {engines,
    [{membase,
      [{engine,"/opt/couchbase/lib/memcached/ep.so"},
       {static_config_string,"failpartialwarmup=false"}]},
     {memcached,
      [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
       {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_rotation_period,39003}]},
 {{node,'ns_1@cb.local',memcached_defaults},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]},
   {max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {je_malloc_conf,undefined},
   {tracing_enabled,true},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>},
   {num_auxio_threads,<<"default">>},
   {num_nonio_threads,<<"default">>},
   {num_storage_threads,<<"default">>},
   {tcp_keepalive_idle,360},
   {tcp_keepalive_interval,10},
   {tcp_keepalive_probes,3},
   {tcp_user_timeout,30},
   {always_collect_trace_info,true},
   {connection_limit_mode,<<"disconnect">>},
   {free_connection_pool_size,0},
   {max_client_connection_details,0}]},
 {memcached,[]},
 {{node,'ns_1@cb.local',audit},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}]},
 {audit,
  [{auditd_enabled,false},
   {rotate_interval,86400},
   {rotate_size,20971520},
   {prune_age,0},
   {disabled,[]},
   {enabled,[]},
   {disabled_users,[]},
   {sync,[]},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
 {{node,'ns_1@cb.local',isasl},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]},
   {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
 {remote_clusters,[]},
 {scramsha_fallback_salt,<<76,22,165,198,5,29,159,21,58,206,10,51>>},
 {client_cert_auth,[{state,"disable"},{prefixes,[]}]},
 {rest_creds,null},
 {{metakv,<<"/analytics/settings/config">>},
  <<"{\"analytics.settings.num_replicas\":0}">>},
 {{metakv,<<"/query/settings/config">>},
  <<"{\"cleanupclientattempts\":true,\"cleanuplostattempts\":true,\"cleanupwindow\":\"60s\",\"completed-limit\":4000,\"completed-threshold\":1000,\"loglevel\":\"info\",\"max-parallelism\":1,\"memory-quota\":0,\"n1ql-feat-ctrl\":76,\"numatrs\":1024,\"pipeline-batch\":16,\"pipeline-cap\":512,\"prepared-limit\":16384,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"query.settings.tmp_space_size\":5120,\"scan-cap\":512,\"timeout\":0,\"txtimeout\":\"0ms\",\"use-cbo\":true}">>},
 {{metakv,<<"/eventing/settings/config">>},<<"{\"ram_quota\":256}">>},
 {{metakv,<<"/indexing/settings/config">>},
  <<"{\"indexer.settings.compaction.abort_exceed_interval\":false,\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.enable_page_bloom_filter\":false,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.log_level\":\"info\",\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.num_replica\":0,\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.storage_mode\":\"\"}">>},
 {{couchdb,max_parallel_replica_indexers},2},
 {{couchdb,max_parallel_indexers},4},
 {quorum_nodes,['ns_1@cb.local']},
 {nodes_wanted,[]},
 {{node,'ns_1@cb.local',compaction_daemon},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]},
   {check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}]},
 {set_view_update_daemon,
  [{update_interval,5000},
   {update_min_changes,5000},
   {replica_update_min_changes,5000}]},
 {max_bucket_count,30},
 {index_aware_rebalance_disabled,false},
 {{node,'ns_1@cb.local',saslauthd_enabled},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   true]},
 {{node,'ns_1@cb.local',is_enterprise},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   true]},
 {{node,'ns_1@cb.local',config_version},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   {7,6}]},
 {{node,'ns_1@cb.local',uuid},
  [{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|
   <<"4e0f9c01d66bba38da51cc6bd58d91b5">>]}]
[ns_server:debug,2025-05-15T21:09:32.477Z,ns_1@cb.local:tombstone_keeper<0.277.0>:tombstone_keeper:handle_call:49]Refreshed with timestamps []
[error_logger:info,2025-05-15T21:09:32.477Z,ns_1@cb.local:ns_config_sup<0.276.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.280.0>},
              {id,ns_config},
              {mfargs,{ns_config,start_link,
                                 ["/opt/couchbase/etc/couchbase/config",
                                  ns_config_default]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:32.478Z,ns_1@cb.local:ns_config_sup<0.276.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.283.0>},
              {id,ns_config_remote},
              {mfargs,{ns_config_replica,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:32.478Z,ns_1@cb.local:ns_config_sup<0.276.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.284.0>},
              {id,ns_config_log},
              {mfargs,{ns_config_log,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:32.478Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.276.0>},
              {id,ns_config_sup},
              {mfargs,{ns_config_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T21:09:32.480Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',n2n_client_cert_auth} ->
[{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|false]
[ns_server:debug,2025-05-15T21:09:32.480Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',erl_external_listeners} ->
[{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]},
 {inet,false}]
[ns_server:debug,2025-05-15T21:09:32.480Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',node_encryption} ->
[{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|false]
[ns_server:debug,2025-05-15T21:09:32.480Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',address_family} ->
[{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]}|inet]
[error_logger:info,2025-05-15T21:09:32.480Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.286.0>},
              {id,netconfig_updater},
              {mfargs,{netconfig_updater,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:32.483Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.289.0>},
              {id,json_rpc_connection_sup},
              {mfargs,{json_rpc_connection_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T21:09:32.488Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.292.0>},
              {id,chronicle_compat_events},
              {mfargs,{chronicle_compat_events,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:32.489Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.297.0>},
              {id,remote_monitors},
              {mfargs,{remote_monitors,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T21:09:32.490Z,ns_1@cb.local:menelaus_barrier<0.298.0>:one_shot_barrier:barrier_body:52]Barrier menelaus_barrier has started
[error_logger:info,2025-05-15T21:09:32.490Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.298.0>},
              {id,menelaus_barrier},
              {mfargs,{menelaus_sup,barrier_start_link,[]}},
              {restart_type,temporary},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T21:09:32.491Z,ns_1@cb.local:<0.301.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,
                                    rest_lhttpc_pool} with delay of 1000
[error_logger:info,2025-05-15T21:09:32.491Z,ns_1@cb.local:<0.302.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.302.0>,suppress_max_restart_intensity}
    started: [{pid,<0.303.0>},
              {id,rest_lhttpc_pool},
              {mfargs,{lhttpc_manager,start_link,
                                      [[{name,rest_lhttpc_pool},
                                        {connection_timeout,120000},
                                        {pool_size,20}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:32.492Z,ns_1@cb.local:<0.300.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.300.0>,suppress_max_restart_intensity}
    started: [{pid,<0.301.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      rest_lhttpc_pool}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,rest_lhttpc_pool},
                       1000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 1,id => rest_lhttpc_pool,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [lhttpc_manager],
                          restart => permanent,shutdown => 1000,
                          start =>
                              {lhttpc_manager,start_link,
                                  [[{name,rest_lhttpc_pool},
                                    {connection_timeout,120000},
                                    {pool_size,20}]]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T21:09:32.492Z,ns_1@cb.local:rest_lhttpc_pool_sup<0.299.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,rest_lhttpc_pool_sup}
    started: [{pid,<0.300.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,rest_lhttpc_pool}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 1,id => rest_lhttpc_pool,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [lhttpc_manager],
                         restart => permanent,shutdown => 1000,
                         start =>
                             {lhttpc_manager,start_link,
                                 [[{name,rest_lhttpc_pool},
                                   {connection_timeout,120000},
                                   {pool_size,20}]]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T21:09:32.492Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.299.0>},
              {id,rest_lhttpc_pool_sup},
              {mfargs,{rest_lhttpc_pool_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T21:09:32.493Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.304.0>},
              {id,memcached_refresh},
              {mfargs,{memcached_refresh,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:32.494Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.305.0>},
              {id,ns_secrets},
              {mfargs,{ns_secrets,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:32.495Z,ns_1@cb.local:ns_ssl_services_sup<0.306.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_ssl_services_sup}
    started: [{pid,<0.307.0>},
              {id,ssl_service_events},
              {mfargs,{gen_event,start_link,[{local,ssl_service_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T21:09:32.497Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:maybe_store_ca_certs:832]Considering to store CA certs
[ns_server:debug,2025-05-15T21:09:32.593Z,ns_1@cb.local:<0.312.0>:goport:handle_eof:592]Stream 'stdout' closed
[ns_server:debug,2025-05-15T21:09:32.593Z,ns_1@cb.local:<0.312.0>:goport:handle_eof:592]Stream 'stderr' closed
[ns_server:info,2025-05-15T21:09:32.593Z,ns_1@cb.local:<0.312.0>:goport:handle_process_exit:573]Port exited with status 0.
[ns_server:debug,2025-05-15T21:09:32.601Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_server_cert:generate_cert_and_pkey:155]Generated certificate and private key in 100972 us
[ns_server:debug,2025-05-15T21:09:32.648Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:maybe_store_ca_certs:847]Updating CA file with 1 certificates
[ns_server:info,2025-05-15T21:09:32.651Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:maybe_store_ca_certs:850]CA file updated: 1 cert(s) written
[ns_server:info,2025-05-15T21:09:32.651Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:should_regenerate_certs:899]Should regenerate node_cert because there are no certs on this node
[ns_server:debug,2025-05-15T21:09:32.790Z,ns_1@cb.local:<0.317.0>:goport:handle_eof:592]Stream 'stdout' closed
[ns_server:debug,2025-05-15T21:09:32.790Z,ns_1@cb.local:<0.317.0>:goport:handle_eof:592]Stream 'stderr' closed
[ns_server:info,2025-05-15T21:09:32.790Z,ns_1@cb.local:<0.317.0>:goport:handle_process_exit:573]Port exited with status 0.
[ns_server:info,2025-05-15T21:09:32.792Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:save_certs:968]New node_cert and pkey are written to tmp file
[ns_server:info,2025-05-15T21:09:32.795Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:save_certs_phase2:995]node_cert cert and pkey files updated
[ns_server:debug,2025-05-15T21:09:32.795Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',node_cert} ->
[{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]},
 {subject,<<"CN=Couchbase Server Node (127.0.0.1)">>},
 {not_after,63985756172},
 {verified_with,<<27,67,103,104,90,220,5,85,48,45,127,94,46,118,249,115>>},
 {load_timestamp,63914562572},
 {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/PiiAAPQIwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciA3ZmM0YzE5ZDAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgN2ZjNGMx\nOWQwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDwZbnZimLPX1G4YQ1m\n44qmBzVzfTSXVtb5fLiPrlDWyNY/GaCDYOVOMLR4R6dbCjjU6wzZvKrD2HZW5VXx\ntUOMbwJyZrA"...>>},
 {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDOTCCAiGgAwIBAgIIGD/PiiPQClgwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciA3ZmM0YzE5ZDAeFw0yNTA1MTQyMTA5MzJaFw0y\nNzA4MTcyMTA5MzJaMCwxKjAoBgNVBAMTIUNvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nMTI3LjAuMC4xKTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAOoAGwmf\nQ/Mee3ev6MReAF18gr0eum3MJ8QJfmWeXUU1klvnpi8Kb5olvncB6a0nJmaqF6Oh\nmYNya0Y"...>>},
 {pkey_passphrase_settings,[]},
 {certs_epoch,0},
 {type,generated},
 {hostname,"127.0.0.1"}]
[ns_server:info,2025-05-15T21:09:32.798Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:should_regenerate_certs:899]Should regenerate client_cert because there are no certs on this node
[ns_server:debug,2025-05-15T21:09:32.912Z,ns_1@cb.local:<0.323.0>:goport:handle_eof:592]Stream 'stdout' closed
[ns_server:debug,2025-05-15T21:09:32.912Z,ns_1@cb.local:<0.323.0>:goport:handle_eof:592]Stream 'stderr' closed
[ns_server:info,2025-05-15T21:09:32.912Z,ns_1@cb.local:<0.323.0>:goport:handle_process_exit:573]Port exited with status 0.
[ns_server:info,2025-05-15T21:09:32.914Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:save_certs:968]New client_cert and pkey are written to tmp file
[ns_server:info,2025-05-15T21:09:32.920Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:save_certs_phase2:995]client_cert cert and pkey files updated
[ns_server:debug,2025-05-15T21:09:32.920Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',client_cert} ->
[{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562572}}]},
 {subject,<<"CN=Couchbase Internal Client (84600751)">>},
 {not_after,63985756172},
 {verified_with,<<27,67,103,104,90,220,5,85,48,45,127,94,46,118,249,115>>},
 {load_timestamp,63914562572},
 {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/PiiAAPQIwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciA3ZmM0YzE5ZDAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgN2ZjNGMx\nOWQwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDwZbnZimLPX1G4YQ1m\n44qmBzVzfTSXVtb5fLiPrlDWyNY/GaCDYOVOMLR4R6dbCjjU6wzZvKrD2HZW5VXx\ntUOMbwJyZrA"...>>},
 {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDWTCCAkGgAwIBAgIIGD/PiiyyIQMwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciA3ZmM0YzE5ZDAeFw0yNTA1MTQyMTA5MzJaFw0y\nNzA4MTcyMTA5MzJaMC8xLTArBgNVBAMTJENvdWNoYmFzZSBJbnRlcm5hbCBDbGll\nbnQgKDg0NjAwNzUxKTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAJ5L\nsHRq2XiSxYw4sJ3iesWlSEnxQuuoiswj7Ev+AyRYeyRGVgDybQl0q6Lz/4TLgL2J\ncqCaX+X"...>>},
 {pkey_passphrase_settings,[]},
 {certs_epoch,0},
 {type,generated},
 {name,"@internal"}]
[ns_server:debug,2025-05-15T21:09:32.938Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:restart_regenerate_client_cert_timer:1447]Time left before client cert regeneration: 70588800000
[ns_server:info,2025-05-15T21:09:32.938Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:handle_info:764]cert_and_pkey changed
[error_logger:info,2025-05-15T21:09:32.938Z,ns_1@cb.local:ns_ssl_services_sup<0.306.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_ssl_services_sup}
    started: [{pid,<0.308.0>},
              {id,ns_ssl_services_setup},
              {mfargs,{ns_ssl_services_setup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T21:09:32.949Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:notify_services:1146]Going to notify following services: [capi_ssl_service,cb_dist_tls,
                                     client_cert_event,memcached,
                                     server_cert_event,ssl_service]
[ns_server:info,2025-05-15T21:09:32.949Z,ns_1@cb.local:<0.342.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service client_cert_event
[ns_server:info,2025-05-15T21:09:32.949Z,ns_1@cb.local:<0.344.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service server_cert_event
[ns_server:debug,2025-05-15T21:09:32.949Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Restarting tls distribution protocols (if any)
[ns_server:debug,2025-05-15T21:09:32.950Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Ensure config is going to change listeners. Will be stopped: [], will be started: [], will be restarted: []
[ns_server:info,2025-05-15T21:09:32.950Z,ns_1@cb.local:<0.341.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service cb_dist_tls
[error_logger:info,2025-05-15T21:09:32.953Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {8283906,#Ref<0.730592461.1793720324.72518>}}}
[ns_server:debug,2025-05-15T21:09:32.953Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T21:09:32.953Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.730592461.1793589252.72524>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T21:09:32.953Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.730592461.1793589252.72524>,
                                  inet_tcp_dist,<0.346.0>,
                                  #Ref<0.730592461.1793589252.72527>}
[error_logger:info,2025-05-15T21:09:32.953Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.346.0>,shutdown}}
[ns_server:debug,2025-05-15T21:09:32.953Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.730592461.1793589252.72524>,
                               inet_tcp_dist,<0.346.0>,
                               #Ref<0.730592461.1793589252.72527>}
[error_logger:info,2025-05-15T21:09:32.953Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T21:09:32.953Z,ns_1@cb.local:<0.340.0>:ns_couchdb_api:rpc_couchdb_node:152]RPC to couchdb node failed for restart_capi_ssl_service with {badrpc,nodedown}
Stack: [{ns_couchdb_api,rpc_couchdb_node,4,
                        [{file,"src/ns_couchdb_api.erl"},{line,150}]},
        {ns_ssl_services_setup,do_notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1203}]},
        {ns_ssl_services_setup,notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1179}]},
        {async,'-async_init/4-fun-1-',3,[{file,"src/async.erl"},{line,199}]}]
[ns_server:warn,2025-05-15T21:09:32.953Z,ns_1@cb.local:<0.340.0>:ns_ssl_services_setup:notify_service:1184]Failed to notify service capi_ssl_service: {'EXIT',{error,{badrpc,nodedown}}}
[ns_server:warn,2025-05-15T21:09:32.954Z,ns_1@cb.local:<0.343.0>:ns_ssl_services_setup:notify_service:1184]Failed to notify service memcached: {error,no_proccess}
[ns_server:info,2025-05-15T21:09:32.959Z,ns_1@cb.local:<0.330.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2025-05-15T21:09:32.960Z,ns_1@cb.local:<0.330.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2025-05-15T21:09:32.960Z,ns_1@cb.local:<0.330.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2025-05-15T21:09:32.960Z,ns_1@cb.local:<0.330.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2025-05-15T21:09:32.960Z,ns_1@cb.local:<0.330.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[error_logger:info,2025-05-15T21:09:32.966Z,ns_1@cb.local:kernel_sup<0.49.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,kernel_sup}
    started: [{pid,<0.349.0>},
              {id,timer_server},
              {mfargs,{timer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-05-15T21:09:32.984Z,ns_1@cb.local:<0.330.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"0.0.0.0"},
 [{keyfile,"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem"},
  {certfile,"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem"},
  {versions,['tlsv1.2','tlsv1.3']},
  {cacerts,[<<48,130,3,12,48,130,1,244,160,3,2,1,2,2,8,24,63,207,138,32,0,61,
              2,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,48,32,
              6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,101,114,
              118,101,114,32,55,102,99,52,99,49,57,100,48,30,23,13,49,51,48,
              49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,51,53,
              57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,117,99,
              104,98,97,115,101,32,83,101,114,118,101,114,32,55,102,99,52,99,
              49,57,100,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,1,5,0,
              3,130,1,15,0,48,130,1,10,2,130,1,1,0,240,101,185,217,138,98,
              207,95,81,184,97,13,102,227,138,166,7,53,115,125,52,151,86,214,
              249,124,184,143,174,80,214,200,214,63,25,160,131,96,229,78,48,
              180,120,71,167,91,10,56,212,235,12,217,188,170,195,216,118,86,
              229,85,241,181,67,140,111,2,114,102,176,60,218,16,44,177,71,
              233,145,168,142,79,93,196,80,30,39,204,58,252,206,117,20,159,
              73,137,212,221,107,85,48,189,249,132,61,157,96,252,173,203,25,
              153,48,74,252,104,216,163,189,10,153,96,72,162,193,16,7,88,140,
              222,143,17,242,109,211,214,34,235,237,231,138,15,24,136,76,219,
              217,249,174,145,146,115,121,28,242,31,191,179,150,87,21,139,
              196,229,214,202,238,56,157,60,117,106,187,40,60,215,75,92,171,
              161,73,112,160,212,39,1,160,33,45,80,210,7,85,131,222,119,243,
              45,220,139,237,116,149,46,153,186,225,234,238,213,160,86,125,
              83,140,88,101,0,36,182,68,138,217,2,108,190,13,89,20,197,17,
              209,199,54,113,154,160,1,251,156,81,225,173,207,104,99,79,172,
              80,153,106,117,63,244,128,189,50,229,2,3,1,0,1,163,66,48,64,48,
              14,6,3,85,29,15,1,1,255,4,4,3,2,1,134,48,15,6,3,85,29,19,1,1,
              255,4,5,48,3,1,1,255,48,29,6,3,85,29,14,4,22,4,20,175,51,235,
              168,237,171,47,66,195,97,24,68,173,225,53,54,75,222,127,70,48,
              13,6,9,42,134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,171,141,68,
              35,75,38,162,213,64,51,7,74,46,17,164,93,72,110,251,0,85,160,
              155,137,146,203,92,66,218,130,144,36,190,236,164,75,91,198,157,
              230,73,129,48,151,42,223,207,237,64,87,221,233,70,251,167,41,
              47,140,161,168,147,251,116,245,102,232,107,195,128,159,79,103,
              1,89,94,170,141,46,5,177,7,213,185,45,218,163,84,76,55,21,19,
              229,245,24,133,85,52,22,249,145,200,158,101,83,216,15,43,210,5,
              74,90,169,119,209,76,5,231,36,50,247,214,107,158,62,82,7,175,
              164,165,193,188,69,121,136,175,68,2,35,195,119,86,79,65,116,
              182,157,60,20,213,20,209,197,16,246,254,3,194,131,74,6,222,167,
              167,201,107,242,114,57,33,230,195,191,2,61,108,188,166,243,150,
              219,32,16,113,196,186,21,165,207,96,155,124,90,105,211,215,36,
              150,56,56,113,2,176,87,68,247,164,43,97,15,185,79,41,12,105,24,
              229,97,60,115,1,93,119,214,180,132,65,180,37,233,89,129,209,72,
              229,225,72,159,222,232,159,53,98,47,210,90,20,143,8,67,112,32,
              230,228,77,110,114>>]},
  {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,
        10,118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,
        158,232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,
        66,211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,
        250,145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,
        104,159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,
        246,169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,
        110,167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,
        118,190,67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,
        74,8,205,174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,
        221,95,184,110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,
        76,187,66,211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,
        69,254,147,103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,
        202,133,173,72,6,69,167,89,112,174,40,229,171,2,1,2>>},
  {ciphers,[#{cipher => aes_256_gcm,key_exchange => any,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm_8,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_ecdsa,
              mac => aead,prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_rsa,
              mac => aead,prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_128_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf},
            #{cipher => aes_128_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf}]},
  {honor_cipher_order,true},
  {secure_renegotiate,true},
  {client_renegotiation,false},
  {password,"********"}],
 {ssl,true},
 {port,18091}]
[error_logger:info,2025-05-15T21:09:32.985Z,ns_1@cb.local:<0.330.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.330.0>,menelaus_web}
    started: [{pid,<0.350.0>},
              {id,menelaus_web_ipv4},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet},
                        {ssl,true},
                        {name,menelaus_web_ssl},
                        {ssl_opts_fun,#Fun<ns_ssl_services_setup.11.39330155>},
                        {port,18091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2025-05-15T21:09:32.986Z,ns_1@cb.local:<0.330.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2025-05-15T21:09:32.986Z,ns_1@cb.local:<0.330.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2025-05-15T21:09:32.986Z,ns_1@cb.local:<0.330.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2025-05-15T21:09:32.986Z,ns_1@cb.local:<0.330.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2025-05-15T21:09:32.986Z,ns_1@cb.local:<0.330.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2025-05-15T21:09:32.988Z,ns_1@cb.local:<0.330.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"::"},
 [{keyfile,"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem"},
  {certfile,"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem"},
  {versions,['tlsv1.2','tlsv1.3']},
  {cacerts,[<<48,130,3,12,48,130,1,244,160,3,2,1,2,2,8,24,63,207,138,32,0,61,
              2,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,48,32,
              6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,101,114,
              118,101,114,32,55,102,99,52,99,49,57,100,48,30,23,13,49,51,48,
              49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,51,53,
              57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,117,99,
              104,98,97,115,101,32,83,101,114,118,101,114,32,55,102,99,52,99,
              49,57,100,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,1,5,0,
              3,130,1,15,0,48,130,1,10,2,130,1,1,0,240,101,185,217,138,98,
              207,95,81,184,97,13,102,227,138,166,7,53,115,125,52,151,86,214,
              249,124,184,143,174,80,214,200,214,63,25,160,131,96,229,78,48,
              180,120,71,167,91,10,56,212,235,12,217,188,170,195,216,118,86,
              229,85,241,181,67,140,111,2,114,102,176,60,218,16,44,177,71,
              233,145,168,142,79,93,196,80,30,39,204,58,252,206,117,20,159,
              73,137,212,221,107,85,48,189,249,132,61,157,96,252,173,203,25,
              153,48,74,252,104,216,163,189,10,153,96,72,162,193,16,7,88,140,
              222,143,17,242,109,211,214,34,235,237,231,138,15,24,136,76,219,
              217,249,174,145,146,115,121,28,242,31,191,179,150,87,21,139,
              196,229,214,202,238,56,157,60,117,106,187,40,60,215,75,92,171,
              161,73,112,160,212,39,1,160,33,45,80,210,7,85,131,222,119,243,
              45,220,139,237,116,149,46,153,186,225,234,238,213,160,86,125,
              83,140,88,101,0,36,182,68,138,217,2,108,190,13,89,20,197,17,
              209,199,54,113,154,160,1,251,156,81,225,173,207,104,99,79,172,
              80,153,106,117,63,244,128,189,50,229,2,3,1,0,1,163,66,48,64,48,
              14,6,3,85,29,15,1,1,255,4,4,3,2,1,134,48,15,6,3,85,29,19,1,1,
              255,4,5,48,3,1,1,255,48,29,6,3,85,29,14,4,22,4,20,175,51,235,
              168,237,171,47,66,195,97,24,68,173,225,53,54,75,222,127,70,48,
              13,6,9,42,134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,171,141,68,
              35,75,38,162,213,64,51,7,74,46,17,164,93,72,110,251,0,85,160,
              155,137,146,203,92,66,218,130,144,36,190,236,164,75,91,198,157,
              230,73,129,48,151,42,223,207,237,64,87,221,233,70,251,167,41,
              47,140,161,168,147,251,116,245,102,232,107,195,128,159,79,103,
              1,89,94,170,141,46,5,177,7,213,185,45,218,163,84,76,55,21,19,
              229,245,24,133,85,52,22,249,145,200,158,101,83,216,15,43,210,5,
              74,90,169,119,209,76,5,231,36,50,247,214,107,158,62,82,7,175,
              164,165,193,188,69,121,136,175,68,2,35,195,119,86,79,65,116,
              182,157,60,20,213,20,209,197,16,246,254,3,194,131,74,6,222,167,
              167,201,107,242,114,57,33,230,195,191,2,61,108,188,166,243,150,
              219,32,16,113,196,186,21,165,207,96,155,124,90,105,211,215,36,
              150,56,56,113,2,176,87,68,247,164,43,97,15,185,79,41,12,105,24,
              229,97,60,115,1,93,119,214,180,132,65,180,37,233,89,129,209,72,
              229,225,72,159,222,232,159,53,98,47,210,90,20,143,8,67,112,32,
              230,228,77,110,114>>]},
  {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,
        10,118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,
        158,232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,
        66,211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,
        250,145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,
        104,159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,
        246,169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,
        110,167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,
        118,190,67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,
        74,8,205,174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,
        221,95,184,110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,
        76,187,66,211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,
        69,254,147,103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,
        202,133,173,72,6,69,167,89,112,174,40,229,171,2,1,2>>},
  {ciphers,[#{cipher => aes_256_gcm,key_exchange => any,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm_8,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_ecdsa,
              mac => aead,prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_rsa,
              mac => aead,prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_128_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf},
            #{cipher => aes_128_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf}]},
  {honor_cipher_order,true},
  {secure_renegotiate,true},
  {client_renegotiation,false},
  {password,"********"}],
 {ssl,true},
 {port,18091}]
[error_logger:info,2025-05-15T21:09:32.989Z,ns_1@cb.local:<0.330.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.330.0>,menelaus_web}
    started: [{pid,<0.369.0>},
              {id,menelaus_web_ipv6},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet6},
                        {ssl,true},
                        {name,menelaus_web_ssl},
                        {ssl_opts_fun,#Fun<ns_ssl_services_setup.11.39330155>},
                        {port,18091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T21:09:32.989Z,ns_1@cb.local:<0.328.0>:restartable:start_child:92]Started child process <0.330.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
[error_logger:info,2025-05-15T21:09:32.989Z,ns_1@cb.local:ns_ssl_services_sup<0.306.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_ssl_services_sup}
    started: [{pid,<0.328.0>},
              {id,ns_rest_ssl_service},
              {mfargs,
                  {restartable,start_link,
                      [{ns_ssl_services_setup,start_link_rest_service,[]},
                       1000]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:32.990Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.306.0>},
              {id,ns_ssl_services_sup},
              {mfargs,{ns_ssl_services_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T21:09:32.990Z,ns_1@cb.local:<0.328.0>:restartable:loop:65]Restarting child <0.330.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
  Shutdown policy: 1000
  Caller: {<0.345.0>,#Ref<0.730592461.1793589253.72828>}
[ns_server:debug,2025-05-15T21:09:32.990Z,ns_1@cb.local:<0.328.0>:restartable:shutdown_child:114]Successfully terminated process <0.330.0>
[ns_server:info,2025-05-15T21:09:32.991Z,ns_1@cb.local:<0.388.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2025-05-15T21:09:32.992Z,ns_1@cb.local:<0.388.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2025-05-15T21:09:32.992Z,ns_1@cb.local:<0.388.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2025-05-15T21:09:32.992Z,ns_1@cb.local:<0.388.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2025-05-15T21:09:32.992Z,ns_1@cb.local:<0.388.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2025-05-15T21:09:32.993Z,ns_1@cb.local:<0.388.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"0.0.0.0"},
 [{keyfile,"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem"},
  {certfile,"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem"},
  {versions,['tlsv1.2','tlsv1.3']},
  {cacerts,[<<48,130,3,12,48,130,1,244,160,3,2,1,2,2,8,24,63,207,138,32,0,61,
              2,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,48,32,
              6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,101,114,
              118,101,114,32,55,102,99,52,99,49,57,100,48,30,23,13,49,51,48,
              49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,51,53,
              57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,117,99,
              104,98,97,115,101,32,83,101,114,118,101,114,32,55,102,99,52,99,
              49,57,100,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,1,5,0,
              3,130,1,15,0,48,130,1,10,2,130,1,1,0,240,101,185,217,138,98,
              207,95,81,184,97,13,102,227,138,166,7,53,115,125,52,151,86,214,
              249,124,184,143,174,80,214,200,214,63,25,160,131,96,229,78,48,
              180,120,71,167,91,10,56,212,235,12,217,188,170,195,216,118,86,
              229,85,241,181,67,140,111,2,114,102,176,60,218,16,44,177,71,
              233,145,168,142,79,93,196,80,30,39,204,58,252,206,117,20,159,
              73,137,212,221,107,85,48,189,249,132,61,157,96,252,173,203,25,
              153,48,74,252,104,216,163,189,10,153,96,72,162,193,16,7,88,140,
              222,143,17,242,109,211,214,34,235,237,231,138,15,24,136,76,219,
              217,249,174,145,146,115,121,28,242,31,191,179,150,87,21,139,
              196,229,214,202,238,56,157,60,117,106,187,40,60,215,75,92,171,
              161,73,112,160,212,39,1,160,33,45,80,210,7,85,131,222,119,243,
              45,220,139,237,116,149,46,153,186,225,234,238,213,160,86,125,
              83,140,88,101,0,36,182,68,138,217,2,108,190,13,89,20,197,17,
              209,199,54,113,154,160,1,251,156,81,225,173,207,104,99,79,172,
              80,153,106,117,63,244,128,189,50,229,2,3,1,0,1,163,66,48,64,48,
              14,6,3,85,29,15,1,1,255,4,4,3,2,1,134,48,15,6,3,85,29,19,1,1,
              255,4,5,48,3,1,1,255,48,29,6,3,85,29,14,4,22,4,20,175,51,235,
              168,237,171,47,66,195,97,24,68,173,225,53,54,75,222,127,70,48,
              13,6,9,42,134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,171,141,68,
              35,75,38,162,213,64,51,7,74,46,17,164,93,72,110,251,0,85,160,
              155,137,146,203,92,66,218,130,144,36,190,236,164,75,91,198,157,
              230,73,129,48,151,42,223,207,237,64,87,221,233,70,251,167,41,
              47,140,161,168,147,251,116,245,102,232,107,195,128,159,79,103,
              1,89,94,170,141,46,5,177,7,213,185,45,218,163,84,76,55,21,19,
              229,245,24,133,85,52,22,249,145,200,158,101,83,216,15,43,210,5,
              74,90,169,119,209,76,5,231,36,50,247,214,107,158,62,82,7,175,
              164,165,193,188,69,121,136,175,68,2,35,195,119,86,79,65,116,
              182,157,60,20,213,20,209,197,16,246,254,3,194,131,74,6,222,167,
              167,201,107,242,114,57,33,230,195,191,2,61,108,188,166,243,150,
              219,32,16,113,196,186,21,165,207,96,155,124,90,105,211,215,36,
              150,56,56,113,2,176,87,68,247,164,43,97,15,185,79,41,12,105,24,
              229,97,60,115,1,93,119,214,180,132,65,180,37,233,89,129,209,72,
              229,225,72,159,222,232,159,53,98,47,210,90,20,143,8,67,112,32,
              230,228,77,110,114>>]},
  {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,
        10,118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,
        158,232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,
        66,211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,
        250,145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,
        104,159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,
        246,169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,
        110,167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,
        118,190,67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,
        74,8,205,174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,
        221,95,184,110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,
        76,187,66,211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,
        69,254,147,103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,
        202,133,173,72,6,69,167,89,112,174,40,229,171,2,1,2>>},
  {ciphers,[#{cipher => aes_256_gcm,key_exchange => any,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm_8,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_ecdsa,
              mac => aead,prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_rsa,
              mac => aead,prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_128_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf},
            #{cipher => aes_128_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf}]},
  {honor_cipher_order,true},
  {secure_renegotiate,true},
  {client_renegotiation,false},
  {password,"********"}],
 {ssl,true},
 {port,18091}]
[error_logger:info,2025-05-15T21:09:32.995Z,ns_1@cb.local:<0.388.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.388.0>,menelaus_web}
    started: [{pid,<0.389.0>},
              {id,menelaus_web_ipv4},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet},
                        {ssl,true},
                        {name,menelaus_web_ssl},
                        {ssl_opts_fun,#Fun<ns_ssl_services_setup.11.39330155>},
                        {port,18091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:32.995Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.408.0>},
              {id,ldap_auth_cache},
              {mfargs,{ldap_auth_cache,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-05-15T21:09:32.996Z,ns_1@cb.local:<0.388.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2025-05-15T21:09:32.996Z,ns_1@cb.local:<0.388.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2025-05-15T21:09:32.996Z,ns_1@cb.local:<0.388.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2025-05-15T21:09:32.996Z,ns_1@cb.local:<0.388.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2025-05-15T21:09:32.996Z,ns_1@cb.local:<0.388.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:debug,2025-05-15T21:09:32.997Z,ns_1@cb.local:cb_saml<0.412.0>:cb_saml:handle_info:316]Settings have changed or this is the first start
[error_logger:info,2025-05-15T21:09:32.997Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.412.0>},
              {id,cb_saml},
              {mfargs,{cb_saml,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T21:09:32.997Z,ns_1@cb.local:cb_saml<0.412.0>:cb_saml:restart_refresh_timer:586]Restarting refresh timer: 38040 ms
[error_logger:info,2025-05-15T21:09:32.998Z,ns_1@cb.local:users_sup<0.431.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_sup}
    started: [{pid,<0.432.0>},
              {id,user_storage_events},
              {mfargs,{gen_event,start_link,[{local,user_storage_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-05-15T21:09:32.997Z,ns_1@cb.local:<0.388.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"::"},
 [{keyfile,"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem"},
  {certfile,"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem"},
  {versions,['tlsv1.2','tlsv1.3']},
  {cacerts,[<<48,130,3,12,48,130,1,244,160,3,2,1,2,2,8,24,63,207,138,32,0,61,
              2,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,48,32,
              6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,101,114,
              118,101,114,32,55,102,99,52,99,49,57,100,48,30,23,13,49,51,48,
              49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,51,53,
              57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,117,99,
              104,98,97,115,101,32,83,101,114,118,101,114,32,55,102,99,52,99,
              49,57,100,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,1,5,0,
              3,130,1,15,0,48,130,1,10,2,130,1,1,0,240,101,185,217,138,98,
              207,95,81,184,97,13,102,227,138,166,7,53,115,125,52,151,86,214,
              249,124,184,143,174,80,214,200,214,63,25,160,131,96,229,78,48,
              180,120,71,167,91,10,56,212,235,12,217,188,170,195,216,118,86,
              229,85,241,181,67,140,111,2,114,102,176,60,218,16,44,177,71,
              233,145,168,142,79,93,196,80,30,39,204,58,252,206,117,20,159,
              73,137,212,221,107,85,48,189,249,132,61,157,96,252,173,203,25,
              153,48,74,252,104,216,163,189,10,153,96,72,162,193,16,7,88,140,
              222,143,17,242,109,211,214,34,235,237,231,138,15,24,136,76,219,
              217,249,174,145,146,115,121,28,242,31,191,179,150,87,21,139,
              196,229,214,202,238,56,157,60,117,106,187,40,60,215,75,92,171,
              161,73,112,160,212,39,1,160,33,45,80,210,7,85,131,222,119,243,
              45,220,139,237,116,149,46,153,186,225,234,238,213,160,86,125,
              83,140,88,101,0,36,182,68,138,217,2,108,190,13,89,20,197,17,
              209,199,54,113,154,160,1,251,156,81,225,173,207,104,99,79,172,
              80,153,106,117,63,244,128,189,50,229,2,3,1,0,1,163,66,48,64,48,
              14,6,3,85,29,15,1,1,255,4,4,3,2,1,134,48,15,6,3,85,29,19,1,1,
              255,4,5,48,3,1,1,255,48,29,6,3,85,29,14,4,22,4,20,175,51,235,
              168,237,171,47,66,195,97,24,68,173,225,53,54,75,222,127,70,48,
              13,6,9,42,134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,171,141,68,
              35,75,38,162,213,64,51,7,74,46,17,164,93,72,110,251,0,85,160,
              155,137,146,203,92,66,218,130,144,36,190,236,164,75,91,198,157,
              230,73,129,48,151,42,223,207,237,64,87,221,233,70,251,167,41,
              47,140,161,168,147,251,116,245,102,232,107,195,128,159,79,103,
              1,89,94,170,141,46,5,177,7,213,185,45,218,163,84,76,55,21,19,
              229,245,24,133,85,52,22,249,145,200,158,101,83,216,15,43,210,5,
              74,90,169,119,209,76,5,231,36,50,247,214,107,158,62,82,7,175,
              164,165,193,188,69,121,136,175,68,2,35,195,119,86,79,65,116,
              182,157,60,20,213,20,209,197,16,246,254,3,194,131,74,6,222,167,
              167,201,107,242,114,57,33,230,195,191,2,61,108,188,166,243,150,
              219,32,16,113,196,186,21,165,207,96,155,124,90,105,211,215,36,
              150,56,56,113,2,176,87,68,247,164,43,97,15,185,79,41,12,105,24,
              229,97,60,115,1,93,119,214,180,132,65,180,37,233,89,129,209,72,
              229,225,72,159,222,232,159,53,98,47,210,90,20,143,8,67,112,32,
              230,228,77,110,114>>]},
  {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,
        10,118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,
        158,232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,
        66,211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,
        250,145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,
        104,159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,
        246,169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,
        110,167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,
        118,190,67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,
        74,8,205,174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,
        221,95,184,110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,
        76,187,66,211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,
        69,254,147,103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,
        202,133,173,72,6,69,167,89,112,174,40,229,171,2,1,2>>},
  {ciphers,[#{cipher => aes_256_gcm,key_exchange => any,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm_8,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_ecdsa,
              mac => aead,prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_rsa,
              mac => aead,prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_128_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf},
            #{cipher => aes_128_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf}]},
  {honor_cipher_order,true},
  {secure_renegotiate,true},
  {client_renegotiation,false},
  {password,"********"}],
 {ssl,true},
 {port,18091}]
[error_logger:info,2025-05-15T21:09:32.999Z,ns_1@cb.local:<0.388.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.388.0>,menelaus_web}
    started: [{pid,<0.410.0>},
              {id,menelaus_web_ipv6},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet6},
                        {ssl,true},
                        {name,menelaus_web_ssl},
                        {ssl_opts_fun,#Fun<ns_ssl_services_setup.11.39330155>},
                        {port,18091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T21:09:32.999Z,ns_1@cb.local:<0.328.0>:restartable:start_child:92]Started child process <0.388.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
[ns_server:info,2025-05-15T21:09:32.999Z,ns_1@cb.local:<0.345.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service ssl_service
[ns_server:info,2025-05-15T21:09:32.999Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:notify_services:1162]Succesfully notified services [ssl_service,server_cert_event,
                               client_cert_event,cb_dist_tls]
[ns_server:info,2025-05-15T21:09:33.001Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:notify_services:1173]Failed to notify some services. Will retry in 5 sec, [{{error,no_proccess},
                                                       memcached},
                                                      {{'EXIT',
                                                        {error,
                                                         {badrpc,nodedown}}},
                                                       capi_ssl_service}]
[ns_server:debug,2025-05-15T21:09:33.001Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:restart_regenerate_client_cert_timer:1447]Time left before client cert regeneration: 70588799000
[ns_server:debug,2025-05-15T21:09:33.001Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:maybe_store_ca_certs:832]Considering to store CA certs
[ns_server:debug,2025-05-15T21:09:33.002Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:notify_services:1146]Going to notify following services: [capi_ssl_service,memcached]
[error_logger:info,2025-05-15T21:09:33.002Z,ns_1@cb.local:users_storage_sup<0.433.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_storage_sup}
    started: [{pid,<0.435.0>},
              {id,users_replicator},
              {mfargs,{menelaus_users,start_replicator,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:warn,2025-05-15T21:09:33.002Z,ns_1@cb.local:<0.441.0>:ns_ssl_services_setup:notify_service:1184]Failed to notify service memcached: {error,no_proccess}
[error_logger:info,2025-05-15T21:09:33.002Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {8283907,#Ref<0.730592461.1793720324.72518>}}}
[ns_server:debug,2025-05-15T21:09:33.002Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T21:09:33.002Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.730592461.1793589251.72421>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T21:09:33.002Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.730592461.1793589251.72421>,
                                  inet_tcp_dist,<0.442.0>,
                                  #Ref<0.730592461.1793589251.72424>}
[ns_server:debug,2025-05-15T21:09:33.003Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.730592461.1793589251.72421>,
                               inet_tcp_dist,<0.442.0>,
                               #Ref<0.730592461.1793589251.72424>}
[error_logger:info,2025-05-15T21:09:33.003Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.442.0>,shutdown}}
[error_logger:info,2025-05-15T21:09:33.003Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T21:09:33.003Z,ns_1@cb.local:<0.440.0>:ns_couchdb_api:rpc_couchdb_node:152]RPC to couchdb node failed for restart_capi_ssl_service with {badrpc,nodedown}
Stack: [{ns_couchdb_api,rpc_couchdb_node,4,
                        [{file,"src/ns_couchdb_api.erl"},{line,150}]},
        {ns_ssl_services_setup,do_notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1203}]},
        {ns_ssl_services_setup,notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1179}]},
        {async,'-async_init/4-fun-1-',3,[{file,"src/async.erl"},{line,199}]}]
[ns_server:warn,2025-05-15T21:09:33.003Z,ns_1@cb.local:<0.440.0>:ns_ssl_services_setup:notify_service:1184]Failed to notify service capi_ssl_service: {'EXIT',{error,{badrpc,nodedown}}}
[ns_server:debug,2025-05-15T21:09:33.003Z,ns_1@cb.local:users_replicator<0.435.0>:replicated_storage:wait_for_startup:47]Start waiting for startup
[ns_server:info,2025-05-15T21:09:33.004Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:notify_services:1173]Failed to notify some services. Will retry in 5 sec, [{{error,no_proccess},
                                                       memcached},
                                                      {{'EXIT',
                                                        {error,
                                                         {badrpc,nodedown}}},
                                                       capi_ssl_service}]
[ns_server:debug,2025-05-15T21:09:33.004Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:restart_regenerate_client_cert_timer:1447]Time left before client cert regeneration: 70588799000
[ns_server:info,2025-05-15T21:09:33.005Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:validate_pkey:1032]Private key (node_cert) passphrase validation suceeded
[ns_server:info,2025-05-15T21:09:33.005Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:validate_pkey:1032]Private key (client_cert) passphrase validation suceeded
[ns_server:debug,2025-05-15T21:09:33.005Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:notify_services:1146]Going to notify following services: [memcached,capi_ssl_service]
[ns_server:warn,2025-05-15T21:09:33.005Z,ns_1@cb.local:<0.450.0>:ns_ssl_services_setup:notify_service:1184]Failed to notify service memcached: {error,no_proccess}
[error_logger:info,2025-05-15T21:09:33.005Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {8283908,#Ref<0.730592461.1793720324.72518>}}}
[ns_server:debug,2025-05-15T21:09:33.005Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T21:09:33.005Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.730592461.1793589250.72533>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T21:09:33.005Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.730592461.1793589250.72533>,
                                  inet_tcp_dist,<0.452.0>,
                                  #Ref<0.730592461.1793589250.72536>}
[error_logger:info,2025-05-15T21:09:33.006Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.452.0>,shutdown}}
[ns_server:debug,2025-05-15T21:09:33.006Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.730592461.1793589250.72533>,
                               inet_tcp_dist,<0.452.0>,
                               #Ref<0.730592461.1793589250.72536>}
[ns_server:debug,2025-05-15T21:09:33.006Z,ns_1@cb.local:users_storage<0.444.0>:replicated_storage:announce_startup:61]Announce my startup to <0.435.0>
[error_logger:info,2025-05-15T21:09:33.006Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T21:09:33.006Z,ns_1@cb.local:<0.451.0>:ns_couchdb_api:rpc_couchdb_node:152]RPC to couchdb node failed for restart_capi_ssl_service with {badrpc,nodedown}
Stack: [{ns_couchdb_api,rpc_couchdb_node,4,
                        [{file,"src/ns_couchdb_api.erl"},{line,150}]},
        {ns_ssl_services_setup,do_notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1203}]},
        {ns_ssl_services_setup,notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1179}]},
        {async,'-async_init/4-fun-1-',3,[{file,"src/async.erl"},{line,199}]}]
[ns_server:debug,2025-05-15T21:09:33.006Z,ns_1@cb.local:users_replicator<0.435.0>:replicated_storage:wait_for_startup:50]Received replicated storage registration from <0.444.0>
[ns_server:debug,2025-05-15T21:09:33.006Z,ns_1@cb.local:users_storage<0.444.0>:replicated_dets:open:177]Opening file "/opt/couchbase/var/lib/couchbase/config/users.dets"
[ns_server:warn,2025-05-15T21:09:33.006Z,ns_1@cb.local:<0.451.0>:ns_ssl_services_setup:notify_service:1184]Failed to notify service capi_ssl_service: {'EXIT',{error,{badrpc,nodedown}}}
[error_logger:info,2025-05-15T21:09:33.006Z,ns_1@cb.local:users_storage_sup<0.433.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_storage_sup}
    started: [{pid,<0.444.0>},
              {id,users_storage},
              {mfargs,{menelaus_users,start_storage,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:33.006Z,ns_1@cb.local:users_sup<0.431.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_sup}
    started: [{pid,<0.433.0>},
              {id,users_storage_sup},
              {mfargs,{users_storage_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T21:09:33.010Z,ns_1@cb.local:compiled_roles_cache<0.455.0>:versioned_cache:init:41]Starting versioned cache compiled_roles_cache
[error_logger:info,2025-05-15T21:09:33.011Z,ns_1@cb.local:users_sup<0.431.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_sup}
    started: [{pid,<0.455.0>},
              {id,compiled_roles_cache},
              {mfargs,{menelaus_roles,start_compiled_roles_cache,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-05-15T21:09:33.011Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:notify_services:1173]Failed to notify some services. Will retry in 5 sec, [{{'EXIT',
                                                        {error,
                                                         {badrpc,nodedown}}},
                                                       capi_ssl_service},
                                                      {{error,no_proccess},
                                                       memcached}]
[error_logger:info,2025-05-15T21:09:33.013Z,ns_1@cb.local:users_sup<0.431.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_sup}
    started: [{pid,<0.458.0>},
              {id,roles_cache},
              {mfargs,{roles_cache,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:33.013Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.431.0>},
              {id,users_sup},
              {mfargs,{users_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T21:09:33.013Z,ns_1@cb.local:kernel_safe_sup<0.67.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,kernel_safe_sup}
    started: [{pid,<0.462.0>},
              {id,dets_sup},
              {mfargs,{dets_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T21:09:33.014Z,ns_1@cb.local:kernel_safe_sup<0.67.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,kernel_safe_sup}
    started: [{pid,<0.463.0>},
              {id,dets},
              {mfargs,{dets_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,2000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T21:09:33.021Z,ns_1@cb.local:<0.468.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,
                                    start_couchdb_node} with delay of 5000
[ns_server:debug,2025-05-15T21:09:33.021Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',cbas_dirs} ->
[{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562573}}]},
 "/opt/couchbase/var/lib/couchbase/data"]
[ns_server:debug,2025-05-15T21:09:33.021Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',eventing_dir} ->
[{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562573}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2025-05-15T21:09:33.027Z,ns_1@cb.local:users_storage<0.444.0>:replicated_dets:select_from_table:312][dets] Starting select with {users_storage,
                                [{{docv2,'_','_','_'},[],['$_']}],
                                100}
[ns_server:debug,2025-05-15T21:09:33.028Z,ns_1@cb.local:users_storage<0.444.0>:replicated_dets:init_after_ack:170]Loading 0 items, 307 words took 21ms
[error_logger:info,2025-05-15T21:09:33.033Z,ns_1@cb.local:<0.469.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.469.0>,suppress_max_restart_intensity}
    started: [{pid,<0.471.0>},
              {id,start_couchdb_node},
              {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,86400000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:33.034Z,ns_1@cb.local:<0.467.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.467.0>,suppress_max_restart_intensity}
    started: [{pid,<0.468.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      start_couchdb_node}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,start_couchdb_node},
                       5000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 5,id => start_couchdb_node,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [],restart => permanent,
                          shutdown => 86400000,
                          start => {ns_server_nodes_sup,start_couchdb_node,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T21:09:33.034Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.467.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,start_couchdb_node}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 5,id => start_couchdb_node,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [],restart => permanent,
                         shutdown => 86400000,
                         start => {ns_server_nodes_sup,start_couchdb_node,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T21:09:33.034Z,ns_1@cb.local:wait_link_to_couchdb_node<0.472.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:158]Waiting for ns_couchdb node to start
[error_logger:info,2025-05-15T21:09:33.034Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {8283909,#Ref<0.730592461.1793720324.72518>}}}
[ns_server:debug,2025-05-15T21:09:33.034Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T21:09:33.034Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.730592461.1793589249.73414>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T21:09:33.034Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.730592461.1793589249.73414>,
                                  inet_tcp_dist,<0.474.0>,
                                  #Ref<0.730592461.1793589249.73417>}
[error_logger:info,2025-05-15T21:09:33.035Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.474.0>,shutdown}}
[error_logger:info,2025-05-15T21:09:33.035Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T21:09:33.035Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.730592461.1793589249.73414>,
                               inet_tcp_dist,<0.474.0>,
                               #Ref<0.730592461.1793589249.73417>}
[ns_server:debug,2025-05-15T21:09:33.035Z,ns_1@cb.local:<0.473.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-05-15T21:09:33.236Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {8283910,#Ref<0.730592461.1793720324.72518>}}}
[ns_server:debug,2025-05-15T21:09:33.236Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T21:09:33.236Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.730592461.1793589249.73429>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T21:09:33.236Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.730592461.1793589249.73429>,
                                  inet_tcp_dist,<0.476.0>,
                                  #Ref<0.730592461.1793589249.73432>}
[error_logger:info,2025-05-15T21:09:33.236Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.476.0>,shutdown}}
[error_logger:info,2025-05-15T21:09:33.236Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T21:09:33.236Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.730592461.1793589249.73429>,
                               inet_tcp_dist,<0.476.0>,
                               #Ref<0.730592461.1793589249.73432>}
[ns_server:debug,2025-05-15T21:09:33.236Z,ns_1@cb.local:<0.473.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-05-15T21:09:33.438Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {8283911,#Ref<0.730592461.1793720324.72518>}}}
[ns_server:debug,2025-05-15T21:09:33.438Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T21:09:33.438Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.730592461.1793589249.73437>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T21:09:33.438Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.730592461.1793589249.73437>,
                                  inet_tcp_dist,<0.478.0>,
                                  #Ref<0.730592461.1793589250.72575>}
[error_logger:info,2025-05-15T21:09:33.468Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.478.0>,{recv_challenge_failed,{error,closed}}}}
[ns_server:debug,2025-05-15T21:09:33.468Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.730592461.1793589249.73437>,
                               inet_tcp_dist,<0.478.0>,
                               #Ref<0.730592461.1793589250.72575>}
[ns_server:debug,2025-05-15T21:09:33.469Z,ns_1@cb.local:<0.473.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-05-15T21:09:33.468Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[error_logger:info,2025-05-15T21:09:33.670Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {8283912,#Ref<0.730592461.1793720324.72518>}}}
[ns_server:debug,2025-05-15T21:09:33.671Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T21:09:33.671Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.730592461.1793589250.72578>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T21:09:33.671Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.730592461.1793589250.72578>,
                                  inet_tcp_dist,<0.480.0>,
                                  #Ref<0.730592461.1793589250.72581>}
[ns_server:debug,2025-05-15T21:09:33.673Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.730592461.1793589250.72578>,
                               inet_tcp_dist,<0.480.0>,
                               #Ref<0.730592461.1793589250.72581>}
[error_logger:info,2025-05-15T21:09:33.673Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.480.0>,{recv_challenge_failed,{error,closed}}}}
[ns_server:info,2025-05-15T21:09:33.673Z,ns_1@cb.local:ns_couchdb_port<0.471.0>:ns_port_server:log:226]ns_couchdb<0.471.0>: =ERROR REPORT==== 15-May-2025::21:09:33.468587 ===
ns_couchdb<0.471.0>: ** Connection attempt to/from node 'ns_1@cb.local' rejected. Cookie is not set. **
ns_couchdb<0.471.0>: 

[error_logger:info,2025-05-15T21:09:33.673Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T21:09:33.673Z,ns_1@cb.local:<0.473.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-05-15T21:09:33.876Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {8283913,#Ref<0.730592461.1793720324.72518>}}}
[ns_server:debug,2025-05-15T21:09:33.876Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T21:09:33.876Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.730592461.1793589254.72387>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T21:09:33.876Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.730592461.1793589254.72387>,
                                  inet_tcp_dist,<0.482.0>,
                                  #Ref<0.730592461.1793589254.72390>}
[error_logger:info,2025-05-15T21:09:33.880Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.482.0>,{recv_challenge_failed,{error,closed}}}}
[ns_server:debug,2025-05-15T21:09:33.880Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.730592461.1793589254.72387>,
                               inet_tcp_dist,<0.482.0>,
                               #Ref<0.730592461.1793589254.72390>}
[error_logger:info,2025-05-15T21:09:33.880Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T21:09:33.880Z,ns_1@cb.local:<0.473.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-05-15T21:09:34.082Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {8283914,#Ref<0.730592461.1793720324.72518>}}}
[ns_server:debug,2025-05-15T21:09:34.082Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T21:09:34.082Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.730592461.1793589250.72589>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T21:09:34.082Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.730592461.1793589250.72589>,
                                  inet_tcp_dist,<0.484.0>,
                                  #Ref<0.730592461.1793589254.72394>}
[ns_server:debug,2025-05-15T21:09:34.083Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.730592461.1793589250.72589>,
                               inet_tcp_dist,<0.484.0>,
                               #Ref<0.730592461.1793589254.72394>}
[error_logger:info,2025-05-15T21:09:34.083Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.484.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2025-05-15T21:09:34.083Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T21:09:34.083Z,ns_1@cb.local:<0.473.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-05-15T21:09:34.285Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {8283915,#Ref<0.730592461.1793720324.72518>}}}
[ns_server:debug,2025-05-15T21:09:34.285Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T21:09:34.285Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.730592461.1793589249.73453>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T21:09:34.285Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.730592461.1793589249.73453>,
                                  inet_tcp_dist,<0.486.0>,
                                  #Ref<0.730592461.1793589249.73456>}
[error_logger:info,2025-05-15T21:09:34.286Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.486.0>,{recv_challenge_failed,{error,closed}}}}
[ns_server:debug,2025-05-15T21:09:34.286Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.730592461.1793589249.73453>,
                               inet_tcp_dist,<0.486.0>,
                               #Ref<0.730592461.1793589249.73456>}
[ns_server:debug,2025-05-15T21:09:34.286Z,ns_1@cb.local:<0.473.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-05-15T21:09:34.286Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[error_logger:info,2025-05-15T21:09:34.487Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {8283916,#Ref<0.730592461.1793720324.72518>}}}
[ns_server:debug,2025-05-15T21:09:34.487Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T21:09:34.487Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.730592461.1793589250.72602>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T21:09:34.487Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.730592461.1793589250.72602>,
                                  inet_tcp_dist,<0.488.0>,
                                  #Ref<0.730592461.1793589250.72605>}
[ns_server:debug,2025-05-15T21:09:34.490Z,ns_1@cb.local:<0.473.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: false
[error_logger:info,2025-05-15T21:09:34.785Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.472.0>},
              {id,wait_for_couchdb_node},
              {mfargs,{erlang,apply,
                              [#Fun<ns_server_nodes_sup.0.120652322>,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T21:09:34.787Z,ns_1@cb.local:<0.494.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,ns_disksup} with delay of 4000
[error_logger:info,2025-05-15T21:09:34.789Z,ns_1@cb.local:<0.495.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.495.0>,suppress_max_restart_intensity}
    started: [{pid,<0.496.0>},
              {id,ns_disksup},
              {mfargs,{ns_disksup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:34.789Z,ns_1@cb.local:<0.493.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.493.0>,suppress_max_restart_intensity}
    started: [{pid,<0.494.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      ns_disksup}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,ns_disksup},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => ns_disksup,inherited_max_r => 20,
                          inherited_max_t => 10,modules => [],
                          restart => permanent,shutdown => 1000,
                          start => {ns_disksup,start_link,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T21:09:34.789Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.493.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,ns_disksup}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => ns_disksup,inherited_max_r => 20,
                         inherited_max_t => 10,modules => [],
                         restart => permanent,shutdown => 1000,
                         start => {ns_disksup,start_link,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T21:09:34.790Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.497.0>},
              {id,diag_handler_worker},
              {mfargs,{work_queue,start_link,[diag_handler_worker]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-05-15T21:09:34.791Z,ns_1@cb.local:ns_server_sup<0.492.0>:dir_size:start_link:33]Starting quick version of dir_size with program name: godu
[error_logger:info,2025-05-15T21:09:34.792Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.498.0>},
              {id,dir_size},
              {mfargs,{dir_size,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:34.793Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.499.0>},
              {id,request_tracker},
              {mfargs,{request_tracker,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:34.794Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.500.0>},
              {id,chronicle_kv_log},
              {mfargs,{chronicle_kv_log,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:warn,2025-05-15T21:09:34.797Z,ns_1@cb.local:ns_log<0.502.0>:gossip_replicator:read_logs:244]Couldn't load logs from "/opt/couchbase/var/lib/couchbase/ns_log" (perhaps it's first
                         startup): {error,enoent}
[error_logger:info,2025-05-15T21:09:34.797Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.502.0>},
              {id,ns_log},
              {mfargs,{ns_log,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:34.797Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.503.0>},
              {id,event_log_events},
              {mfargs,{gen_event,start_link,[{local,event_log_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:warn,2025-05-15T21:09:34.798Z,ns_1@cb.local:event_log_server<0.504.0>:gossip_replicator:read_logs:244]Couldn't load logs from "/opt/couchbase/var/lib/couchbase/event_log" (perhaps it's first
                         startup): {error,enoent}
[error_logger:info,2025-05-15T21:09:34.798Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.504.0>},
              {id,event_log_server},
              {mfargs,{event_log_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:34.805Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.506.0>},
              {id,initargs_updater},
              {mfargs,{initargs_updater,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:34.808Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.508.0>},
              {id,timer_lag_recorder},
              {mfargs,{timer_lag_recorder,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T21:09:34.808Z,ns_1@cb.local:<0.510.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,
                                    ns_babysitter_log_consumer} with delay of 4000
[error_logger:info,2025-05-15T21:09:34.808Z,ns_1@cb.local:<0.511.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.511.0>,suppress_max_restart_intensity}
    started: [{pid,<0.512.0>},
              {id,ns_babysitter_log_consumer},
              {mfargs,{ns_log,start_link_babysitter_log_consumer,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:34.808Z,ns_1@cb.local:<0.509.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.509.0>,suppress_max_restart_intensity}
    started: [{pid,<0.510.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      ns_babysitter_log_consumer}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,
                           ns_babysitter_log_consumer},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => ns_babysitter_log_consumer,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [],restart => permanent,shutdown => 1000,
                          start =>
                              {ns_log,start_link_babysitter_log_consumer,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T21:09:34.808Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.509.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,
                      ns_babysitter_log_consumer}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => ns_babysitter_log_consumer,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [],restart => permanent,shutdown => 1000,
                         start =>
                             {ns_log,start_link_babysitter_log_consumer,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:info,2025-05-15T21:09:34.826Z,ns_1@cb.local:ns_couchdb_port<0.471.0>:ns_port_server:log:226]ns_couchdb<0.471.0>: Apache CouchDB v4.5.1-330-g3e5b8f24 (LogLevel=info) is starting.
ns_couchdb<0.471.0>: Apache CouchDB has started. Time to relax.
ns_couchdb<0.471.0>: 217: Booted. Waiting for shutdown request
ns_couchdb<0.471.0>: 217: Booted. Waiting for shutdown request
ns_couchdb<0.471.0>: working as port

[ns_server:debug,2025-05-15T21:09:34.888Z,ns_1@cb.local:<0.517.0>:goport:handle_eof:592]Stream 'stdout' closed
[ns_server:debug,2025-05-15T21:09:34.888Z,ns_1@cb.local:<0.517.0>:goport:handle_eof:592]Stream 'stderr' closed
[ns_server:info,2025-05-15T21:09:34.889Z,ns_1@cb.local:<0.517.0>:goport:handle_process_exit:573]Port exited with status 0.
[ns_server:debug,2025-05-15T21:09:34.891Z,ns_1@cb.local:prometheus_cfg<0.513.0>:prometheus_cfg:ensure_prometheus_config:932]Updating prometheus config file: /opt/couchbase/var/lib/couchbase/config/prometheus.yml
[ns_server:debug,2025-05-15T21:09:34.893Z,ns_1@cb.local:prometheus_cfg<0.513.0>:prometheus_cfg:ensure_prometheus_config:932]Updating prometheus config file: /opt/couchbase/var/lib/couchbase/config/prometheus_rules.yml
[ns_server:debug,2025-05-15T21:09:34.910Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',prometheus_auth_info} ->
[{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562574}}]}|
 {"@prometheus",
  {auth,
   [{<<"plain">>,
     {sanitized,<<"ERw+o+BSDz2di/hpDb2qs5Okj8eHAZOnGp+wiVsTes0=">>}},
    {<<"sha512">>,
     {[{<<"s">>,
        <<"f9TK8MzYTyU3arwQ1YHUSPfhN1vHOULyYBrwxb7K/a88Wr8WZCQRvfBS8WHHSl5fk8TZIEkNDBSavgE+vZefGg==">>},
       {<<"h">>,
        {sanitized,<<"OXz0VbpFyQPt2m3RAcdmNwyNAR2MjwfnPBamGHZc1XA=">>}},
       {<<"i">>,15000}]}},
    {<<"sha256">>,
     {[{<<"s">>,<<"P/9vODMlovd+zeV0mn/55AD4rfHlTUNOahtrIBjh/k0=">>},
       {<<"h">>,
        {sanitized,<<"jJTS6fLZCFiCHpcNTS9JjTQdZ89XJN7kMOLS8e7P/Uw=">>}},
       {<<"i">>,15000}]}},
    {<<"sha1">>,
     {[{<<"s">>,<<"nd0d+jY3E9ZAQK0ppR6AmM/uyck=">>},
       {<<"h">>,
        {sanitized,<<"mSKPPqbbfsHNfnfyLn8JkxFN9Z+O7xry+EllgXpWyqg=">>}},
       {<<"i">>,15000}]}}]}}]
[ns_server:debug,2025-05-15T21:09:34.913Z,ns_1@cb.local:prometheus_cfg<0.513.0>:prometheus_cfg:apply_config:724]Restarting Prometheus as the start specs have changed
[error_logger:info,2025-05-15T21:09:34.914Z,ns_1@cb.local:ale_dynamic_sup<0.78.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ale_dynamic_sup}
    started: [{pid,<0.520.0>},
              {id,'sink-prometheus'},
              {mfargs,
                  {ale_dynamic_sup,delay_death,
                      [{ale_disk_sink,start_link,
                           ['sink-prometheus',
                            "/opt/couchbase/var/lib/couchbase/logs/prometheus.log",
                            [{rotation,
                                 [{compress,true},
                                  {size,41943040},
                                  {num_files,10},
                                  {buffer_size_max,52428800}]}]]},
                       1000]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.017Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.513.0>},
              {id,prometheus_cfg},
              {mfargs,{prometheus_cfg,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T21:09:35.020Z,ns_1@cb.local:memcached_passwords<0.526.0>:memcached_cfg:init:60]Init config writer for memcached_passwords, "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2025-05-15T21:09:35.023Z,ns_1@cb.local:memcached_passwords<0.526.0>:memcached_cfg:write_cfg:156]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2025-05-15T21:09:35.035Z,ns_1@cb.local:memcached_passwords<0.526.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,{auth,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2025-05-15T21:09:35.036Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:handle_call:57]File rename from "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw" is requested
[ns_server:debug,2025-05-15T21:09:35.036Z,ns_1@cb.local:memcached_passwords<0.526.0>:memcached_cfg:rename_and_refresh:178]Successfully renamed "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2025-05-15T21:09:35.036Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:handle_cast:67]Refresh of isasl requested
[ns_server:debug,2025-05-15T21:09:35.036Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:update_refresh_state:137]Refresh of [isasl] skipped. Retry in 1000 ms.
[error_logger:info,2025-05-15T21:09:35.036Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.526.0>},
              {id,memcached_passwords},
              {mfargs,{memcached_passwords,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T21:09:35.038Z,ns_1@cb.local:memcached_permissions<0.529.0>:memcached_cfg:init:60]Init config writer for memcached_permissions, "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2025-05-15T21:09:35.041Z,ns_1@cb.local:memcached_permissions<0.529.0>:memcached_cfg:write_cfg:156]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2025-05-15T21:09:35.042Z,ns_1@cb.local:memcached_permissions<0.529.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,{user,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2025-05-15T21:09:35.042Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:handle_call:57]File rename from "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac" is requested
[ns_server:debug,2025-05-15T21:09:35.043Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:handle_cast:67]Refresh of rbac requested
[ns_server:debug,2025-05-15T21:09:35.043Z,ns_1@cb.local:memcached_permissions<0.529.0>:memcached_cfg:rename_and_refresh:178]Successfully renamed "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[error_logger:info,2025-05-15T21:09:35.043Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.529.0>},
              {id,memcached_permissions},
              {mfargs,{memcached_permissions,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.047Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.536.0>},
              {id,ns_email_alert},
              {mfargs,{ns_email_alert,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.049Z,ns_1@cb.local:ns_node_disco_sup<0.539.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_node_disco_sup}
    started: [{pid,<0.540.0>},
              {id,ns_node_disco_events},
              {mfargs,{gen_event,start_link,[{local,ns_node_disco_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T21:09:35.049Z,ns_1@cb.local:ns_node_disco<0.541.0>:ns_node_disco:init:111]Initting ns_node_disco with []
[error_logger:info,2025-05-15T21:09:35.050Z,ns_1@cb.local:ns_node_disco_sup<0.539.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_node_disco_sup}
    started: [{pid,<0.541.0>},
              {id,ns_node_disco},
              {mfargs,{ns_node_disco,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T21:09:35.050Z,ns_1@cb.local:ns_cookie_manager<0.238.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[user:info,2025-05-15T21:09:35.050Z,ns_1@cb.local:ns_cookie_manager<0.238.0>:ns_cookie_manager:do_cookie_init:78]Initial otp cookie generated: {sanitized,
                                  <<"NbWTbhHMLhxhsB8GT+S7UdJUEAJmbSMtoeSB69ItQbY=">>}
[ns_server:debug,2025-05-15T21:09:35.050Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
otp ->
[{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562575}}]},
 {cookie,{sanitized,<<"NbWTbhHMLhxhsB8GT+S7UdJUEAJmbSMtoeSB69ItQbY=">>}}]
[ns_server:debug,2025-05-15T21:09:35.050Z,ns_1@cb.local:ns_cookie_manager<0.238.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[ns_server:debug,2025-05-15T21:09:35.050Z,ns_1@cb.local:<0.546.0>:ns_node_disco:do_nodes_wanted_updated_fun:210]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"NbWTbhHMLhxhsB8GT+S7UdJUEAJmbSMtoeSB69ItQbY=">>}
[ns_server:debug,2025-05-15T21:09:35.050Z,ns_1@cb.local:<0.543.0>:ns_node_disco:do_nodes_wanted_updated_fun:210]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"NbWTbhHMLhxhsB8GT+S7UdJUEAJmbSMtoeSB69ItQbY=">>}
[error_logger:info,2025-05-15T21:09:35.053Z,ns_1@cb.local:ns_node_disco_sup<0.539.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_node_disco_sup}
    started: [{pid,<0.547.0>},
              {id,ns_node_disco_log},
              {mfargs,{ns_node_disco_log,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T21:09:35.054Z,ns_1@cb.local:<0.546.0>:ns_node_disco:do_nodes_wanted_updated_fun:213]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"NbWTbhHMLhxhsB8GT+S7UdJUEAJmbSMtoeSB69ItQbY=">>}
[ns_server:debug,2025-05-15T21:09:35.054Z,ns_1@cb.local:<0.543.0>:ns_node_disco:do_nodes_wanted_updated_fun:213]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"NbWTbhHMLhxhsB8GT+S7UdJUEAJmbSMtoeSB69ItQbY=">>}
[error_logger:info,2025-05-15T21:09:35.057Z,ns_1@cb.local:ns_config_rep_sup<0.548.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_rep_sup}
    started: [{pid,<0.550.0>},
              {id,ns_config_rep_merger},
              {mfargs,{ns_config_rep,start_link_merger,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[ns_server:debug,2025-05-15T21:09:35.058Z,ns_1@cb.local:ns_config_rep<0.551.0>:ns_config_rep:init:80]init pulling
[ns_server:debug,2025-05-15T21:09:35.058Z,ns_1@cb.local:ns_config_rep<0.551.0>:ns_config_rep:init:82]init pushing
[error_logger:info,2025-05-15T21:09:35.059Z,ns_1@cb.local:ns_config_rep_sup<0.548.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_rep_sup}
    started: [{pid,<0.551.0>},
              {id,ns_config_rep},
              {mfargs,{ns_config_rep,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.059Z,ns_1@cb.local:ns_node_disco_sup<0.539.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_node_disco_sup}
    started: [{pid,<0.548.0>},
              {id,ns_config_rep_sup},
              {mfargs,{ns_config_rep_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T21:09:35.059Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.539.0>},
              {id,ns_node_disco_sup},
              {mfargs,{ns_node_disco_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T21:09:35.059Z,ns_1@cb.local:tombstone_keeper<0.277.0>:tombstone_keeper:handle_call:49]Refreshed with timestamps []
[error_logger:info,2025-05-15T21:09:35.059Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.562.0>},
              {id,tombstone_agent},
              {mfargs,{tombstone_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.061Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.569.0>},
              {id,vbucket_map_mirror},
              {mfargs,{vbucket_map_mirror,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.063Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.571.0>},
              {id,capi_url_cache},
              {mfargs,{capi_url_cache,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.064Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.575.0>},
              {id,bucket_info_cache},
              {mfargs,{bucket_info_cache,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.064Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.578.0>},
              {id,ns_tick_event},
              {mfargs,{gen_event,start_link,[{local,ns_tick_event}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.064Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.579.0>},
              {id,buckets_events},
              {mfargs,{gen_event,start_link,[{local,buckets_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.064Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.580.0>},
              {id,ns_stats_event},
              {mfargs,{gen_event,start_link,[{local,ns_stats_event}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.067Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.581.0>},
              {id,samples_loader_tasks},
              {mfargs,{samples_loader_tasks,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.070Z,ns_1@cb.local:ns_heart_sup<0.582.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_heart_sup}
    started: [{pid,<0.583.0>},
              {id,ns_heart},
              {mfargs,{ns_heart,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.071Z,ns_1@cb.local:ns_heart_sup<0.582.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_heart_sup}
    started: [{pid,<0.585.0>},
              {id,ns_heart_slow_updater},
              {mfargs,{ns_heart,start_link_slow_updater,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.071Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.582.0>},
              {id,ns_heart_sup},
              {mfargs,{ns_heart_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T21:09:35.072Z,ns_1@cb.local:ns_doctor_sup<0.587.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_doctor_sup}
    started: [{pid,<0.588.0>},
              {id,ns_doctor_events},
              {mfargs,{gen_event,start_link,[{local,ns_doctor_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.078Z,ns_1@cb.local:ns_doctor_sup<0.587.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_doctor_sup}
    started: [{pid,<0.591.0>},
              {id,ns_doctor},
              {mfargs,{ns_doctor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T21:09:35.078Z,ns_1@cb.local:<0.586.0>:restartable:start_child:92]Started child process <0.587.0>
  MFA: {ns_doctor_sup,start_link,[]}
[error_logger:info,2025-05-15T21:09:35.078Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.586.0>},
              {id,ns_doctor_sup},
              {mfargs,{restartable,start_link,
                                   [{ns_doctor_sup,start_link,[]},infinity]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T21:09:35.078Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.594.0>},
              {id,master_activity_events},
              {mfargs,{gen_event,start_link,[{local,master_activity_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.081Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.595.0>},
              {id,xdcr_ckpt_store},
              {mfargs,{simple_store,start_link,[xdcr_ckpt_data]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.081Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.596.0>},
              {id,metakv_worker},
              {mfargs,{work_queue,start_link,[metakv_worker]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.081Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.597.0>},
              {id,index_events},
              {mfargs,{gen_event,start_link,[{local,index_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.081Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.598.0>},
              {id,index_settings_manager},
              {mfargs,{index_settings_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.081Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.600.0>},
              {id,query_settings_manager},
              {mfargs,{query_settings_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.081Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.602.0>},
              {id,eventing_settings_manager},
              {mfargs,{eventing_settings_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.081Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.604.0>},
              {id,analytics_settings_manager},
              {mfargs,{analytics_settings_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.082Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.606.0>},
              {id,audit_events},
              {mfargs,{gen_event,start_link,[{local,audit_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[ns_server:debug,2025-05-15T21:09:35.082Z,ns_1@cb.local:<0.608.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,
                                    encryption_service} with delay of 1000
[ns_server:debug,2025-05-15T21:09:35.082Z,ns_1@cb.local:encryption_service<0.610.0>:encryption_service:recover:199]Config marker /opt/couchbase/var/lib/couchbase/config/sm_load_config_marker doesn't exist
[error_logger:info,2025-05-15T21:09:35.082Z,ns_1@cb.local:<0.609.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.609.0>,suppress_max_restart_intensity}
    started: [{pid,<0.610.0>},
              {id,encryption_service},
              {mfargs,{encryption_service,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.082Z,ns_1@cb.local:<0.607.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.607.0>,suppress_max_restart_intensity}
    started: [{pid,<0.608.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      encryption_service}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,encryption_service},
                       1000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 1,id => encryption_service,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [encryption_service],
                          restart => permanent,shutdown => 5000,
                          start => {encryption_service,start_link,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T21:09:35.082Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.607.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,encryption_service}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 1,id => encryption_service,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [encryption_service],
                         restart => permanent,shutdown => 5000,
                         start => {encryption_service,start_link,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T21:09:35.088Z,ns_1@cb.local:menelaus_sup<0.612.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.615.0>},
              {id,menelaus_ui_auth},
              {mfargs,{menelaus_ui_auth,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.088Z,ns_1@cb.local:menelaus_sup<0.612.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.617.0>},
              {id,scram_sha},
              {mfargs,{scram_sha,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.091Z,ns_1@cb.local:menelaus_sup<0.612.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.618.0>},
              {id,menelaus_local_auth},
              {mfargs,{menelaus_local_auth,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.095Z,ns_1@cb.local:menelaus_sup<0.612.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.619.0>},
              {id,menelaus_web_cache},
              {mfargs,{menelaus_web_cache,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.099Z,ns_1@cb.local:menelaus_sup<0.612.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.620.0>},
              {id,menelaus_stats_gatherer},
              {mfargs,{menelaus_stats_gatherer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.099Z,ns_1@cb.local:menelaus_sup<0.612.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.621.0>},
              {id,json_rpc_events},
              {mfargs,{gen_event,start_link,[{local,json_rpc_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.110Z,ns_1@cb.local:menelaus_web_sup<0.622.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_web_sup}
    started: [{pid,<0.623.0>},
              {id,menelaus_event},
              {mfargs,{menelaus_event,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2025-05-15T21:09:35.112Z,ns_1@cb.local:<0.626.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2025-05-15T21:09:35.113Z,ns_1@cb.local:<0.626.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2025-05-15T21:09:35.113Z,ns_1@cb.local:<0.626.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2025-05-15T21:09:35.114Z,ns_1@cb.local:<0.626.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[error_logger:info,2025-05-15T21:09:35.114Z,ns_1@cb.local:inet_gethost_native_sup<0.627.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,inet_gethost_native_sup}
    started: [{pid,<0.628.0>},{mfa,{inet_gethost_native,init,[[]]}}]

[error_logger:info,2025-05-15T21:09:35.114Z,ns_1@cb.local:kernel_safe_sup<0.67.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,kernel_safe_sup}
    started: [{pid,<0.627.0>},
              {id,inet_gethost_native_sup},
              {mfargs,{inet_gethost_native,start_link,[]}},
              {restart_type,temporary},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-05-15T21:09:35.114Z,ns_1@cb.local:<0.626.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2025-05-15T21:09:35.114Z,ns_1@cb.local:<0.626.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"0.0.0.0"},{port,8091}]
[error_logger:info,2025-05-15T21:09:35.115Z,ns_1@cb.local:<0.626.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.626.0>,menelaus_web}
    started: [{pid,<0.629.0>},
              {id,menelaus_web_ipv4},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet},
                        {name,menelaus_web},
                        {port,8091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2025-05-15T21:09:35.115Z,ns_1@cb.local:<0.626.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2025-05-15T21:09:35.115Z,ns_1@cb.local:<0.626.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2025-05-15T21:09:35.115Z,ns_1@cb.local:<0.626.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2025-05-15T21:09:35.115Z,ns_1@cb.local:<0.626.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2025-05-15T21:09:35.116Z,ns_1@cb.local:<0.626.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2025-05-15T21:09:35.116Z,ns_1@cb.local:<0.626.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"::"},{port,8091}]
[error_logger:info,2025-05-15T21:09:35.116Z,ns_1@cb.local:<0.626.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.626.0>,menelaus_web}
    started: [{pid,<0.646.0>},
              {id,menelaus_web_ipv6},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet6},
                        {name,menelaus_web},
                        {port,8091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T21:09:35.116Z,ns_1@cb.local:<0.625.0>:restartable:start_child:92]Started child process <0.626.0>
  MFA: {menelaus_web,start_link,[]}
[error_logger:info,2025-05-15T21:09:35.116Z,ns_1@cb.local:menelaus_web_sup<0.622.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_web_sup}
    started: [{pid,<0.625.0>},
              {id,menelaus_web},
              {mfargs,{restartable,start_link,
                                   [{menelaus_web,start_link,[]},infinity]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[user:info,2025-05-15T21:09:35.116Z,ns_1@cb.local:menelaus_sup<0.612.0>:menelaus_web_sup:start_link:38]Couchbase Server has started on web port 8091 on node 'ns_1@cb.local'. Version: "7.6.2-3721-enterprise".
[error_logger:info,2025-05-15T21:09:35.116Z,ns_1@cb.local:menelaus_sup<0.612.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.622.0>},
              {id,menelaus_web_sup},
              {mfargs,{menelaus_web_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T21:09:35.116Z,ns_1@cb.local:menelaus_sup<0.612.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.663.0>},
              {id,menelaus_web_alerts_srv},
              {mfargs,{menelaus_web_alerts_srv,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.119Z,ns_1@cb.local:menelaus_sup<0.612.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.665.0>},
              {id,guardrail_monitor},
              {mfargs,{guardrail_monitor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.122Z,ns_1@cb.local:menelaus_sup<0.612.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.666.0>},
              {id,menelaus_cbauth},
              {mfargs,{menelaus_cbauth,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.122Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.612.0>},
              {id,menelaus},
              {mfargs,{menelaus_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T21:09:35.122Z,ns_1@cb.local:<0.673.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,ns_ports_setup} with delay of 4000
[error_logger:info,2025-05-15T21:09:35.122Z,ns_1@cb.local:<0.674.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.674.0>,suppress_max_restart_intensity}
    started: [{pid,<0.675.0>},
              {id,ns_ports_setup},
              {mfargs,{ns_ports_setup,start,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.122Z,ns_1@cb.local:<0.672.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.672.0>,suppress_max_restart_intensity}
    started: [{pid,<0.673.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      ns_ports_setup}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,ns_ports_setup},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => ns_ports_setup,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [],restart => permanent,
                          shutdown => brutal_kill,
                          start => {ns_ports_setup,start,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T21:09:35.122Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.672.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,ns_ports_setup}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => ns_ports_setup,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [],restart => permanent,
                         shutdown => brutal_kill,
                         start => {ns_ports_setup,start,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T21:09:35.124Z,ns_1@cb.local:ns_ports_setup<0.675.0>:ns_ports_manager:set_dynamic_children:48]Setting children [memcached,saslauthd_port,goxdcr]
[error_logger:info,2025-05-15T21:09:35.124Z,ns_1@cb.local:service_agent_sup<0.678.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_agent_sup}
    started: [{pid,<0.679.0>},
              {id,service_agent_children_sup},
              {mfargs,{supervisor,start_link,
                                  [{local,service_agent_children_sup},
                                   service_agent_sup,child]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T21:09:35.125Z,ns_1@cb.local:service_agent_sup<0.678.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_agent_sup}
    started: [{pid,<0.680.0>},
              {id,service_agent_worker},
              {mfargs,{erlang,apply,[#Fun<service_agent_sup.0.125430937>,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.125Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.678.0>},
              {id,service_agent_sup},
              {mfargs,{service_agent_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T21:09:35.130Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.682.0>},
              {id,ns_memcached_sockets_pool},
              {mfargs,{ns_memcached_sockets_pool,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T21:09:35.133Z,ns_1@cb.local:memcached_auth_server<0.683.0>:memcached_auth_server:reconnect:239]Skipping creation of 'Auth provider' connection because external users are disabled
[error_logger:info,2025-05-15T21:09:35.133Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.683.0>},
              {id,memcached_auth_server},
              {mfargs,{memcached_auth_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T21:09:35.133Z,ns_1@cb.local:<0.686.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,ns_audit_cfg} with delay of 4000
[ns_server:debug,2025-05-15T21:09:35.133Z,ns_1@cb.local:ns_audit_cfg<0.688.0>:ns_audit_cfg:write_audit_json:238]Writing new content to "/opt/couchbase/var/lib/couchbase/config/audit.json", Params [{descriptors_path,
                                                                                      "/opt/couchbase/etc/security"},
                                                                                     {version,
                                                                                      2},
                                                                                     {uuid,
                                                                                      "33985209"},
                                                                                     {event_states,
                                                                                      {[]}},
                                                                                     {filtering_enabled,
                                                                                      true},
                                                                                     {disabled_userids,
                                                                                      []},
                                                                                     {auditd_enabled,
                                                                                      false},
                                                                                     {log_path,
                                                                                      "/opt/couchbase/var/lib/couchbase/logs"},
                                                                                     {prune_age,
                                                                                      0},
                                                                                     {rotate_interval,
                                                                                      86400},
                                                                                     {rotate_size,
                                                                                      20971520},
                                                                                     {sync,
                                                                                      []}]
[ns_server:debug,2025-05-15T21:09:35.136Z,ns_1@cb.local:ns_audit_cfg<0.688.0>:ns_audit_cfg:notify_memcached:152]Instruct memcached to reload audit config
[error_logger:info,2025-05-15T21:09:35.136Z,ns_1@cb.local:<0.687.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.687.0>,suppress_max_restart_intensity}
    started: [{pid,<0.688.0>},
              {id,ns_audit_cfg},
              {mfargs,{ns_audit_cfg,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.137Z,ns_1@cb.local:<0.685.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.685.0>,suppress_max_restart_intensity}
    started: [{pid,<0.686.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      ns_audit_cfg}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,ns_audit_cfg},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => ns_audit_cfg,inherited_max_r => 20,
                          inherited_max_t => 10,modules => [],
                          restart => permanent,shutdown => 1000,
                          start => {ns_audit_cfg,start_link,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T21:09:35.137Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.685.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,ns_audit_cfg}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => ns_audit_cfg,inherited_max_r => 20,
                         inherited_max_t => 10,modules => [],
                         restart => permanent,shutdown => 1000,
                         start => {ns_audit_cfg,start_link,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:warn,2025-05-15T21:09:35.137Z,ns_1@cb.local:<0.692.0>:ns_memcached:connect:1460]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}, retrying.
[ns_server:debug,2025-05-15T21:09:35.137Z,ns_1@cb.local:<0.694.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,ns_audit} with delay of 4000
[error_logger:info,2025-05-15T21:09:35.140Z,ns_1@cb.local:<0.695.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.695.0>,suppress_max_restart_intensity}
    started: [{pid,<0.696.0>},
              {id,ns_audit},
              {mfargs,{ns_audit,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.140Z,ns_1@cb.local:<0.693.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.693.0>,suppress_max_restart_intensity}
    started: [{pid,<0.694.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      ns_audit}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,ns_audit},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => ns_audit,inherited_max_r => 20,
                          inherited_max_t => 10,modules => [],
                          restart => permanent,shutdown => 1000,
                          start => {ns_audit,start_link,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T21:09:35.140Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.693.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,ns_audit}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => ns_audit,inherited_max_r => 20,
                         inherited_max_t => 10,modules => [],
                         restart => permanent,shutdown => 1000,
                         start => {ns_audit,start_link,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T21:09:35.140Z,ns_1@cb.local:<0.699.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,
                                    memcached_config_mgr} with delay of 4000
[ns_server:debug,2025-05-15T21:09:35.141Z,ns_1@cb.local:memcached_config_mgr<0.701.0>:memcached_config_mgr:memcached_port_pid:154]waiting for completion of initial ns_ports_setup round
[error_logger:info,2025-05-15T21:09:35.141Z,ns_1@cb.local:<0.700.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.700.0>,suppress_max_restart_intensity}
    started: [{pid,<0.701.0>},
              {id,memcached_config_mgr},
              {mfargs,{memcached_config_mgr,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.141Z,ns_1@cb.local:<0.698.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.698.0>,suppress_max_restart_intensity}
    started: [{pid,<0.699.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      memcached_config_mgr}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,memcached_config_mgr},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => memcached_config_mgr,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [],restart => permanent,shutdown => 1000,
                          start => {memcached_config_mgr,start_link,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T21:09:35.141Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.698.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,memcached_config_mgr}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => memcached_config_mgr,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [],restart => permanent,shutdown => 1000,
                         start => {memcached_config_mgr,start_link,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:info,2025-05-15T21:09:35.144Z,ns_1@cb.local:<0.703.0>:ns_memcached_log_rotator:init:36]Starting log rotator on "/opt/couchbase/var/lib/couchbase/logs"/"memcached.log"* with an initial period of 39003ms
[error_logger:info,2025-05-15T21:09:35.144Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.703.0>},
              {id,ns_memcached_log_rotator},
              {mfargs,{ns_memcached_log_rotator,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.145Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.704.0>},
              {id,testconditions_store},
              {mfargs,{simple_store,start_link,[testconditions]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T21:09:35.149Z,ns_1@cb.local:<0.705.0>:memcached_config_mgr:memcached_port_pid:154]waiting for completion of initial ns_ports_setup round
[error_logger:info,2025-05-15T21:09:35.149Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.705.0>},
              {id,terse_cluster_info_uploader},
              {mfargs,{terse_cluster_info_uploader,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.156Z,ns_1@cb.local:ns_bucket_worker_sup<0.707.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_bucket_worker_sup}
    started: [{pid,<0.708.0>},
              {id,ns_bucket_sup},
              {mfargs,{ns_bucket_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T21:09:35.158Z,ns_1@cb.local:ns_bucket_worker_sup<0.707.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_bucket_worker_sup}
    started: [{pid,<0.709.0>},
              {id,ns_bucket_worker},
              {mfargs,{ns_bucket_worker,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.158Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.707.0>},
              {id,ns_bucket_worker_sup},
              {mfargs,{ns_bucket_worker_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:warn,2025-05-15T21:09:35.159Z,ns_1@cb.local:<0.712.0>:ns_memcached:connect:1460]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}, retrying.
[error_logger:info,2025-05-15T21:09:35.164Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.713.0>},
              {id,ns_server_stats},
              {mfargs,{ns_server_stats,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T21:09:35.169Z,ns_1@cb.local:<0.715.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ale_stats_events,<0.713.0>} exited with reason {noproc,
                                                                                {gen_statem,
                                                                                 call,
                                                                                 [mb_master,
                                                                                  master_node,
                                                                                  infinity]}}
[error_logger:info,2025-05-15T21:09:35.173Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.717.0>},
              {id,{stats_reader,"@system"}},
              {mfargs,{stats_reader,start_link,["@system"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.173Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.719.0>},
              {id,{stats_reader,"@system-processes"}},
              {mfargs,{stats_reader,start_link,["@system-processes"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.174Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.721.0>},
              {id,{stats_reader,"@query"}},
              {mfargs,{stats_reader,start_link,["@query"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.174Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.723.0>},
              {id,{stats_reader,"@global"}},
              {mfargs,{stats_reader,start_link,["@global"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.175Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.725.0>},
              {id,goxdcr_status_keeper},
              {mfargs,{goxdcr_status_keeper,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.178Z,ns_1@cb.local:services_stats_sup<0.726.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,services_stats_sup}
    started: [{pid,<0.727.0>},
              {id,service_stats_children_sup},
              {mfargs,{supervisor,start_link,
                                  [{local,service_stats_children_sup},
                                   services_stats_sup,child]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T21:09:35.181Z,ns_1@cb.local:service_status_keeper_sup<0.728.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_status_keeper_sup}
    started: [{pid,<0.729.0>},
              {id,service_status_keeper_worker},
              {mfargs,{work_queue,start_link,[service_status_keeper_worker]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.187Z,ns_1@cb.local:service_status_keeper_sup<0.728.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_status_keeper_sup}
    started: [{pid,<0.731.0>},
              {id,service_status_keeper_index},
              {mfargs,{service_index,start_keeper,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T21:09:35.189Z,ns_1@cb.local:goxdcr_status_keeper<0.725.0>:goxdcr_rest:get_from_goxdcr:139]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2025-05-15T21:09:35.189Z,ns_1@cb.local:goxdcr_status_keeper<0.725.0>:goxdcr_rest:get_from_goxdcr:139]Goxdcr is temporary not available. Return empty list.
[error_logger:info,2025-05-15T21:09:35.190Z,ns_1@cb.local:service_status_keeper_sup<0.728.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_status_keeper_sup}
    started: [{pid,<0.735.0>},
              {id,service_status_keeper_fts},
              {mfargs,{service_fts,start_keeper,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.191Z,ns_1@cb.local:service_status_keeper_sup<0.728.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_status_keeper_sup}
    started: [{pid,<0.738.0>},
              {id,service_status_keeper_eventing},
              {mfargs,{service_eventing,start_keeper,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.191Z,ns_1@cb.local:services_stats_sup<0.726.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,services_stats_sup}
    started: [{pid,<0.728.0>},
              {id,service_status_keeper_sup},
              {mfargs,{service_status_keeper_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T21:09:35.192Z,ns_1@cb.local:services_stats_sup<0.726.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,services_stats_sup}
    started: [{pid,<0.741.0>},
              {id,service_stats_worker},
              {mfargs,{erlang,apply,[#Fun<services_stats_sup.0.35188242>,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.192Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.726.0>},
              {id,services_stats_sup},
              {mfargs,{services_stats_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T21:09:35.192Z,ns_1@cb.local:<0.744.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,
                                    compaction_daemon} with delay of 4000
[ns_server:debug,2025-05-15T21:09:35.196Z,ns_1@cb.local:<0.748.0>:new_concurrency_throttle:init:109]init concurrent throttle process, pid: <0.748.0>, type: kv_throttle# of available token: 1
[ns_server:debug,2025-05-15T21:09:35.197Z,ns_1@cb.local:compaction_daemon<0.746.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_kv. Rescheduling compaction.
[error_logger:info,2025-05-15T21:09:35.197Z,ns_1@cb.local:<0.745.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.745.0>,suppress_max_restart_intensity}
    started: [{pid,<0.746.0>},
              {id,compaction_daemon},
              {mfargs,{compaction_daemon,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,86400000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T21:09:35.197Z,ns_1@cb.local:compaction_daemon<0.746.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T21:09:35.197Z,ns_1@cb.local:compaction_daemon<0.746.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2025-05-15T21:09:35.197Z,ns_1@cb.local:compaction_daemon<0.746.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T21:09:35.197Z,ns_1@cb.local:compaction_daemon<0.746.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2025-05-15T21:09:35.197Z,ns_1@cb.local:compaction_daemon<0.746.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_master too soon. Next run will be in 3600s
[error_logger:info,2025-05-15T21:09:35.197Z,ns_1@cb.local:<0.743.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.743.0>,suppress_max_restart_intensity}
    started: [{pid,<0.744.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      compaction_daemon}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,compaction_daemon},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => compaction_daemon,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [compaction_daemon],
                          restart => permanent,shutdown => 86400000,
                          start => {compaction_daemon,start_link,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T21:09:35.197Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.743.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,compaction_daemon}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => compaction_daemon,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [compaction_daemon],
                         restart => permanent,shutdown => 86400000,
                         start => {compaction_daemon,start_link,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T21:09:35.200Z,ns_1@cb.local:cluster_logs_sup<0.749.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,cluster_logs_sup}
    started: [{pid,<0.750.0>},
              {id,ets_holder},
              {mfargs,{cluster_logs_collection_task,start_link_ets_holder,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.200Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.749.0>},
              {id,cluster_logs_sup},
              {mfargs,{cluster_logs_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T21:09:35.200Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.751.0>},
              {id,collections},
              {mfargs,{collections,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.200Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.753.0>},
              {id,leader_events},
              {mfargs,{gen_event,start_link,[{local,leader_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.207Z,ns_1@cb.local:leader_leases_sup<0.756.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_leases_sup}
    started: [{pid,<0.757.0>},
              {id,leader_activities},
              {mfargs,{leader_activities,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,10000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.211Z,ns_1@cb.local:leader_leases_sup<0.756.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_leases_sup}
    started: [{pid,<0.762.0>},
              {id,leader_lease_agent},
              {mfargs,{leader_lease_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.211Z,ns_1@cb.local:leader_services_sup<0.755.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_services_sup}
    started: [{pid,<0.756.0>},
              {id,leader_leases_sup},
              {mfargs,{leader_leases_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T21:09:35.213Z,ns_1@cb.local:ns_heart<0.583.0>:goxdcr_rest:get_from_goxdcr:139]Goxdcr is temporary not available. Return empty list.
[error_logger:info,2025-05-15T21:09:35.214Z,ns_1@cb.local:leader_registry_sup<0.764.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_registry_sup}
    started: [{pid,<0.765.0>},
              {id,leader_registry},
              {mfargs,{leader_registry,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T21:09:35.214Z,ns_1@cb.local:leader_registry_sup<0.764.0>:mb_master:check_master_takeover_needed:183]Sending master node question to the following nodes: []
[ns_server:debug,2025-05-15T21:09:35.214Z,ns_1@cb.local:leader_registry_sup<0.764.0>:mb_master:check_master_takeover_needed:187]Got replies: []
[ns_server:debug,2025-05-15T21:09:35.214Z,ns_1@cb.local:leader_registry_sup<0.764.0>:mb_master:check_master_takeover_needed:193]Was unable to discover master, not going to force mastership takeover
[ns_server:debug,2025-05-15T21:09:35.214Z,ns_1@cb.local:mb_master<0.767.0>:mb_master:init:86]Heartbeat interval is 2000
[user:info,2025-05-15T21:09:35.214Z,ns_1@cb.local:mb_master<0.767.0>:mb_master:init:91]I'm the only node, so I'm the master.
[ns_server:debug,2025-05-15T21:09:35.214Z,ns_1@cb.local:leader_registry<0.765.0>:leader_registry:handle_new_leader:275]New leader is 'ns_1@cb.local'. Invalidating name cache.
[ns_server:debug,2025-05-15T21:09:35.223Z,ns_1@cb.local:mb_master<0.767.0>:master_activity_events:submit_cast:75]Failed to send master activity event {became_master,'ns_1@cb.local'}: {error,
                                                                       badarg}
[ns_server:debug,2025-05-15T21:09:35.225Z,ns_1@cb.local:ns_heart_slow_status_updater<0.585.0>:goxdcr_rest:get_from_goxdcr:139]Goxdcr is temporary not available. Return empty list.
[error_logger:info,2025-05-15T21:09:35.226Z,ns_1@cb.local:mb_master_sup<0.784.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.788.0>},
              {id,leader_lease_acquirer},
              {mfargs,{leader_lease_acquirer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,10000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T21:09:35.227Z,ns_1@cb.local:leader_quorum_nodes_manager<0.792.0>:leader_quorum_nodes_manager:pull_config:99]Attempting to pull config from nodes:
[]
[ns_server:debug,2025-05-15T21:09:35.227Z,ns_1@cb.local:leader_quorum_nodes_manager<0.792.0>:leader_quorum_nodes_manager:pull_config:104]Pulled config successfully.
[error_logger:info,2025-05-15T21:09:35.227Z,ns_1@cb.local:mb_master_sup<0.784.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.792.0>},
              {id,leader_quorum_nodes_manager},
              {mfargs,{leader_quorum_nodes_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-05-15T21:09:35.230Z,ns_1@cb.local:mb_master_sup<0.784.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,ns_tick},
                                         ns_tick,[],[]]): started as <0.800.0> on 'ns_1@cb.local'

[error_logger:info,2025-05-15T21:09:35.230Z,ns_1@cb.local:mb_master_sup<0.784.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.800.0>},
              {id,ns_tick},
              {mfargs,{ns_tick,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,10},
              {child_type,worker}]

[ns_server:debug,2025-05-15T21:09:35.232Z,ns_1@cb.local:leader_lease_agent<0.762.0>:leader_lease_agent:do_handle_acquire_lease:140]Granting lease to {lease_holder,<<"64babe628cd2c80b0066759dc260e4f3">>,
                                'ns_1@cb.local'} for 15000ms
[ns_server:debug,2025-05-15T21:09:35.235Z,ns_1@cb.local:<0.803.0>:chronicle_master:do_init:115]Starting with SelfRef = #Ref<0.730592461.1793589250.72703>
[ns_server:info,2025-05-15T21:09:35.235Z,ns_1@cb.local:mb_master_sup<0.784.0>:misc:start_singleton:913]start_singleton(gen_server2, start_link, [{via,leader_registry,
                                           chronicle_master},
                                          chronicle_master,[],[]]): started as <0.803.0> on 'ns_1@cb.local'

[error_logger:info,2025-05-15T21:09:35.235Z,ns_1@cb.local:mb_master_sup<0.784.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.803.0>},
              {id,chronicle_master},
              {mfargs,{chronicle_master,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-05-15T21:09:35.236Z,ns_1@cb.local:<0.799.0>:leader_lease_acquire_worker:handle_fresh_lease_acquired:296]Acquired lease from node 'ns_1@cb.local' (lease uuid: <<"64babe628cd2c80b0066759dc260e4f3">>)
[error_logger:info,2025-05-15T21:09:35.238Z,ns_1@cb.local:ns_orchestrator_sup<0.805.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.806.0>},
              {id,compat_mode_events},
              {mfargs,{gen_event,start_link,[{local,compat_mode_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-05-15T21:09:35.243Z,ns_1@cb.local:compat_mode_manager<0.807.0>:cluster_compat_mode:do_upgrades:205]Initiating rbac upgrade due to version change from [7,1] to [7,6] (target version: [7,
                                                                                    6])
[ns_server:info,2025-05-15T21:09:35.243Z,ns_1@cb.local:compat_mode_manager<0.807.0>:menelaus_users:upgrade:1057]Upgrading users database to [7,6]
[ns_server:debug,2025-05-15T21:09:35.243Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
rbac_upgrade ->
[{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562575}}]}|
 started]
[ns_server:debug,2025-05-15T21:09:35.243Z,ns_1@cb.local:ns_config_rep<0.551.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([rbac_upgrade]..)
[ns_server:debug,2025-05-15T21:09:35.243Z,ns_1@cb.local:ns_config_rep<0.551.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 27 us
[ns_server:debug,2025-05-15T21:09:35.243Z,ns_1@cb.local:ns_config_rep<0.551.0>:ns_config_rep:handle_call:149]Got full synchronization request from 'ns_1@cb.local'
[ns_server:debug,2025-05-15T21:09:35.243Z,ns_1@cb.local:ns_config_rep<0.551.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 40 us
[ns_server:debug,2025-05-15T21:09:35.244Z,ns_1@cb.local:users_storage<0.444.0>:replicated_storage:handle_call:151]Received sync_to_me with timeout = 60000, nodes = ['ns_1@cb.local']
[ns_server:debug,2025-05-15T21:09:35.244Z,ns_1@cb.local:users_replicator<0.435.0>:doc_replicator:loop:100]Received sync_to_me with timeout = 60000, nodes = ['ns_1@cb.local']
[ns_server:debug,2025-05-15T21:09:35.244Z,ns_1@cb.local:users_storage<0.444.0>:replicated_storage:handle_call:146]Received sync_token from {<0.823.0>,
                          [alias|#Ref<0.730592461.1793654789.73227>]}
[ns_server:debug,2025-05-15T21:09:35.244Z,ns_1@cb.local:users_replicator<0.435.0>:doc_replicator:loop:95]Received sync_token from {<0.823.0>,
                          [alias|#Ref<0.730592461.1793654789.73227>]}
[ns_server:debug,2025-05-15T21:09:35.244Z,ns_1@cb.local:<0.820.0>:replicated_storage:handle_call:157]sync_to_me reply: ok
[ns_server:debug,2025-05-15T21:09:35.244Z,ns_1@cb.local:users_storage<0.444.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,'_','_','_'},[],['$_']}],
                               100}
[ns_server:info,2025-05-15T21:09:35.244Z,ns_1@cb.local:compat_mode_manager<0.807.0>:menelaus_users:upgrade:1071]Users database was upgraded to [7,6]
[ns_server:debug,2025-05-15T21:09:35.244Z,ns_1@cb.local:users_storage<0.444.0>:replicated_storage:handle_call:151]Received sync_to_me with timeout = 60000, nodes = ['ns_1@cb.local']
[ns_server:debug,2025-05-15T21:09:35.244Z,ns_1@cb.local:users_replicator<0.435.0>:doc_replicator:loop:100]Received sync_to_me with timeout = 60000, nodes = ['ns_1@cb.local']
[ns_server:debug,2025-05-15T21:09:35.244Z,ns_1@cb.local:users_storage<0.444.0>:replicated_storage:handle_call:146]Received sync_token from {<0.827.0>,
                          [alias|#Ref<0.730592461.1793654787.72776>]}
[ns_server:debug,2025-05-15T21:09:35.244Z,ns_1@cb.local:users_replicator<0.435.0>:doc_replicator:loop:95]Received sync_token from {<0.827.0>,
                          [alias|#Ref<0.730592461.1793654787.72776>]}
[ns_server:debug,2025-05-15T21:09:35.244Z,ns_1@cb.local:<0.824.0>:replicated_storage:handle_call:157]sync_to_me reply: ok
[ns_server:info,2025-05-15T21:09:35.244Z,ns_1@cb.local:compat_mode_manager<0.807.0>:menelaus_users:upgrade:1073]Users database upgrade was delivered to ['ns_1@cb.local']
[ns_server:info,2025-05-15T21:09:35.244Z,ns_1@cb.local:kv<0.268.0>:chronicle_upgrade:upgrade_loop:114]Upgrading chronicle from [7,1]. Final version = [7,6]
[ns_server:info,2025-05-15T21:09:35.244Z,ns_1@cb.local:kv<0.268.0>:chronicle_upgrade:upgrade_loop:114]Upgrading chronicle from [7,2]. Final version = [7,6]
[ns_server:debug,2025-05-15T21:09:35.255Z,ns_1@cb.local:ns_ports_setup<0.675.0>:ns_ports_setup:set_children:65]Monitor ns_child_ports_sup <16971.133.0>
[ns_server:debug,2025-05-15T21:09:35.255Z,ns_1@cb.local:<0.705.0>:memcached_config_mgr:memcached_port_pid:156]ns_ports_setup seems to be ready
[ns_server:debug,2025-05-15T21:09:35.255Z,ns_1@cb.local:memcached_config_mgr<0.701.0>:memcached_config_mgr:memcached_port_pid:156]ns_ports_setup seems to be ready
[ns_server:debug,2025-05-15T21:09:35.255Z,ns_1@cb.local:memcached_config_mgr<0.701.0>:memcached_config_mgr:find_port_pid_loop:164]Found memcached port <16971.139.0>
[ns_server:debug,2025-05-15T21:09:35.255Z,ns_1@cb.local:<0.705.0>:memcached_config_mgr:find_port_pid_loop:164]Found memcached port <16971.139.0>
[ns_server:debug,2025-05-15T21:09:35.261Z,ns_1@cb.local:memcached_config_mgr<0.701.0>:memcached_config_mgr:init:93]wrote memcached config to /opt/couchbase/var/lib/couchbase/config/memcached.json. Will activate memcached port server
[ns_server:debug,2025-05-15T21:09:35.261Z,ns_1@cb.local:<0.705.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":12,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true,\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\"]}}">>
[ns_server:warn,2025-05-15T21:09:35.262Z,ns_1@cb.local:<0.834.0>:ns_memcached:connect:1460]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}, retrying.
[ns_server:debug,2025-05-15T21:09:35.262Z,ns_1@cb.local:memcached_config_mgr<0.701.0>:memcached_config_mgr:init:97]activated memcached port server
[ns_server:info,2025-05-15T21:09:35.263Z,ns_1@cb.local:memcached_config_mgr<0.701.0>:memcached_config_mgr:push_tls_config:233]Pushing TLS config to memcached:
{[{<<"private key">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem">>},
  {<<"certificate chain">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem">>},
  {<<"CA file">>,<<"/opt/couchbase/var/lib/couchbase/config/certs/ca.pem">>},
  {<<"minimum version">>,<<"TLS 1.2">>},
  {<<"cipher list">>,
   {[{<<"TLS 1.2">>,<<"HIGH">>},
     {<<"TLS 1.3">>,
      <<"TLS_AES_256_GCM_SHA384:TLS_AES_128_GCM_SHA256:TLS_CHACHA20_POLY1305_SHA256">>}]}},
  {<<"cipher order">>,true},
  {<<"client cert auth">>,<<"disabled">>}]}
[ns_server:warn,2025-05-15T21:09:35.264Z,ns_1@cb.local:<0.836.0>:ns_memcached:connect:1460]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}, retrying.
[ns_server:info,2025-05-15T21:09:35.268Z,ns_1@cb.local:ns_config<0.280.0>:ns_config:do_upgrade_config:733]Upgrading config by changes:
[{set,cluster_compat_version,[7,1]}]

[ns_server:info,2025-05-15T21:09:35.268Z,ns_1@cb.local:ns_config<0.280.0>:ns_online_config_upgrader:do_upgrade_config:59]Performing online config upgrade to [7,2]
[ns_server:info,2025-05-15T21:09:35.268Z,ns_1@cb.local:ns_config<0.280.0>:ns_config:do_upgrade_config:733]Upgrading config by changes:
[{set,cluster_compat_version,[7,2]},
 {set,auto_failover_cfg,
      [{enabled,true},
       {timeout,120},
       {count,0},
       {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
       {failover_server_group,false},
       {max_count,1},
       {failed_over_server_groups,[]},
       {can_abort_rebalance,true},
       {failover_preserve_durability_majority,false}]}]

[ns_server:debug,2025-05-15T21:09:35.270Z,ns_1@cb.local:chronicle_kv_log<0.500.0>:chronicle_kv_log:log:59]update (key: cluster_compat_version, rev: {<<"29b17497cea731943091527ebf7f7991">>,
                                           5})
[7,6]
[ns_server:debug,2025-05-15T21:09:35.270Z,ns_1@cb.local:roles_cache<0.458.0>:active_cache:clean:181]Clearing the roles_cache cache
[ns_server:debug,2025-05-15T21:09:35.270Z,ns_1@cb.local:memcached_permissions<0.529.0>:memcached_permissions:producer:512]Skipping update during users upgrade
[ns_server:debug,2025-05-15T21:09:35.270Z,ns_1@cb.local:ns_cookie_manager<0.238.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[ns_server:debug,2025-05-15T21:09:35.270Z,ns_1@cb.local:tombstone_keeper<0.277.0>:tombstone_keeper:handle_call:49]Refreshed with timestamps []
[ns_server:debug,2025-05-15T21:09:35.270Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:maybe_store_ca_certs:832]Considering to store CA certs
[ns_server:debug,2025-05-15T21:09:35.270Z,ns_1@cb.local:<0.837.0>:ns_node_disco:do_nodes_wanted_updated_fun:210]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"NbWTbhHMLhxhsB8GT+S7UdJUEAJmbSMtoeSB69ItQbY=">>}
[ns_server:debug,2025-05-15T21:09:35.270Z,ns_1@cb.local:compiled_roles_cache<0.455.0>:versioned_cache:handle_info:89]Flushing cache compiled_roles_cache due to version change from undefined to {[7,
                                                                              6],
                                                                             {0,
                                                                              725580152},
                                                                             {0,
                                                                              725580152},
                                                                             false,
                                                                             []}
[ns_server:debug,2025-05-15T21:09:35.270Z,ns_1@cb.local:<0.837.0>:ns_node_disco:do_nodes_wanted_updated_fun:213]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"NbWTbhHMLhxhsB8GT+S7UdJUEAJmbSMtoeSB69ItQbY=">>}
[ns_server:debug,2025-05-15T21:09:35.275Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:notify_services:1146]Going to notify following services: [capi_ssl_service,memcached]
[ns_server:info,2025-05-15T21:09:35.276Z,ns_1@cb.local:<0.846.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service memcached
[ns_server:info,2025-05-15T21:09:35.282Z,ns_1@cb.local:ns_config<0.280.0>:ns_online_config_upgrader:do_upgrade_config:59]Performing online config upgrade to [7,6]
[ns_server:info,2025-05-15T21:09:35.292Z,ns_1@cb.local:ns_config<0.280.0>:ns_config:do_upgrade_config:733]Upgrading config by changes:
[{set,cluster_compat_version,[7,6]},
 {set,audit_decriptors,
      [{8243,
        [{name,<<"mutate document">>},
         {description,<<"Document was mutated via the REST API">>},
         {enabled,true},
         {module,ns_server}]},
       {8255,
        [{name,<<"read document">>},
         {description,<<"Document was read via the REST API">>},
         {enabled,false},
         {module,ns_server}]},
       {8257,
        [{name,<<"alert email sent">>},
         {description,<<"An alert email was successfully sent">>},
         {enabled,true},
         {module,ns_server}]},
       {8265,
        [{name,<<"RBAC information retrieved">>},
         {description,<<"RBAC information was retrieved">>},
         {enabled,true},
         {module,ns_server}]},
       {16384,
        [{name,<<"remote cluster ref creation">>},
         {description,<<"created remote cluster ref">>},
         {enabled,true},
         {module,xdcr}]},
       {16385,
        [{name,<<"remote cluster ref update">>},
         {description,<<"updated remote cluster ref">>},
         {enabled,true},
         {module,xdcr}]},
       {16386,
        [{name,<<"remote cluster ref deletion">>},
         {description,<<"deleted remote cluster ref">>},
         {enabled,true},
         {module,xdcr}]},
       {16387,
        [{name,<<"replication creation">>},
         {description,<<"created replication">>},
         {enabled,true},
         {module,xdcr}]},
       {16388,
        [{name,<<"replication pause">>},
         {description,<<"paused replication">>},
         {enabled,true},
         {module,xdcr}]},
       {16389,
        [{name,<<"replication resume">>},
         {description,<<"resumed replication">>},
         {enabled,true},
         {module,xdcr}]},
       {16390,
        [{name,<<"replication cancellation">>},
         {description,<<"canceled replication">>},
         {enabled,true},
         {module,xdcr}]},
       {16391,
        [{name,<<"default replication settings update">>},
         {description,<<"updated default replication settings">>},
         {enabled,true},
         {module,xdcr}]},
       {16392,
        [{name,<<"individual replication settings update">>},
         {description,<<"updated individual replication settings">>},
         {enabled,true},
         {module,xdcr}]},
       {16393,
        [{name,<<"bucket settings update">>},
         {description,<<"updated bucket settings">>},
         {enabled,true},
         {module,xdcr}]},
       {16394,
        [{name,<<"authorization failure while adding remote cluster ref">>},
         {description,<<"failed to add remote cluster ref because of authorization failure">>},
         {enabled,true},
         {module,xdcr}]},
       {16395,
        [{name,<<"authorization failure while updating remote cluster ref">>},
         {description,<<"failed to update remote cluster ref because of authorization failure">>},
         {enabled,true},
         {module,xdcr}]},
       {16396,
        [{name,<<"access denied">>},
         {description,<<"access denied">>},
         {enabled,true},
         {module,xdcr}]},
       {20480,
        [{name,<<"opened DCP connection">>},
         {description,<<"opened DCP connection">>},
         {enabled,true},
         {module,memcached}]},
       {20482,
        [{name,<<"external memcached bucket flush">>},
         {description,<<"External user flushed the content of a memcached bucket">>},
         {enabled,true},
         {module,memcached}]},
       {20483,
        [{name,<<"invalid packet">>},
         {description,<<"Rejected an invalid packet">>},
         {enabled,true},
         {module,memcached}]},
       {20485,
        [{name,<<"authentication succeeded">>},
         {description,<<"Authentication to the cluster succeeded">>},
         {enabled,false},
         {module,memcached}]},
       {20488,
        [{name,<<"document read">>},
         {description,<<"Document was read">>},
         {enabled,false},
         {module,memcached}]},
       {20489,
        [{name,<<"document locked">>},
         {description,<<"Document was locked">>},
         {enabled,false},
         {module,memcached}]},
       {20490,
        [{name,<<"document modify">>},
         {description,<<"Document was modified">>},
         {enabled,false},
         {module,memcached}]},
       {20491,
        [{name,<<"document delete">>},
         {description,<<"Document was deleted">>},
         {enabled,false},
         {module,memcached}]},
       {20492,
        [{name,<<"select bucket">>},
         {description,<<"The specified bucket was selected">>},
         {enabled,true},
         {module,memcached}]},
       {20493,
        [{name,<<"session terminated">>},
         {description,<<"Session to the cluster has terminated">>},
         {enabled,false},
         {module,memcached}]},
       {20494,
        [{name,<<"tenant rate limited">>},
         {description,<<"The given tenant was rate limited">>},
         {enabled,true},
         {module,memcached}]},
       {24576,
        [{name,<<"Delete index">>},
         {description,<<"FTS index was deleted">>},
         {enabled,true},
         {module,fts}]},
       {24577,
        [{name,<<"Create/Update index">>},
         {description,<<"FTS index was created/Updated">>},
         {enabled,true},
         {module,fts}]},
       {24579,
        [{name,<<"Control index">>},
         {description,<<"FTS index control command was issued">>},
         {enabled,true},
         {module,fts}]},
       {24582,
        [{name,<<"GC run">>},
         {description,<<"GC run was triggered">>},
         {enabled,true},
         {module,fts}]},
       {24583,
        [{name,<<"CPU profile">>},
         {description,<<"CPU profiling was started">>},
         {enabled,true},
         {module,fts}]},
       {24584,
        [{name,<<"Memory profile">>},
         {description,<<"Memory profiling was started">>},
         {enabled,true},
         {module,fts}]},
       {28672,
        [{name,<<"SELECT statement">>},
         {description,<<"A N1QL SELECT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28673,
        [{name,<<"EXPLAIN statement">>},
         {description,<<"A N1QL EXPLAIN statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28674,
        [{name,<<"PREPARE statement">>},
         {description,<<"A N1QL PREPARE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28675,
        [{name,<<"INFER statement">>},
         {description,<<"A N1QL INFER statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28676,
        [{name,<<"INSERT statement">>},
         {description,<<"A N1QL INSERT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28677,
        [{name,<<"UPSERT statement">>},
         {description,<<"A N1QL UPSERT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28678,
        [{name,<<"DELETE statement">>},
         {description,<<"A N1QL DELETE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28679,
        [{name,<<"UPDATE statement">>},
         {description,<<"A N1QL UPDATE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28680,
        [{name,<<"MERGE statement">>},
         {description,<<"A N1QL MERGE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28681,
        [{name,<<"CREATE INDEX statement">>},
         {description,<<"A N1QL CREATE INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28682,
        [{name,<<"DROP INDEX statement">>},
         {description,<<"A N1QL DROP INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28683,
        [{name,<<"ALTER INDEX statement">>},
         {description,<<"A N1QL ALTER INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28684,
        [{name,<<"BUILD INDEX statement">>},
         {description,<<"A N1QL BUILD INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28685,
        [{name,<<"GRANT ROLE statement">>},
         {description,<<"A N1QL GRANT ROLE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28686,
        [{name,<<"REVOKE ROLE statement">>},
         {description,<<"A N1QL REVOKE ROLE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28687,
        [{name,<<"UNRECOGNIZED statement">>},
         {description,<<"An unrecognized statement was received by the N1QL query engine">>},
         {enabled,false},
         {module,n1ql}]},
       {28688,
        [{name,<<"CREATE PRIMARY INDEX statement">>},
         {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28689,
        [{name,<<"/admin/stats API request">>},
         {description,<<"An HTTP request was made to the API at /admin/stats.">>},
         {enabled,false},
         {module,n1ql}]},
       {28690,
        [{name,<<"/admin/vitals API request">>},
         {description,<<"An HTTP request was made to the API at /admin/vitals.">>},
         {enabled,false},
         {module,n1ql}]},
       {28691,
        [{name,<<"/admin/prepareds API request">>},
         {description,<<"An HTTP request was made to the API at /admin/prepareds.">>},
         {enabled,false},
         {module,n1ql}]},
       {28692,
        [{name,<<"/admin/active_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/active_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28693,
        [{name,<<"/admin/indexes/prepareds API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
         {enabled,false},
         {module,n1ql}]},
       {28694,
        [{name,<<"/admin/indexes/active_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28695,
        [{name,<<"/admin/indexes/completed_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28697,
        [{name,<<"/admin/ping API request">>},
         {description,<<"An HTTP request was made to the API at /admin/ping.">>},
         {enabled,false},
         {module,n1ql}]},
       {28698,
        [{name,<<"/admin/config API request">>},
         {description,<<"An HTTP request was made to the API at /admin/config.">>},
         {enabled,false},
         {module,n1ql}]},
       {28699,
        [{name,<<"/admin/ssl_cert API request">>},
         {description,<<"An HTTP request was made to the API at /admin/ssl_cert.">>},
         {enabled,false},
         {module,n1ql}]},
       {28700,
        [{name,<<"/admin/settings API request">>},
         {description,<<"An HTTP request was made to the API at /admin/settings.">>},
         {enabled,false},
         {module,n1ql}]},
       {28701,
        [{name,<<"/admin/clusters API request">>},
         {description,<<"An HTTP request was made to the API at /admin/clusters.">>},
         {enabled,false},
         {module,n1ql}]},
       {28702,
        [{name,<<"/admin/completed_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/completed_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28704,
        [{name,<<"/admin/functions API request">>},
         {description,<<"An HTTP request was made to the API at /admin/functions.">>},
         {enabled,false},
         {module,n1ql}]},
       {28705,
        [{name,<<"/admin/indexes/functions API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/functions.">>},
         {enabled,false},
         {module,n1ql}]},
       {28706,
        [{name,<<"CREATE FUNCTION statement">>},
         {description,<<"A N1QL CREATE FUNCTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28707,
        [{name,<<"DROP FUNCTION statement">>},
         {description,<<"A N1QL DROP FUNCTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28708,
        [{name,<<"EXECUTE FUNCTION statement">>},
         {description,<<"A N1QL EXECUTE FUNCTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28709,
        [{name,<<"/admin/tasks API request">>},
         {description,<<"An HTTP request was made to the API at /admin/tasks.">>},
         {enabled,false},
         {module,n1ql}]},
       {28710,
        [{name,<<"/admin/indexes/tasks API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/tasks.">>},
         {enabled,false},
         {module,n1ql}]},
       {28711,
        [{name,<<"/admin/dictionary_cache API request">>},
         {description,<<"An HTTP request was made to the API at /admin/dictionary_cache.">>},
         {enabled,false},
         {module,n1ql}]},
       {28712,
        [{name,<<"/admin/indexes/dictionary_cache API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/dictionary_cache.">>},
         {enabled,false},
         {module,n1ql}]},
       {28713,
        [{name,<<"CREATE SCOPE statement">>},
         {description,<<"A N1QL CREATE SCOPE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28714,
        [{name,<<"DROP SCOPE statement">>},
         {description,<<"A N1QL DROP SCOPE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28715,
        [{name,<<"CREATE COLLECTION statement">>},
         {description,<<"A N1QL CREATE COLLECTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28716,
        [{name,<<"DROP COLLECTION statement">>},
         {description,<<"A N1QL DROP COLLECTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28717,
        [{name,<<"FLUSH COLLECTION statement">>},
         {description,<<"A N1QL FLUSH COLLECTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28718,
        [{name,<<"UPDATE STATISTICS statement">>},
         {description,<<"A N1QL UPDATE STATISTICS statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28719,
        [{name,<<"ADVISE statement">>},
         {description,<<"A N1QL ADVISE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28720,
        [{name,<<"START TRANSACTION statement">>},
         {description,<<"A N1QL START TRANSACTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28721,
        [{name,<<"COMMIT TRANSACTION statement">>},
         {description,<<"A N1QL COMMIT TRANSACTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28722,
        [{name,<<"ROLLBACK TRANSACTION statement">>},
         {description,<<"A N1QL ROLLBACK TRANSACTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28723,
        [{name,<<"ROLLBACK TRANSACTION TO SAVEPOINT statement">>},
         {description,<<"A N1QL ROLLBACK TRANSACTION TO SAVEPOINT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28724,
        [{name,<<"SET TRANSACTION ISOLATION statement">>},
         {description,<<"A N1QL SET TRANSACTION ISOLATION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28725,
        [{name,<<"SAVEPOINT statement">>},
         {description,<<"A N1QL SAVEPOINT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28726,
        [{name,<<"/admin/transactions API request">>},
         {description,<<"An HTTP request was made to the API at /admin/transactions.">>},
         {enabled,false},
         {module,n1ql}]},
       {28727,
        [{name,<<"/admin/indexes/transactions API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/transactions.">>},
         {enabled,false},
         {module,n1ql}]},
       {28728,
        [{name,<<"N1QL backup / restore API request">>},
         {description,<<"An HTTP request was made to archive or restore N1QL metadata">>},
         {enabled,false},
         {module,n1ql}]},
       {28729,
        [{name,<<"/admin/shutdown API request">>},
         {description,<<"An HTTP request was made to initiate graceful shutdown">>},
         {enabled,false},
         {module,n1ql}]},
       {28730,
        [{name,<<"/admin/gc API request">>},
         {description,<<"An HTTP request was made to run garbage collection">>},
         {enabled,false},
         {module,n1ql}]},
       {28731,
        [{name,<<"/admin/ffdc API request">>},
         {description,<<"An HTTP request was made to run an FFDC collection">>},
         {enabled,false},
         {module,n1ql}]},
       {28732,
        [{name,<<"/admin/log/ API request">>},
         {description,<<"An HTTP request was made to access diagnostic logs">>},
         {enabled,false},
         {module,n1ql}]},
       {28733,
        [{name,<<"/admin/sequences_cache API request">>},
         {description,<<"An HTTP request was made to access sequences">>},
         {enabled,false},
         {module,n1ql}]},
       {28734,
        [{name,<<"CREATE SEQUENCE statement">>},
         {description,<<"A N1QL CREATE SEQUENCE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28735,
        [{name,<<"ALTER SEQUENCE statement">>},
         {description,<<"A N1QL ALTER SEQUENCE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28736,
        [{name,<<"DROP SEQUENCE statement">>},
         {description,<<"A N1QL DROP SEQUENCE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28737,
        [{name,<<"Migration abort">>},
         {description,<<"Migration was aborted">>},
         {enabled,false},
         {module,n1ql}]},
       {32768,
        [{name,<<"Create Function">>},
         {description,<<"Request to create or update eventing function definition">>},
         {enabled,true},
         {module,eventing}]},
       {32769,
        [{name,<<"Delete Function">>},
         {description,<<"Request to delete eventing function definition">>},
         {enabled,true},
         {module,eventing}]},
       {32770,
        [{name,<<"Fetch Functions">>},
         {description,<<"Request to fetch eventing function definition">>},
         {enabled,false},
         {module,eventing}]},
       {32771,
        [{name,<<"List Deployed">>},
         {description,<<"Request to fetch eventing deployed functions list">>},
         {enabled,false},
         {module,eventing}]},
       {32772,
        [{name,<<"Fetch Drafts">>},
         {description,<<"Request to fetch eventing function draft definitions">>},
         {enabled,false},
         {module,eventing}]},
       {32773,
        [{name,<<"Delete Drafts">>},
         {description,<<"Request to delete eventing function draft definitions">>},
         {enabled,true},
         {module,eventing}]},
       {32774,
        [{name,<<"Save Draft">>},
         {description,<<"Request to save a draft definition">>},
         {enabled,true},
         {module,eventing}]},
       {32775,
        [{name,<<"Start Debug">>},
         {description,<<"Request to start eventing function debugger">>},
         {enabled,true},
         {module,eventing}]},
       {32776,
        [{name,<<"Stop Debug">>},
         {description,<<"Request to stop eventing function debugger">>},
         {enabled,true},
         {module,eventing}]},
       {32777,
        [{name,<<"Start Tracing">>},
         {description,<<"Request to start tracing eventing function execution">>},
         {enabled,true},
         {module,eventing}]},
       {32778,
        [{name,<<"Stop Tracing">>},
         {description,<<"Request to stop tracing eventing function execution">>},
         {enabled,true},
         {module,eventing}]},
       {32779,
        [{name,<<"Set Settings">>},
         {description,<<"Request to save settings for an eventing function">>},
         {enabled,true},
         {module,eventing}]},
       {32780,
        [{name,<<"Fetch Config">>},
         {description,<<"Request to fetch eventing config">>},
         {enabled,false},
         {module,eventing}]},
       {32781,
        [{name,<<"Save Config">>},
         {description,<<"Request to save eventing config">>},
         {enabled,true},
         {module,eventing}]},
       {32783,
        [{name,<<"Get Settings">>},
         {description,<<"Request to fetch eventing function settings">>},
         {enabled,false},
         {module,eventing}]},
       {32784,
        [{name,<<"Import Functions">>},
         {description,<<"Request to import one or more eventing functions">>},
         {enabled,true},
         {module,eventing}]},
       {32785,
        [{name,<<"Export Functions">>},
         {description,<<"Request to export all eventing functions">>},
         {enabled,false},
         {module,eventing}]},
       {32786,
        [{name,<<"List Running">>},
         {description,<<"Request to fetch eventing running function list">>},
         {enabled,false},
         {module,eventing}]},
       {32789,
        [{name,<<"Deploy Function">>},
         {description,<<"Request to deploy eventing function">>},
         {enabled,true},
         {module,eventing}]},
       {32790,
        [{name,<<"Undeploy Function">>},
         {description,<<"Request to undeploy eventing function">>},
         {enabled,true},
         {module,eventing}]},
       {32791,
        [{name,<<"Pause Function">>},
         {description,<<"Request to pause eventing function">>},
         {enabled,true},
         {module,eventing}]},
       {32792,
        [{name,<<"Resume Function">>},
         {description,<<"Request to resume eventing function">>},
         {enabled,true},
         {module,eventing}]},
       {32793,
        [{name,<<"Backup Functions">>},
         {description,<<"Request to backup one or more eventing functions">>},
         {enabled,false},
         {module,eventing}]},
       {32794,
        [{name,<<"Restore Functions">>},
         {description,<<"Request to restore one or more eventing functions from a backup">>},
         {enabled,true},
         {module,eventing}]},
       {32795,
        [{name,<<"List Function">>},
         {description,<<"Request to fetch eventing functions">>},
         {enabled,false},
         {module,eventing}]},
       {32796,
        [{name,<<"Function Status">>},
         {description,<<"Request to fetch eventing function status">>},
         {enabled,false},
         {module,eventing}]},
       {32797,
        [{name,<<"Clear Stats">>},
         {description,<<"Request to reset eventing function stats">>},
         {enabled,true},
         {module,eventing}]},
       {32798,
        [{name,<<"Fetch Stats">>},
         {description,<<"Request to fetch eventing function stats">>},
         {enabled,false},
         {module,eventing}]},
       {32799,
        [{name,<<"Eventing Cluster Stats">>},
         {description,<<"Request to fetch eventing cluster stats">>},
         {enabled,false},
         {module,eventing}]},
       {32801,
        [{name,<<"Eventing System Event">>},
         {description,<<"Request to execute eventing node related functions">>},
         {enabled,false},
         {module,eventing}]},
       {32802,
        [{name,<<"Get User Info">>},
         {description,<<"Request to get real_userid eventing permissions">>},
         {enabled,false},
         {module,eventing}]},
       {36865,
        [{name,<<"Service configuration change">>},
         {description,<<"A successful service configuration change was made.">>},
         {enabled,true},
         {module,analytics}]},
       {36866,
        [{name,<<"Node configuration change">>},
         {description,<<"A successful node configuration change was made.">>},
         {enabled,true},
         {module,analytics}]},
       {36867,
        [{name,<<"SELECT statement">>},
         {description,<<"A N1QL SELECT statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36868,
        [{name,<<"CREATE DATAVERSE statement">>},
         {description,<<"A N1QL CREATE DATAVERSE statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36869,
        [{name,<<"DROP DATAVERSE statement">>},
         {description,<<"A N1QL DROP DATAVERSE statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36870,
        [{name,<<"CREATE DATASET statement">>},
         {description,<<"A N1QL CREATE DATASET statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36871,
        [{name,<<"DROP DATASET statement">>},
         {description,<<"A N1QL DROP DATASET statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36872,
        [{name,<<"CREATE INDEX statement">>},
         {description,<<"A N1QL CREATE INDEX statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36873,
        [{name,<<"DROP INDEX statement">>},
         {description,<<"A N1QL DROP INDEX statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36877,
        [{name,<<"CONNECT LINK statement">>},
         {description,<<"A N1QL CONNECT LINK statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36878,
        [{name,<<"DISCONNECT LINK statement">>},
         {description,<<"A N1QL DISCONNECT LINK statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36879,
        [{name,<<"UNRECOGNIZED statement">>},
         {description,<<"An UNRECOGNIZED N1QL statement was encountered">>},
         {enabled,false},
         {module,analytics}]},
       {36880,
        [{name,<<"ALTER COLLECTION statement">>},
         {description,<<"A N1QL ALTER COLLECTION statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {40960,
        [{name,<<"Create Design Doc">>},
         {description,<<"Design Doc is Created">>},
         {enabled,true},
         {module,view_engine}]},
       {40961,
        [{name,<<"Delete Design Doc">>},
         {description,<<"Design Doc is Deleted">>},
         {enabled,true},
         {module,view_engine}]},
       {40962,
        [{name,<<"Query DDoc Meta Data">>},
         {description,<<"Design Doc Meta Data Query Request">>},
         {enabled,true},
         {module,view_engine}]},
       {40963,
        [{name,<<"View Query">>},
         {description,<<"View Query Request">>},
         {enabled,false},
         {module,view_engine}]},
       {40964,
        [{name,<<"Update Design Doc">>},
         {description,<<"Design Doc is Updated">>},
         {enabled,true},
         {module,view_engine}]},
       {40966,
        [{name,<<"Access denied">>},
         {description,<<"Access denied to the REST API due to invalid permissions or credentials">>},
         {enabled,true},
         {module,view_engine}]},
       {45056,
        [{name,<<"Modify configuration">>},
         {description,<<"Backup service configuration was modified">>},
         {enabled,true},
         {module,backup}]},
       {45057,
        [{name,<<"Fetch configuration">>},
         {description,<<"Backup service configuration was retrieved">>},
         {enabled,false},
         {module,backup}]},
       {45058,
        [{name,<<"Add plan">>},
         {description,<<"A new backup plan was added">>},
         {enabled,true},
         {module,backup}]},
       {45059,
        [{name,<<"Modify plan">>},
         {description,<<"Existing backup plan was modified">>},
         {enabled,true},
         {module,backup}]},
       {45060,
        [{name,<<"Delete plan">>},
         {description,<<"A backup plan was removed">>},
         {enabled,true},
         {module,backup}]},
       {45061,
        [{name,<<"Fetch plan">>},
         {description,<<"One or more backup plans where fetched">>},
         {enabled,false},
         {module,backup}]},
       {45062,
        [{name,<<"Add repository">>},
         {description,<<"A new active backup repository was added">>},
         {enabled,true},
         {module,backup}]},
       {45063,
        [{name,<<"Archive repository">>},
         {description,<<"An active repository was archived">>},
         {enabled,true},
         {module,backup}]},
       {45064,
        [{name,<<"Pause repository">>},
         {description,<<"An active repository was paused">>},
         {enabled,true},
         {module,backup}]},
       {45065,
        [{name,<<"Resume repository">>},
         {description,<<"An active repository was resumed">>},
         {enabled,true},
         {module,backup}]},
       {45066,
        [{name,<<"Fetch repository">>},
         {description,<<"A repository was fetched">>},
         {enabled,false},
         {module,backup}]},
       {45067,
        [{name,<<"Restore repository">>},
         {description,<<"The repository data was restored">>},
         {enabled,true},
         {module,backup}]},
       {45068,
        [{name,<<"Backup repository">>},
         {description,<<"A manual backup was triggered on an active repository">>},
         {enabled,true},
         {module,backup}]},
       {45069,
        [{name,<<"Merge repository">>},
         {description,<<"A manual merge was triggered on an active repository">>},
         {enabled,true},
         {module,backup}]},
       {45070,
        [{name,<<"Info repository">>},
         {description,<<"Information about the structure and contents of the backup repository was fetched.">>},
         {enabled,false},
         {module,backup}]},
       {45071,
        [{name,<<"Examine repository">>},
         {description,<<"A document was retrieved from the repository backups">>},
         {enabled,true},
         {module,backup}]},
       {45072,
        [{name,<<"Delete repository">>},
         {description,<<"A repository was deleted">>},
         {enabled,true},
         {module,backup}]},
       {45073,
        [{name,<<"Delete backup">>},
         {description,<<"An active repository backup was deleted">>},
         {enabled,true},
         {module,backup}]},
       {45074,
        [{name,<<"Access denied">>},
         {description,<<"A user has been denied access to the REST API due to invalid permissions or credentials">>},
         {enabled,true},
         {module,backup}]}]},
 {delete,rbac_upgrade},
 {set,auto_failover_cfg,
      [{disable_max_count,false},
       {enabled,true},
       {timeout,120},
       {count,0},
       {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
       {failover_server_group,false},
       {max_count,1},
       {failed_over_server_groups,[]},
       {can_abort_rebalance,true},
       {failover_preserve_durability_majority,false}]},
 {delete,memory_alert_email},
 {delete,memory_alert_popup},
 {delete,popup_alerts_auto_failover_upgrade_70_fixed},
 {set,{metakv,<<"/indexing/settings/config">>},
      <<"{\"indexer.settings.compaction.abort_exceed_interval\":false,\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.enable_page_bloom_filter\":false,\"indexer.settings.enable_shard_affinity\":false,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.log_level\":\"info\",\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.num_replica\":0,\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.rebalance.blob_storage_bucket\":\"\",\"indexer.settings.rebalance.blob_storage_prefix\":\"\",\"indexer.settings.rebalance.blob_storage_region\":\"\",\"indexer.settings.rebalance.blob_storage_scheme\":\"\",\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.thresholds.mem_high\":70,\"indexer.settings.thresholds.mem_low\":50,\"indexer.settings.thresholds.units_high\":60,\"indexer.settings.thresholds.units_low\":40}">>},
 {set,{metakv,<<"/query/settings/config">>},
      <<"{\"cleanupclientattempts\":true,\"cleanuplostattempts\":true,\"cleanupwindow\":\"60s\",\"completed-limit\":4000,\"completed-max-plan-size\":262144,\"completed-threshold\":1000,\"loglevel\":\"info\",\"max-parallelism\":1,\"memory-quota\":0,\"n1ql-feat-ctrl\":76,\"node-quota\":0,\"node-quota-val-percent\":67,\"num-cpus\":0,\"numatrs\":1024,\"pipeline-batch\":16,\"pipeline-cap\":512,\"prepared-limit\":16384,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"query.settings.tmp_space_size\":5120,\"scan-cap\":512,\"timeout\":0,\"txtimeout\":\"0ms\",\"use-cbo\":true,\"use-replica\":\"unset\"}">>},
 {set,{metakv,<<"/analytics/settings/config">>},
      <<"{\"analytics.settings.blob_storage_bucket\":\"\",\"analytics.settings.blob_storage_prefix\":\"\",\"analytics.settings.blob_storage_region\":\"\",\"analytics.settings.blob_storage_scheme\":\"\",\"analytics.settings.num_replicas\":0}">>},
 {delete,mb33750_workaround_enabled},
 {delete,cert_and_pkey},
 {set,resource_management,
      [{bucket,[{resident_ratio,[{enabled,false},
                                 {couchstore_minimum,1},
                                 {magma_minimum,0.2}]},
                {data_size,[{enabled,false},
                            {couchstore_maximum,2},
                            {magma_maximum,16}]}]},
       {index,[]},
       {cores_per_bucket,[{enabled,false},{minimum,0.4}]},
       {disk_usage,[{enabled,false},{maximum,96}]},
       {collections_per_quota,[{enabled,false},{maximum,1}]}]}]

[ns_server:debug,2025-05-15T21:09:35.296Z,ns_1@cb.local:ns_config_rep<0.551.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([audit_decriptors,auto_failover_cfg,
                               cluster_compat_version,rbac_upgrade,
                               resource_management,
                               {metakv,<<"/analytics/settings/config">>},
                               {metakv,<<"/indexing/settings/config">>},
                               {metakv,<<"/query/settings/config">>}]..)
[ns_server:debug,2025-05-15T21:09:35.297Z,ns_1@cb.local:roles_cache<0.458.0>:active_cache:clean:181]Clearing the roles_cache cache
[ns_server:debug,2025-05-15T21:09:35.297Z,ns_1@cb.local:tombstone_keeper<0.277.0>:tombstone_keeper:handle_call:49]Refreshed with timestamps []
[ns_server:debug,2025-05-15T21:09:35.297Z,ns_1@cb.local:ns_cookie_manager<0.238.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[ns_server:debug,2025-05-15T21:09:35.297Z,ns_1@cb.local:<0.851.0>:ns_node_disco:do_nodes_wanted_updated_fun:210]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"NbWTbhHMLhxhsB8GT+S7UdJUEAJmbSMtoeSB69ItQbY=">>}
[ns_server:debug,2025-05-15T21:09:35.297Z,ns_1@cb.local:<0.851.0>:ns_node_disco:do_nodes_wanted_updated_fun:213]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"NbWTbhHMLhxhsB8GT+S7UdJUEAJmbSMtoeSB69ItQbY=">>}
[ns_server:debug,2025-05-15T21:09:35.297Z,ns_1@cb.local:memcached_passwords<0.526.0>:memcached_cfg:write_cfg:156]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2025-05-15T21:09:35.298Z,ns_1@cb.local:memcached_permissions<0.529.0>:memcached_cfg:write_cfg:156]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2025-05-15T21:09:35.298Z,ns_1@cb.local:ns_config_rep<0.551.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 70 us
[ns_server:debug,2025-05-15T21:09:35.298Z,ns_1@cb.local:ns_config_rep<0.551.0>:ns_config_rep:handle_call:149]Got full synchronization request from 'ns_1@cb.local'
[ns_server:debug,2025-05-15T21:09:35.298Z,ns_1@cb.local:ns_config_rep<0.551.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 3 us
[ns_server:debug,2025-05-15T21:09:35.298Z,ns_1@cb.local:memcached_permissions<0.529.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,{user,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[user:warn,2025-05-15T21:09:35.298Z,ns_1@cb.local:compat_mode_manager<0.807.0>:compat_mode_manager:handle_consider_switching_compat_mode:43]Changed cluster compat mode from [7,1] to [7,6]
[ns_server:debug,2025-05-15T21:09:35.299Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:handle_call:57]File rename from "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac" is requested
[ns_server:debug,2025-05-15T21:09:35.300Z,ns_1@cb.local:memcached_permissions<0.529.0>:memcached_cfg:rename_and_refresh:178]Successfully renamed "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2025-05-15T21:09:35.300Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:handle_cast:67]Refresh of rbac requested
[error_logger:info,2025-05-15T21:09:35.298Z,ns_1@cb.local:ns_orchestrator_sup<0.805.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.807.0>},
              {id,compat_mode_manager},
              {mfargs,{compat_mode_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T21:09:35.305Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
audit_decriptors ->
[{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562575}}]},
 {8243,
  [{name,<<"mutate document">>},
   {description,<<"Document was mutated via the REST API">>},
   {enabled,true},
   {module,ns_server}]},
 {8255,
  [{name,<<"read document">>},
   {description,<<"Document was read via the REST API">>},
   {enabled,false},
   {module,ns_server}]},
 {8257,
  [{name,<<"alert email sent">>},
   {description,<<"An alert email was successfully sent">>},
   {enabled,true},
   {module,ns_server}]},
 {8265,
  [{name,<<"RBAC information retrieved">>},
   {description,<<"RBAC information was retrieved">>},
   {enabled,true},
   {module,ns_server}]},
 {16384,
  [{name,<<"remote cluster ref creation">>},
   {description,<<"created remote cluster ref">>},
   {enabled,true},
   {module,xdcr}]},
 {16385,
  [{name,<<"remote cluster ref update">>},
   {description,<<"updated remote cluster ref">>},
   {enabled,true},
   {module,xdcr}]},
 {16386,
  [{name,<<"remote cluster ref deletion">>},
   {description,<<"deleted remote cluster ref">>},
   {enabled,true},
   {module,xdcr}]},
 {16387,
  [{name,<<"replication creation">>},
   {description,<<"created replication">>},
   {enabled,true},
   {module,xdcr}]},
 {16388,
  [{name,<<"replication pause">>},
   {description,<<"paused replication">>},
   {enabled,true},
   {module,xdcr}]},
 {16389,
  [{name,<<"replication resume">>},
   {description,<<"resumed replication">>},
   {enabled,true},
   {module,xdcr}]},
 {16390,
  [{name,<<"replication cancellation">>},
   {description,<<"canceled replication">>},
   {enabled,true},
   {module,xdcr}]},
 {16391,
  [{name,<<"default replication settings update">>},
   {description,<<"updated default replication settings">>},
   {enabled,true},
   {module,xdcr}]},
 {16392,
  [{name,<<"individual replication settings update">>},
   {description,<<"updated individual replication settings">>},
   {enabled,true},
   {module,xdcr}]},
 {16393,
  [{name,<<"bucket settings update">>},
   {description,<<"updated bucket settings">>},
   {enabled,true},
   {module,xdcr}]},
 {16394,
  [{name,<<"authorization failure while adding remote cluster ref">>},
   {description,<<"failed to add remote cluster ref because of authorization failure">>},
   {enabled,true},
   {module,xdcr}]},
 {16395,
  [{name,<<"authorization failure while updating remote cluster ref">>},
   {description,<<"failed to update remote cluster ref because of authorization failure">>},
   {enabled,true},
   {module,xdcr}]},
 {16396,
  [{name,<<"access denied">>},
   {description,<<"access denied">>},
   {enabled,true},
   {module,xdcr}]},
 {20480,
  [{name,<<"opened DCP connection">>},
   {description,<<"opened DCP connection">>},
   {enabled,true},
   {module,memcached}]},
 {20482,
  [{name,<<"external memcached bucket flush">>},
   {description,<<"External user flushed the content of a memcached bucket">>},
   {enabled,true},
   {module,memcached}]},
 {20483,
  [{name,<<"invalid packet">>},
   {description,<<"Rejected an invalid packet">>},
   {enabled,true},
   {module,memcached}]},
 {20485,
  [{name,<<"authentication succeeded">>},
   {description,<<"Authentication to the cluster succeeded">>},
   {enabled,false},
   {module,memcached}]},
 {20488,
  [{name,<<"document read">>},
   {description,<<"Document was read">>},
   {enabled,false},
   {module,memcached}]},
 {20489,
  [{name,<<"document locked">>},
   {description,<<"Document was locked">>},
   {enabled,false},
   {module,memcached}]},
 {20490,
  [{name,<<"document modify">>},
   {description,<<"Document was modified">>},
   {enabled,false},
   {module,memcached}]},
 {20491,
  [{name,<<"document delete">>},
   {description,<<"Document was deleted">>},
   {enabled,false},
   {module,memcached}]},
 {20492,
  [{name,<<"select bucket">>},
   {description,<<"The specified bucket was selected">>},
   {enabled,true},
   {module,memcached}]},
 {20493,
  [{name,<<"session terminated">>},
   {description,<<"Session to the cluster has terminated">>},
   {enabled,false},
   {module,memcached}]},
 {20494,
  [{name,<<"tenant rate limited">>},
   {description,<<"The given tenant was rate limited">>},
   {enabled,true},
   {module,memcached}]},
 {24576,
  [{name,<<"Delete index">>},
   {description,<<"FTS index was deleted">>},
   {enabled,true},
   {module,fts}]},
 {24577,
  [{name,<<"Create/Update index">>},
   {description,<<"FTS index was created/Updated">>},
   {enabled,true},
   {module,fts}]},
 {24579,
  [{name,<<"Control index">>},
   {description,<<"FTS index control command was issued">>},
   {enabled,true},
   {module,fts}]},
 {24582,
  [{name,<<"GC run">>},
   {description,<<"GC run was triggered">>},
   {enabled,true},
   {module,fts}]},
 {24583,
  [{name,<<"CPU profile">>},
   {description,<<"CPU profiling was started">>},
   {enabled,true},
   {module,fts}]},
 {24584,
  [{name,<<"Memory profile">>},
   {description,<<"Memory profiling was started">>},
   {enabled,true},
   {module,fts}]},
 {28672,
  [{name,<<"SELECT statement">>},
   {description,<<"A N1QL SELECT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28673,
  [{name,<<"EXPLAIN statement">>},
   {description,<<"A N1QL EXPLAIN statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28674,
  [{name,<<"PREPARE statement">>},
   {description,<<"A N1QL PREPARE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28675,
  [{name,<<"INFER statement">>},
   {description,<<"A N1QL INFER statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28676,
  [{name,<<"INSERT statement">>},
   {description,<<"A N1QL INSERT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28677,
  [{name,<<"UPSERT statement">>},
   {description,<<"A N1QL UPSERT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28678,
  [{name,<<"DELETE statement">>},
   {description,<<"A N1QL DELETE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28679,
  [{name,<<"UPDATE statement">>},
   {description,<<"A N1QL UPDATE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28680,
  [{name,<<"MERGE statement">>},
   {description,<<"A N1QL MERGE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28681,
  [{name,<<"CREATE INDEX statement">>},
   {description,<<"A N1QL CREATE INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28682,
  [{name,<<"DROP INDEX statement">>},
   {description,<<"A N1QL DROP INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28683,
  [{name,<<"ALTER INDEX statement">>},
   {description,<<"A N1QL ALTER INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28684,
  [{name,<<"BUILD INDEX statement">>},
   {description,<<"A N1QL BUILD INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28685,
  [{name,<<"GRANT ROLE statement">>},
   {description,<<"A N1QL GRANT ROLE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28686,
  [{name,<<"REVOKE ROLE statement">>},
   {description,<<"A N1QL REVOKE ROLE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28687,
  [{name,<<"UNRECOGNIZED statement">>},
   {description,<<"An unrecognized statement was received by the N1QL query engine">>},
   {enabled,false},
   {module,n1ql}]},
 {28688,
  [{name,<<"CREATE PRIMARY INDEX statement">>},
   {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28689,
  [{name,<<"/admin/stats API request">>},
   {description,<<"An HTTP request was made to the API at /admin/stats.">>},
   {enabled,false},
   {module,n1ql}]},
 {28690,
  [{name,<<"/admin/vitals API request">>},
   {description,<<"An HTTP request was made to the API at /admin/vitals.">>},
   {enabled,false},
   {module,n1ql}]},
 {28691,
  [{name,<<"/admin/prepareds API request">>},
   {description,<<"An HTTP request was made to the API at /admin/prepareds.">>},
   {enabled,false},
   {module,n1ql}]},
 {28692,
  [{name,<<"/admin/active_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/active_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28693,
  [{name,<<"/admin/indexes/prepareds API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
   {enabled,false},
   {module,n1ql}]},
 {28694,
  [{name,<<"/admin/indexes/active_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28695,
  [{name,<<"/admin/indexes/completed_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28697,
  [{name,<<"/admin/ping API request">>},
   {description,<<"An HTTP request was made to the API at /admin/ping.">>},
   {enabled,false},
   {module,n1ql}]},
 {28698,
  [{name,<<"/admin/config API request">>},
   {description,<<"An HTTP request was made to the API at /admin/config.">>},
   {enabled,false},
   {module,n1ql}]},
 {28699,
  [{name,<<"/admin/ssl_cert API request">>},
   {description,<<"An HTTP request was made to the API at /admin/ssl_cert.">>},
   {enabled,false},
   {module,n1ql}]},
 {28700,
  [{name,<<"/admin/settings API request">>},
   {description,<<"An HTTP request was made to the API at /admin/settings.">>},
   {enabled,false},
   {module,n1ql}]},
 {28701,
  [{name,<<"/admin/clusters API request">>},
   {description,<<"An HTTP request was made to the API at /admin/clusters.">>},
   {enabled,false},
   {module,n1ql}]},
 {28702,
  [{name,<<"/admin/completed_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/completed_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28704,
  [{name,<<"/admin/functions API request">>},
   {description,<<"An HTTP request was made to the API at /admin/functions.">>},
   {enabled,false},
   {module,n1ql}]},
 {28705,
  [{name,<<"/admin/indexes/functions API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/functions.">>},
   {enabled,false},
   {module,n1ql}]},
 {28706,
  [{name,<<"CREATE FUNCTION statement">>},
   {description,<<"A N1QL CREATE FUNCTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28707,
  [{name,<<"DROP FUNCTION statement">>},
   {description,<<"A N1QL DROP FUNCTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28708,
  [{name,<<"EXECUTE FUNCTION statement">>},
   {description,<<"A N1QL EXECUTE FUNCTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28709,
  [{name,<<"/admin/tasks API request">>},
   {description,<<"An HTTP request was made to the API at /admin/tasks.">>},
   {enabled,false},
   {module,n1ql}]},
 {28710,
  [{name,<<"/admin/indexes/tasks API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/tasks.">>},
   {enabled,false},
   {module,n1ql}]},
 {28711,
  [{name,<<"/admin/dictionary_cache API request">>},
   {description,<<"An HTTP request was made to the API at /admin/dictionary_cache.">>},
   {enabled,false},
   {module,n1ql}]},
 {28712,
  [{name,<<"/admin/indexes/dictionary_cache API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/dictionary_cache.">>},
   {enabled,false},
   {module,n1ql}]},
 {28713,
  [{name,<<"CREATE SCOPE statement">>},
   {description,<<"A N1QL CREATE SCOPE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28714,
  [{name,<<"DROP SCOPE statement">>},
   {description,<<"A N1QL DROP SCOPE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28715,
  [{name,<<"CREATE COLLECTION statement">>},
   {description,<<"A N1QL CREATE COLLECTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28716,
  [{name,<<"DROP COLLECTION statement">>},
   {description,<<"A N1QL DROP COLLECTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28717,
  [{name,<<"FLUSH COLLECTION statement">>},
   {description,<<"A N1QL FLUSH COLLECTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28718,
  [{name,<<"UPDATE STATISTICS statement">>},
   {description,<<"A N1QL UPDATE STATISTICS statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28719,
  [{name,<<"ADVISE statement">>},
   {description,<<"A N1QL ADVISE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28720,
  [{name,<<"START TRANSACTION statement">>},
   {description,<<"A N1QL START TRANSACTION statement was execu"...>>},
   {enabled,false},
   {module,n1ql}]},
 {28721,
  [{name,<<"COMMIT TRANSACTION statement">>},
   {description,<<"A N1QL COMMIT TRANSACTION statement was "...>>},
   {enabled,false},
   {module,n1ql}]},
 {28722,
  [{name,<<"ROLLBACK TRANSACTION statement">>},
   {description,<<"A N1QL ROLLBACK TRANSACTION statemen"...>>},
   {enabled,false},
   {module,n1ql}]},
 {28723,
  [{name,<<"ROLLBACK TRANSACTION TO SAVEPOINT st"...>>},
   {description,<<"A N1QL ROLLBACK TRANSACTION TO S"...>>},
   {enabled,false},
   {module,n1ql}]},
 {28724,
  [{name,<<"SET TRANSACTION ISOLATION statem"...>>},
   {description,<<"A N1QL SET TRANSACTION ISOLA"...>>},
   {enabled,false},
   {module,n1ql}]},
 {28725,
  [{name,<<"SAVEPOINT statement">>},
   {description,<<"A N1QL SAVEPOINT stateme"...>>},
   {enabled,false},
   {module,n1ql}]},
 {28726,
  [{name,<<"/admin/transactions API "...>>},
   {description,<<"An HTTP request was "...>>},
   {enabled,false},
   {module,n1ql}]},
 {28727,
  [{name,<<"/admin/indexes/trans"...>>},
   {description,<<"An HTTP request "...>>},
   {enabled,false},
   {module,n1ql}]},
 {28728,
  [{name,<<"N1QL backup / re"...>>},
   {description,<<"An HTTP requ"...>>},
   {enabled,false},
   {module,n1ql}]},
 {28729,
  [{name,<<"/admin/shutd"...>>},
   {description,<<"An HTTP "...>>},
   {enabled,false},
   {module,n1ql}]},
 {28730,
  [{name,<<"/admin/g"...>>},
   {description,<<"An H"...>>},
   {enabled,false},
   {module,...}]},
 {28731,[{name,<<"/adm"...>>},{description,<<...>>},{enabled,...},{...}]},
 {28732,[{name,<<...>>},{description,...},{...}|...]},
 {28733,[{name,...},{...}|...]},
 {28734,[{...}|...]},
 {28735,[...]},
 {28736,...},
 {...}|...]
[ns_server:debug,2025-05-15T21:09:35.307Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
cluster_compat_version ->
[{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{3,63914562575}}]},7,6]
[ns_server:debug,2025-05-15T21:09:35.307Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
rbac_upgrade ->
[{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{2,63914562575}}]}|
 '_deleted']
[ns_server:debug,2025-05-15T21:09:35.307Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
auto_failover_cfg ->
[{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{2,63914562575}}]},
 {disable_max_count,false},
 {enabled,true},
 {timeout,120},
 {count,0},
 {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
 {failover_server_group,false},
 {max_count,1},
 {failed_over_server_groups,[]},
 {can_abort_rebalance,true},
 {failover_preserve_durability_majority,false}]
[ns_server:debug,2025-05-15T21:09:35.307Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
resource_management ->
[{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562575}}]},
 {bucket,[{resident_ratio,[{enabled,false},
                           {couchstore_minimum,1},
                           {magma_minimum,0.2}]},
          {data_size,[{enabled,false},
                      {couchstore_maximum,2},
                      {magma_maximum,16}]}]},
 {index,[]},
 {cores_per_bucket,[{enabled,false},{minimum,0.4}]},
 {disk_usage,[{enabled,false},{maximum,96}]},
 {collections_per_quota,[{enabled,false},{maximum,1}]}]
[ns_server:debug,2025-05-15T21:09:35.307Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/analytics/settings/config">>} ->
[{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562575}}]}|
 <<"{\"analytics.settings.blob_storage_bucket\":\"\",\"analytics.settings.blob_storage_prefix\":\"\",\"analytics.settings.blob_storage_region\":\"\",\"analytics.settings.blob_storage_scheme\":\"\",\"analytics.settings.num_replicas\":0}">>]
[ns_server:debug,2025-05-15T21:09:35.307Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562575}}]}|
 <<"{\"indexer.settings.compaction.abort_exceed_interval\":false,\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.enable_page_bloom_filter\":false,\"indexer.settings.enable_"...>>]
[ns_server:debug,2025-05-15T21:09:35.307Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/query/settings/config">>} ->
[{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562575}}]}|
 <<"{\"cleanupclientattempts\":true,\"cleanuplostattempts\":true,\"cleanupwindow\":\"60s\",\"completed-limit\":4000,\"completed-max-plan-size\":262144,\"completed-threshold\":1000,\"loglevel\":\"info\",\"max-parallelism\":1,\"memory-quota\":0,\"n1ql-feat-ctrl\":76,\"node-quota\":0,\"node-quota-val-percent\":67,\"num-cpus\":0,\"numatrs\":1024,\"pipeline-batch\":16,\"pipeline-cap\":512,\"prepared-limit\":16384,\"query.settings.cu"...>>]
[ns_server:info,2025-05-15T21:09:35.307Z,ns_1@cb.local:<0.845.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service capi_ssl_service
[ns_server:info,2025-05-15T21:09:35.307Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:notify_services:1162]Succesfully notified services [memcached,capi_ssl_service]
[ns_server:debug,2025-05-15T21:09:35.307Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',prometheus_auth_info} ->
[{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{2,63914562575}}]}|
 {"@prometheus",
  {auth,
   [{<<"hash">>,
     {[{<<"hashes">>,
        {sanitized,<<"ZiyjTLslTqGlVUQGVBxgadN9XygNm1EEU3z94EjZMa4=">>}},
       {<<"algorithm">>,<<"argon2id">>},
       {<<"salt">>,<<"20RZ6WctwVVGcJXAGAUPKQ==">>},
       {<<"parallelism">>,1},
       {<<"time">>,3},
       {<<"memory">>,524288}]}},
    {<<"scram-sha-512">>,
     {[{<<"salt">>,
        <<"dbaAnetqfyDMqiH1DibtF1xNnMbddf9NNXEHwHs9+wonRGNrgBb9j3jLqL2N2j4fwtWtoowCRsoCuFD0fTZ92w==">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"OAiak3jdrtXgJNQ9PBNpQ3tJIEeCg7aPKve22uOmIfY=">>}}]}},
    {<<"scram-sha-256">>,
     {[{<<"salt">>,<<"YDvmvp12eAgvW9blhAmadDC7MAAmrTZf54LUcjD1NVM=">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"JUEefAm9QXFnuT7NcGNNkc7AZ59B5kWUEZ/aMtMMOGo=">>}}]}},
    {<<"scram-sha-1">>,
     {[{<<"salt">>,<<"5LS0UaewkGLRjk8nFbddN5cqRgo=">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"AABCEoJMDX6rYIY/7qE/U9faVRz9BmaD4fzwbX/relc=">>}}]}}]}}]
[ns_server:debug,2025-05-15T21:09:35.307Z,ns_1@cb.local:prometheus_cfg<0.513.0>:prometheus_cfg:maybe_apply_new_settings:704]Settings didn't change, ignoring update
[ns_server:debug,2025-05-15T21:09:35.308Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:restart_regenerate_client_cert_timer:1447]Time left before client cert regeneration: 70588797000
[ns_server:debug,2025-05-15T21:09:35.308Z,ns_1@cb.local:prometheus_cfg<0.513.0>:prometheus_cfg:maybe_apply_new_settings:704]Settings didn't change, ignoring update
[ns_server:debug,2025-05-15T21:09:35.308Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:maybe_store_ca_certs:832]Considering to store CA certs
[ns_server:debug,2025-05-15T21:09:35.308Z,ns_1@cb.local:ns_config_rep<0.551.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([{node,'ns_1@cb.local',prometheus_auth_info}]..)
[ns_server:debug,2025-05-15T21:09:35.309Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:restart_regenerate_client_cert_timer:1447]Time left before client cert regeneration: 70588797000
[ns_server:debug,2025-05-15T21:09:35.315Z,ns_1@cb.local:memcached_passwords<0.526.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,{auth,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2025-05-15T21:09:35.316Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:handle_call:57]File rename from "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw" is requested
[ns_server:debug,2025-05-15T21:09:35.316Z,ns_1@cb.local:memcached_passwords<0.526.0>:memcached_cfg:rename_and_refresh:178]Successfully renamed "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2025-05-15T21:09:35.316Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:handle_cast:67]Refresh of isasl requested
[error_logger:info,2025-05-15T21:09:35.316Z,ns_1@cb.local:ns_orchestrator_child_sup<0.873.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.874.0>},
              {id,ns_janitor_server},
              {mfargs,{ns_janitor_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T21:09:35.316Z,ns_1@cb.local:memcached_passwords<0.526.0>:memcached_cfg:write_cfg:156]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:info,2025-05-15T21:09:35.328Z,ns_1@cb.local:ns_orchestrator_child_sup<0.873.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          auto_reprovision},
                                         auto_reprovision,[],[]]): started as <0.875.0> on 'ns_1@cb.local'

[error_logger:info,2025-05-15T21:09:35.329Z,ns_1@cb.local:ns_orchestrator_child_sup<0.873.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.875.0>},
              {id,auto_reprovision},
              {mfargs,{auto_reprovision,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T21:09:35.329Z,ns_1@cb.local:memcached_passwords<0.526.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,{auth,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2025-05-15T21:09:35.330Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:handle_call:57]File rename from "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw" is requested
[ns_server:info,2025-05-15T21:09:35.331Z,ns_1@cb.local:ns_orchestrator_child_sup<0.873.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,auto_rebalance},
                                         auto_rebalance,[],[]]): started as <0.876.0> on 'ns_1@cb.local'

[error_logger:info,2025-05-15T21:09:35.331Z,ns_1@cb.local:ns_orchestrator_child_sup<0.873.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.876.0>},
              {id,auto_rebalance},
              {mfargs,{auto_rebalance,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-05-15T21:09:35.331Z,ns_1@cb.local:ns_orchestrator_child_sup<0.873.0>:misc:start_singleton:913]start_singleton(gen_statem, start_link, [{via,leader_registry,ns_orchestrator},
                                         ns_orchestrator,[],[]]): started as <0.877.0> on 'ns_1@cb.local'

[error_logger:info,2025-05-15T21:09:35.331Z,ns_1@cb.local:ns_orchestrator_child_sup<0.873.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.877.0>},
              {id,ns_orchestrator},
              {mfargs,{ns_orchestrator,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.331Z,ns_1@cb.local:ns_orchestrator_sup<0.805.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.873.0>},
              {id,ns_orchestrator_child_sup},
              {mfargs,{ns_orchestrator_child_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T21:09:35.331Z,ns_1@cb.local:memcached_passwords<0.526.0>:memcached_cfg:rename_and_refresh:178]Successfully renamed "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2025-05-15T21:09:35.331Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:handle_cast:67]Refresh of isasl requested
[ns_server:debug,2025-05-15T21:09:35.331Z,ns_1@cb.local:<0.879.0>:auto_failover:init:223]init auto_failover.
[user:info,2025-05-15T21:09:35.334Z,ns_1@cb.local:<0.879.0>:auto_failover:enable_auto_failover:460]Enabled auto-failover with timeout 120 and max count 1
[ns_server:debug,2025-05-15T21:09:35.334Z,ns_1@cb.local:<0.879.0>:auto_failover:update_and_save_auto_failover_state:479]No change in timeout 120
[ns_server:debug,2025-05-15T21:09:35.334Z,ns_1@cb.local:<0.879.0>:auto_failover:update_and_save_auto_failover_state:487]No change in max count 1
[ns_server:debug,2025-05-15T21:09:35.338Z,ns_1@cb.local:<0.879.0>:auto_failover:init_logic_state:249]Using auto-failover logic state {state,[],
                                    [{service_state,kv,nil,false},
                                     {service_state,n1ql,nil,false},
                                     {service_state,index,nil,false},
                                     {service_state,fts,nil,false},
                                     {service_state,cbas,nil,false},
                                     {service_state,eventing,nil,false},
                                     {service_state,backup,nil,false}],
                                    118}
[ns_server:debug,2025-05-15T21:09:35.338Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
auto_failover_cfg ->
[{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{3,63914562575}}]},
 {enabled,true},
 {timeout,120},
 {count,0},
 {max_count,1},
 {disable_max_count,false},
 {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
 {failover_server_group,false},
 {failed_over_server_groups,[]},
 {can_abort_rebalance,true},
 {failover_preserve_durability_majority,false}]
[ns_server:info,2025-05-15T21:09:35.338Z,ns_1@cb.local:ns_orchestrator_sup<0.805.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,auto_failover},
                                         auto_failover,[],[]]): started as <0.879.0> on 'ns_1@cb.local'

[ns_server:debug,2025-05-15T21:09:35.338Z,ns_1@cb.local:ns_config_rep<0.551.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([auto_failover_cfg]..)
[error_logger:info,2025-05-15T21:09:35.339Z,ns_1@cb.local:ns_orchestrator_sup<0.805.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.879.0>},
              {id,auto_failover},
              {mfargs,{auto_failover,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.339Z,ns_1@cb.local:mb_master_sup<0.784.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.805.0>},
              {id,ns_orchestrator_sup},
              {mfargs,{ns_orchestrator_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:info,2025-05-15T21:09:35.340Z,ns_1@cb.local:mb_master_sup<0.784.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          tombstone_purger},
                                         tombstone_purger,[],[]]): started as <0.891.0> on 'ns_1@cb.local'

[error_logger:info,2025-05-15T21:09:35.340Z,ns_1@cb.local:mb_master_sup<0.784.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.891.0>},
              {id,tombstone_purger},
              {mfargs,{tombstone_purger,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.340Z,ns_1@cb.local:mb_master_sup<0.784.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.892.0>},
              {id,global_tasks},
              {mfargs,{global_tasks,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.342Z,ns_1@cb.local:mb_master_sup<0.784.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.893.0>},
              {id,guardrail_enforcer},
              {mfargs,{guardrail_enforcer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T21:09:35.342Z,ns_1@cb.local:guardrail_enforcer<0.893.0>:guardrail_enforcer:maybe_notify_services:137]Changed Statuses: #{}
[ns_server:debug,2025-05-15T21:09:35.344Z,ns_1@cb.local:<0.895.0>:license_reporting:init:60]Starting license_reporting server
[ns_server:info,2025-05-15T21:09:35.344Z,ns_1@cb.local:mb_master_sup<0.784.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          license_reporting},
                                         license_reporting,[],[]]): started as <0.895.0> on 'ns_1@cb.local'

[error_logger:info,2025-05-15T21:09:35.344Z,ns_1@cb.local:mb_master_sup<0.784.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.895.0>},
              {id,license_reporting},
              {mfargs,{license_reporting,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.344Z,ns_1@cb.local:leader_registry_sup<0.764.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_registry_sup}
    started: [{pid,<0.767.0>},
              {id,mb_master},
              {mfargs,{mb_master,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T21:09:35.344Z,ns_1@cb.local:leader_services_sup<0.755.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_services_sup}
    started: [{pid,<0.764.0>},
              {id,leader_registry_sup},
              {mfargs,{leader_registry_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T21:09:35.344Z,ns_1@cb.local:<0.754.0>:restartable:start_child:92]Started child process <0.755.0>
  MFA: {leader_services_sup,start_link,[]}
[error_logger:info,2025-05-15T21:09:35.344Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.754.0>},
              {id,leader_services_sup},
              {mfargs,{restartable,start_link,
                                   [{leader_services_sup,start_link,[]},
                                    infinity]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T21:09:35.346Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.897.0>},
              {id,ns_tick_agent},
              {mfargs,{ns_tick_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.346Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.899.0>},
              {id,master_activity_events_ingress},
              {mfargs,{gen_event,start_link,
                                 [{local,master_activity_events_ingress}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.346Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.900.0>},
              {id,master_activity_events_timestamper},
              {mfargs,{master_activity_events,start_link_timestamper,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.347Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.901.0>},
              {id,master_activity_events_pids_watcher},
              {mfargs,{master_activity_events_pids_watcher,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.349Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.902.0>},
              {id,master_activity_events_keeper},
              {mfargs,{master_activity_events_keeper,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.351Z,ns_1@cb.local:health_monitor_sup<0.904.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.905.0>},
              {id,ns_server_monitor},
              {mfargs,{ns_server_monitor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.352Z,ns_1@cb.local:health_monitor_sup<0.904.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.907.0>},
              {id,service_monitor_children_sup},
              {mfargs,{supervisor,start_link,
                                  [{local,service_monitor_children_sup},
                                   health_monitor_sup,child]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T21:09:35.352Z,ns_1@cb.local:health_monitor_sup<0.904.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.908.0>},
              {id,service_monitor_worker},
              {mfargs,{erlang,apply,[#Fun<health_monitor_sup.0.30770475>,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.354Z,ns_1@cb.local:health_monitor_sup<0.904.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.914.0>},
              {id,node_monitor},
              {mfargs,{node_monitor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.356Z,ns_1@cb.local:health_monitor_sup<0.904.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.921.0>},
              {id,node_status_analyzer},
              {mfargs,{node_status_analyzer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.356Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.904.0>},
              {id,health_monitor_sup},
              {mfargs,{health_monitor_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T21:09:35.358Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.924.0>},
              {id,rebalance_agent},
              {mfargs,{rebalance_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.360Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.925.0>},
              {id,kv_hibernation_agent},
              {mfargs,{kv_hibernation_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.363Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.926.0>},
              {id,ns_rebalance_report_manager},
              {mfargs,{ns_rebalance_report_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T21:09:35.363Z,ns_1@cb.local:chronicle_kv_log<0.500.0>:chronicle_kv_log:log:59]update (key: tasks, rev: {<<"29b17497cea731943091527ebf7f7991">>,6})
[]
[ns_server:debug,2025-05-15T21:09:35.365Z,ns_1@cb.local:cb_creds_rotation<0.928.0>:cb_creds_rotation:start_rotate_timer:214]Starting creds rotation timer (1800000ms)
[error_logger:info,2025-05-15T21:09:35.366Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.928.0>},
              {id,creds_rotation},
              {mfargs,{cb_creds_rotation,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:error,2025-05-15T21:09:35.366Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {local,ns_server_sup}
    errorContext: child_terminated
    reason: {noproc,{gen_statem,call,[mb_master,master_node,infinity]}}
    offender: [{pid,<0.713.0>},
               {id,ns_server_stats},
               {mfargs,{ns_server_stats,start_link,[]}},
               {restart_type,permanent},
               {significant,false},
               {shutdown,1000},
               {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.366Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.492.0>},
              {id,ns_server_sup},
              {mfargs,{ns_server_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T21:09:35.366Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:one_shot_barrier:notify:21]Notifying on barrier menelaus_barrier
[ns_server:debug,2025-05-15T21:09:35.366Z,ns_1@cb.local:menelaus_barrier<0.298.0>:one_shot_barrier:barrier_body:56]Barrier menelaus_barrier got notification from <0.291.0>
[ns_server:debug,2025-05-15T21:09:35.366Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:one_shot_barrier:notify:26]Successfuly notified on barrier menelaus_barrier
[ns_server:debug,2025-05-15T21:09:35.366Z,ns_1@cb.local:<0.290.0>:restartable:start_child:92]Started child process <0.291.0>
  MFA: {ns_server_nodes_sup,start_link,[]}
[error_logger:info,2025-05-15T21:09:35.366Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.290.0>},
              {id,ns_server_nodes_sup},
              {mfargs,{restartable,start_link,
                                   [{ns_server_nodes_sup,start_link,[]},
                                    infinity]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T21:09:35.366Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.930.0>},
              {id,ns_server_stats},
              {mfargs,{ns_server_stats,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.368Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.934.0>},
              {id,remote_api},
              {mfargs,{remote_api,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T21:09:35.370Z,ns_1@cb.local:compiled_roles_cache<0.455.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@",admin}
[error_logger:info,2025-05-15T21:09:35.374Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.935.0>},
              {id,ns_gc_runner},
              {mfargs,{ns_gc_runner,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:09:35.374Z,ns_1@cb.local:root_sup<0.214.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,root_sup}
    started: [{pid,<0.235.0>},
              {id,ns_server_cluster_sup},
              {mfargs,{ns_server_cluster_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T21:09:35.374Z,ns_1@cb.local:application_controller<0.44.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    application: ns_server
    started_at: 'ns_1@cb.local'

[ns_server:debug,2025-05-15T21:09:35.374Z,ns_1@cb.local:<0.9.0>:child_erlang:child_loop:147]119: Entered child_loop
[ns_server:debug,2025-05-15T21:09:35.374Z,ns_1@cb.local:<0.9.0>:child_erlang:child_loop:147]119: Entered child_loop
[ns_server:debug,2025-05-15T21:09:35.379Z,ns_1@cb.local:json_rpc_connection-goxdcr-cbauth<0.938.0>:json_rpc_connection:init:71]Observed revrpc connection: label "goxdcr-cbauth", handling process <0.938.0>
[ns_server:debug,2025-05-15T21:09:35.379Z,ns_1@cb.local:json_rpc_connection-saslauthd-saslauthd-port<0.937.0>:json_rpc_connection:init:71]Observed revrpc connection: label "saslauthd-saslauthd-port", handling process <0.937.0>
[ns_server:debug,2025-05-15T21:09:35.379Z,ns_1@cb.local:menelaus_cbauth<0.666.0>:menelaus_cbauth:handle_cast:203]Observed json rpc process {"goxdcr-cbauth",[{internal,true}],<0.938.0>} started
[ns_server:debug,2025-05-15T21:09:35.384Z,ns_1@cb.local:compiled_roles_cache<0.455.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@goxdcr",admin}
[ns_server:debug,2025-05-15T21:09:35.385Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/query/sequences_cache/revision">>} ->
[{'_vclock',[{<<"4e0f9c01d66bba38da51cc6bd58d91b5">>,{1,63914562575}}]}|
 <<"0">>]
[ns_server:debug,2025-05-15T21:09:35.386Z,ns_1@cb.local:ns_config_rep<0.551.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([{metakv,<<"/query/sequences_cache/revision">>}]..)
[ns_server:debug,2025-05-15T21:09:35.765Z,ns_1@cb.local:compiled_roles_cache<0.455.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@ns_server",admin}
[ns_server:debug,2025-05-15T21:09:36.047Z,ns_1@cb.local:<0.962.0>:memcached_refresh:do_refresh:153]Successfully connected to memcached, Trying to refresh [rbac,isasl]
[ns_server:debug,2025-05-15T21:09:36.050Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:update_refresh_state:116]Refresh of [rbac,isasl] succeeded
[ns_server:debug,2025-05-15T21:09:36.270Z,ns_1@cb.local:<0.705.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":18,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true,\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[ns_server:info,2025-05-15T21:09:36.276Z,ns_1@cb.local:memcached_config_mgr<0.701.0>:memcached_config_mgr:push_tls_config:237]Successfully pushed TLS config to memcached
[ns_server:info,2025-05-15T21:09:36.277Z,ns_1@cb.local:memcached_config_mgr<0.701.0>:memcached_config_mgr:push_tls_config:233]Pushing TLS config to memcached:
{[{<<"private key">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem">>},
  {<<"certificate chain">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem">>},
  {<<"CA file">>,<<"/opt/couchbase/var/lib/couchbase/config/certs/ca.pem">>},
  {<<"minimum version">>,<<"TLS 1.2">>},
  {<<"cipher list">>,
   {[{<<"TLS 1.2">>,<<"HIGH">>},
     {<<"TLS 1.3">>,
      <<"TLS_AES_256_GCM_SHA384:TLS_AES_128_GCM_SHA256:TLS_CHACHA20_POLY1305_SHA256">>}]}},
  {<<"cipher order">>,true},
  {<<"client cert auth">>,<<"disabled">>}]}
[ns_server:info,2025-05-15T21:09:36.280Z,ns_1@cb.local:memcached_config_mgr<0.701.0>:memcached_config_mgr:push_tls_config:237]Successfully pushed TLS config to memcached
[ns_server:debug,2025-05-15T21:09:36.336Z,ns_1@cb.local:<0.879.0>:auto_failover_logic:log_master_activity:130]Transitioned node {'ns_1@cb.local',<<"4e0f9c01d66bba38da51cc6bd58d91b5">>} state new -> up
[ns_server:debug,2025-05-15T21:09:36.376Z,ns_1@cb.local:ns_gc_runner<0.935.0>:ns_gc_runner:handle_info:125]GC populating new pid list of size=591, prevMaxGcDuration=0 us
[ns_server:debug,2025-05-15T21:09:49.023Z,ns_1@cb.local:compiled_roles_cache<0.455.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@prometheus",stats_reader}
[ns_server:debug,2025-05-15T21:10:05.199Z,ns_1@cb.local:compaction_daemon<0.746.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2025-05-15T21:10:05.200Z,ns_1@cb.local:compaction_daemon<0.746.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T21:10:05.201Z,ns_1@cb.local:compaction_daemon<0.746.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2025-05-15T21:10:05.201Z,ns_1@cb.local:compaction_daemon<0.746.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T21:10:11.041Z,ns_1@cb.local:cb_saml<0.412.0>:cb_saml:refresh_metadata:526]Refreshing metadata
[ns_server:debug,2025-05-15T21:10:11.042Z,ns_1@cb.local:cb_saml<0.412.0>:cb_saml:restart_refresh_timer:583]Disabling refresh timer
[ns_server:debug,2025-05-15T21:10:35.204Z,ns_1@cb.local:compaction_daemon<0.746.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2025-05-15T21:10:35.204Z,ns_1@cb.local:compaction_daemon<0.746.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T21:10:35.205Z,ns_1@cb.local:compaction_daemon<0.746.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2025-05-15T21:10:35.205Z,ns_1@cb.local:compaction_daemon<0.746.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T21:10:47.998Z,ns_1@cb.local:ldap_auth_cache<0.408.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2025-05-15T21:11:04.332Z,ns_1@cb.local:<0.953.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.634.0>} exited with reason normal
[ns_server:debug,2025-05-15T21:11:04.332Z,ns_1@cb.local:<0.982.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.641.0>} exited with reason normal
[ns_server:debug,2025-05-15T21:11:04.332Z,ns_1@cb.local:json_rpc_connection-goxdcr-cbauth<0.938.0>:json_rpc_connection:handle_info:142]Socket closed
[ns_server:debug,2025-05-15T21:11:04.332Z,ns_1@cb.local:<0.980.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.640.0>} exited with reason normal
[ns_server:info,2025-05-15T21:11:04.332Z,ns_1@cb.local:menelaus_cbauth_worker-goxdcr-cbauth<0.945.0>:menelaus_cbauth_worker:handle_info:93]Observed json rpc process <0.938.0> died with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.332Z,ns_1@cb.local:<0.961.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.636.0>} exited with reason normal
[ns_server:debug,2025-05-15T21:11:04.332Z,ns_1@cb.local:<0.941.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.938.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.332Z,ns_1@cb.local:<0.958.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.638.0>} exited with reason normal
[ns_server:debug,2025-05-15T21:11:04.332Z,ns_1@cb.local:menelaus_cbauth<0.666.0>:menelaus_cbauth:handle_info:258]Observed worker process {worker,<0.945.0>,"goxdcr-cbauth",internal,
                                #Ref<0.730592461.1793589250.72818>,<0.938.0>} died with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.345Z,ns_1@cb.local:json_rpc_connection-saslauthd-saslauthd-port<0.937.0>:json_rpc_connection:handle_info:142]Socket closed
[ns_server:debug,2025-05-15T21:11:04.345Z,ns_1@cb.local:<0.942.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.937.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.478Z,ns_1@cb.local:<0.830.0>:remote_monitors:monitor_loop:122]Monitored remote process <16971.139.0> went down with: shutdown
[ns_server:debug,2025-05-15T21:11:04.478Z,ns_1@cb.local:<0.831.0>:remote_monitors:monitor_loop:122]Monitored remote process <16971.139.0> went down with: shutdown
[ns_server:debug,2025-05-15T21:11:04.478Z,ns_1@cb.local:<0.829.0>:remote_monitors:monitor_loop:122]Monitored remote process <16971.133.0> went down with: shutdown
[ns_server:debug,2025-05-15T21:11:04.478Z,ns_1@cb.local:<0.705.0>:terse_cluster_info_uploader:handle_info:64]Got DOWN with reason: shutdown from memcached port server: <16971.139.0>. Shutting down
[ns_server:debug,2025-05-15T21:11:04.478Z,ns_1@cb.local:ns_ports_setup<0.675.0>:ns_ports_setup:children_loop_continue:101]ns_child_ports_sup <16971.133.0> died on babysitter node with shutdown. Restart.
[ns_server:debug,2025-05-15T21:11:04.478Z,ns_1@cb.local:<0.706.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {bucket_info_cache_invalidations,<0.705.0>} exited with reason {shutdown,
                                                                                               {memcached_port_server_down,
                                                                                                <16971.139.0>,
                                                                                                shutdown}}
[ns_server:debug,2025-05-15T21:11:04.478Z,ns_1@cb.local:<0.9.0>:child_erlang:child_loop:151]119: Got EOL
[ns_server:info,2025-05-15T21:11:04.479Z,ns_1@cb.local:<0.9.0>:ns_bootstrap:stop:36]Initiated server shutdown
[ns_server:debug,2025-05-15T21:11:04.478Z,ns_1@cb.local:memcached_config_mgr<0.701.0>:memcached_config_mgr:handle_info:198]Got DOWN with reason: shutdown from memcached port server: <16971.139.0>. Shutting down
[ns_server:debug,2025-05-15T21:11:04.479Z,ns_1@cb.local:<0.832.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.701.0>} exited with reason {shutdown,
                                                                                {memcached_port_server_down,
                                                                                 <16971.139.0>,
                                                                                 shutdown}}
[error_logger:error,2025-05-15T21:11:04.479Z,ns_1@cb.local:<0.700.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {<0.700.0>,suppress_max_restart_intensity}
    errorContext: child_terminated
    reason: {shutdown,{memcached_port_server_down,<16971.139.0>,shutdown}}
    offender: [{pid,<0.701.0>},
               {id,memcached_config_mgr},
               {mfargs,{memcached_config_mgr,start_link,[]}},
               {restart_type,permanent},
               {significant,false},
               {shutdown,1000},
               {child_type,worker}]

[error_logger:error,2025-05-15T21:11:04.479Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {local,ns_server_sup}
    errorContext: child_terminated
    reason: {shutdown,{memcached_port_server_down,<16971.139.0>,shutdown}}
    offender: [{pid,<0.705.0>},
               {id,terse_cluster_info_uploader},
               {mfargs,{terse_cluster_info_uploader,start_link,[]}},
               {restart_type,permanent},
               {significant,false},
               {shutdown,1000},
               {child_type,worker}]

[ns_server:debug,2025-05-15T21:11:04.479Z,ns_1@cb.local:memcached_config_mgr<0.4023.0>:memcached_config_mgr:memcached_port_pid:154]waiting for completion of initial ns_ports_setup round
[error_logger:info,2025-05-15T21:11:04.479Z,ns_1@cb.local:<0.9.0>:ale_error_logger_handler:do_log:101]Initiated server shutdown
[ns_server:debug,2025-05-15T21:11:04.479Z,ns_1@cb.local:<0.4022.0>:memcached_config_mgr:memcached_port_pid:154]waiting for completion of initial ns_ports_setup round
[error_logger:info,2025-05-15T21:11:04.479Z,ns_1@cb.local:<0.700.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.700.0>,suppress_max_restart_intensity}
    started: [{pid,<0.4023.0>},
              {id,memcached_config_mgr},
              {mfargs,{memcached_config_mgr,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T21:11:04.479Z,ns_1@cb.local:ns_server_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.4022.0>},
              {id,terse_cluster_info_uploader},
              {mfargs,{terse_cluster_info_uploader,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T21:11:04.480Z,ns_1@cb.local:<0.936.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.935.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.480Z,ns_1@cb.local:<0.929.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.928.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.480Z,ns_1@cb.local:<0.927.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.926.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.480Z,ns_1@cb.local:<0.923.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.921.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.480Z,ns_1@cb.local:<0.915.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.914.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.480Z,ns_1@cb.local:<0.922.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.921.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.480Z,ns_1@cb.local:<0.916.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.914.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.480Z,ns_1@cb.local:<0.909.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.908.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.480Z,ns_1@cb.local:<0.906.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.905.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.480Z,ns_1@cb.local:<0.903.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {master_activity_events,<0.902.0>} exited with reason killed
[ns_server:debug,2025-05-15T21:11:04.480Z,ns_1@cb.local:<0.898.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {leader_events,<0.897.0>} exited with reason shutdown
[ns_server:info,2025-05-15T21:11:04.480Z,ns_1@cb.local:mb_master<0.767.0>:mb_master:terminate:247]Synchronously shutting down child mb_master_sup
[ns_server:debug,2025-05-15T21:11:04.480Z,ns_1@cb.local:<0.896.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.895.0>} exited with reason shutdown
[ns_server:info,2025-05-15T21:11:04.480Z,ns_1@cb.local:leader_registry<0.765.0>:leader_registry:handle_down:286]Process <0.895.0> registered as 'license_reporting' terminated.
[ns_server:debug,2025-05-15T21:11:04.480Z,ns_1@cb.local:<0.894.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.893.0>} exited with reason shutdown
[ns_server:info,2025-05-15T21:11:04.480Z,ns_1@cb.local:leader_registry<0.765.0>:leader_registry:handle_down:286]Process <0.891.0> registered as 'tombstone_purger' terminated.
[ns_server:debug,2025-05-15T21:11:04.480Z,ns_1@cb.local:<0.881.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.879.0>} exited with reason shutdown
[ns_server:info,2025-05-15T21:11:04.481Z,ns_1@cb.local:leader_registry<0.765.0>:leader_registry:handle_down:286]Process <0.879.0> registered as 'auto_failover' terminated.
[ns_server:debug,2025-05-15T21:11:04.480Z,ns_1@cb.local:<0.880.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {compat_mode_events,<0.879.0>} exited with reason shutdown
[ns_server:info,2025-05-15T21:11:04.481Z,ns_1@cb.local:leader_registry<0.765.0>:leader_registry:handle_down:286]Process <0.877.0> registered as 'ns_orchestrator' terminated.
[ns_server:debug,2025-05-15T21:11:04.481Z,ns_1@cb.local:<0.804.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {'kv-events',<0.803.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.481Z,ns_1@cb.local:<0.795.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.792.0>} exited with reason shutdown
[ns_server:info,2025-05-15T21:11:04.481Z,ns_1@cb.local:leader_registry<0.765.0>:leader_registry:handle_down:286]Process <0.876.0> registered as 'auto_rebalance' terminated.
[ns_server:debug,2025-05-15T21:11:04.481Z,ns_1@cb.local:leader_activities<0.757.0>:leader_activities:handle_internal_process_down:440]Process {quorum_nodes_manager,<0.792.0>} terminated with reason shutdown
[ns_server:info,2025-05-15T21:11:04.481Z,ns_1@cb.local:leader_registry<0.765.0>:leader_registry:handle_down:286]Process <0.875.0> registered as 'auto_reprovision' terminated.
[ns_server:info,2025-05-15T21:11:04.481Z,ns_1@cb.local:leader_registry<0.765.0>:leader_registry:handle_down:286]Process <0.803.0> registered as 'chronicle_master' terminated.
[ns_server:debug,2025-05-15T21:11:04.481Z,ns_1@cb.local:leader_activities<0.757.0>:leader_activities:handle_internal_process_down:440]Process {acquirer,<0.788.0>} terminated with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.481Z,ns_1@cb.local:leader_lease_agent<0.762.0>:leader_lease_agent:handle_abolish_lease:246]Received abolish lease request from {lease_holder,
                                     <<"64babe628cd2c80b0066759dc260e4f3">>,
                                     'ns_1@cb.local'} when lease is {lease,
                                                                     {lease_holder,
                                                                      <<"64babe628cd2c80b0066759dc260e4f3">>,
                                                                      'ns_1@cb.local'},
                                                                     -576460658067140290,
                                                                     -576460643067140290,
                                                                     {timer,
                                                                      #Ref<0.730592461.1793589251.77482>,
                                                                      {lease_expired,
                                                                       {lease_holder,
                                                                        <<"64babe628cd2c80b0066759dc260e4f3">>,
                                                                        'ns_1@cb.local'}}},
                                                                     active}
[ns_server:debug,2025-05-15T21:11:04.481Z,ns_1@cb.local:<0.789.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_node_disco_events,<0.788.0>} exited with reason shutdown
[ns_server:info,2025-05-15T21:11:04.481Z,ns_1@cb.local:leader_registry<0.765.0>:leader_registry:handle_down:286]Process <0.800.0> registered as 'ns_tick' terminated.
[ns_server:debug,2025-05-15T21:11:04.481Z,ns_1@cb.local:<0.768.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.767.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.481Z,ns_1@cb.local:<0.766.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {leader_events,<0.765.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.481Z,ns_1@cb.local:leader_lease_agent<0.762.0>:leader_lease_agent:handle_abolish_lease:251]Expiring abolished lease
[ns_server:warn,2025-05-15T21:11:04.481Z,ns_1@cb.local:leader_lease_agent<0.762.0>:leader_lease_agent:handle_terminate:308]Terminating with reason shutdown when lease is expiring:
{lease,
    {lease_holder,<<"64babe628cd2c80b0066759dc260e4f3">>,'ns_1@cb.local'},
    -576460658067140290,-576460643067140290,
    {timer,undefined,
        {lease_expired,
            {lease_holder,<<"64babe628cd2c80b0066759dc260e4f3">>,
                'ns_1@cb.local'}}},
    expiring}
Removing the persisted lease.
[ns_server:debug,2025-05-15T21:11:04.484Z,ns_1@cb.local:leader_activities<0.757.0>:leader_activities:handle_internal_process_down:440]Process {agent,<0.762.0>} terminated with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.484Z,ns_1@cb.local:<0.754.0>:restartable:shutdown_child:114]Successfully terminated process <0.755.0>
[ns_server:debug,2025-05-15T21:11:04.485Z,ns_1@cb.local:<0.752.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.751.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.485Z,ns_1@cb.local:<0.747.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.746.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.485Z,ns_1@cb.local:<0.742.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.741.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.485Z,ns_1@cb.local:<0.740.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_node_disco_events,<0.738.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.485Z,ns_1@cb.local:<0.739.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.738.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.485Z,ns_1@cb.local:<0.737.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_node_disco_events,<0.735.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.485Z,ns_1@cb.local:<0.732.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.731.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.485Z,ns_1@cb.local:<0.733.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_node_disco_events,<0.731.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.485Z,ns_1@cb.local:<0.736.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.735.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.485Z,ns_1@cb.local:<0.724.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_tick_event,<0.723.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.485Z,ns_1@cb.local:<0.722.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_tick_event,<0.721.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.485Z,ns_1@cb.local:<0.720.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_tick_event,<0.719.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.485Z,ns_1@cb.local:<0.718.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_tick_event,<0.717.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.485Z,ns_1@cb.local:<0.932.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ale_stats_events,<0.930.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.485Z,ns_1@cb.local:<0.710.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.709.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.485Z,ns_1@cb.local:<0.4024.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {bucket_info_cache_invalidations,<0.4022.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.486Z,ns_1@cb.local:<0.689.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.688.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.486Z,ns_1@cb.local:<0.684.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.683.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.486Z,ns_1@cb.local:<0.681.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.680.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.486Z,ns_1@cb.local:<0.676.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.675.0>} exited with reason killed
[ns_server:debug,2025-05-15T21:11:04.486Z,ns_1@cb.local:<0.677.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {user_storage_events,<0.675.0>} exited with reason killed
[ns_server:info,2025-05-15T21:11:04.486Z,ns_1@cb.local:menelaus_cbauth<0.666.0>:menelaus_cbauth:terminate_external_connections:159]External connections to be terminated: []
[ns_server:debug,2025-05-15T21:11:04.486Z,ns_1@cb.local:<0.668.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_node_disco_events,<0.666.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.486Z,ns_1@cb.local:<0.670.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {user_storage_events,<0.666.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.486Z,ns_1@cb.local:<0.667.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {json_rpc_events,<0.666.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.486Z,ns_1@cb.local:<0.669.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.666.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.486Z,ns_1@cb.local:<0.671.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ssl_service_events,<0.666.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.486Z,ns_1@cb.local:<0.664.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.663.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.487Z,ns_1@cb.local:<0.625.0>:restartable:shutdown_child:114]Successfully terminated process <0.626.0>
[ns_server:debug,2025-05-15T21:11:04.487Z,ns_1@cb.local:<0.616.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.615.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.487Z,ns_1@cb.local:<0.605.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.604.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.487Z,ns_1@cb.local:<0.603.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.602.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.487Z,ns_1@cb.local:<0.601.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.600.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.487Z,ns_1@cb.local:<0.592.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.591.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.487Z,ns_1@cb.local:<0.599.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.598.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.487Z,ns_1@cb.local:<0.586.0>:restartable:shutdown_child:114]Successfully terminated process <0.587.0>
[ns_server:debug,2025-05-15T21:11:04.488Z,ns_1@cb.local:<0.584.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {buckets_events,<0.583.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.510Z,ns_1@cb.local:chronicle_kv_log<0.500.0>:chronicle_kv_log:log:59]update (key: tasks, rev: {<<"29b17497cea731943091527ebf7f7991">>,13})
[]
[ns_server:debug,2025-05-15T21:11:04.510Z,ns_1@cb.local:<0.577.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.575.0>} exited with reason killed
[ns_server:debug,2025-05-15T21:11:04.510Z,ns_1@cb.local:<0.563.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.562.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.510Z,ns_1@cb.local:<0.572.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.571.0>} exited with reason killed
[ns_server:debug,2025-05-15T21:11:04.510Z,ns_1@cb.local:<0.570.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.569.0>} exited with reason killed
[ns_server:debug,2025-05-15T21:11:04.510Z,ns_1@cb.local:<0.555.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_node_disco_events,<0.551.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.510Z,ns_1@cb.local:<0.554.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.551.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.510Z,ns_1@cb.local:<0.552.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events_local,<0.551.0>} exited with reason shutdown
[error_logger:error,2025-05-15T21:11:04.510Z,ns_1@cb.local:bucket_info_cache_invalidations<0.576.0>:ale_error_logger_handler:do_log:101]
=========================CRASH REPORT=========================
  crasher:
    initial call: gen_event:init_it/6
    pid: <0.576.0>
    registered_name: bucket_info_cache_invalidations
    exception exit: killed
      in function  gen_event:terminate_server/4 (gen_event.erl, line 580)
    ancestors: [bucket_info_cache,ns_server_sup,ns_server_nodes_sup,
                  <0.290.0>,ns_server_cluster_sup,root_sup,<0.154.0>]
    message_queue_len: 0
    messages: []
    links: []
    dictionary: []
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 28
    reductions: 601
  neighbours:

[ns_server:debug,2025-05-15T21:11:04.510Z,ns_1@cb.local:<0.530.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.529.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.510Z,ns_1@cb.local:<0.531.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {user_storage_events,<0.529.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.510Z,ns_1@cb.local:<0.542.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.541.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.510Z,ns_1@cb.local:<0.528.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {user_storage_events,<0.526.0>} exited with reason shutdown
[ns_server:info,2025-05-15T21:11:04.510Z,ns_1@cb.local:prometheus_cfg<0.513.0>:prometheus_cfg:terminate:609]Terminate: shutdown
[ns_server:debug,2025-05-15T21:11:04.510Z,ns_1@cb.local:prometheus_cfg<0.513.0>:prometheus_cfg:terminate_prometheus:773]Terminating Prometheus gracefully
[ns_server:debug,2025-05-15T21:11:04.510Z,ns_1@cb.local:<0.527.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.526.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.514Z,ns_1@cb.local:prometheus-goport<0.525.0>:goport:handle_eof:592]Stream 'stdout' closed
[ns_server:debug,2025-05-15T21:11:04.515Z,ns_1@cb.local:prometheus-goport<0.525.0>:goport:handle_eof:592]Stream 'stderr' closed
[ns_server:info,2025-05-15T21:11:04.515Z,ns_1@cb.local:prometheus-goport<0.525.0>:goport:handle_process_exit:573]Port exited with status 0.
[ns_server:info,2025-05-15T21:11:04.515Z,ns_1@cb.local:<0.519.0>:ns_port_server:handle_info:149]Got {exit_status,0} from port prometheus. Exiting normally
[ns_server:debug,2025-05-15T21:11:04.515Z,ns_1@cb.local:prometheus_cfg<0.513.0>:prometheus_cfg:terminate_prometheus:794]Prometheus port server stopped successfully
[ns_server:debug,2025-05-15T21:11:04.515Z,ns_1@cb.local:<0.514.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.513.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.515Z,ns_1@cb.local:<0.507.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.506.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.517Z,ns_1@cb.local:<0.505.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.504.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.519Z,ns_1@cb.local:<0.501.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_kv_event_manager,<0.500.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.519Z,ns_1@cb.local:<0.491.0>:remote_monitors:handle_down:151]Caller of remote monitor <0.472.0> died with shutdown. Exiting
[ns_server:debug,2025-05-15T21:11:04.519Z,ns_1@cb.local:ns_couchdb_port<0.471.0>:ns_port_server:terminate:198]Shutting down port ns_couchdb
[ns_server:debug,2025-05-15T21:11:04.519Z,ns_1@cb.local:ns_couchdb_port<0.471.0>:ns_port_server:port_shutdown:348]Shutdown command: "shutdown"
[ns_server:debug,2025-05-15T21:11:04.526Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.730592461.1793589250.72602>,
                               inet_tcp_dist,<0.488.0>,
                               #Ref<0.730592461.1793589250.72605>}
[error_logger:info,2025-05-15T21:11:04.527Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',#Port<0.30>,normal}}
[error_logger:info,2025-05-15T21:11:04.527Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.488.0>,connection_closed}}
[error_logger:info,2025-05-15T21:11:04.527Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {8283917,#Ref<0.730592461.1793720324.72518>}}}
[ns_server:debug,2025-05-15T21:11:04.527Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T21:11:04.527Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.730592461.1793589251.77571>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T21:11:04.527Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.730592461.1793589251.77571>,
                                  inet_tcp_dist,<0.4036.0>,
                                  #Ref<0.730592461.1793589251.77574>}
[ns_server:info,2025-05-15T21:11:04.527Z,ns_1@cb.local:ns_couchdb_port<0.471.0>:ns_port_server:handle_info:149]Got {exit_status,0} from port ns_couchdb. Exiting normally
[ns_server:debug,2025-05-15T21:11:04.527Z,ns_1@cb.local:ns_couchdb_port<0.471.0>:ns_port_server:terminate:201]ns_couchdb has exited
[ns_server:info,2025-05-15T21:11:04.527Z,ns_1@cb.local:ns_couchdb_port<0.471.0>:ns_port_server:log:226]ns_couchdb<0.471.0>: 217: got shutdown request. Exiting
ns_couchdb<0.471.0>: [os_mon] cpu supervisor port (cpu_sup): Erlang has closed
ns_couchdb<0.471.0>: [os_mon] memory supervisor port (memsup): Erlang has closed

[ns_server:debug,2025-05-15T21:11:04.528Z,ns_1@cb.local:<0.459.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.458.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.528Z,ns_1@cb.local:<0.456.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {user_storage_events,<0.455.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.528Z,ns_1@cb.local:<0.461.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.458.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.528Z,ns_1@cb.local:<0.445.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.444.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.528Z,ns_1@cb.local:<0.460.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {user_storage_events,<0.458.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.528Z,ns_1@cb.local:<0.457.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.455.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.528Z,ns_1@cb.local:<0.413.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.412.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.528Z,ns_1@cb.local:<0.409.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.408.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.528Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.730592461.1793589251.77571>,
                               inet_tcp_dist,<0.4036.0>,
                               #Ref<0.730592461.1793589251.77574>}
[error_logger:info,2025-05-15T21:11:04.529Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.4036.0>,shutdown}}
[error_logger:info,2025-05-15T21:11:04.529Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T21:11:04.529Z,ns_1@cb.local:<0.328.0>:restartable:shutdown_child:114]Successfully terminated process <0.388.0>
[ns_server:debug,2025-05-15T21:11:04.529Z,ns_1@cb.local:<0.309.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.308.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.529Z,ns_1@cb.local:<0.296.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {'kv-events',<0.292.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.529Z,ns_1@cb.local:<0.295.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.292.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.529Z,ns_1@cb.local:<0.290.0>:restartable:shutdown_child:114]Successfully terminated process <0.291.0>
[ns_server:debug,2025-05-15T21:11:04.529Z,ns_1@cb.local:<0.285.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.284.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.529Z,ns_1@cb.local:ns_config<0.280.0>:ns_config:terminate:859]Config is terminating with reason shutdown
[ns_server:debug,2025-05-15T21:11:04.529Z,ns_1@cb.local:ns_config<0.280.0>:ns_config:wait_saver:844]Done waiting for saver.
[ns_server:debug,2025-05-15T21:11:04.530Z,ns_1@cb.local:<0.274.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_external_events,<0.273.0>} exited with reason shutdown
[error_logger:info,2025-05-15T21:11:04.532Z,ns_1@cb.local:application_controller<0.44.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
    application: ns_server
    exited: stopped
    type: permanent

[ns_server:info,2025-05-15T21:11:04.532Z,ns_1@cb.local:<0.9.0>:ns_bootstrap:stop:40]Successfully stopped ns_server
