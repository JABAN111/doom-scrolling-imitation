[ns_server:info,2025-05-15T18:46:45.319Z,nonode@nohost:<0.154.0>:ns_server:init_logging:180]Started & configured logging
[ns_server:warn,2025-05-15T18:46:45.364Z,nonode@nohost:<0.154.0>:config_profile:load:123]Could not load profile file ("/etc/couchbase.d/config_profile") because it does not exist
[ns_server:debug,2025-05-15T18:46:45.367Z,nonode@nohost:<0.154.0>:ns_server:setup_server_profile:108]Using profile 'default': [{name,"default"},
                          {{indexer,disable_shard_affinity},true}]
[ns_server:warn,2025-05-15T18:46:45.370Z,nonode@nohost:<0.154.0>:ns_server:config_profile_continuity_checker:129]Writing config_profile '"default"' to disk.
[ns_server:info,2025-05-15T18:46:45.377Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]Static config terms:
[{error_logger_mf_dir,"/opt/couchbase/var/lib/couchbase/logs"},
 {path_config_bindir,"/opt/couchbase/bin"},
 {path_config_etcdir,"/opt/couchbase/etc/couchbase"},
 {path_config_libdir,"/opt/couchbase/lib"},
 {path_config_datadir,"/opt/couchbase/var/lib/couchbase"},
 {path_config_tmpdir,"/opt/couchbase/var/lib/couchbase/tmp"},
 {path_config_secdir,"/opt/couchbase/etc/security"},
 {nodefile,"/opt/couchbase/var/lib/couchbase/couchbase-server.node"},
 {loglevel_default,debug},
 {loglevel_couchdb,info},
 {loglevel_ns_server,debug},
 {loglevel_error_logger,debug},
 {loglevel_user,debug},
 {loglevel_menelaus,debug},
 {loglevel_ns_doctor,debug},
 {loglevel_stats,debug},
 {loglevel_rebalance,debug},
 {loglevel_cluster,debug},
 {loglevel_views,debug},
 {loglevel_mapreduce_errors,debug},
 {loglevel_xdcr,debug},
 {loglevel_access,info},
 {loglevel_cbas,debug},
 {disk_sink_opts,
     [{rotation,
          [{compress,true},
           {size,41943040},
           {num_files,10},
           {buffer_size_max,52428800}]}]},
 {disk_sink_opts_disk_json_rpc,
     [{rotation,
          [{compress,true},
           {size,41943040},
           {num_files,2},
           {buffer_size_max,52428800}]}]},
 {disk_sink_opts_disk_tls_key_log,
     [{rotation,
          [{compress,true},
           {size,10485760},
           {num_files,1},
           {buffer_size_max,13107200}]}]},
 {net_kernel_verbosity,10}]
[ns_server:warn,2025-05-15T18:46:45.378Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter error_logger_mf_dir, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.378Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter path_config_bindir, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.378Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter path_config_etcdir, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.378Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter path_config_libdir, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.378Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter path_config_datadir, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.378Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter path_config_tmpdir, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.378Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter path_config_secdir, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.378Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter nodefile, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.378Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_default, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.378Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_couchdb, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.379Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_ns_server, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.379Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_error_logger, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.379Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_user, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.379Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_menelaus, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.380Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_ns_doctor, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.380Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_stats, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.380Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_rebalance, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.380Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_cluster, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.380Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_views, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.380Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_mapreduce_errors, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.380Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_xdcr, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.380Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_access, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.380Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_cbas, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.380Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter disk_sink_opts, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.380Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter disk_sink_opts_disk_json_rpc, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.380Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter disk_sink_opts_disk_tls_key_log, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.380Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter net_kernel_verbosity, which is given from command line
[ns_server:info,2025-05-15T18:46:45.407Z,nonode@nohost:dist_manager<0.215.0>:dist_manager:read_address_config_from_path:83]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip_start"
[ns_server:info,2025-05-15T18:46:45.412Z,nonode@nohost:dist_manager<0.215.0>:dist_manager:read_address_config_from_path:83]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2025-05-15T18:46:45.412Z,nonode@nohost:dist_manager<0.215.0>:dist_manager:init:180]ip config not found. Looks like we're brand new node
[ns_server:info,2025-05-15T18:46:45.423Z,nonode@nohost:dist_manager<0.215.0>:dist_manager:bringup:246]Attempting to bring up net_kernel with name 'ns_1@cb.local'
[error_logger:info,2025-05-15T18:46:45.457Z,nonode@nohost:ssl_dist_admin_sup<0.218.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_admin_sup}
    started: [{pid,<0.219.0>},
              {id,ssl_pem_cache_dist},
              {mfargs,{ssl_pem_cache,start_link_dist,[[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.458Z,nonode@nohost:ssl_dist_admin_sup<0.218.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_admin_sup}
    started: [{pid,<0.220.0>},
              {id,ssl_dist_manager},
              {mfargs,{ssl_manager,start_link_dist,[[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.458Z,nonode@nohost:ssl_dist_sup<0.217.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_sup}
    started: [{pid,<0.218.0>},
              {id,ssl_dist_admin_sup},
              {mfargs,{ssl_dist_admin_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:45.467Z,nonode@nohost:tls_dist_sup<0.221.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_sup}
    started: [{pid,<0.222.0>},
              {id,dist_tls_connection_sup},
              {mfargs,{tls_connection_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:45.481Z,nonode@nohost:tls_dist_server_sup<0.223.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_server_sup}
    started: [{pid,<0.224.0>},
              {id,dist_ssl_listen_tracker_sup},
              {mfargs,{ssl_listen_tracker_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:45.481Z,nonode@nohost:tls_dist_server_sup<0.223.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_server_sup}
    started: [{pid,<0.225.0>},
              {id,dist_tls_server_session_ticket},
              {mfargs,{tls_server_session_ticket_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:45.481Z,nonode@nohost:tls_dist_server_sup<0.223.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_server_sup}
    started: [{pid,<0.226.0>},
              {id,dist_ssl_upgrade_server_session_cache_sup},
              {mfargs,
                  {ssl_upgrade_server_session_cache_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:45.482Z,nonode@nohost:tls_dist_sup<0.221.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_sup}
    started: [{pid,<0.223.0>},
              {id,tls_dist_server_sup},
              {mfargs,{tls_dist_server_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:45.482Z,nonode@nohost:ssl_dist_sup<0.217.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_sup}
    started: [{pid,<0.221.0>},
              {id,tls_dist_sup},
              {mfargs,{tls_dist_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:45.482Z,nonode@nohost:net_sup<0.216.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.217.0>},
              {id,ssl_dist_sup},
              {mfargs,{ssl_dist_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:45.482Z,nonode@nohost:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Starting cb_dist with config [{config_vsn,2}]
[error_logger:info,2025-05-15T18:46:45.483Z,nonode@nohost:net_sup<0.216.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.227.0>},
              {id,cb_dist},
              {mfargs,{cb_dist,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.486Z,nonode@nohost:net_sup<0.216.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.228.0>},
              {id,auth},
              {mfargs,{auth,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,2000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:45.492Z,nonode@nohost:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Initial protos: [{external,inet_tcp_dist}], required protos: [{external,
                                                                        inet_tcp_dist}]
[ns_server:debug,2025-05-15T18:46:45.492Z,nonode@nohost:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Starting {external,inet_tcp_dist} listener on 21100...
[ns_server:debug,2025-05-15T18:46:45.497Z,nonode@nohost:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Started listener: inet_tcp_dist
[ns_server:debug,2025-05-15T18:46:45.551Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated server ssl_dist_opts (keep_secrets) - false
[ns_server:debug,2025-05-15T18:46:45.551Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Started acceptor inet_tcp_dist: <0.231.0>
[ns_server:debug,2025-05-15T18:46:45.551Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Ensure config is going to change listeners. Will be stopped: [], will be started: [], will be restarted: []
[error_logger:info,2025-05-15T18:46:45.551Z,ns_1@cb.local:net_sup<0.216.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.229.0>},
              {id,net_kernel},
              {mfargs,{net_kernel,start_link,
                                  [#{clean_halt => false,
                                     name => 'ns_1@cb.local',
                                     name_domain => longnames,
                                     net_tickintensity => 4,
                                     net_ticktime => 60,
                                     supervisor => net_sup_dynamic}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,2000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.552Z,ns_1@cb.local:kernel_sup<0.49.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,kernel_sup}
    started: [{pid,<0.216.0>},
              {id,net_sup_dynamic},
              {mfargs,
                  {erl_distribution,start_link,
                      [#{clean_halt => false,name => 'ns_1@cb.local',
                         name_domain => longnames,net_tickintensity => 4,
                         net_ticktime => 60,supervisor => net_sup_dynamic}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,60000},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:45.553Z,ns_1@cb.local:dist_manager<0.215.0>:dist_manager:configure_net_kernel:302]Set net_kernel vebosity to 10 -> 0
[ns_server:info,2025-05-15T18:46:45.569Z,ns_1@cb.local:dist_manager<0.215.0>:dist_manager:save_node:160]saving node name '"ns_1@cb.local"' to "/opt/couchbase/var/lib/couchbase/couchbase-server.node"
[ns_server:debug,2025-05-15T18:46:45.578Z,ns_1@cb.local:dist_manager<0.215.0>:dist_manager:bringup:269]Attempted to save node name to disk: ok
[ns_server:debug,2025-05-15T18:46:45.579Z,ns_1@cb.local:dist_manager<0.215.0>:dist_manager:wait_for_node:276]Waiting for connection to node 'babysitter_of_ns_1@cb.local' to be established
[error_logger:info,2025-05-15T18:46:45.579Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{connect,normal,'babysitter_of_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:46:45.583Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'babysitter_of_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:46:45.583Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3529285563.3650617345.3721>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:46:45.583Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3529285563.3650617345.3721>,
                                  inet_tcp_dist,<0.233.0>,
                                  #Ref<0.3529285563.3650617345.3724>}
[ns_server:debug,2025-05-15T18:46:45.597Z,ns_1@cb.local:dist_manager<0.215.0>:dist_manager:wait_for_node:288]Observed node 'babysitter_of_ns_1@cb.local' to come up
[ns_server:info,2025-05-15T18:46:45.597Z,ns_1@cb.local:dist_manager<0.215.0>:dist_manager:save_address_config:147]Deleting irrelevant ip file "/opt/couchbase/var/lib/couchbase/ip_start": {error,
                                                                          enoent}
[ns_server:info,2025-05-15T18:46:45.597Z,ns_1@cb.local:dist_manager<0.215.0>:dist_manager:save_address_config:148]saving ip config to "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2025-05-15T18:46:45.601Z,ns_1@cb.local:dist_manager<0.215.0>:dist_manager:save_address_config:151]Persisted the address successfully
[error_logger:info,2025-05-15T18:46:45.606Z,ns_1@cb.local:root_sup<0.214.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,root_sup}
    started: [{pid,<0.215.0>},
              {id,dist_manager},
              {mfargs,{dist_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.609Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.236.0>},
              {id,local_tasks},
              {mfargs,{local_tasks,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:45.614Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:log_os_info:start_link:19]OS type: {unix,linux} Version: {6,10,14}
Runtime info: [{otp_release,"25"},
               {erl_version,"13.2.2.3"},
               {erl_version_long,
                   "Erlang/OTP 25 [erts-13.2.2.3] [source-15104f9619] [64-bit] [smp:10:10] [ds:10:10:10] [async-threads:16] [jit]\n"},
               {system_arch_raw,"aarch64-unknown-linux-gnu"},
               {system_arch,"aarch64-unknown-linux-gnu"},
               {localtime,{{2025,5,15},{18,46,45}}},
               {memory,
                   [{total,44134408},
                    {processes,9978312},
                    {processes_used,9970432},
                    {system,34156096},
                    {atom,540873},
                    {atom_used,522143},
                    {binary,130136},
                    {code,9676246},
                    {ets,2639568}]},
               {loaded,
                   [ns_info,log_os_info,local_tasks,restartable,
                    ns_server_cluster_sup,ns_cluster,dist_util,ns_node_disco,
                    crypto,inet_tcp,re,auth,tls_dist_server_sup,tls_dist_sup,
                    ssl_dist_admin_sup,ssl_dist_sup,inet_tls_dist,
                    inet_tcp_dist,cb_epmd,gen_udp,inet_hosts,dist_manager,
                    root_sup,cb_dist,path_config,config_profile,
                    ns_server_stats,calendar,ale_default_formatter,
                    'ale_logger-tls_key','ale_logger-metakv',
                    'ale_logger-rebalance','ale_logger-chronicle',
                    'ale_logger-ns_server_trace','ale_logger-menelaus',
                    'ale_logger-stats','ale_logger-json_rpc',
                    'ale_logger-access','ale_logger-ns_server',
                    'ale_logger-user','ale_logger-ns_doctor',
                    'ale_logger-cluster','ale_logger-xdcr',erl_bits,
                    otp_internal,cb_log_counter_sink,ns_log_sink,
                    ale_disk_sink,misc,couch_util,ns_server,esaml_util,esaml,
                    filelib,cpu_sup,ale_error_logger_handler,timer,memsup,
                    disksup,os_mon,unicode_util,string,io,release_handler,
                    alarm_handler,sasl,httpd_sup,httpc_handler_sup,
                    httpc_cookie,inets_trace,httpc_manager,httpc,
                    httpc_profile_sup,httpc_sup,inets_sup,inets_app,ssl,
                    lhttpc_manager,lhttpc_sup,lhttpc,
                    dtls_server_session_cache_sup,dtls_listener_sup,
                    dtls_server_sup,dtls_connection_sup,dtls_sup,
                    ssl_upgrade_server_session_cache_sup,
                    ssl_server_session_cache_sup,
                    tls_server_session_ticket_sup,ssl_listen_tracker_sup,
                    tls_server_sup,tls_connection_sup,tls_sup,
                    ssl_connection_sup,tls_client_ticket_store,
                    ssl_client_session_cache_db,ssl_config,ssl_manager,
                    ssl_pkix_db,ssl_pem_cache,ssl_admin_sup,ssl_sup,
                    logger_h_common,logger_std_h,ssl_logger,ssl_app,
                    'ale_logger-trace_logger','ale_logger-ale_logger',
                    'ale_logger-error_logger',beam_opcodes,beam_dict,beam_asm,
                    beam_z,beam_flatten,beam_trim,beam_clean,beam_block,
                    beam_utils,beam_jump,beam_a,beam_validator,
                    beam_ssa_codegen,beam_ssa_pre_codegen,beam_ssa_throw,
                    beam_ssa_dead,beam_call_types,beam_types,beam_ssa_type,
                    beam_ssa_bc_size,beam_ssa_opt,beam_ssa_bsm,beam_ssa_recv,
                    beam_ssa_share,beam_ssa_bool,beam_ssa,beam_kernel_to_ssa,
                    v3_kernel,sys_core_bsm,sys_core_alias,erl_bifs,
                    cerl_clauses,sys_core_fold,sys_core_inline,cerl_trees,
                    core_lib,cerl,sets,v3_core,erl_expand_records,sofs,
                    erl_internal,ordsets,compile,dynamic_compile,ale_utils,
                    io_lib_pretty,io_lib_format,io_lib,ale_codegen,dict,ale,
                    ale_dynamic_sup,ale_sup,ale_app,ns_bootstrap,child_erlang,
                    raw_file_io,orddict,c,erl_signal_handler,
                    logger_handler_watcher,logger_sup,kernel_refc,
                    kernel_config,user_io,user_sup,supervisor_bridge,
                    standard_error,erpc,global_group,erl_distribution,maps,
                    rand,net_kernel,global,rpc,epp,inet_parse,inet,inet_udp,
                    inet_config,inet_db,unicode,os,gb_trees,gb_sets,binary,
                    beam_lib,peer,erl_anno,erl_features,proplists,erl_scan,
                    queue,logger_olp,logger_proxy,application,error_handler,
                    code,file_server,file_io_server,code_server,error_logger,
                    kernel,application_master,erl_eval,heart,
                    application_controller,logger,logger_backend,
                    logger_filters,logger_simple_h,logger_config,file,gen,
                    logger_server,proc_lib,gen_event,filename,lists,
                    gen_server,supervisor,ets,erl_parse,erl_lint,
                    persistent_term,counters,atomics,
                    erts_dirty_process_signal_handler,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,prim_net,prim_socket,
                    socket_registry,zlib,prim_file,prim_inet,prim_eval,
                    prim_buffer,init,erl_init,erts_code_purger]},
               {applications,
                   [{stdlib,"ERTS  CXC 138 10","4.3.1.2"},
                    {sasl,"SASL  CXC 138 11","4.2"},
                    {public_key,"Public key infrastructure","1.13.3.1"},
                    {crypto,"CRYPTO","5.1.4.1"},
                    {lhttpc,"Lightweight HTTP Client","1.3.0"},
                    {asn1,"The Erlang ASN1 compiler version 5.0.21","5.0.21"},
                    {ssl,"Erlang/OTP SSL application","10.9.1.2"},
                    {esaml,"SAML Server Provider library for erlang","4.4.0"},
                    {ale,"Another Logger for Erlang","0.0.0"},
                    {xmerl,"XML parser","1.3.31.1"},
                    {inets,"INETS  CXC 138 49","8.3.1.2"},
                    {os_mon,"CPO  CXC 138 46","2.8.2"},
                    {ns_server,"Couchbase server","7.6.2-3721-enterprise"},
                    {kernel,"ERTS  CXC 138 10","8.5.4.2"}]},
               {pre_loaded,
                   [persistent_term,counters,atomics,
                    erts_dirty_process_signal_handler,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,prim_net,prim_socket,
                    socket_registry,zlib,prim_file,prim_inet,prim_eval,
                    prim_buffer,init,erl_init,erts_code_purger]},
               {process_count,159},
               {node,'ns_1@cb.local'},
               {nodes,[]},
               {registered,
                   [ssl_connection_sup,standard_error,net_kernel,
                    ssl_upgrade_server_session_cache_sup,kernel_refc,
                    'sink-disk_debug',ale_sup,httpd_sup,
                    ssl_upgrade_server_session_cache_sup_dist,
                    tls_client_ticket_store,dtls_server_session_cache_sup,
                    'sink-disk_trace',inets_sup,tls_sup,erts_code_purger,
                    'sink-disk_tls_key_log',dist_manager,ssl_pem_cache_dist,
                    cb_dist,ale,erl_prim_loader,global_group,cpu_sup,init,
                    application_controller,ns_server_cluster_sup,
                    httpc_manager,erl_signal_server,ssl_dist_sup,
                    tls_dist_connection_sup,ssl_sup,esaml,logger_sup,
                    'sink-disk_error',tls_server_session_ticket_sup_dist,
                    ssl_listen_tracker_sup,kernel_safe_sup,os_mon_sup,
                    'sink-disk_reports',dtls_sup,rex,tls_dist_sup,
                    tls_server_session_ticket_sup,'sink-disk_metakv',disksup,
                    ssl_manager,'sink-ns_log',logger_proxy,
                    'sink-disk_default',ale_dynamic_sup,
                    ssl_listen_tracker_sup_dist,auth,logger_std_h_ssl_handler,
                    standard_error_sup,'sink-disk_stats',httpc_profile_sup,
                    kernel_sup,ssl_admin_sup,inet_db,user,sasl_safe_sup,
                    release_handler,'sink-disk_access_int',
                    ssl_server_session_cache_sup,root_sup,logger,
                    alarm_handler,lhttpc_manager,dtls_server_sup,
                    logger_handler_watcher,httpc_sup,tls_dist_server_sup,
                    global_group_check,'sink-disk_json_rpc',local_tasks,
                    memsup,ssl_pem_cache,'sink-disk_xdcr',dtls_connection_sup,
                    global_name_server,file_server_2,httpc_handler_sup,
                    sasl_sup,tls_connection_sup,ssl_manager_dist,lhttpc_sup,
                    'sink-disk_access',dtls_listener_sup,ale_stats_events,
                    socket_registry,code_server,net_sup,esaml_ets_table_owner,
                    'sink-cb_log_counter',tls_server_sup,ssl_dist_admin_sup]},
               {cookie,nocookie},
               {wordsize,8},
               {wall_clock,5}]
[ns_server:info,2025-05-15T18:46:45.619Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:log_os_info:start_link:21]Manifest:
["<?xml version=\"1.0\" encoding=\"UTF-8\"?>","<manifest>",
 "  <remote name=\"blevesearch\" fetch=\"https://github.com/blevesearch/\"/>",
 "  <remote name=\"couchbase\" fetch=\"https://github.com/couchbase/\" review=\"review.couchbase.org\"/>",
 "  <remote name=\"couchbase-priv\" fetch=\"ssh://git@github.com/couchbase/\" review=\"review.couchbase.org\"/>",
 "  <remote name=\"couchbasedeps\" fetch=\"https://github.com/couchbasedeps/\" review=\"review.couchbase.org\"/>",
 "  <remote name=\"couchbaselabs\" fetch=\"https://github.com/couchbaselabs/\" review=\"review.couchbase.org\"/>",
 "  ","  <default remote=\"couchbase\" revision=\"master\"/>","  ",
 "  <project name=\"HdrHistogram_c\" path=\"third_party/HdrHistogram_c\" remote=\"couchbasedeps\" revision=\"9ead6b88adbf8d6131e5ae7a3a699c477a3b4195\" groups=\"kv\"/>",
 "  <project name=\"analytics-dcp-client\" path=\"analytics/java-dcp-client\" revision=\"081d9934d4a28b4abdadcd13891792ea423416c0\" upstream=\"trinity\" dest-branch=\"trinity\" groups=\"notdefault,enterprise,analytics\"/>",
 "  <project name=\"asterixdb\" path=\"analytics/asterixdb\" revision=\"6a10f3f81db977c706447ece476b487cbe56414c\" groups=\"notdefault,enterprise,analytics\"/>",
 "  <project name=\"backup\" remote=\"couchbase-priv\" revision=\"192d7500ba2a7b5281d2c61af126c8027bbb858d\" groups=\"backup,notdefault,enterprise\"/>",
 "  <project name=\"build\" path=\"cbbuild\" revision=\"cedf9d4ec929eac7e61f8e86488aeac5402c8563\" groups=\"notdefault,build\">",
 "    <annotation name=\"RELEASE\" value=\"trinity\"/>",
 "    <annotation name=\"PRODUCT\" value=\"couchbase-server\"/>",
 "    <annotation name=\"BLD_NUM\" value=\"3721\"/>",
 "    <annotation name=\"VERSION\" value=\"7.6.2\"/>","  </project>",
 "  <project name=\"cbas\" path=\"goproj/src/github.com/couchbase/cbas\" remote=\"couchbase-priv\" revision=\"2db6eb59fd5af47a1ce81d53c8f4e58c7a14df3a\" groups=\"notdefault,enterprise,analytics\"/>",
 "  <project name=\"cbas-core\" path=\"analytics\" remote=\"couchbase-priv\" revision=\"2c1d23ee3aba4c80196d9d94ceaca3917b8ea8a7\" upstream=\"7.6.2\" dest-branch=\"7.6.2\" groups=\"notdefault,enterprise,analytics\"/>",
 "  <project name=\"cbas-ui\" revision=\"704db180d01de15f70cacc9fc11c5d8d8d4ff965\" groups=\"notdefault,enterprise,analytics\"/>",
 "  <project name=\"cbauth\" path=\"goproj/src/github.com/couchbase/cbauth\" revision=\"a9992170165a1d330cb5a9918a29d5bd417c5e46\" groups=\"backup\"/>",
 "  <project name=\"cbbs\" remote=\"couchbase-priv\" revision=\"f1d0272decc7f1b445b08e56e1f75e99f743aa90\" groups=\"backup,notdefault,enterprise\"/>",
 "  <project name=\"cbft\" revision=\"69d32cca4a8eca6e5aad5dad689795ab72ecdd6e\"/>",
 "  <project name=\"cbftx\" remote=\"couchbase-priv\" revision=\"258e3829db59f06a202ea2435c776a351a590eba\" groups=\"notdefault,enterprise\"/>",
 "  <project name=\"cbgt\" revision=\"b7dd01a11c5c56fbca88b9b950d8eca4dacce36f\"/>",
 "  <project name=\"cbsummary\" path=\"goproj/src/github.com/couchbase/cbsummary\" revision=\"fb656c91554a97318c44f58e3cc7f166f1eef4fc\"/>",
 "  <project name=\"chronicle\" path=\"ns_server/deps/chronicle\" revision=\"8d1feeb0d8b15e2b6a4c1a417addfd159b422a71\"/>",
 "  <project name=\"client_golang\" path=\"godeps/src/github.com/prometheus/client_golang\" remote=\"couchbasedeps\" revision=\"2e1c4818ccfdcf953ce399cadad615ff2bed968c\" upstream=\"refs/tags/v1.12.1\" dest-branch=\"refs/tags/v1.12.1\"/>",
 "  <project name=\"client_model\" path=\"godeps/src/github.com/prometheus/client_model\" remote=\"couchbasedeps\" revision=\"6dc836ede0b5b08c61893c3ffeb474498b18bb83\"/>",
 "  <project name=\"clog\" path=\"godeps/src/github.com/couchbase/clog\" revision=\"f935d1fdfc36541b505cf86fea4822e4067f9c39\"/>",
 "  <project name=\"common\" path=\"godeps/src/github.com/prometheus/common\" remote=\"couchbasedeps\" revision=\"902cb39e6c079571d32c2db8da220da13c11b562\" upstream=\"refs/tags/v0.33.0\" dest-branch=\"refs/tags/v0.33.0\"/>",
 "  <project name=\"couchbase-cli\" revision=\"941f6d7bbac8f8a42870c3f5459376b9f19ef1fd\" groups=\"kv\"/>",
 "  <project name=\"couchdb\" revision=\"3e5b8f248d77dd9317b36b50eed2567bcfb5f4cf\" dest-branch=\"unstable\"/>",
 "  <project name=\"couchdbx-app\" revision=\"702647dd015e7443de9cdb789806351774e85463\" groups=\"notdefault,packaging\"/>",
 "  <project name=\"couchstore\" revision=\"ce7305bab3feb64bd2504f34d24a1419008e8bda\" groups=\"kv\"/>",
 "  <project name=\"crypto\" path=\"godeps/src/golang.org/x/crypto\" remote=\"couchbasedeps\" revision=\"eb61739cd99fb244c7cd188d3c5bae54824e781d\" upstream=\"refs/tags/v0.15.0\" dest-branch=\"refs/tags/v0.15.0\"/>",
 "  <project name=\"docloader\" path=\"goproj/src/github.com/couchbase/docloader\" revision=\"cf3254d7dfb042192c9a23bd2e64a281c32a29d8\"/>",
 "  <project name=\"errors\" path=\"godeps/src/github.com/pkg/errors\" remote=\"couchbasedeps\" revision=\"30136e27e2ac8d167177e8a583aa4c3fea5be833\"/>",
 "  <project name=\"eventing\" path=\"goproj/src/github.com/couchbase/eventing\" revision=\"047b756132464b8f756cc35e02a15b5f498f80d5\" dest-branch=\"unstable\" groups=\"notdefault,enterprise\"/>",
 "  <project name=\"eventing-ee\" path=\"goproj/src/github.com/couchbase/eventing-ee\" remote=\"couchbase-priv\" revision=\"5425f180a0756868524081f889ab224cfc10b70d\" dest-branch=\"unstable\" groups=\"notdefault,enterprise\"/>",
 "  <project name=\"flatbuffers\" path=\"godeps/src/github.com/google/flatbuffers\" remote=\"couchbasedeps\" revision=\"1a8968225130caeddd16e227678e6f8af1926303\"/>",
 "  <project name=\"forestdb\" revision=\"9efe6d75d7d61e742af70fb47fe97ad1d04ba86f\" groups=\"backup\"/>",
 "  <project name=\"geocouch\" revision=\"68f3b9d36630682d17ca5232770f1693b9b8fa18\"/>",
 "  <project name=\"go-couchbase\" path=\"goproj/src/github.com/couchbase/go-couchbase\" revision=\"959eaf944140a6c660990f38b1db310ddd6d8e42\" groups=\"backup\"/>",
 "  <project name=\"go-metrics\" path=\"godeps/src/github.com/rcrowley/go-metrics\" remote=\"couchbasedeps\" revision=\"cf1acfcdf4751e0554ffa765d03e479ec491cad6\"/>",
 "  <project name=\"go_json\" path=\"goproj/src/github.com/couchbase/go_json\" revision=\"d6e17ad2b9a218e82569e09b761c226fa8df726a\"/>",
 "  <project name=\"gocb\" path=\"godeps/src/github.com/couchbase/gocb/v2\" revision=\"40020eef484873e507745107edbf99c421927a93\" upstream=\"refs/tags/v2.2.5\" dest-branch=\"refs/tags/v2.2.5\"/>",
 "  <project name=\"gocbcore\" path=\"godeps/src/github.com/couchbase/gocbcore/v9\" revision=\"0ece206041d8cf5f5fcd919767446603691bdb69\" upstream=\"refs/tags/v9.1.6\" dest-branch=\"refs/tags/v9.1.6\"/>",
 "  <project name=\"godbc\" path=\"goproj/src/github.com/couchbase/godbc\" revision=\"2d3ecc3de903a5e4d0bc9181adedb5e637f83435\"/>",
 "  <project name=\"gojsonsm\" path=\"godeps/src/github.com/couchbaselabs/gojsonsm\" remote=\"couchbaselabs\" revision=\"8db06ae62940835d35db4de075bd68f0e00ea6b7\" groups=\"bsl\"/>",
 "  <project name=\"golang\" remote=\"couchbaselabs\" revision=\"4dd1b189981c94835b61c1607ca765e88604ce5a\" groups=\"kv\"/>",
 "  <project name=\"golang-pkg-pcre\" path=\"godeps/src/github.com/glenn-brown/golang-pkg-pcre\" remote=\"couchbasedeps\" revision=\"48bb82a8b8ceea98f4e97825b43870f6ba1970d6\"/>",
 "  <project name=\"golang-snappy\" path=\"godeps/src/github.com/golang/snappy\" remote=\"couchbasedeps\" revision=\"723cc1e459b8eea2dea4583200fd60757d40097a\"/>",
 "  <project name=\"golang-tools\" path=\"godeps/src/golang.org/x/tools\" remote=\"couchbasedeps\" revision=\"a28dfb48e06b2296b66678872c2cb638f0304f20\"/>",
 "  <project name=\"golang_protobuf_extensions\" path=\"godeps/src/github.com/matttproud/golang_protobuf_extensions\" remote=\"couchbasedeps\" revision=\"c182affec369e30f25d3eb8cd8a478dee585ae7d\"/>",
 "  <project name=\"gomemcached\" path=\"goproj/src/github.com/couchbase/gomemcached\" revision=\"689b8f03386ba2e7bac304bfd3a525b1e1427675\" groups=\"backup\"/>",
 "  <project name=\"gometa\" path=\"goproj/src/github.com/couchbase/gometa\" revision=\"816f7d6346c9fc5473c4a11e3efe9ed29a2f7f72\"/>",
 "  <project name=\"goutils\" path=\"goproj/src/github.com/couchbase/goutils\" revision=\"30adfca73d8113b5b217097414d7c3adeeef849a\" groups=\"bsl\"/>",
 "  <project name=\"goxdcr\" path=\"goproj/src/github.com/couchbase/goxdcr\" revision=\"4c570a31e5a6f3e087e147edf781022352497f64\" groups=\"bsl\"/>",
 "  <project name=\"gsl-lite\" path=\"third_party/gsl-lite\" remote=\"couchbasedeps\" revision=\"e1c381746c2625a76227255f999ae9f14a062208\" upstream=\"refs/tags/v0.38.1\" dest-branch=\"refs/tags/v0.38.1\" groups=\"kv\"/>",
 "  <project name=\"hebrew\" remote=\"couchbase-priv\" revision=\"c57616b187889a5318688f49817ccaceb9c098b9\" groups=\"notdefault,enterprise\"/>",
 "  <project name=\"indexing\" path=\"goproj/src/github.com/couchbase/indexing\" revision=\"7e924978fef8498113ae2d6e2be8ccd27da70d2d\" groups=\"bsl\"/>",
 "  <project name=\"jsonschema\" path=\"godeps/src/github.com/santhosh-tekuri/jsonschema\" remote=\"couchbasedeps\" revision=\"137f44a49015e5060a447c331aa37de6e0f50267\"/>",
 "  <project name=\"kv_engine\" revision=\"cfff435cf5fe6efcf1a18aa5580993fb8a4b2010\" groups=\"kv,bsl\"/>",
 "  <project name=\"libcouchbase\" revision=\"684931e59cd87e0c6292e8142c2b18897be5b10c\"/>",
 "  <project name=\"magma\" remote=\"couchbase-priv\" revision=\"86c2233ab8780e7aa71e0199bb957dcda2cf6cd1\" groups=\"notdefault,enterprise,kv_ee\"/>",
 "  <project name=\"mux\" path=\"godeps/src/github.com/gorilla/mux\" remote=\"couchbasedeps\" revision=\"043ee6597c29786140136a5747b6a886364f5282\"/>",
 "  <project name=\"n1fty\" path=\"goproj/src/github.com/couchbase/n1fty\" revision=\"a1fc533c18e5094ce75262c9e711d7189d256cd2\" groups=\"bsl\"/>",
 "  <project name=\"nitro\" path=\"goproj/src/github.com/couchbase/nitro\" revision=\"b70d849f0207f7cfe7ebf32b2db35b534929e041\" groups=\"bsl\"/>",
 "  <project name=\"ns_server\" revision=\"6954b533143cf0a8f906ef9086a8337ee50004a6\" groups=\"bsl\"/>",
 "  <project name=\"participle\" path=\"godeps/src/github.com/alecthomas/participle\" remote=\"couchbasedeps\" revision=\"d638c6e1953ed899e05a34da3935146790c60e46\"/>",
 "  <project name=\"perks\" path=\"godeps/src/github.com/beorn7/perks\" remote=\"couchbasedeps\" revision=\"37c8de3658fcb183f997c4e13e8337516ab753e6\" upstream=\"refs/tags/v1.0.1\" dest-branch=\"refs/tags/v1.0.1\"/>",
 "  <project name=\"phosphor\" revision=\"c0a034fe407eec4723f2e01db2d72762efdbc276\" groups=\"bsl,kv\"/>",
 "  <project name=\"pkcs8\" path=\"godeps/src/github.com/youmark/pkcs8\" remote=\"couchbasedeps\" revision=\"1be2e3e5546da8a58903ff4adcfab015022538ea\" upstream=\"refs/tags/v1.1\" dest-branch=\"refs/tags/v1.1\"/>",
 "  <project name=\"plasma\" path=\"goproj/src/github.com/couchbase/plasma\" remote=\"couchbase-priv\" revision=\"34f14cf1d8cc543932e60d9780cc13d48fa7ea5c\" groups=\"bsl,notdefault,enterprise\"/>",
 "  <project name=\"platform\" revision=\"a158d359293665b6251973868fdc42c3b642474c\" groups=\"bsl,kv\"/>",
 "  <project name=\"procfs\" path=\"godeps/src/github.com/prometheus/procfs\" remote=\"couchbasedeps\" revision=\"76fc8b844e3a18c31bf689e4fe7efdd5a2f41298\"/>",
 "  <project name=\"product-metadata\" revision=\"1bd027c34f33919f7005ddae0ba032a3120fe776\" groups=\"notdefault,packaging\"/>",
 "  <project name=\"product-texts\" revision=\"ec39f811376df6d18e56c81873fd565093666505\" upstream=\"master\" dest-branch=\"master\"/>",
 "  <project name=\"protobuf\" path=\"godeps/src/github.com/golang/protobuf\" remote=\"couchbasedeps\" revision=\"d04d7b157bb510b1e0c10132224b616ac0e26b17\" upstream=\"refs/tags/v1.4.2\" dest-branch=\"refs/tags/v1.4.2\"/>",
 "  <project name=\"protobuf-go\" path=\"godeps/src/google.golang.org/protobuf\" remote=\"couchbasedeps\" revision=\"32051b4f86e54c2142c7c05362c6e96ae3454a1c\" upstream=\"refs/tags/v1.28.0\" dest-branch=\"refs/tags/v1.28.0\"/>",
 "  <project name=\"query\" path=\"goproj/src/github.com/couchbase/query\" revision=\"5da4cb71df5aa00aad1e01e9a3b9d5be0c4f2769\" groups=\"bsl\"/>",
 "  <project name=\"query-ee\" path=\"goproj/src/github.com/couchbase/query-ee\" remote=\"couchbase-priv\" revision=\"6924a352019351c746fe08a2cf9a1993b54093e8\" groups=\"notdefault,enterprise\"/>",
 "  <project name=\"query-ui\" revision=\"abcc90e091c46ad74a59bb2fe768b6f09864ddbf\" groups=\"bsl\"/>",
 "  <project name=\"regulator\" path=\"goproj/src/github.com/couchbase/regulator\" remote=\"couchbase-priv\" revision=\"4ef404748ecc34fd87bdebc56074ebe99d240464\" groups=\"notdefault,enterprise\"/>",
 "  <project name=\"sigar\" revision=\"2da0c123cfb45ae39e76e730bd960db8812e3f20\" groups=\"kv\"/>",
 "  <project name=\"simdutf\" path=\"third_party/simdutf\" remote=\"couchbasedeps\" revision=\"4a212616ba23c65c7048f9604faccbff5353300f\" upstream=\"refs/tags/v3.2.14\" dest-branch=\"refs/tags/v3.2.14\" groups=\"kv\"/>",
 "  <project name=\"subjson\" revision=\"a619faccb30e43a4bc0708ee11b1b24abb349f18\" groups=\"bsl,kv\"/>",
 "  <project name=\"sys\" path=\"godeps/src/golang.org/x/sys\" remote=\"couchbasedeps\" revision=\"d36c6a25d886e7c9975d5bf247ac24887ba6da37\"/>",
 "  <project name=\"testrunner\" revision=\"b2d76b82c7d75a75ae78f787a76dc8300c427adf\" upstream=\"trinity\" dest-branch=\"trinity\"/>",
 "  <project name=\"tlm\" revision=\"0c610d8e4738567440ffb1f557dfa15bff81b99d\" upstream=\"7.6.2\" dest-branch=\"7.6.2\" groups=\"bsl,kv\">",
 "    <copyfile src=\"Build.sh\" dest=\"Build.sh\"/>",
 "    <copyfile src=\"GNUmakefile\" dest=\"GNUmakefile\"/>",
 "    <copyfile src=\"Makefile\" dest=\"Makefile\"/>",
 "    <copyfile src=\"CMakeLists.txt\" dest=\"CMakeLists.txt\"/>",
 "    <copyfile src=\"dot-clang-format\" dest=\".clang-format\"/>",
 "    <copyfile src=\"dot-clang-tidy\" dest=\".clang-tidy\"/>",
 "    <copyfile src=\"third-party-CMakeLists.txt\" dest=\"third_party/CMakeLists.txt\"/>",
 "  </project>",
 "  <project name=\"udf-api\" path=\"goproj/src/github.com/couchbase/udf-api\" revision=\"b2788ae3d412356a330b36d7f38ad2c66edb5879\"/>",
 "  <project name=\"uuid\" path=\"godeps/src/github.com/google/uuid\" remote=\"couchbasedeps\" revision=\"dec09d789f3dba190787f8b4454c7d3c936fed9e\"/>",
 "  <project name=\"vbmap\" revision=\"6cce93c4af4497d8108c3ed31b84d7139321cc82\"/>",
 "  <project name=\"voltron\" remote=\"couchbase-priv\" revision=\"19881dacfffb6d834a7aaa4a6d1925a904ea387f\" groups=\"notdefault,packaging\"/>",
 "  <project name=\"xxhash\" path=\"goproj/src/github.com/cespare/xxhash\" remote=\"couchbasedeps\" revision=\"e7a6b52374f7e2abfb8abb27249d53a1997b09a7\" upstream=\"refs/tags/v2.1.2\" dest-branch=\"refs/tags/v2.1.2\"/>",
 "</manifest>"]

[error_logger:info,2025-05-15T18:46:45.623Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.237.0>},
              {id,timeout_diag_logger},
              {mfargs,{timeout_diag_logger,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.625Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.238.0>},
              {id,ns_cookie_manager},
              {mfargs,{ns_cookie_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:45.628Z,ns_1@cb.local:chronicle_local<0.239.0>:chronicle_local:init:54]Ensure chronicle is started
[error_logger:info,2025-05-15T18:46:45.664Z,ns_1@cb.local:chronicle_sup<0.244.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.245.0>},
              {id,chronicle_events},
              {mfargs,{chronicle_events,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.665Z,ns_1@cb.local:chronicle_sup<0.244.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.246.0>},
              {id,chronicle_external_events},
              {mfargs,{gen_event,start_link,
                                 [{local,chronicle_external_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.669Z,ns_1@cb.local:chronicle_sup<0.244.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.247.0>},
              {id,chronicle_ets},
              {mfargs,{chronicle_ets,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.671Z,ns_1@cb.local:chronicle_sup<0.244.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.248.0>},
              {id,chronicle_settings},
              {mfargs,{chronicle_settings,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.676Z,ns_1@cb.local:chronicle_agent_sup<0.249.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_agent_sup}
    started: [{pid,<0.250.0>},
              {id,chronicle_snapshot_mgr},
              {mfargs,{chronicle_snapshot_mgr,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.676Z,ns_1@cb.local:chronicle_agent_sup<0.249.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_agent_sup}
    started: [{pid,<0.251.0>},
              {id,chronicle_rsm_events},
              {mfargs,{chronicle_events,start_link,[chronicle_rsm_events]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[chronicle:info,2025-05-15T18:46:45.737Z,ns_1@cb.local:chronicle_agent<0.252.0>:chronicle_storage:open_logs:180]Reading log files [{0,
                    "/opt/couchbase/var/lib/couchbase/config/chronicle/logs/0.log"}]
[chronicle:info,2025-05-15T18:46:45.742Z,ns_1@cb.local:chronicle_agent<0.252.0>:chronicle_storage:open_current_log:232]Error while opening log file "/opt/couchbase/var/lib/couchbase/config/chronicle/logs/0.log": no_header
[chronicle:info,2025-05-15T18:46:45.742Z,ns_1@cb.local:chronicle_agent<0.252.0>:chronicle_storage:create_log:280]Creating log file /opt/couchbase/var/lib/couchbase/config/chronicle/logs/0.log (high seqno = 0)
[chronicle:info,2025-05-15T18:46:45.748Z,ns_1@cb.local:chronicle_agent<0.252.0>:chronicle_agent:maybe_seed_storage:2444]Found empty storage. Seeding it with default metadata:
#{committed_seqno => 0,history_id => <<"no-history">>,peer => nonode@nohost,
  peer_id => <<>>,pending_branch => undefined,state => not_provisioned,
  term => {0,nonode@nohost}}
[error_logger:info,2025-05-15T18:46:45.749Z,ns_1@cb.local:chronicle_agent_sup<0.249.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_agent_sup}
    started: [{pid,<0.252.0>},
              {id,chronicle_agent},
              {mfargs,{chronicle_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.749Z,ns_1@cb.local:chronicle_sup<0.244.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.249.0>},
              {id,chronicle_agent_sup},
              {mfargs,{chronicle_agent_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:45.756Z,ns_1@cb.local:chronicle_sup<0.244.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.254.0>},
              {id,chronicle_secondary_sup},
              {mfargs,{chronicle_secondary_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:45.757Z,ns_1@cb.local:application_controller<0.44.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    application: chronicle
    started_at: 'ns_1@cb.local'

[ns_server:debug,2025-05-15T18:46:45.760Z,ns_1@cb.local:chronicle_local<0.239.0>:chronicle_local:init:58]Chronicle state is: not_provisioned
[ns_server:debug,2025-05-15T18:46:45.760Z,ns_1@cb.local:chronicle_local<0.239.0>:chronicle_local:provision:139]Provision chronicle on this node
[chronicle:debug,2025-05-15T18:46:45.762Z,ns_1@cb.local:chronicle_agent<0.252.0>:chronicle_agent:handle_provision:1199]Provisioning with history <<"ea19d13c61d8435b9ad1a5f6af1aa5eb">>. Config:
{config,undefined,0,undefined,
        #{'ns_1@cb.local' =>
              #{id => <<"4002160e672d4042c281f80158fca980">>,role => voter}},
        undefined,
        #{chronicle_config_rsm => {rsm_config,chronicle_config_rsm,[]},
          kv => {rsm_config,chronicle_kv,[]}},
        #{},undefined,
        [{<<"ea19d13c61d8435b9ad1a5f6af1aa5eb">>,0}]}
[error_logger:info,2025-05-15T18:46:45.765Z,ns_1@cb.local:<0.255.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.255.0>,dynamic_supervisor}
    started: [{pid,<0.256.0>},
              {id,chronicle_leader},
              {mfargs,{chronicle_leader,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.775Z,ns_1@cb.local:chronicle_secondary_restartable_sup<0.257.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_secondary_restartable_sup}
    started: [{pid,<0.258.0>},
              {id,chronicle_status},
              {mfargs,{chronicle_status,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.784Z,ns_1@cb.local:chronicle_secondary_restartable_sup<0.257.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_secondary_restartable_sup}
    started: [{pid,<0.259.0>},
              {id,chronicle_failover},
              {mfargs,{chronicle_failover,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.785Z,ns_1@cb.local:<0.255.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.255.0>,dynamic_supervisor}
    started: [{pid,<0.257.0>},
              {id,chronicle_secondary_restartable_sup},
              {mfargs,{chronicle_secondary_restartable_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:45.789Z,ns_1@cb.local:<0.255.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.255.0>,dynamic_supervisor}
    started: [{pid,<0.260.0>},
              {id,chronicle_server},
              {mfargs,{chronicle_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[chronicle:info,2025-05-15T18:46:45.798Z,ns_1@cb.local:chronicle_config_rsm<0.264.0>:chronicle_rsm:get_incarnation:1594]No incarnation file found at '/opt/couchbase/var/lib/couchbase/config/chronicle/rsms/chronicle_config_rsm/incarnation'
[chronicle:debug,2025-05-15T18:46:45.800Z,ns_1@cb.local:chronicle_server<0.260.0>:chronicle_server:handle_register_rsm:361]Registering RSM chronicle_config_rsm with pid <0.264.0>
[error_logger:info,2025-05-15T18:46:45.800Z,ns_1@cb.local:<0.263.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.263.0>,chronicle_single_rsm_sup}
    started: [{pid,<0.264.0>},
              {id,chronicle_config_rsm},
              {mfargs,{chronicle_rsm,start_link,
                                     [chronicle_config_rsm,
                                      <<"4002160e672d4042c281f80158fca980">>,
                                      chronicle_config_rsm,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.800Z,ns_1@cb.local:<0.262.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.262.0>,dynamic_supervisor}
    started: [{pid,<0.263.0>},
              {id,chronicle_config_rsm},
              {mfargs,
                  {chronicle_single_rsm_sup,start_link,
                      [chronicle_config_rsm,
                       <<"4002160e672d4042c281f80158fca980">>,
                       chronicle_config_rsm,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:45.804Z,ns_1@cb.local:<0.266.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.266.0>,chronicle_single_rsm_sup}
    started: [{pid,<0.267.0>},
              {id,'kv-events'},
              {mfargs,{gen_event,start_link,[{local,'kv-events'}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[chronicle:info,2025-05-15T18:46:45.805Z,ns_1@cb.local:kv<0.268.0>:chronicle_rsm:get_incarnation:1594]No incarnation file found at '/opt/couchbase/var/lib/couchbase/config/chronicle/rsms/kv/incarnation'
[error_logger:info,2025-05-15T18:46:45.807Z,ns_1@cb.local:<0.266.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.266.0>,chronicle_single_rsm_sup}
    started: [{pid,<0.268.0>},
              {id,kv},
              {mfargs,{chronicle_rsm,start_link,
                                     [kv,
                                      <<"4002160e672d4042c281f80158fca980">>,
                                      chronicle_kv,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[chronicle:debug,2025-05-15T18:46:45.807Z,ns_1@cb.local:chronicle_server<0.260.0>:chronicle_server:handle_register_rsm:361]Registering RSM kv with pid <0.268.0>
[error_logger:info,2025-05-15T18:46:45.808Z,ns_1@cb.local:<0.262.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.262.0>,dynamic_supervisor}
    started: [{pid,<0.266.0>},
              {id,kv},
              {mfargs,
                  {chronicle_single_rsm_sup,start_link,
                      [kv,<<"4002160e672d4042c281f80158fca980">>,chronicle_kv,
                       []]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:45.808Z,ns_1@cb.local:<0.255.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.255.0>,dynamic_supervisor}
    started: [{pid,<0.261.0>},
              {id,chronicle_rsm_sup},
              {mfargs,{chronicle_rsm_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:info,2025-05-15T18:46:45.809Z,ns_1@cb.local:chronicle_local<0.239.0>:chronicle_upgrade:initialize:70]Setup initial chronicle content [{set,counters,[]},
                                 {set,auto_reprovision_cfg,
                                  [{enabled,true},{max_nodes,1},{count,0}]},
                                 {set,bucket_names,[]},
                                 {set,nodes_wanted,['ns_1@cb.local']},
                                 {set,server_groups,
                                  [[{uuid,<<"0">>},
                                    {name,<<"Group 1">>},
                                    {nodes,['ns_1@cb.local']}]]},
                                 {set,
                                  {node,'ns_1@cb.local',membership},
                                  active},
                                 {set,autocompaction,
                                  [{database_fragmentation_threshold,
                                    {30,undefined}},
                                   {view_fragmentation_threshold,
                                    {30,undefined}},
                                   {magma_fragmentation_percentage,50}]}]
[chronicle:debug,2025-05-15T18:46:46.102Z,ns_1@cb.local:chronicle_leader<0.256.0>:chronicle_leader:handle_state_timeout:608]State timeout when state is: {observer,true,false}
[chronicle:info,2025-05-15T18:46:46.102Z,ns_1@cb.local:<0.270.0>:chronicle_leader:do_election_worker:892]Starting election.
History ID: <<"ea19d13c61d8435b9ad1a5f6af1aa5eb">>
Log position: {{1,'ns_1@cb.local'},1}
Peers: ['ns_1@cb.local']
Required quorum: {majority,{set,1,16,16,8,80,48,
                                {[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],
                                 []},
                                {{[],[],[],[],[],[],[],[],[],[],[],[],
                                  ['ns_1@cb.local'],
                                  [],[],[]}}}}
[chronicle:info,2025-05-15T18:46:46.103Z,ns_1@cb.local:<0.270.0>:chronicle_leader:do_election_worker:901]I'm the only peer, so I'm the leader.
[chronicle:info,2025-05-15T18:46:46.103Z,ns_1@cb.local:chronicle_leader<0.256.0>:chronicle_leader:handle_election_result:691]Going to become a leader in term {2,'ns_1@cb.local'} (history id <<"ea19d13c61d8435b9ad1a5f6af1aa5eb">>)
[chronicle:debug,2025-05-15T18:46:46.107Z,ns_1@cb.local:chronicle_agent<0.252.0>:chronicle_agent:handle_establish_term:1529]Accepted term {2,'ns_1@cb.local'} in history <<"ea19d13c61d8435b9ad1a5f6af1aa5eb">>
[chronicle:debug,2025-05-15T18:46:46.107Z,ns_1@cb.local:chronicle_proposer<0.271.0>:chronicle_proposer:establish_term_init:367]Going to establish term {2,'ns_1@cb.local'} (history id <<"ea19d13c61d8435b9ad1a5f6af1aa5eb">>).
Quorum peers: ['ns_1@cb.local']
Metadata:
{metadata,'ns_1@cb.local',<<"4002160e672d4042c281f80158fca980">>,
          <<"ea19d13c61d8435b9ad1a5f6af1aa5eb">>,
          {1,'ns_1@cb.local'},
          {1,'ns_1@cb.local'},
          1,1,
          {log_entry,<<"ea19d13c61d8435b9ad1a5f6af1aa5eb">>,
                     {1,'ns_1@cb.local'},
                     1,
                     {config,undefined,0,undefined,
                             #{'ns_1@cb.local' =>
                                   #{id =>
                                         <<"4002160e672d4042c281f80158fca980">>,
                                     role => voter}},
                             undefined,
                             #{chronicle_config_rsm =>
                                   {rsm_config,chronicle_config_rsm,[]},
                               kv => {rsm_config,chronicle_kv,[]}},
                             #{},undefined,
                             [{<<"ea19d13c61d8435b9ad1a5f6af1aa5eb">>,0}]}},
          {log_entry,<<"ea19d13c61d8435b9ad1a5f6af1aa5eb">>,
                     {1,'ns_1@cb.local'},
                     1,
                     {config,undefined,0,undefined,
                             #{'ns_1@cb.local' =>
                                   #{id =>
                                         <<"4002160e672d4042c281f80158fca980">>,
                                     role => voter}},
                             undefined,
                             #{chronicle_config_rsm =>
                                   {rsm_config,chronicle_config_rsm,[]},
                               kv => {rsm_config,chronicle_kv,[]}},
                             #{},undefined,
                             [{<<"ea19d13c61d8435b9ad1a5f6af1aa5eb">>,0}]}},
          undefined}
[chronicle:debug,2025-05-15T18:46:46.107Z,ns_1@cb.local:chronicle_proposer<0.271.0>:chronicle_proposer:establish_term_maybe_transition:686]Established term {2,'ns_1@cb.local'} (history id <<"ea19d13c61d8435b9ad1a5f6af1aa5eb">>) successfully.
Votes: ['ns_1@cb.local']
[chronicle:debug,2025-05-15T18:46:46.107Z,ns_1@cb.local:chronicle_proposer<0.271.0>:chronicle_proposer:handle_state_enter:284]Starting recovery for term {2,'ns_1@cb.local'} in history <<"ea19d13c61d8435b9ad1a5f6af1aa5eb">>
[chronicle:debug,2025-05-15T18:46:46.109Z,ns_1@cb.local:chronicle_proposer<0.271.0>:chronicle_proposer:handle_state_enter:296]Proposer for term {2,'ns_1@cb.local'} in history <<"ea19d13c61d8435b9ad1a5f6af1aa5eb">> is ready. Committed seqno: 2
[chronicle:info,2025-05-15T18:46:46.109Z,ns_1@cb.local:chronicle_leader<0.256.0>:chronicle_leader:handle_note_term_status:596]Term {2,'ns_1@cb.local'} established.
[ns_server:info,2025-05-15T18:46:46.137Z,ns_1@cb.local:chronicle_local<0.239.0>:chronicle_upgrade:initialize:72]Chronicle content was initialized. Rev = {<<"ea19d13c61d8435b9ad1a5f6af1aa5eb">>,
                                          3}.
[error_logger:info,2025-05-15T18:46:46.137Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.239.0>},
              {id,chronicle_local},
              {mfargs,{chronicle_local,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:46.139Z,ns_1@cb.local:ns_cluster<0.273.0>:ns_cluster:handle_info:514]Chronicle state is: provisioned
[error_logger:info,2025-05-15T18:46:46.139Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.273.0>},
              {id,ns_cluster},
              {mfargs,{ns_cluster,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:46.140Z,ns_1@cb.local:sigar<0.275.0>:sigar:spawn_sigar:134]Spawning sigar process 'portsigar for ns_1@cb.local'("/opt/couchbase/bin/sigar_port") with babysitter pid: 41 and log file "/opt/couchbase/var/lib/couchbase/logs/sigar_port.log"
[error_logger:info,2025-05-15T18:46:46.141Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.275.0>},
              {id,sigar},
              {mfargs,{sigar,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:46.142Z,ns_1@cb.local:ns_config_sup<0.276.0>:ns_config_sup:init:26]loading static ns_config from "/opt/couchbase/etc/couchbase/config"
[error_logger:info,2025-05-15T18:46:46.144Z,ns_1@cb.local:ns_config_sup<0.276.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.277.0>},
              {id,tombstone_keeper},
              {mfargs,{tombstone_keeper,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:46.144Z,ns_1@cb.local:ns_config_sup<0.276.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.278.0>},
              {id,ns_config_events},
              {mfargs,{gen_event,start_link,[{local,ns_config_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:46.145Z,ns_1@cb.local:ns_config_sup<0.276.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.279.0>},
              {id,ns_config_events_local},
              {mfargs,{gen_event,start_link,[{local,ns_config_events_local}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:46.228Z,ns_1@cb.local:ns_config<0.280.0>:ns_config:load_config:1113]Loading static config from "/opt/couchbase/etc/couchbase/config"
[ns_server:info,2025-05-15T18:46:46.229Z,ns_1@cb.local:ns_config<0.280.0>:ns_config:load_config:1127]Loading dynamic config from "/opt/couchbase/var/lib/couchbase/config/config.dat"
[ns_server:info,2025-05-15T18:46:46.230Z,ns_1@cb.local:ns_config<0.280.0>:ns_config:load_config:1132]No dynamic config file found. Assuming we're brand new node
[ns_server:debug,2025-05-15T18:46:46.235Z,ns_1@cb.local:ns_config<0.280.0>:ns_config:load_config:1135]Here's full dynamic config we loaded:
[[]]
[ns_server:info,2025-05-15T18:46:46.239Z,ns_1@cb.local:ns_config<0.280.0>:ns_config:load_config:1157]Here's full dynamic config we loaded + static & default config:
[{resource_management,
  [{bucket,
    [{resident_ratio,
      [{enabled,false},{couchstore_minimum,1},{magma_minimum,0.2}]},
     {data_size,[{enabled,false},{couchstore_maximum,2},{magma_maximum,16}]}]},
   {index,[]},
   {cores_per_bucket,[{enabled,false},{minimum,0.4}]},
   {disk_usage,[{enabled,false},{maximum,96}]},
   {collections_per_quota,[{enabled,false},{maximum,1}]}]},
 {{node,'ns_1@cb.local',index_dir},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]},
   47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
   98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
 {{node,'ns_1@cb.local',database_dir},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]},
   47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
   98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
 {auto_failover_cfg,
  [{enabled,true},
   {timeout,120},
   {count,0},
   {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
   {failover_server_group,false},
   {max_count,1},
   {failed_over_server_groups,[]},
   {can_abort_rebalance,true}]},
 {retry_rebalance,[{enabled,false},{after_time_period,300},{max_attempts,1}]},
 {{node,'ns_1@cb.local',{project_intact,is_vulnerable}},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   false]},
 {{node,'ns_1@cb.local',ssl_capi_port},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   18092]},
 {{node,'ns_1@cb.local',capi_port},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   8092]},
 {{node,'ns_1@cb.local',backup_grpc_port},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   9124]},
 {{node,'ns_1@cb.local',backup_https_port},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   18097]},
 {{node,'ns_1@cb.local',backup_http_port},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   8097]},
 {{node,'ns_1@cb.local',prometheus_http_port},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   9123]},
 {{node,'ns_1@cb.local',cbas_debug_port},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|-1]},
 {{node,'ns_1@cb.local',cbas_parent_port},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   9122]},
 {{node,'ns_1@cb.local',cbas_metadata_port},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   9121]},
 {{node,'ns_1@cb.local',cbas_replication_port},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   9120]},
 {{node,'ns_1@cb.local',cbas_metadata_callback_port},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   9119]},
 {{node,'ns_1@cb.local',cbas_messaging_port},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   9118]},
 {{node,'ns_1@cb.local',cbas_result_port},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   9117]},
 {{node,'ns_1@cb.local',cbas_data_port},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   9116]},
 {{node,'ns_1@cb.local',cbas_cluster_port},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   9115]},
 {{node,'ns_1@cb.local',cbas_console_port},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   9114]},
 {{node,'ns_1@cb.local',cbas_cc_client_port},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   9113]},
 {{node,'ns_1@cb.local',cbas_cc_cluster_port},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   9112]},
 {{node,'ns_1@cb.local',cbas_cc_http_port},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   9111]},
 {{node,'ns_1@cb.local',cbas_admin_port},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   9110]},
 {{node,'ns_1@cb.local',cbas_ssl_port},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   18095]},
 {{node,'ns_1@cb.local',cbas_http_port},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   8095]},
 {{node,'ns_1@cb.local',eventing_https_port},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   18096]},
 {{node,'ns_1@cb.local',eventing_debug_port},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   9140]},
 {{node,'ns_1@cb.local',eventing_http_port},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   8096]},
 {{node,'ns_1@cb.local',fts_grpc_ssl_port},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   19130]},
 {{node,'ns_1@cb.local',fts_grpc_port},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   9130]},
 {{node,'ns_1@cb.local',fts_ssl_port},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   18094]},
 {{node,'ns_1@cb.local',fts_http_port},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   8094]},
 {{node,'ns_1@cb.local',indexer_https_port},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   19102]},
 {{node,'ns_1@cb.local',indexer_stmaint_port},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   9105]},
 {{node,'ns_1@cb.local',indexer_stcatchup_port},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   9104]},
 {{node,'ns_1@cb.local',indexer_stinit_port},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   9103]},
 {{node,'ns_1@cb.local',indexer_http_port},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   9102]},
 {{node,'ns_1@cb.local',indexer_scan_port},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   9101]},
 {{node,'ns_1@cb.local',indexer_admin_port},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   9100]},
 {{node,'ns_1@cb.local',ssl_query_port},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   18093]},
 {{node,'ns_1@cb.local',query_port},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   8093]},
 {{node,'ns_1@cb.local',projector_ssl_port},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   9999]},
 {{node,'ns_1@cb.local',projector_port},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   9999]},
 {{node,'ns_1@cb.local',memcached_prometheus},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   11280]},
 {{node,'ns_1@cb.local',memcached_dedicated_ssl_port},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   11206]},
 {{node,'ns_1@cb.local',xdcr_rest_port},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   9998]},
 {{node,'ns_1@cb.local',ssl_rest_port},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   18091]},
 {{node,'ns_1@cb.local',rest},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]},
   {port,8091},
   {port_meta,global}]},
 {rest,[{port,8091}]},
 {password_policy,[{min_length,6},{must_present,[]}]},
 {health_monitor_refresh_interval,[]},
 {service_orchestrator_weight,
  [{kv,10000},
   {index,1000},
   {fts,1000},
   {cbas,1000},
   {n1ql,100},
   {eventing,100},
   {backup,10}]},
 {log_redaction_default_cfg,[{redact_level,none}]},
 {replication,[{enabled,true}]},
 {alert_limits,
  [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
 {email_alerts,
  [{recipients,["root@localhost"]},
   {sender,"couchbase@localhost"},
   {enabled,false},
   {email_server,
    [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
   {alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     indexer_low_resident_percentage,ep_clock_cas_drift_threshold_exceeded,
     communication_issue,time_out_of_sync,disk_usage_analyzer_stuck,
     cert_expires_soon,cert_expired,memory_threshold,history_size_warning,
     stuck_rebalance,memcached_connections]},
   {pop_up_alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     indexer_low_resident_percentage,ep_clock_cas_drift_threshold_exceeded,
     communication_issue,time_out_of_sync,disk_usage_analyzer_stuck,
     cert_expires_soon,cert_expired,memory_threshold,history_size_warning,
     stuck_rebalance,memcached_connections]}]},
 {{node,'ns_1@cb.local',event_log},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/event_log"}]},
 {{node,'ns_1@cb.local',ns_log},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
 {{node,'ns_1@cb.local',port_servers},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}]},
 {secure_headers,[]},
 {buckets,[{configs,[]}]},
 {cbas_memory_quota,1549},
 {fts_memory_quota,512},
 {memory_quota,3406},
 {{node,'ns_1@cb.local',memcached_config},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   {[{interfaces,{memcached_config_mgr,get_interfaces,[]}},
     {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
     {connection_idle_time,connection_idle_time},
     {privilege_debug,privilege_debug},
     {breakpad,
      {[{enabled,breakpad_enabled},
        {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
     {deployment_model,{memcached_config_mgr,get_config_profile,[]}},
     {verbosity,verbosity},
     {audit_file,{"~s",[audit_file]}},
     {rbac_file,{"~s",[rbac_file]}},
     {dedupe_nmvb_maps,dedupe_nmvb_maps},
     {tracing_enabled,tracing_enabled},
     {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
     {xattr_enabled,true},
     {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
     {collections_enabled,true},
     {max_connections,max_connections},
     {system_connections,system_connections},
     {num_reader_threads,num_reader_threads},
     {num_writer_threads,num_writer_threads},
     {num_auxio_threads,num_auxio_threads},
     {num_nonio_threads,num_nonio_threads},
     {num_storage_threads,num_storage_threads},
     {logger,
      {[{filename,{"~s/~s",[log_path,log_prefix]}},
        {cyclesize,log_cyclesize}]}},
     {external_auth_service,
      {memcached_config_mgr,get_external_auth_service,[]}},
     {active_external_users_push_interval,
      {memcached_config_mgr,get_external_users_push_interval,[]}},
     {prometheus,{memcached_config_mgr,prometheus_cfg,[]}},
     {sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
     {ssl_sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
     {tcp_keepalive_idle,tcp_keepalive_idle},
     {tcp_keepalive_interval,tcp_keepalive_interval},
     {tcp_keepalive_probes,tcp_keepalive_probes},
     {tcp_user_timeout,tcp_user_timeout},
     {always_collect_trace_info,always_collect_trace_info},
     {connection_limit_mode,connection_limit_mode},
     {free_connection_pool_size,free_connection_pool_size},
     {max_client_connection_details,max_client_connection_details}]}]},
 {{node,'ns_1@cb.local',memcached},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]},
   {port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,11206},
   {ssl_port,11207},
   {admin_user,"@ns_server"},
   {other_users,
    ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing","@cbas",
     "@backup"]},
   {admin_pass,"*****"},
   {engines,
    [{membase,
      [{engine,"/opt/couchbase/lib/memcached/ep.so"},
       {static_config_string,"failpartialwarmup=false"}]},
     {memcached,
      [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
       {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_rotation_period,39003}]},
 {{node,'ns_1@cb.local',memcached_defaults},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]},
   {max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {je_malloc_conf,undefined},
   {tracing_enabled,true},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>},
   {num_auxio_threads,<<"default">>},
   {num_nonio_threads,<<"default">>},
   {num_storage_threads,<<"default">>},
   {tcp_keepalive_idle,360},
   {tcp_keepalive_interval,10},
   {tcp_keepalive_probes,3},
   {tcp_user_timeout,30},
   {always_collect_trace_info,true},
   {connection_limit_mode,<<"disconnect">>},
   {free_connection_pool_size,0},
   {max_client_connection_details,0}]},
 {memcached,[]},
 {{node,'ns_1@cb.local',audit},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}]},
 {audit,
  [{auditd_enabled,false},
   {rotate_interval,86400},
   {rotate_size,20971520},
   {prune_age,0},
   {disabled,[]},
   {enabled,[]},
   {disabled_users,[]},
   {sync,[]},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
 {{node,'ns_1@cb.local',isasl},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]},
   {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
 {remote_clusters,[]},
 {scramsha_fallback_salt,<<107,122,33,140,105,162,110,2,109,167,244,101>>},
 {client_cert_auth,[{state,"disable"},{prefixes,[]}]},
 {rest_creds,null},
 {{metakv,<<"/analytics/settings/config">>},
  <<"{\"analytics.settings.num_replicas\":0}">>},
 {{metakv,<<"/query/settings/config">>},
  <<"{\"cleanupclientattempts\":true,\"cleanuplostattempts\":true,\"cleanupwindow\":\"60s\",\"completed-limit\":4000,\"completed-threshold\":1000,\"loglevel\":\"info\",\"max-parallelism\":1,\"memory-quota\":0,\"n1ql-feat-ctrl\":76,\"numatrs\":1024,\"pipeline-batch\":16,\"pipeline-cap\":512,\"prepared-limit\":16384,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"query.settings.tmp_space_size\":5120,\"scan-cap\":512,\"timeout\":0,\"txtimeout\":\"0ms\",\"use-cbo\":true}">>},
 {{metakv,<<"/eventing/settings/config">>},<<"{\"ram_quota\":256}">>},
 {{metakv,<<"/indexing/settings/config">>},
  <<"{\"indexer.settings.compaction.abort_exceed_interval\":false,\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.enable_page_bloom_filter\":false,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.log_level\":\"info\",\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.num_replica\":0,\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.storage_mode\":\"\"}">>},
 {{couchdb,max_parallel_replica_indexers},2},
 {{couchdb,max_parallel_indexers},4},
 {quorum_nodes,['ns_1@cb.local']},
 {nodes_wanted,[]},
 {{node,'ns_1@cb.local',compaction_daemon},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]},
   {check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}]},
 {set_view_update_daemon,
  [{update_interval,5000},
   {update_min_changes,5000},
   {replica_update_min_changes,5000}]},
 {max_bucket_count,30},
 {index_aware_rebalance_disabled,false},
 {{node,'ns_1@cb.local',saslauthd_enabled},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   true]},
 {{node,'ns_1@cb.local',is_enterprise},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   true]},
 {{node,'ns_1@cb.local',config_version},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   {7,6}]},
 {{node,'ns_1@cb.local',uuid},
  [{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|
   <<"e0d520cb35700b4a473cf359e3354528">>]}]
[ns_server:debug,2025-05-15T18:46:46.243Z,ns_1@cb.local:tombstone_keeper<0.277.0>:tombstone_keeper:handle_call:49]Refreshed with timestamps []
[error_logger:info,2025-05-15T18:46:46.243Z,ns_1@cb.local:ns_config_sup<0.276.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.280.0>},
              {id,ns_config},
              {mfargs,{ns_config,start_link,
                                 ["/opt/couchbase/etc/couchbase/config",
                                  ns_config_default]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:46.245Z,ns_1@cb.local:ns_config_sup<0.276.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.283.0>},
              {id,ns_config_remote},
              {mfargs,{ns_config_replica,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:46.245Z,ns_1@cb.local:ns_config_sup<0.276.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.284.0>},
              {id,ns_config_log},
              {mfargs,{ns_config_log,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:46.245Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.276.0>},
              {id,ns_config_sup},
              {mfargs,{ns_config_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:46.247Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',n2n_client_cert_auth} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|false]
[ns_server:debug,2025-05-15T18:46:46.247Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',erl_external_listeners} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]},
 {inet,false}]
[error_logger:info,2025-05-15T18:46:46.247Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.286.0>},
              {id,netconfig_updater},
              {mfargs,{netconfig_updater,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:46.247Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',node_encryption} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|false]
[ns_server:debug,2025-05-15T18:46:46.247Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',address_family} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]}|inet]
[error_logger:info,2025-05-15T18:46:46.250Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.289.0>},
              {id,json_rpc_connection_sup},
              {mfargs,{json_rpc_connection_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:46.266Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.292.0>},
              {id,chronicle_compat_events},
              {mfargs,{chronicle_compat_events,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:46.268Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.297.0>},
              {id,remote_monitors},
              {mfargs,{remote_monitors,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:46.270Z,ns_1@cb.local:menelaus_barrier<0.298.0>:one_shot_barrier:barrier_body:52]Barrier menelaus_barrier has started
[error_logger:info,2025-05-15T18:46:46.270Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.298.0>},
              {id,menelaus_barrier},
              {mfargs,{menelaus_sup,barrier_start_link,[]}},
              {restart_type,temporary},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:46.273Z,ns_1@cb.local:<0.301.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,
                                    rest_lhttpc_pool} with delay of 1000
[error_logger:info,2025-05-15T18:46:46.274Z,ns_1@cb.local:<0.302.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.302.0>,suppress_max_restart_intensity}
    started: [{pid,<0.303.0>},
              {id,rest_lhttpc_pool},
              {mfargs,{lhttpc_manager,start_link,
                                      [[{name,rest_lhttpc_pool},
                                        {connection_timeout,120000},
                                        {pool_size,20}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:46.274Z,ns_1@cb.local:<0.300.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.300.0>,suppress_max_restart_intensity}
    started: [{pid,<0.301.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      rest_lhttpc_pool}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,rest_lhttpc_pool},
                       1000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 1,id => rest_lhttpc_pool,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [lhttpc_manager],
                          restart => permanent,shutdown => 1000,
                          start =>
                              {lhttpc_manager,start_link,
                                  [[{name,rest_lhttpc_pool},
                                    {connection_timeout,120000},
                                    {pool_size,20}]]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:46.274Z,ns_1@cb.local:rest_lhttpc_pool_sup<0.299.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,rest_lhttpc_pool_sup}
    started: [{pid,<0.300.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,rest_lhttpc_pool}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 1,id => rest_lhttpc_pool,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [lhttpc_manager],
                         restart => permanent,shutdown => 1000,
                         start =>
                             {lhttpc_manager,start_link,
                                 [[{name,rest_lhttpc_pool},
                                   {connection_timeout,120000},
                                   {pool_size,20}]]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:46.274Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.299.0>},
              {id,rest_lhttpc_pool_sup},
              {mfargs,{rest_lhttpc_pool_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:46.280Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.304.0>},
              {id,memcached_refresh},
              {mfargs,{memcached_refresh,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:46.282Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.305.0>},
              {id,ns_secrets},
              {mfargs,{ns_secrets,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:46.283Z,ns_1@cb.local:ns_ssl_services_sup<0.306.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_ssl_services_sup}
    started: [{pid,<0.307.0>},
              {id,ssl_service_events},
              {mfargs,{gen_event,start_link,[{local,ssl_service_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:46.288Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:maybe_store_ca_certs:832]Considering to store CA certs
[ns_server:debug,2025-05-15T18:46:46.660Z,ns_1@cb.local:<0.312.0>:goport:handle_eof:592]Stream 'stdout' closed
[ns_server:debug,2025-05-15T18:46:46.660Z,ns_1@cb.local:<0.312.0>:goport:handle_eof:592]Stream 'stderr' closed
[ns_server:info,2025-05-15T18:46:46.660Z,ns_1@cb.local:<0.312.0>:goport:handle_process_exit:573]Port exited with status 0.
[ns_server:debug,2025-05-15T18:46:46.666Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_server_cert:generate_cert_and_pkey:155]Generated certificate and private key in 374226 us
[ns_server:debug,2025-05-15T18:46:46.723Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:maybe_store_ca_certs:847]Updating CA file with 1 certificates
[ns_server:info,2025-05-15T18:46:46.727Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:maybe_store_ca_certs:850]CA file updated: 1 cert(s) written
[ns_server:info,2025-05-15T18:46:46.727Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:should_regenerate_certs:899]Should regenerate node_cert because there are no certs on this node
[ns_server:debug,2025-05-15T18:46:46.843Z,ns_1@cb.local:<0.317.0>:goport:handle_eof:592]Stream 'stderr' closed
[ns_server:debug,2025-05-15T18:46:46.843Z,ns_1@cb.local:<0.317.0>:goport:handle_eof:592]Stream 'stdout' closed
[ns_server:info,2025-05-15T18:46:46.843Z,ns_1@cb.local:<0.317.0>:goport:handle_process_exit:573]Port exited with status 0.
[ns_server:info,2025-05-15T18:46:46.845Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:save_certs:968]New node_cert and pkey are written to tmp file
[ns_server:info,2025-05-15T18:46:46.851Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:save_certs_phase2:995]node_cert cert and pkey files updated
[ns_server:debug,2025-05-15T18:46:46.851Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',node_cert} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554006}}]},
 {subject,<<"CN=Couchbase Server Node (127.0.0.1)">>},
 {not_after,63985747606},
 {verified_with,<<107,108,26,174,73,161,129,96,235,159,4,83,123,189,82,186>>},
 {load_timestamp,63914554006},
 {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv7ZdwOswDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNGFmZjU2YzAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzRhZmY1\nNmMwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDvE0P4l48WhjwU/MMT\njgReGICRQusZuf0g2Co1QK5tYQ69lC7J9eYYoM0snH5sM4JAx3DTHZY3ctRXc2+r\nW9PRBFALBMG"...>>},
 {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDOTCCAiGgAwIBAgIIGD/Hv7tjtawwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNGFmZjU2YzAeFw0yNTA1MTQxODQ2NDZaFw0y\nNzA4MTcxODQ2NDZaMCwxKjAoBgNVBAMTIUNvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nMTI3LjAuMC4xKTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAKjp7XXy\nPwREAonHdJTbJ98z796XzNMY3kqOOVNbdTrEt7H8deKOSdtwJcAtkS8sdxFnmyF0\nEvTiWd0"...>>},
 {pkey_passphrase_settings,[]},
 {certs_epoch,0},
 {type,generated},
 {hostname,"127.0.0.1"}]
[ns_server:info,2025-05-15T18:46:46.859Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:should_regenerate_certs:899]Should regenerate client_cert because there are no certs on this node
[ns_server:debug,2025-05-15T18:46:47.057Z,ns_1@cb.local:<0.323.0>:goport:handle_eof:592]Stream 'stdout' closed
[ns_server:debug,2025-05-15T18:46:47.057Z,ns_1@cb.local:<0.323.0>:goport:handle_eof:592]Stream 'stderr' closed
[ns_server:info,2025-05-15T18:46:47.057Z,ns_1@cb.local:<0.323.0>:goport:handle_process_exit:573]Port exited with status 0.
[ns_server:info,2025-05-15T18:46:47.060Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:save_certs:968]New client_cert and pkey are written to tmp file
[ns_server:info,2025-05-15T18:46:47.068Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:save_certs_phase2:995]client_cert cert and pkey files updated
[ns_server:debug,2025-05-15T18:46:47.069Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',client_cert} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554007}}]},
 {subject,<<"CN=Couchbase Internal Client (10007120)">>},
 {not_after,63985747606},
 {verified_with,<<107,108,26,174,73,161,129,96,235,159,4,83,123,189,82,186>>},
 {load_timestamp,63914554007},
 {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv7ZdwOswDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNGFmZjU2YzAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzRhZmY1\nNmMwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDvE0P4l48WhjwU/MMT\njgReGICRQusZuf0g2Co1QK5tYQ69lC7J9eYYoM0snH5sM4JAx3DTHZY3ctRXc2+r\nW9PRBFALBMG"...>>},
 {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDWTCCAkGgAwIBAgIIGD/Hv8LbyDswDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNGFmZjU2YzAeFw0yNTA1MTQxODQ2NDZaFw0y\nNzA4MTcxODQ2NDZaMC8xLTArBgNVBAMTJENvdWNoYmFzZSBJbnRlcm5hbCBDbGll\nbnQgKDEwMDA3MTIwKTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMua\nXqo6pUD2FK1W7gKuO/tOz4RDUbxQOkR2K2wIxXPMJhLI5QEiHZv5ssicaR2o+zQu\nHWCECH8"...>>},
 {pkey_passphrase_settings,[]},
 {certs_epoch,0},
 {type,generated},
 {name,"@internal"}]
[ns_server:debug,2025-05-15T18:46:47.093Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:restart_regenerate_client_cert_timer:1447]Time left before client cert regeneration: 70588799000
[ns_server:info,2025-05-15T18:46:47.093Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:handle_info:764]cert_and_pkey changed
[error_logger:info,2025-05-15T18:46:47.093Z,ns_1@cb.local:ns_ssl_services_sup<0.306.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_ssl_services_sup}
    started: [{pid,<0.308.0>},
              {id,ns_ssl_services_setup},
              {mfargs,{ns_ssl_services_setup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:47.104Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:notify_services:1146]Going to notify following services: [capi_ssl_service,cb_dist_tls,
                                     client_cert_event,memcached,
                                     server_cert_event,ssl_service]
[ns_server:info,2025-05-15T18:46:47.104Z,ns_1@cb.local:<0.342.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service client_cert_event
[ns_server:info,2025-05-15T18:46:47.104Z,ns_1@cb.local:<0.344.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service server_cert_event
[ns_server:debug,2025-05-15T18:46:47.104Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Restarting tls distribution protocols (if any)
[ns_server:debug,2025-05-15T18:46:47.106Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Ensure config is going to change listeners. Will be stopped: [], will be started: [], will be restarted: []
[ns_server:info,2025-05-15T18:46:47.106Z,ns_1@cb.local:<0.341.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service cb_dist_tls
[error_logger:info,2025-05-15T18:46:47.108Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {15946781,#Ref<0.3529285563.3650748419.3237>}}}
[ns_server:debug,2025-05-15T18:46:47.108Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:46:47.108Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3529285563.3650617347.3243>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:46:47.109Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3529285563.3650617347.3243>,
                                  inet_tcp_dist,<0.346.0>,
                                  #Ref<0.3529285563.3650617347.3246>}
[ns_server:debug,2025-05-15T18:46:47.109Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3529285563.3650617347.3243>,
                               inet_tcp_dist,<0.346.0>,
                               #Ref<0.3529285563.3650617347.3246>}
[error_logger:info,2025-05-15T18:46:47.109Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.346.0>,shutdown}}
[error_logger:info,2025-05-15T18:46:47.109Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:46:47.109Z,ns_1@cb.local:<0.340.0>:ns_couchdb_api:rpc_couchdb_node:152]RPC to couchdb node failed for restart_capi_ssl_service with {badrpc,nodedown}
Stack: [{ns_couchdb_api,rpc_couchdb_node,4,
                        [{file,"src/ns_couchdb_api.erl"},{line,150}]},
        {ns_ssl_services_setup,do_notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1203}]},
        {ns_ssl_services_setup,notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1179}]},
        {async,'-async_init/4-fun-1-',3,[{file,"src/async.erl"},{line,199}]}]
[ns_server:warn,2025-05-15T18:46:47.109Z,ns_1@cb.local:<0.340.0>:ns_ssl_services_setup:notify_service:1184]Failed to notify service capi_ssl_service: {'EXIT',{error,{badrpc,nodedown}}}
[ns_server:warn,2025-05-15T18:46:47.113Z,ns_1@cb.local:<0.343.0>:ns_ssl_services_setup:notify_service:1184]Failed to notify service memcached: {error,no_proccess}
[ns_server:info,2025-05-15T18:46:47.130Z,ns_1@cb.local:<0.330.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2025-05-15T18:46:47.130Z,ns_1@cb.local:<0.330.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2025-05-15T18:46:47.130Z,ns_1@cb.local:<0.330.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2025-05-15T18:46:47.131Z,ns_1@cb.local:<0.330.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2025-05-15T18:46:47.131Z,ns_1@cb.local:<0.330.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[error_logger:info,2025-05-15T18:46:47.141Z,ns_1@cb.local:kernel_sup<0.49.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,kernel_sup}
    started: [{pid,<0.349.0>},
              {id,timer_server},
              {mfargs,{timer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:47.166Z,ns_1@cb.local:<0.330.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"0.0.0.0"},
 [{keyfile,"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem"},
  {certfile,"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem"},
  {versions,['tlsv1.2','tlsv1.3']},
  {cacerts,[<<48,130,3,12,48,130,1,244,160,3,2,1,2,2,8,24,63,199,191,182,93,
              192,235,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,
              48,32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,
              101,114,118,101,114,32,99,52,97,102,102,53,54,99,48,30,23,13,
              49,51,48,49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,
              50,51,53,57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,
              117,99,104,98,97,115,101,32,83,101,114,118,101,114,32,99,52,97,
              102,102,53,54,99,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,
              1,1,5,0,3,130,1,15,0,48,130,1,10,2,130,1,1,0,239,19,67,248,151,
              143,22,134,60,20,252,195,19,142,4,94,24,128,145,66,235,25,185,
              253,32,216,42,53,64,174,109,97,14,189,148,46,201,245,230,24,
              160,205,44,156,126,108,51,130,64,199,112,211,29,150,55,114,212,
              87,115,111,171,91,211,209,4,80,11,4,193,158,105,110,58,22,11,
              11,233,217,15,75,246,230,250,194,222,10,140,189,52,107,169,72,
              160,110,117,228,32,232,48,150,30,69,209,54,212,98,17,225,175,
              64,186,132,144,132,127,38,143,177,100,233,245,49,196,8,82,86,
              55,48,149,219,162,224,146,96,81,40,160,101,55,23,220,164,17,
              136,135,153,40,106,119,177,8,162,177,146,159,161,247,38,51,209,
              50,246,203,9,187,138,173,122,229,135,29,19,58,163,54,97,183,
              240,75,136,100,238,206,152,105,50,38,52,63,211,25,147,70,69,3,
              151,146,206,165,213,48,190,208,106,173,123,128,168,37,158,43,
              24,2,18,14,74,129,208,86,132,29,51,18,243,241,188,209,186,130,
              39,39,211,90,112,68,219,123,222,17,126,105,63,125,240,25,157,
              62,125,104,250,158,67,245,97,24,43,2,3,1,0,1,163,66,48,64,48,
              14,6,3,85,29,15,1,1,255,4,4,3,2,1,134,48,15,6,3,85,29,19,1,1,
              255,4,5,48,3,1,1,255,48,29,6,3,85,29,14,4,22,4,20,195,20,145,
              17,76,20,39,101,25,197,156,185,193,171,223,93,10,165,210,191,
              48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,120,203,
              72,57,92,116,122,174,212,40,240,208,113,77,217,197,208,114,232,
              156,132,111,84,12,212,179,98,155,89,207,33,189,199,14,30,113,
              136,47,23,96,202,61,115,9,135,46,202,177,176,224,133,194,238,
              37,246,69,184,170,79,48,4,28,153,121,118,29,9,128,228,245,135,
              65,24,126,23,90,111,189,19,189,130,176,154,245,134,33,87,178,
              238,156,112,206,37,154,23,125,253,153,150,189,114,6,53,194,121,
              143,243,60,171,251,95,79,137,161,179,148,50,193,0,161,1,69,205,
              230,159,122,203,205,22,158,229,163,8,38,49,224,246,14,114,59,
              205,240,98,222,168,36,160,100,180,134,22,114,187,56,241,110,48,
              179,195,1,230,193,101,186,176,175,39,182,136,205,54,99,251,226,
              134,82,229,254,48,251,1,211,72,32,51,34,200,58,88,252,44,60,81,
              95,87,50,39,254,172,242,96,187,59,51,186,47,75,33,233,8,189,
              159,111,26,95,104,52,53,170,228,89,207,253,109,140,167,146,122,
              230,240,11,34,21,166,171,118,81,188,248,27,44,157,72,200,83,85,
              132,118,65,231,178,134,240,116,169,232>>]},
  {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,
        10,118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,
        158,232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,
        66,211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,
        250,145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,
        104,159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,
        246,169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,
        110,167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,
        118,190,67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,
        74,8,205,174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,
        221,95,184,110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,
        76,187,66,211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,
        69,254,147,103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,
        202,133,173,72,6,69,167,89,112,174,40,229,171,2,1,2>>},
  {ciphers,[#{cipher => aes_256_gcm,key_exchange => any,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm_8,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_ecdsa,
              mac => aead,prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_rsa,
              mac => aead,prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_128_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf},
            #{cipher => aes_128_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf}]},
  {honor_cipher_order,true},
  {secure_renegotiate,true},
  {client_renegotiation,false},
  {password,"********"}],
 {ssl,true},
 {port,18091}]
[error_logger:info,2025-05-15T18:46:47.169Z,ns_1@cb.local:<0.330.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.330.0>,menelaus_web}
    started: [{pid,<0.350.0>},
              {id,menelaus_web_ipv4},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet},
                        {ssl,true},
                        {name,menelaus_web_ssl},
                        {ssl_opts_fun,#Fun<ns_ssl_services_setup.11.39330155>},
                        {port,18091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:47.170Z,ns_1@cb.local:<0.330.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2025-05-15T18:46:47.170Z,ns_1@cb.local:<0.330.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2025-05-15T18:46:47.171Z,ns_1@cb.local:<0.330.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2025-05-15T18:46:47.171Z,ns_1@cb.local:<0.330.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2025-05-15T18:46:47.171Z,ns_1@cb.local:<0.330.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2025-05-15T18:46:47.173Z,ns_1@cb.local:<0.330.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"::"},
 [{keyfile,"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem"},
  {certfile,"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem"},
  {versions,['tlsv1.2','tlsv1.3']},
  {cacerts,[<<48,130,3,12,48,130,1,244,160,3,2,1,2,2,8,24,63,199,191,182,93,
              192,235,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,
              48,32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,
              101,114,118,101,114,32,99,52,97,102,102,53,54,99,48,30,23,13,
              49,51,48,49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,
              50,51,53,57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,
              117,99,104,98,97,115,101,32,83,101,114,118,101,114,32,99,52,97,
              102,102,53,54,99,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,
              1,1,5,0,3,130,1,15,0,48,130,1,10,2,130,1,1,0,239,19,67,248,151,
              143,22,134,60,20,252,195,19,142,4,94,24,128,145,66,235,25,185,
              253,32,216,42,53,64,174,109,97,14,189,148,46,201,245,230,24,
              160,205,44,156,126,108,51,130,64,199,112,211,29,150,55,114,212,
              87,115,111,171,91,211,209,4,80,11,4,193,158,105,110,58,22,11,
              11,233,217,15,75,246,230,250,194,222,10,140,189,52,107,169,72,
              160,110,117,228,32,232,48,150,30,69,209,54,212,98,17,225,175,
              64,186,132,144,132,127,38,143,177,100,233,245,49,196,8,82,86,
              55,48,149,219,162,224,146,96,81,40,160,101,55,23,220,164,17,
              136,135,153,40,106,119,177,8,162,177,146,159,161,247,38,51,209,
              50,246,203,9,187,138,173,122,229,135,29,19,58,163,54,97,183,
              240,75,136,100,238,206,152,105,50,38,52,63,211,25,147,70,69,3,
              151,146,206,165,213,48,190,208,106,173,123,128,168,37,158,43,
              24,2,18,14,74,129,208,86,132,29,51,18,243,241,188,209,186,130,
              39,39,211,90,112,68,219,123,222,17,126,105,63,125,240,25,157,
              62,125,104,250,158,67,245,97,24,43,2,3,1,0,1,163,66,48,64,48,
              14,6,3,85,29,15,1,1,255,4,4,3,2,1,134,48,15,6,3,85,29,19,1,1,
              255,4,5,48,3,1,1,255,48,29,6,3,85,29,14,4,22,4,20,195,20,145,
              17,76,20,39,101,25,197,156,185,193,171,223,93,10,165,210,191,
              48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,120,203,
              72,57,92,116,122,174,212,40,240,208,113,77,217,197,208,114,232,
              156,132,111,84,12,212,179,98,155,89,207,33,189,199,14,30,113,
              136,47,23,96,202,61,115,9,135,46,202,177,176,224,133,194,238,
              37,246,69,184,170,79,48,4,28,153,121,118,29,9,128,228,245,135,
              65,24,126,23,90,111,189,19,189,130,176,154,245,134,33,87,178,
              238,156,112,206,37,154,23,125,253,153,150,189,114,6,53,194,121,
              143,243,60,171,251,95,79,137,161,179,148,50,193,0,161,1,69,205,
              230,159,122,203,205,22,158,229,163,8,38,49,224,246,14,114,59,
              205,240,98,222,168,36,160,100,180,134,22,114,187,56,241,110,48,
              179,195,1,230,193,101,186,176,175,39,182,136,205,54,99,251,226,
              134,82,229,254,48,251,1,211,72,32,51,34,200,58,88,252,44,60,81,
              95,87,50,39,254,172,242,96,187,59,51,186,47,75,33,233,8,189,
              159,111,26,95,104,52,53,170,228,89,207,253,109,140,167,146,122,
              230,240,11,34,21,166,171,118,81,188,248,27,44,157,72,200,83,85,
              132,118,65,231,178,134,240,116,169,232>>]},
  {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,
        10,118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,
        158,232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,
        66,211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,
        250,145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,
        104,159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,
        246,169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,
        110,167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,
        118,190,67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,
        74,8,205,174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,
        221,95,184,110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,
        76,187,66,211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,
        69,254,147,103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,
        202,133,173,72,6,69,167,89,112,174,40,229,171,2,1,2>>},
  {ciphers,[#{cipher => aes_256_gcm,key_exchange => any,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm_8,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_ecdsa,
              mac => aead,prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_rsa,
              mac => aead,prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_128_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf},
            #{cipher => aes_128_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf}]},
  {honor_cipher_order,true},
  {secure_renegotiate,true},
  {client_renegotiation,false},
  {password,"********"}],
 {ssl,true},
 {port,18091}]
[error_logger:info,2025-05-15T18:46:47.176Z,ns_1@cb.local:<0.330.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.330.0>,menelaus_web}
    started: [{pid,<0.369.0>},
              {id,menelaus_web_ipv6},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet6},
                        {ssl,true},
                        {name,menelaus_web_ssl},
                        {ssl_opts_fun,#Fun<ns_ssl_services_setup.11.39330155>},
                        {port,18091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:47.177Z,ns_1@cb.local:<0.327.0>:restartable:start_child:92]Started child process <0.330.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
[error_logger:info,2025-05-15T18:46:47.177Z,ns_1@cb.local:ns_ssl_services_sup<0.306.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_ssl_services_sup}
    started: [{pid,<0.327.0>},
              {id,ns_rest_ssl_service},
              {mfargs,
                  {restartable,start_link,
                      [{ns_ssl_services_setup,start_link_rest_service,[]},
                       1000]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:47.177Z,ns_1@cb.local:<0.327.0>:restartable:loop:65]Restarting child <0.330.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
  Shutdown policy: 1000
  Caller: {<0.345.0>,#Ref<0.3529285563.3650617346.3031>}
[error_logger:info,2025-05-15T18:46:47.177Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.306.0>},
              {id,ns_ssl_services_sup},
              {mfargs,{ns_ssl_services_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:47.179Z,ns_1@cb.local:<0.327.0>:restartable:shutdown_child:114]Successfully terminated process <0.330.0>
[ns_server:info,2025-05-15T18:46:47.180Z,ns_1@cb.local:<0.388.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2025-05-15T18:46:47.181Z,ns_1@cb.local:<0.388.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2025-05-15T18:46:47.181Z,ns_1@cb.local:<0.388.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2025-05-15T18:46:47.181Z,ns_1@cb.local:<0.388.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2025-05-15T18:46:47.181Z,ns_1@cb.local:<0.388.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2025-05-15T18:46:47.183Z,ns_1@cb.local:<0.388.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"0.0.0.0"},
 [{keyfile,"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem"},
  {certfile,"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem"},
  {versions,['tlsv1.2','tlsv1.3']},
  {cacerts,[<<48,130,3,12,48,130,1,244,160,3,2,1,2,2,8,24,63,199,191,182,93,
              192,235,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,
              48,32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,
              101,114,118,101,114,32,99,52,97,102,102,53,54,99,48,30,23,13,
              49,51,48,49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,
              50,51,53,57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,
              117,99,104,98,97,115,101,32,83,101,114,118,101,114,32,99,52,97,
              102,102,53,54,99,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,
              1,1,5,0,3,130,1,15,0,48,130,1,10,2,130,1,1,0,239,19,67,248,151,
              143,22,134,60,20,252,195,19,142,4,94,24,128,145,66,235,25,185,
              253,32,216,42,53,64,174,109,97,14,189,148,46,201,245,230,24,
              160,205,44,156,126,108,51,130,64,199,112,211,29,150,55,114,212,
              87,115,111,171,91,211,209,4,80,11,4,193,158,105,110,58,22,11,
              11,233,217,15,75,246,230,250,194,222,10,140,189,52,107,169,72,
              160,110,117,228,32,232,48,150,30,69,209,54,212,98,17,225,175,
              64,186,132,144,132,127,38,143,177,100,233,245,49,196,8,82,86,
              55,48,149,219,162,224,146,96,81,40,160,101,55,23,220,164,17,
              136,135,153,40,106,119,177,8,162,177,146,159,161,247,38,51,209,
              50,246,203,9,187,138,173,122,229,135,29,19,58,163,54,97,183,
              240,75,136,100,238,206,152,105,50,38,52,63,211,25,147,70,69,3,
              151,146,206,165,213,48,190,208,106,173,123,128,168,37,158,43,
              24,2,18,14,74,129,208,86,132,29,51,18,243,241,188,209,186,130,
              39,39,211,90,112,68,219,123,222,17,126,105,63,125,240,25,157,
              62,125,104,250,158,67,245,97,24,43,2,3,1,0,1,163,66,48,64,48,
              14,6,3,85,29,15,1,1,255,4,4,3,2,1,134,48,15,6,3,85,29,19,1,1,
              255,4,5,48,3,1,1,255,48,29,6,3,85,29,14,4,22,4,20,195,20,145,
              17,76,20,39,101,25,197,156,185,193,171,223,93,10,165,210,191,
              48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,120,203,
              72,57,92,116,122,174,212,40,240,208,113,77,217,197,208,114,232,
              156,132,111,84,12,212,179,98,155,89,207,33,189,199,14,30,113,
              136,47,23,96,202,61,115,9,135,46,202,177,176,224,133,194,238,
              37,246,69,184,170,79,48,4,28,153,121,118,29,9,128,228,245,135,
              65,24,126,23,90,111,189,19,189,130,176,154,245,134,33,87,178,
              238,156,112,206,37,154,23,125,253,153,150,189,114,6,53,194,121,
              143,243,60,171,251,95,79,137,161,179,148,50,193,0,161,1,69,205,
              230,159,122,203,205,22,158,229,163,8,38,49,224,246,14,114,59,
              205,240,98,222,168,36,160,100,180,134,22,114,187,56,241,110,48,
              179,195,1,230,193,101,186,176,175,39,182,136,205,54,99,251,226,
              134,82,229,254,48,251,1,211,72,32,51,34,200,58,88,252,44,60,81,
              95,87,50,39,254,172,242,96,187,59,51,186,47,75,33,233,8,189,
              159,111,26,95,104,52,53,170,228,89,207,253,109,140,167,146,122,
              230,240,11,34,21,166,171,118,81,188,248,27,44,157,72,200,83,85,
              132,118,65,231,178,134,240,116,169,232>>]},
  {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,
        10,118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,
        158,232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,
        66,211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,
        250,145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,
        104,159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,
        246,169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,
        110,167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,
        118,190,67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,
        74,8,205,174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,
        221,95,184,110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,
        76,187,66,211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,
        69,254,147,103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,
        202,133,173,72,6,69,167,89,112,174,40,229,171,2,1,2>>},
  {ciphers,[#{cipher => aes_256_gcm,key_exchange => any,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm_8,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_ecdsa,
              mac => aead,prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_rsa,
              mac => aead,prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_128_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf},
            #{cipher => aes_128_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf}]},
  {honor_cipher_order,true},
  {secure_renegotiate,true},
  {client_renegotiation,false},
  {password,"********"}],
 {ssl,true},
 {port,18091}]
[error_logger:info,2025-05-15T18:46:47.184Z,ns_1@cb.local:<0.388.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.388.0>,menelaus_web}
    started: [{pid,<0.389.0>},
              {id,menelaus_web_ipv4},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet},
                        {ssl,true},
                        {name,menelaus_web_ssl},
                        {ssl_opts_fun,#Fun<ns_ssl_services_setup.11.39330155>},
                        {port,18091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:47.186Z,ns_1@cb.local:<0.388.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2025-05-15T18:46:47.187Z,ns_1@cb.local:<0.388.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2025-05-15T18:46:47.187Z,ns_1@cb.local:<0.388.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2025-05-15T18:46:47.188Z,ns_1@cb.local:<0.388.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[error_logger:info,2025-05-15T18:46:47.188Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.408.0>},
              {id,ldap_auth_cache},
              {mfargs,{ldap_auth_cache,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:47.188Z,ns_1@cb.local:<0.388.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2025-05-15T18:46:47.189Z,ns_1@cb.local:<0.388.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"::"},
 [{keyfile,"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem"},
  {certfile,"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem"},
  {versions,['tlsv1.2','tlsv1.3']},
  {cacerts,[<<48,130,3,12,48,130,1,244,160,3,2,1,2,2,8,24,63,199,191,182,93,
              192,235,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,
              48,32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,
              101,114,118,101,114,32,99,52,97,102,102,53,54,99,48,30,23,13,
              49,51,48,49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,
              50,51,53,57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,
              117,99,104,98,97,115,101,32,83,101,114,118,101,114,32,99,52,97,
              102,102,53,54,99,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,
              1,1,5,0,3,130,1,15,0,48,130,1,10,2,130,1,1,0,239,19,67,248,151,
              143,22,134,60,20,252,195,19,142,4,94,24,128,145,66,235,25,185,
              253,32,216,42,53,64,174,109,97,14,189,148,46,201,245,230,24,
              160,205,44,156,126,108,51,130,64,199,112,211,29,150,55,114,212,
              87,115,111,171,91,211,209,4,80,11,4,193,158,105,110,58,22,11,
              11,233,217,15,75,246,230,250,194,222,10,140,189,52,107,169,72,
              160,110,117,228,32,232,48,150,30,69,209,54,212,98,17,225,175,
              64,186,132,144,132,127,38,143,177,100,233,245,49,196,8,82,86,
              55,48,149,219,162,224,146,96,81,40,160,101,55,23,220,164,17,
              136,135,153,40,106,119,177,8,162,177,146,159,161,247,38,51,209,
              50,246,203,9,187,138,173,122,229,135,29,19,58,163,54,97,183,
              240,75,136,100,238,206,152,105,50,38,52,63,211,25,147,70,69,3,
              151,146,206,165,213,48,190,208,106,173,123,128,168,37,158,43,
              24,2,18,14,74,129,208,86,132,29,51,18,243,241,188,209,186,130,
              39,39,211,90,112,68,219,123,222,17,126,105,63,125,240,25,157,
              62,125,104,250,158,67,245,97,24,43,2,3,1,0,1,163,66,48,64,48,
              14,6,3,85,29,15,1,1,255,4,4,3,2,1,134,48,15,6,3,85,29,19,1,1,
              255,4,5,48,3,1,1,255,48,29,6,3,85,29,14,4,22,4,20,195,20,145,
              17,76,20,39,101,25,197,156,185,193,171,223,93,10,165,210,191,
              48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,120,203,
              72,57,92,116,122,174,212,40,240,208,113,77,217,197,208,114,232,
              156,132,111,84,12,212,179,98,155,89,207,33,189,199,14,30,113,
              136,47,23,96,202,61,115,9,135,46,202,177,176,224,133,194,238,
              37,246,69,184,170,79,48,4,28,153,121,118,29,9,128,228,245,135,
              65,24,126,23,90,111,189,19,189,130,176,154,245,134,33,87,178,
              238,156,112,206,37,154,23,125,253,153,150,189,114,6,53,194,121,
              143,243,60,171,251,95,79,137,161,179,148,50,193,0,161,1,69,205,
              230,159,122,203,205,22,158,229,163,8,38,49,224,246,14,114,59,
              205,240,98,222,168,36,160,100,180,134,22,114,187,56,241,110,48,
              179,195,1,230,193,101,186,176,175,39,182,136,205,54,99,251,226,
              134,82,229,254,48,251,1,211,72,32,51,34,200,58,88,252,44,60,81,
              95,87,50,39,254,172,242,96,187,59,51,186,47,75,33,233,8,189,
              159,111,26,95,104,52,53,170,228,89,207,253,109,140,167,146,122,
              230,240,11,34,21,166,171,118,81,188,248,27,44,157,72,200,83,85,
              132,118,65,231,178,134,240,116,169,232>>]},
  {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,
        10,118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,
        158,232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,
        66,211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,
        250,145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,
        104,159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,
        246,169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,
        110,167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,
        118,190,67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,
        74,8,205,174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,
        221,95,184,110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,
        76,187,66,211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,
        69,254,147,103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,
        202,133,173,72,6,69,167,89,112,174,40,229,171,2,1,2>>},
  {ciphers,[#{cipher => aes_256_gcm,key_exchange => any,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm_8,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_ecdsa,
              mac => aead,prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_rsa,
              mac => aead,prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_128_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf},
            #{cipher => aes_128_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf}]},
  {honor_cipher_order,true},
  {secure_renegotiate,true},
  {client_renegotiation,false},
  {password,"********"}],
 {ssl,true},
 {port,18091}]
[error_logger:info,2025-05-15T18:46:47.195Z,ns_1@cb.local:<0.388.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.388.0>,menelaus_web}
    started: [{pid,<0.410.0>},
              {id,menelaus_web_ipv6},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet6},
                        {ssl,true},
                        {name,menelaus_web_ssl},
                        {ssl_opts_fun,#Fun<ns_ssl_services_setup.11.39330155>},
                        {port,18091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:47.196Z,ns_1@cb.local:cb_saml<0.429.0>:cb_saml:handle_info:316]Settings have changed or this is the first start
[ns_server:debug,2025-05-15T18:46:47.196Z,ns_1@cb.local:cb_saml<0.429.0>:cb_saml:restart_refresh_timer:586]Restarting refresh timer: 6058 ms
[error_logger:info,2025-05-15T18:46:47.196Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.429.0>},
              {id,cb_saml},
              {mfargs,{cb_saml,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:47.196Z,ns_1@cb.local:<0.327.0>:restartable:start_child:92]Started child process <0.388.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
[ns_server:info,2025-05-15T18:46:47.197Z,ns_1@cb.local:<0.345.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service ssl_service
[ns_server:info,2025-05-15T18:46:47.197Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:notify_services:1162]Succesfully notified services [ssl_service,server_cert_event,
                               client_cert_event,cb_dist_tls]
[error_logger:info,2025-05-15T18:46:47.200Z,ns_1@cb.local:users_sup<0.431.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_sup}
    started: [{pid,<0.432.0>},
              {id,user_storage_events},
              {mfargs,{gen_event,start_link,[{local,user_storage_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:47.201Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:notify_services:1173]Failed to notify some services. Will retry in 5 sec, [{{error,no_proccess},
                                                       memcached},
                                                      {{'EXIT',
                                                        {error,
                                                         {badrpc,nodedown}}},
                                                       capi_ssl_service}]
[ns_server:debug,2025-05-15T18:46:47.202Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:restart_regenerate_client_cert_timer:1447]Time left before client cert regeneration: 70588799000
[ns_server:debug,2025-05-15T18:46:47.202Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:maybe_store_ca_certs:832]Considering to store CA certs
[ns_server:debug,2025-05-15T18:46:47.209Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:notify_services:1146]Going to notify following services: [capi_ssl_service,memcached]
[ns_server:warn,2025-05-15T18:46:47.209Z,ns_1@cb.local:<0.439.0>:ns_ssl_services_setup:notify_service:1184]Failed to notify service memcached: {error,no_proccess}
[error_logger:info,2025-05-15T18:46:47.209Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {15946782,#Ref<0.3529285563.3650748419.3237>}}}
[ns_server:debug,2025-05-15T18:46:47.209Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:46:47.209Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3529285563.3650617346.3066>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:46:47.209Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3529285563.3650617346.3066>,
                                  inet_tcp_dist,<0.441.0>,
                                  #Ref<0.3529285563.3650617346.3069>}
[error_logger:info,2025-05-15T18:46:47.210Z,ns_1@cb.local:users_storage_sup<0.433.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_storage_sup}
    started: [{pid,<0.443.0>},
              {id,users_replicator},
              {mfargs,{menelaus_users,start_replicator,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:47.210Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.441.0>,shutdown}}
[ns_server:debug,2025-05-15T18:46:47.210Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3529285563.3650617346.3066>,
                               inet_tcp_dist,<0.441.0>,
                               #Ref<0.3529285563.3650617346.3069>}
[error_logger:info,2025-05-15T18:46:47.210Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:46:47.210Z,ns_1@cb.local:<0.440.0>:ns_couchdb_api:rpc_couchdb_node:152]RPC to couchdb node failed for restart_capi_ssl_service with {badrpc,nodedown}
Stack: [{ns_couchdb_api,rpc_couchdb_node,4,
                        [{file,"src/ns_couchdb_api.erl"},{line,150}]},
        {ns_ssl_services_setup,do_notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1203}]},
        {ns_ssl_services_setup,notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1179}]},
        {async,'-async_init/4-fun-1-',3,[{file,"src/async.erl"},{line,199}]}]
[ns_server:warn,2025-05-15T18:46:47.210Z,ns_1@cb.local:<0.440.0>:ns_ssl_services_setup:notify_service:1184]Failed to notify service capi_ssl_service: {'EXIT',{error,{badrpc,nodedown}}}
[ns_server:debug,2025-05-15T18:46:47.213Z,ns_1@cb.local:users_replicator<0.443.0>:replicated_storage:wait_for_startup:47]Start waiting for startup
[ns_server:info,2025-05-15T18:46:47.214Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:notify_services:1173]Failed to notify some services. Will retry in 5 sec, [{{error,no_proccess},
                                                       memcached},
                                                      {{'EXIT',
                                                        {error,
                                                         {badrpc,nodedown}}},
                                                       capi_ssl_service}]
[ns_server:debug,2025-05-15T18:46:47.214Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:restart_regenerate_client_cert_timer:1447]Time left before client cert regeneration: 70588799000
[ns_server:info,2025-05-15T18:46:47.215Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:validate_pkey:1032]Private key (node_cert) passphrase validation suceeded
[ns_server:info,2025-05-15T18:46:47.216Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:validate_pkey:1032]Private key (client_cert) passphrase validation suceeded
[ns_server:debug,2025-05-15T18:46:47.216Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:notify_services:1146]Going to notify following services: [memcached,capi_ssl_service]
[ns_server:debug,2025-05-15T18:46:47.216Z,ns_1@cb.local:users_storage<0.444.0>:replicated_storage:announce_startup:61]Announce my startup to <0.443.0>
[ns_server:warn,2025-05-15T18:46:47.216Z,ns_1@cb.local:<0.450.0>:ns_ssl_services_setup:notify_service:1184]Failed to notify service memcached: {error,no_proccess}
[ns_server:debug,2025-05-15T18:46:47.216Z,ns_1@cb.local:users_replicator<0.443.0>:replicated_storage:wait_for_startup:50]Received replicated storage registration from <0.444.0>
[ns_server:debug,2025-05-15T18:46:47.216Z,ns_1@cb.local:users_storage<0.444.0>:replicated_dets:open:177]Opening file "/opt/couchbase/var/lib/couchbase/config/users.dets"
[error_logger:info,2025-05-15T18:46:47.216Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {15946783,#Ref<0.3529285563.3650748419.3237>}}}
[error_logger:info,2025-05-15T18:46:47.216Z,ns_1@cb.local:users_storage_sup<0.433.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_storage_sup}
    started: [{pid,<0.444.0>},
              {id,users_storage},
              {mfargs,{menelaus_users,start_storage,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:47.216Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2025-05-15T18:46:47.216Z,ns_1@cb.local:users_sup<0.431.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_sup}
    started: [{pid,<0.433.0>},
              {id,users_storage_sup},
              {mfargs,{users_storage_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:47.216Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3529285563.3650617345.4413>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:46:47.216Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3529285563.3650617345.4413>,
                                  inet_tcp_dist,<0.453.0>,
                                  #Ref<0.3529285563.3650617345.4416>}
[error_logger:info,2025-05-15T18:46:47.217Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.453.0>,shutdown}}
[ns_server:debug,2025-05-15T18:46:47.217Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3529285563.3650617345.4413>,
                               inet_tcp_dist,<0.453.0>,
                               #Ref<0.3529285563.3650617345.4416>}
[error_logger:info,2025-05-15T18:46:47.217Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:46:47.217Z,ns_1@cb.local:<0.451.0>:ns_couchdb_api:rpc_couchdb_node:152]RPC to couchdb node failed for restart_capi_ssl_service with {badrpc,nodedown}
Stack: [{ns_couchdb_api,rpc_couchdb_node,4,
                        [{file,"src/ns_couchdb_api.erl"},{line,150}]},
        {ns_ssl_services_setup,do_notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1203}]},
        {ns_ssl_services_setup,notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1179}]},
        {async,'-async_init/4-fun-1-',3,[{file,"src/async.erl"},{line,199}]}]
[ns_server:warn,2025-05-15T18:46:47.217Z,ns_1@cb.local:<0.451.0>:ns_ssl_services_setup:notify_service:1184]Failed to notify service capi_ssl_service: {'EXIT',{error,{badrpc,nodedown}}}
[ns_server:info,2025-05-15T18:46:47.219Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:notify_services:1173]Failed to notify some services. Will retry in 5 sec, [{{'EXIT',
                                                        {error,
                                                         {badrpc,nodedown}}},
                                                       capi_ssl_service},
                                                      {{error,no_proccess},
                                                       memcached}]
[ns_server:debug,2025-05-15T18:46:47.222Z,ns_1@cb.local:compiled_roles_cache<0.455.0>:versioned_cache:init:41]Starting versioned cache compiled_roles_cache
[error_logger:info,2025-05-15T18:46:47.223Z,ns_1@cb.local:users_sup<0.431.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_sup}
    started: [{pid,<0.455.0>},
              {id,compiled_roles_cache},
              {mfargs,{menelaus_roles,start_compiled_roles_cache,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:47.227Z,ns_1@cb.local:users_sup<0.431.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_sup}
    started: [{pid,<0.458.0>},
              {id,roles_cache},
              {mfargs,{roles_cache,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:47.227Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.431.0>},
              {id,users_sup},
              {mfargs,{users_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:47.229Z,ns_1@cb.local:kernel_safe_sup<0.67.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,kernel_safe_sup}
    started: [{pid,<0.462.0>},
              {id,dets_sup},
              {mfargs,{dets_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:47.229Z,ns_1@cb.local:kernel_safe_sup<0.67.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,kernel_safe_sup}
    started: [{pid,<0.463.0>},
              {id,dets},
              {mfargs,{dets_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,2000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:47.242Z,ns_1@cb.local:<0.468.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,
                                    start_couchdb_node} with delay of 5000
[ns_server:debug,2025-05-15T18:46:47.242Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',cbas_dirs} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554007}}]},
 "/opt/couchbase/var/lib/couchbase/data"]
[ns_server:debug,2025-05-15T18:46:47.242Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',eventing_dir} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554007}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2025-05-15T18:46:47.251Z,ns_1@cb.local:users_storage<0.444.0>:replicated_dets:select_from_table:312][dets] Starting select with {users_storage,
                                [{{docv2,'_','_','_'},[],['$_']}],
                                100}
[ns_server:debug,2025-05-15T18:46:47.251Z,ns_1@cb.local:users_storage<0.444.0>:replicated_dets:init_after_ack:170]Loading 0 items, 307 words took 35ms
[error_logger:info,2025-05-15T18:46:47.260Z,ns_1@cb.local:<0.469.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.469.0>,suppress_max_restart_intensity}
    started: [{pid,<0.471.0>},
              {id,start_couchdb_node},
              {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,86400000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:47.261Z,ns_1@cb.local:<0.467.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.467.0>,suppress_max_restart_intensity}
    started: [{pid,<0.468.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      start_couchdb_node}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,start_couchdb_node},
                       5000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 5,id => start_couchdb_node,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [],restart => permanent,
                          shutdown => 86400000,
                          start => {ns_server_nodes_sup,start_couchdb_node,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:47.261Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.467.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,start_couchdb_node}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 5,id => start_couchdb_node,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [],restart => permanent,
                         shutdown => 86400000,
                         start => {ns_server_nodes_sup,start_couchdb_node,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:47.261Z,ns_1@cb.local:wait_link_to_couchdb_node<0.472.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:158]Waiting for ns_couchdb node to start
[error_logger:info,2025-05-15T18:46:47.261Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {15946784,#Ref<0.3529285563.3650748419.3237>}}}
[ns_server:debug,2025-05-15T18:46:47.261Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:46:47.261Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3529285563.3650617347.3451>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:46:47.261Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3529285563.3650617347.3451>,
                                  inet_tcp_dist,<0.474.0>,
                                  #Ref<0.3529285563.3650617347.3454>}
[ns_server:debug,2025-05-15T18:46:47.262Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3529285563.3650617347.3451>,
                               inet_tcp_dist,<0.474.0>,
                               #Ref<0.3529285563.3650617347.3454>}
[error_logger:info,2025-05-15T18:46:47.262Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.474.0>,shutdown}}
[error_logger:info,2025-05-15T18:46:47.262Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:46:47.262Z,ns_1@cb.local:<0.473.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-05-15T18:46:47.467Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {15946785,#Ref<0.3529285563.3650748419.3237>}}}
[ns_server:debug,2025-05-15T18:46:47.467Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:46:47.468Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3529285563.3650617345.4434>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:46:47.468Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3529285563.3650617345.4434>,
                                  inet_tcp_dist,<0.476.0>,
                                  #Ref<0.3529285563.3650617345.4437>}
[error_logger:info,2025-05-15T18:46:47.468Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.476.0>,shutdown}}
[ns_server:debug,2025-05-15T18:46:47.468Z,ns_1@cb.local:<0.473.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2025-05-15T18:46:47.468Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3529285563.3650617345.4434>,
                               inet_tcp_dist,<0.476.0>,
                               #Ref<0.3529285563.3650617345.4437>}
[error_logger:info,2025-05-15T18:46:47.468Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[error_logger:info,2025-05-15T18:46:47.670Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {15946786,#Ref<0.3529285563.3650748419.3237>}}}
[ns_server:debug,2025-05-15T18:46:47.671Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:46:47.671Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3529285563.3650617349.3012>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:46:47.671Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3529285563.3650617349.3012>,
                                  inet_tcp_dist,<0.478.0>,
                                  #Ref<0.3529285563.3650617345.4444>}
[ns_server:debug,2025-05-15T18:46:47.712Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3529285563.3650617349.3012>,
                               inet_tcp_dist,<0.478.0>,
                               #Ref<0.3529285563.3650617345.4444>}
[error_logger:info,2025-05-15T18:46:47.712Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.478.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2025-05-15T18:46:47.712Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:46:47.712Z,ns_1@cb.local:<0.473.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-05-15T18:46:47.913Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {15946787,#Ref<0.3529285563.3650748419.3237>}}}
[ns_server:debug,2025-05-15T18:46:47.914Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:46:47.914Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3529285563.3650617349.3023>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:46:47.914Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3529285563.3650617349.3023>,
                                  inet_tcp_dist,<0.480.0>,
                                  #Ref<0.3529285563.3650617349.3026>}
[ns_server:info,2025-05-15T18:46:47.939Z,ns_1@cb.local:ns_couchdb_port<0.471.0>:ns_port_server:log:226]ns_couchdb<0.471.0>: =ERROR REPORT==== 15-May-2025::18:46:47.711987 ===
ns_couchdb<0.471.0>: ** Connection attempt to/from node 'ns_1@cb.local' rejected. Cookie is not set. **
ns_couchdb<0.471.0>: 

[error_logger:info,2025-05-15T18:46:47.958Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.480.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2025-05-15T18:46:47.958Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:46:47.958Z,ns_1@cb.local:<0.473.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2025-05-15T18:46:47.958Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3529285563.3650617349.3023>,
                               inet_tcp_dist,<0.480.0>,
                               #Ref<0.3529285563.3650617349.3026>}
[error_logger:info,2025-05-15T18:46:48.159Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {15946788,#Ref<0.3529285563.3650748419.3237>}}}
[ns_server:info,2025-05-15T18:46:48.159Z,ns_1@cb.local:ns_couchdb_port<0.471.0>:ns_port_server:log:226]ns_couchdb<0.471.0>: =ERROR REPORT==== 15-May-2025::18:46:47.957793 ===
ns_couchdb<0.471.0>: ** Connection attempt to/from node 'ns_1@cb.local' rejected. Cookie is not set. **
ns_couchdb<0.471.0>: 

[ns_server:debug,2025-05-15T18:46:48.159Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:46:48.160Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3529285563.3650617347.3462>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:46:48.160Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3529285563.3650617347.3462>,
                                  inet_tcp_dist,<0.482.0>,
                                  #Ref<0.3529285563.3650617347.3465>}
[error_logger:info,2025-05-15T18:46:48.168Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.482.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2025-05-15T18:46:48.168Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:46:48.168Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3529285563.3650617347.3462>,
                               inet_tcp_dist,<0.482.0>,
                               #Ref<0.3529285563.3650617347.3465>}
[ns_server:debug,2025-05-15T18:46:48.168Z,ns_1@cb.local:<0.473.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-05-15T18:46:48.373Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {15946789,#Ref<0.3529285563.3650748419.3237>}}}
[ns_server:debug,2025-05-15T18:46:48.373Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:46:48.374Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3529285563.3650617346.3148>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:46:48.374Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3529285563.3650617346.3148>,
                                  inet_tcp_dist,<0.484.0>,
                                  #Ref<0.3529285563.3650617346.3151>}
[error_logger:info,2025-05-15T18:46:48.381Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.484.0>,{recv_challenge_failed,{error,closed}}}}
[ns_server:debug,2025-05-15T18:46:48.381Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3529285563.3650617346.3148>,
                               inet_tcp_dist,<0.484.0>,
                               #Ref<0.3529285563.3650617346.3151>}
[ns_server:debug,2025-05-15T18:46:48.381Z,ns_1@cb.local:<0.473.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-05-15T18:46:48.381Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[error_logger:info,2025-05-15T18:46:48.582Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {15946790,#Ref<0.3529285563.3650748419.3237>}}}
[ns_server:debug,2025-05-15T18:46:48.582Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:46:48.583Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3529285563.3650617345.4465>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:46:48.583Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3529285563.3650617345.4465>,
                                  inet_tcp_dist,<0.486.0>,
                                  #Ref<0.3529285563.3650617345.4468>}
[error_logger:info,2025-05-15T18:46:48.583Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.486.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2025-05-15T18:46:48.584Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:46:48.584Z,ns_1@cb.local:<0.473.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2025-05-15T18:46:48.584Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3529285563.3650617345.4465>,
                               inet_tcp_dist,<0.486.0>,
                               #Ref<0.3529285563.3650617345.4468>}
[error_logger:info,2025-05-15T18:46:48.785Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {15946791,#Ref<0.3529285563.3650748419.3237>}}}
[ns_server:debug,2025-05-15T18:46:48.785Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:46:48.786Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3529285563.3650617349.3042>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:46:48.786Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3529285563.3650617349.3042>,
                                  inet_tcp_dist,<0.488.0>,
                                  #Ref<0.3529285563.3650617348.3747>}
[error_logger:info,2025-05-15T18:46:48.790Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.488.0>,{recv_challenge_failed,{error,closed}}}}
[ns_server:debug,2025-05-15T18:46:48.790Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3529285563.3650617349.3042>,
                               inet_tcp_dist,<0.488.0>,
                               #Ref<0.3529285563.3650617348.3747>}
[error_logger:info,2025-05-15T18:46:48.790Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:46:48.790Z,ns_1@cb.local:<0.473.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-05-15T18:46:48.992Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {15946792,#Ref<0.3529285563.3650748419.3237>}}}
[ns_server:debug,2025-05-15T18:46:48.993Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:46:48.993Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3529285563.3650617348.3755>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:46:48.993Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3529285563.3650617348.3755>,
                                  inet_tcp_dist,<0.490.0>,
                                  #Ref<0.3529285563.3650617348.3757>}
[error_logger:info,2025-05-15T18:46:48.993Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.490.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2025-05-15T18:46:48.994Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:46:48.994Z,ns_1@cb.local:<0.473.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2025-05-15T18:46:48.994Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3529285563.3650617348.3755>,
                               inet_tcp_dist,<0.490.0>,
                               #Ref<0.3529285563.3650617348.3757>}
[error_logger:info,2025-05-15T18:46:49.195Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {15946793,#Ref<0.3529285563.3650748419.3237>}}}
[ns_server:debug,2025-05-15T18:46:49.196Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:46:49.196Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3529285563.3650617348.3769>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:46:49.196Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3529285563.3650617348.3769>,
                                  inet_tcp_dist,<0.492.0>,
                                  #Ref<0.3529285563.3650617348.3772>}
[ns_server:debug,2025-05-15T18:46:49.200Z,ns_1@cb.local:<0.473.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: false
[ns_server:info,2025-05-15T18:46:49.654Z,ns_1@cb.local:ns_couchdb_port<0.471.0>:ns_port_server:log:226]ns_couchdb<0.471.0>: Apache CouchDB v4.5.1-330-g3e5b8f24 (LogLevel=info) is starting.
ns_couchdb<0.471.0>: Apache CouchDB has started. Time to relax.

[error_logger:info,2025-05-15T18:46:49.764Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.472.0>},
              {id,wait_for_couchdb_node},
              {mfargs,{erlang,apply,
                              [#Fun<ns_server_nodes_sup.0.120652322>,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:49.768Z,ns_1@cb.local:<0.498.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,ns_disksup} with delay of 4000
[error_logger:info,2025-05-15T18:46:49.773Z,ns_1@cb.local:<0.499.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.499.0>,suppress_max_restart_intensity}
    started: [{pid,<0.500.0>},
              {id,ns_disksup},
              {mfargs,{ns_disksup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.773Z,ns_1@cb.local:<0.497.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.497.0>,suppress_max_restart_intensity}
    started: [{pid,<0.498.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      ns_disksup}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,ns_disksup},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => ns_disksup,inherited_max_r => 20,
                          inherited_max_t => 10,modules => [],
                          restart => permanent,shutdown => 1000,
                          start => {ns_disksup,start_link,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:49.773Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.497.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,ns_disksup}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => ns_disksup,inherited_max_r => 20,
                         inherited_max_t => 10,modules => [],
                         restart => permanent,shutdown => 1000,
                         start => {ns_disksup,start_link,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:49.775Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.501.0>},
              {id,diag_handler_worker},
              {mfargs,{work_queue,start_link,[diag_handler_worker]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:49.778Z,ns_1@cb.local:ns_server_sup<0.496.0>:dir_size:start_link:33]Starting quick version of dir_size with program name: godu
[error_logger:info,2025-05-15T18:46:49.779Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.502.0>},
              {id,dir_size},
              {mfargs,{dir_size,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.780Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.503.0>},
              {id,request_tracker},
              {mfargs,{request_tracker,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.782Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.504.0>},
              {id,chronicle_kv_log},
              {mfargs,{chronicle_kv_log,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:warn,2025-05-15T18:46:49.786Z,ns_1@cb.local:ns_log<0.506.0>:gossip_replicator:read_logs:244]Couldn't load logs from "/opt/couchbase/var/lib/couchbase/ns_log" (perhaps it's first
                         startup): {error,enoent}
[error_logger:info,2025-05-15T18:46:49.787Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.506.0>},
              {id,ns_log},
              {mfargs,{ns_log,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.787Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.507.0>},
              {id,event_log_events},
              {mfargs,{gen_event,start_link,[{local,event_log_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:warn,2025-05-15T18:46:49.791Z,ns_1@cb.local:event_log_server<0.508.0>:gossip_replicator:read_logs:244]Couldn't load logs from "/opt/couchbase/var/lib/couchbase/event_log" (perhaps it's first
                         startup): {error,enoent}
[error_logger:info,2025-05-15T18:46:49.792Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.508.0>},
              {id,event_log_server},
              {mfargs,{event_log_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.800Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.510.0>},
              {id,initargs_updater},
              {mfargs,{initargs_updater,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.803Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.512.0>},
              {id,timer_lag_recorder},
              {mfargs,{timer_lag_recorder,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:49.803Z,ns_1@cb.local:<0.514.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,
                                    ns_babysitter_log_consumer} with delay of 4000
[error_logger:info,2025-05-15T18:46:49.803Z,ns_1@cb.local:<0.515.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.515.0>,suppress_max_restart_intensity}
    started: [{pid,<0.516.0>},
              {id,ns_babysitter_log_consumer},
              {mfargs,{ns_log,start_link_babysitter_log_consumer,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.804Z,ns_1@cb.local:<0.513.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.513.0>,suppress_max_restart_intensity}
    started: [{pid,<0.514.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      ns_babysitter_log_consumer}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,
                           ns_babysitter_log_consumer},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => ns_babysitter_log_consumer,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [],restart => permanent,shutdown => 1000,
                          start =>
                              {ns_log,start_link_babysitter_log_consumer,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:49.804Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.513.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,
                      ns_babysitter_log_consumer}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => ns_babysitter_log_consumer,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [],restart => permanent,shutdown => 1000,
                         start =>
                             {ns_log,start_link_babysitter_log_consumer,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:49.898Z,ns_1@cb.local:<0.521.0>:goport:handle_eof:592]Stream 'stdout' closed
[ns_server:debug,2025-05-15T18:46:49.898Z,ns_1@cb.local:<0.521.0>:goport:handle_eof:592]Stream 'stderr' closed
[ns_server:info,2025-05-15T18:46:49.898Z,ns_1@cb.local:<0.521.0>:goport:handle_process_exit:573]Port exited with status 0.
[ns_server:debug,2025-05-15T18:46:49.902Z,ns_1@cb.local:prometheus_cfg<0.517.0>:prometheus_cfg:ensure_prometheus_config:932]Updating prometheus config file: /opt/couchbase/var/lib/couchbase/config/prometheus.yml
[ns_server:debug,2025-05-15T18:46:49.905Z,ns_1@cb.local:prometheus_cfg<0.517.0>:prometheus_cfg:ensure_prometheus_config:932]Updating prometheus config file: /opt/couchbase/var/lib/couchbase/config/prometheus_rules.yml
[ns_server:debug,2025-05-15T18:46:49.926Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',prometheus_auth_info} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554009}}]}|
 {"@prometheus",
  {auth,
   [{<<"plain">>,
     {sanitized,<<"5SIDHzZbdOZ2ttFcD0psuX/t0NnhgGkCk8uNVd/6MA0=">>}},
    {<<"sha512">>,
     {[{<<"s">>,
        <<"tpB/VaV2ixt8INd0O8MjQszCiXCYxTRZmYjCgKqZRx/RiAxKzQkiOCFeOasM5pR5RT0gA4NvQSALJE35heUOMw==">>},
       {<<"h">>,
        {sanitized,<<"BLdUyQsl7LP+kuiNiK9x8TbFAGoqCyLCY0iI/VkdoPg=">>}},
       {<<"i">>,15000}]}},
    {<<"sha256">>,
     {[{<<"s">>,<<"25hVgKVBA+xTPb/dd1pZVSgKtu7rhuNaEI2LZ5X1c4I=">>},
       {<<"h">>,
        {sanitized,<<"UbJVRccFcXqLpWGRF9F9oiypZdnzyVRO7AQtwVs0gbc=">>}},
       {<<"i">>,15000}]}},
    {<<"sha1">>,
     {[{<<"s">>,<<"pI6wZ5CzIUXInSDp07Fg8a9e36o=">>},
       {<<"h">>,
        {sanitized,<<"fNJUkUIvKmfsbVPpdtN3XtG1JWaHMpHqnTWuz8Xz1RY=">>}},
       {<<"i">>,15000}]}}]}}]
[ns_server:debug,2025-05-15T18:46:49.929Z,ns_1@cb.local:prometheus_cfg<0.517.0>:prometheus_cfg:apply_config:724]Restarting Prometheus as the start specs have changed
[error_logger:info,2025-05-15T18:46:49.931Z,ns_1@cb.local:ale_dynamic_sup<0.78.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ale_dynamic_sup}
    started: [{pid,<0.524.0>},
              {id,'sink-prometheus'},
              {mfargs,
                  {ale_dynamic_sup,delay_death,
                      [{ale_disk_sink,start_link,
                           ['sink-prometheus',
                            "/opt/couchbase/var/lib/couchbase/logs/prometheus.log",
                            [{rotation,
                                 [{compress,true},
                                  {size,41943040},
                                  {num_files,10},
                                  {buffer_size_max,52428800}]}]]},
                       1000]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:49.965Z,ns_1@cb.local:ns_couchdb_port<0.471.0>:ns_port_server:log:226]ns_couchdb<0.471.0>: 221: Booted. Waiting for shutdown request
ns_couchdb<0.471.0>: 221: Booted. Waiting for shutdown request
ns_couchdb<0.471.0>: working as port

[error_logger:info,2025-05-15T18:46:50.063Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.517.0>},
              {id,prometheus_cfg},
              {mfargs,{prometheus_cfg,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:50.072Z,ns_1@cb.local:memcached_passwords<0.530.0>:memcached_cfg:init:60]Init config writer for memcached_passwords, "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2025-05-15T18:46:50.076Z,ns_1@cb.local:memcached_passwords<0.530.0>:memcached_cfg:write_cfg:156]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2025-05-15T18:46:50.097Z,ns_1@cb.local:memcached_passwords<0.530.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,{auth,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2025-05-15T18:46:50.098Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:handle_call:57]File rename from "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw" is requested
[ns_server:debug,2025-05-15T18:46:50.100Z,ns_1@cb.local:memcached_passwords<0.530.0>:memcached_cfg:rename_and_refresh:178]Successfully renamed "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2025-05-15T18:46:50.100Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:handle_cast:67]Refresh of isasl requested
[ns_server:debug,2025-05-15T18:46:50.100Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:update_refresh_state:137]Refresh of [isasl] skipped. Retry in 1000 ms.
[error_logger:info,2025-05-15T18:46:50.100Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.530.0>},
              {id,memcached_passwords},
              {mfargs,{memcached_passwords,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:50.107Z,ns_1@cb.local:memcached_permissions<0.533.0>:memcached_cfg:init:60]Init config writer for memcached_permissions, "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2025-05-15T18:46:50.109Z,ns_1@cb.local:memcached_permissions<0.533.0>:memcached_cfg:write_cfg:156]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2025-05-15T18:46:50.110Z,ns_1@cb.local:memcached_permissions<0.533.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,{user,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2025-05-15T18:46:50.111Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:handle_call:57]File rename from "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac" is requested
[ns_server:debug,2025-05-15T18:46:50.112Z,ns_1@cb.local:memcached_permissions<0.533.0>:memcached_cfg:rename_and_refresh:178]Successfully renamed "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2025-05-15T18:46:50.112Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:handle_cast:67]Refresh of rbac requested
[error_logger:info,2025-05-15T18:46:50.112Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.533.0>},
              {id,memcached_permissions},
              {mfargs,{memcached_permissions,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.117Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.536.0>},
              {id,ns_email_alert},
              {mfargs,{ns_email_alert,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.123Z,ns_1@cb.local:ns_node_disco_sup<0.537.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_node_disco_sup}
    started: [{pid,<0.538.0>},
              {id,ns_node_disco_events},
              {mfargs,{gen_event,start_link,[{local,ns_node_disco_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:50.123Z,ns_1@cb.local:ns_node_disco<0.539.0>:ns_node_disco:init:111]Initting ns_node_disco with []
[error_logger:info,2025-05-15T18:46:50.123Z,ns_1@cb.local:ns_node_disco_sup<0.537.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_node_disco_sup}
    started: [{pid,<0.539.0>},
              {id,ns_node_disco},
              {mfargs,{ns_node_disco,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:50.123Z,ns_1@cb.local:ns_cookie_manager<0.238.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[user:info,2025-05-15T18:46:50.124Z,ns_1@cb.local:ns_cookie_manager<0.238.0>:ns_cookie_manager:do_cookie_init:78]Initial otp cookie generated: {sanitized,
                                  <<"1ZVTl+fgxazsGs5XltMEJFjUhLpt0If4lYDK2laySmI=">>}
[ns_server:debug,2025-05-15T18:46:50.124Z,ns_1@cb.local:<0.541.0>:ns_node_disco:do_nodes_wanted_updated_fun:210]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"1ZVTl+fgxazsGs5XltMEJFjUhLpt0If4lYDK2laySmI=">>}
[ns_server:debug,2025-05-15T18:46:50.124Z,ns_1@cb.local:ns_cookie_manager<0.238.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[ns_server:debug,2025-05-15T18:46:50.124Z,ns_1@cb.local:<0.543.0>:ns_node_disco:do_nodes_wanted_updated_fun:210]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"1ZVTl+fgxazsGs5XltMEJFjUhLpt0If4lYDK2laySmI=">>}
[ns_server:debug,2025-05-15T18:46:50.124Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
otp ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554010}}]},
 {cookie,{sanitized,<<"1ZVTl+fgxazsGs5XltMEJFjUhLpt0If4lYDK2laySmI=">>}}]
[error_logger:info,2025-05-15T18:46:50.127Z,ns_1@cb.local:ns_node_disco_sup<0.537.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_node_disco_sup}
    started: [{pid,<0.544.0>},
              {id,ns_node_disco_log},
              {mfargs,{ns_node_disco_log,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:50.129Z,ns_1@cb.local:<0.541.0>:ns_node_disco:do_nodes_wanted_updated_fun:213]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"1ZVTl+fgxazsGs5XltMEJFjUhLpt0If4lYDK2laySmI=">>}
[ns_server:debug,2025-05-15T18:46:50.129Z,ns_1@cb.local:<0.543.0>:ns_node_disco:do_nodes_wanted_updated_fun:213]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"1ZVTl+fgxazsGs5XltMEJFjUhLpt0If4lYDK2laySmI=">>}
[error_logger:info,2025-05-15T18:46:50.133Z,ns_1@cb.local:ns_config_rep_sup<0.545.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_rep_sup}
    started: [{pid,<0.547.0>},
              {id,ns_config_rep_merger},
              {mfargs,{ns_config_rep,start_link_merger,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:50.133Z,ns_1@cb.local:ns_config_rep<0.548.0>:ns_config_rep:init:80]init pulling
[ns_server:debug,2025-05-15T18:46:50.133Z,ns_1@cb.local:ns_config_rep<0.548.0>:ns_config_rep:init:82]init pushing
[error_logger:info,2025-05-15T18:46:50.134Z,ns_1@cb.local:ns_config_rep_sup<0.545.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_rep_sup}
    started: [{pid,<0.548.0>},
              {id,ns_config_rep},
              {mfargs,{ns_config_rep,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.134Z,ns_1@cb.local:ns_node_disco_sup<0.537.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_node_disco_sup}
    started: [{pid,<0.545.0>},
              {id,ns_config_rep_sup},
              {mfargs,{ns_config_rep_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:50.134Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.537.0>},
              {id,ns_node_disco_sup},
              {mfargs,{ns_node_disco_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:50.136Z,ns_1@cb.local:tombstone_keeper<0.277.0>:tombstone_keeper:handle_call:49]Refreshed with timestamps []
[error_logger:info,2025-05-15T18:46:50.136Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.558.0>},
              {id,tombstone_agent},
              {mfargs,{tombstone_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.143Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.565.0>},
              {id,vbucket_map_mirror},
              {mfargs,{vbucket_map_mirror,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.145Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.568.0>},
              {id,capi_url_cache},
              {mfargs,{capi_url_cache,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.150Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.570.0>},
              {id,bucket_info_cache},
              {mfargs,{bucket_info_cache,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.151Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.573.0>},
              {id,ns_tick_event},
              {mfargs,{gen_event,start_link,[{local,ns_tick_event}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.151Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.574.0>},
              {id,buckets_events},
              {mfargs,{gen_event,start_link,[{local,buckets_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.151Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.575.0>},
              {id,ns_stats_event},
              {mfargs,{gen_event,start_link,[{local,ns_stats_event}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.154Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.576.0>},
              {id,samples_loader_tasks},
              {mfargs,{samples_loader_tasks,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.158Z,ns_1@cb.local:ns_heart_sup<0.577.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_heart_sup}
    started: [{pid,<0.578.0>},
              {id,ns_heart},
              {mfargs,{ns_heart,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.158Z,ns_1@cb.local:ns_heart_sup<0.577.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_heart_sup}
    started: [{pid,<0.580.0>},
              {id,ns_heart_slow_updater},
              {mfargs,{ns_heart,start_link_slow_updater,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.158Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.577.0>},
              {id,ns_heart_sup},
              {mfargs,{ns_heart_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:50.166Z,ns_1@cb.local:ns_doctor_sup<0.590.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_doctor_sup}
    started: [{pid,<0.591.0>},
              {id,ns_doctor_events},
              {mfargs,{gen_event,start_link,[{local,ns_doctor_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.173Z,ns_1@cb.local:ns_doctor_sup<0.590.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_doctor_sup}
    started: [{pid,<0.594.0>},
              {id,ns_doctor},
              {mfargs,{ns_doctor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:50.174Z,ns_1@cb.local:<0.582.0>:restartable:start_child:92]Started child process <0.590.0>
  MFA: {ns_doctor_sup,start_link,[]}
[error_logger:info,2025-05-15T18:46:50.174Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.582.0>},
              {id,ns_doctor_sup},
              {mfargs,{restartable,start_link,
                                   [{ns_doctor_sup,start_link,[]},infinity]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:50.174Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.597.0>},
              {id,master_activity_events},
              {mfargs,{gen_event,start_link,[{local,master_activity_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.177Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.600.0>},
              {id,xdcr_ckpt_store},
              {mfargs,{simple_store,start_link,[xdcr_ckpt_data]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.177Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.601.0>},
              {id,metakv_worker},
              {mfargs,{work_queue,start_link,[metakv_worker]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.177Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.602.0>},
              {id,index_events},
              {mfargs,{gen_event,start_link,[{local,index_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.177Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.603.0>},
              {id,index_settings_manager},
              {mfargs,{index_settings_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.178Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.605.0>},
              {id,query_settings_manager},
              {mfargs,{query_settings_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.178Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.607.0>},
              {id,eventing_settings_manager},
              {mfargs,{eventing_settings_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.178Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.609.0>},
              {id,analytics_settings_manager},
              {mfargs,{analytics_settings_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.178Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.611.0>},
              {id,audit_events},
              {mfargs,{gen_event,start_link,[{local,audit_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:50.178Z,ns_1@cb.local:<0.613.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,
                                    encryption_service} with delay of 1000
[ns_server:debug,2025-05-15T18:46:50.178Z,ns_1@cb.local:encryption_service<0.615.0>:encryption_service:recover:199]Config marker /opt/couchbase/var/lib/couchbase/config/sm_load_config_marker doesn't exist
[error_logger:info,2025-05-15T18:46:50.178Z,ns_1@cb.local:<0.614.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.614.0>,suppress_max_restart_intensity}
    started: [{pid,<0.615.0>},
              {id,encryption_service},
              {mfargs,{encryption_service,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.178Z,ns_1@cb.local:<0.612.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.612.0>,suppress_max_restart_intensity}
    started: [{pid,<0.613.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      encryption_service}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,encryption_service},
                       1000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 1,id => encryption_service,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [encryption_service],
                          restart => permanent,shutdown => 5000,
                          start => {encryption_service,start_link,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:50.178Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.612.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,encryption_service}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 1,id => encryption_service,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [encryption_service],
                         restart => permanent,shutdown => 5000,
                         start => {encryption_service,start_link,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:50.184Z,ns_1@cb.local:menelaus_sup<0.617.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.618.0>},
              {id,menelaus_ui_auth},
              {mfargs,{menelaus_ui_auth,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.185Z,ns_1@cb.local:menelaus_sup<0.617.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.620.0>},
              {id,scram_sha},
              {mfargs,{scram_sha,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.189Z,ns_1@cb.local:menelaus_sup<0.617.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.621.0>},
              {id,menelaus_local_auth},
              {mfargs,{menelaus_local_auth,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.194Z,ns_1@cb.local:menelaus_sup<0.617.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.622.0>},
              {id,menelaus_web_cache},
              {mfargs,{menelaus_web_cache,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.208Z,ns_1@cb.local:menelaus_sup<0.617.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.623.0>},
              {id,menelaus_stats_gatherer},
              {mfargs,{menelaus_stats_gatherer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.208Z,ns_1@cb.local:menelaus_sup<0.617.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.624.0>},
              {id,json_rpc_events},
              {mfargs,{gen_event,start_link,[{local,json_rpc_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.233Z,ns_1@cb.local:menelaus_web_sup<0.625.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_web_sup}
    started: [{pid,<0.627.0>},
              {id,menelaus_event},
              {mfargs,{menelaus_event,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:50.233Z,ns_1@cb.local:<0.629.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2025-05-15T18:46:50.235Z,ns_1@cb.local:<0.629.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2025-05-15T18:46:50.235Z,ns_1@cb.local:<0.629.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2025-05-15T18:46:50.235Z,ns_1@cb.local:<0.629.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2025-05-15T18:46:50.235Z,ns_1@cb.local:<0.629.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[error_logger:info,2025-05-15T18:46:50.236Z,ns_1@cb.local:inet_gethost_native_sup<0.631.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,inet_gethost_native_sup}
    started: [{pid,<0.632.0>},{mfa,{inet_gethost_native,init,[[]]}}]

[ns_server:info,2025-05-15T18:46:50.236Z,ns_1@cb.local:<0.629.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"0.0.0.0"},{port,8091}]
[error_logger:info,2025-05-15T18:46:50.236Z,ns_1@cb.local:kernel_safe_sup<0.67.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,kernel_safe_sup}
    started: [{pid,<0.631.0>},
              {id,inet_gethost_native_sup},
              {mfargs,{inet_gethost_native,start_link,[]}},
              {restart_type,temporary},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.236Z,ns_1@cb.local:<0.629.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.629.0>,menelaus_web}
    started: [{pid,<0.630.0>},
              {id,menelaus_web_ipv4},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet},
                        {name,menelaus_web},
                        {port,8091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:50.237Z,ns_1@cb.local:<0.629.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2025-05-15T18:46:50.237Z,ns_1@cb.local:<0.629.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2025-05-15T18:46:50.237Z,ns_1@cb.local:<0.629.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2025-05-15T18:46:50.237Z,ns_1@cb.local:<0.629.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2025-05-15T18:46:50.237Z,ns_1@cb.local:<0.629.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2025-05-15T18:46:50.238Z,ns_1@cb.local:<0.629.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"::"},{port,8091}]
[error_logger:info,2025-05-15T18:46:50.238Z,ns_1@cb.local:<0.629.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.629.0>,menelaus_web}
    started: [{pid,<0.649.0>},
              {id,menelaus_web_ipv6},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet6},
                        {name,menelaus_web},
                        {port,8091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:50.238Z,ns_1@cb.local:<0.628.0>:restartable:start_child:92]Started child process <0.629.0>
  MFA: {menelaus_web,start_link,[]}
[error_logger:info,2025-05-15T18:46:50.238Z,ns_1@cb.local:menelaus_web_sup<0.625.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_web_sup}
    started: [{pid,<0.628.0>},
              {id,menelaus_web},
              {mfargs,{restartable,start_link,
                                   [{menelaus_web,start_link,[]},infinity]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[user:info,2025-05-15T18:46:50.238Z,ns_1@cb.local:menelaus_sup<0.617.0>:menelaus_web_sup:start_link:38]Couchbase Server has started on web port 8091 on node 'ns_1@cb.local'. Version: "7.6.2-3721-enterprise".
[error_logger:info,2025-05-15T18:46:50.238Z,ns_1@cb.local:menelaus_sup<0.617.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.625.0>},
              {id,menelaus_web_sup},
              {mfargs,{menelaus_web_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:50.239Z,ns_1@cb.local:menelaus_sup<0.617.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.666.0>},
              {id,menelaus_web_alerts_srv},
              {mfargs,{menelaus_web_alerts_srv,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.242Z,ns_1@cb.local:menelaus_sup<0.617.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.668.0>},
              {id,guardrail_monitor},
              {mfargs,{guardrail_monitor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.246Z,ns_1@cb.local:menelaus_sup<0.617.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.669.0>},
              {id,menelaus_cbauth},
              {mfargs,{menelaus_cbauth,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.247Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.617.0>},
              {id,menelaus},
              {mfargs,{menelaus_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:50.247Z,ns_1@cb.local:<0.676.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,ns_ports_setup} with delay of 4000
[error_logger:info,2025-05-15T18:46:50.247Z,ns_1@cb.local:<0.677.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.677.0>,suppress_max_restart_intensity}
    started: [{pid,<0.678.0>},
              {id,ns_ports_setup},
              {mfargs,{ns_ports_setup,start,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.247Z,ns_1@cb.local:<0.675.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.675.0>,suppress_max_restart_intensity}
    started: [{pid,<0.676.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      ns_ports_setup}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,ns_ports_setup},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => ns_ports_setup,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [],restart => permanent,
                          shutdown => brutal_kill,
                          start => {ns_ports_setup,start,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:50.247Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.675.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,ns_ports_setup}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => ns_ports_setup,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [],restart => permanent,
                         shutdown => brutal_kill,
                         start => {ns_ports_setup,start,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:50.252Z,ns_1@cb.local:ns_ports_setup<0.678.0>:ns_ports_manager:set_dynamic_children:48]Setting children [memcached,saslauthd_port,goxdcr]
[error_logger:info,2025-05-15T18:46:50.254Z,ns_1@cb.local:service_agent_sup<0.681.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_agent_sup}
    started: [{pid,<0.682.0>},
              {id,service_agent_children_sup},
              {mfargs,{supervisor,start_link,
                                  [{local,service_agent_children_sup},
                                   service_agent_sup,child]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:50.255Z,ns_1@cb.local:service_agent_sup<0.681.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_agent_sup}
    started: [{pid,<0.683.0>},
              {id,service_agent_worker},
              {mfargs,{erlang,apply,[#Fun<service_agent_sup.0.125430937>,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.255Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.681.0>},
              {id,service_agent_sup},
              {mfargs,{service_agent_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:50.259Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.685.0>},
              {id,ns_memcached_sockets_pool},
              {mfargs,{ns_memcached_sockets_pool,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:50.270Z,ns_1@cb.local:memcached_auth_server<0.686.0>:memcached_auth_server:reconnect:239]Skipping creation of 'Auth provider' connection because external users are disabled
[error_logger:info,2025-05-15T18:46:50.270Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.686.0>},
              {id,memcached_auth_server},
              {mfargs,{memcached_auth_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:50.270Z,ns_1@cb.local:<0.689.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,ns_audit_cfg} with delay of 4000
[ns_server:debug,2025-05-15T18:46:50.270Z,ns_1@cb.local:ns_audit_cfg<0.691.0>:ns_audit_cfg:write_audit_json:238]Writing new content to "/opt/couchbase/var/lib/couchbase/config/audit.json", Params [{descriptors_path,
                                                                                      "/opt/couchbase/etc/security"},
                                                                                     {version,
                                                                                      2},
                                                                                     {uuid,
                                                                                      "33985209"},
                                                                                     {event_states,
                                                                                      {[]}},
                                                                                     {filtering_enabled,
                                                                                      true},
                                                                                     {disabled_userids,
                                                                                      []},
                                                                                     {auditd_enabled,
                                                                                      false},
                                                                                     {log_path,
                                                                                      "/opt/couchbase/var/lib/couchbase/logs"},
                                                                                     {prune_age,
                                                                                      0},
                                                                                     {rotate_interval,
                                                                                      86400},
                                                                                     {rotate_size,
                                                                                      20971520},
                                                                                     {sync,
                                                                                      []}]
[ns_server:debug,2025-05-15T18:46:50.275Z,ns_1@cb.local:ns_audit_cfg<0.691.0>:ns_audit_cfg:notify_memcached:152]Instruct memcached to reload audit config
[error_logger:info,2025-05-15T18:46:50.276Z,ns_1@cb.local:<0.690.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.690.0>,suppress_max_restart_intensity}
    started: [{pid,<0.691.0>},
              {id,ns_audit_cfg},
              {mfargs,{ns_audit_cfg,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.276Z,ns_1@cb.local:<0.688.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.688.0>,suppress_max_restart_intensity}
    started: [{pid,<0.689.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      ns_audit_cfg}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,ns_audit_cfg},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => ns_audit_cfg,inherited_max_r => 20,
                          inherited_max_t => 10,modules => [],
                          restart => permanent,shutdown => 1000,
                          start => {ns_audit_cfg,start_link,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:50.276Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.688.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,ns_audit_cfg}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => ns_audit_cfg,inherited_max_r => 20,
                         inherited_max_t => 10,modules => [],
                         restart => permanent,shutdown => 1000,
                         start => {ns_audit_cfg,start_link,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:50.276Z,ns_1@cb.local:<0.695.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,ns_audit} with delay of 4000
[ns_server:warn,2025-05-15T18:46:50.277Z,ns_1@cb.local:<0.698.0>:ns_memcached:connect:1460]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}, retrying.
[error_logger:info,2025-05-15T18:46:50.282Z,ns_1@cb.local:<0.696.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.696.0>,suppress_max_restart_intensity}
    started: [{pid,<0.699.0>},
              {id,ns_audit},
              {mfargs,{ns_audit,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.282Z,ns_1@cb.local:<0.694.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.694.0>,suppress_max_restart_intensity}
    started: [{pid,<0.695.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      ns_audit}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,ns_audit},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => ns_audit,inherited_max_r => 20,
                          inherited_max_t => 10,modules => [],
                          restart => permanent,shutdown => 1000,
                          start => {ns_audit,start_link,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:50.282Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.694.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,ns_audit}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => ns_audit,inherited_max_r => 20,
                         inherited_max_t => 10,modules => [],
                         restart => permanent,shutdown => 1000,
                         start => {ns_audit,start_link,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:50.283Z,ns_1@cb.local:<0.702.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,
                                    memcached_config_mgr} with delay of 4000
[ns_server:debug,2025-05-15T18:46:50.283Z,ns_1@cb.local:memcached_config_mgr<0.704.0>:memcached_config_mgr:memcached_port_pid:154]waiting for completion of initial ns_ports_setup round
[error_logger:info,2025-05-15T18:46:50.283Z,ns_1@cb.local:<0.703.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.703.0>,suppress_max_restart_intensity}
    started: [{pid,<0.704.0>},
              {id,memcached_config_mgr},
              {mfargs,{memcached_config_mgr,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.283Z,ns_1@cb.local:<0.701.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.701.0>,suppress_max_restart_intensity}
    started: [{pid,<0.702.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      memcached_config_mgr}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,memcached_config_mgr},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => memcached_config_mgr,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [],restart => permanent,shutdown => 1000,
                          start => {memcached_config_mgr,start_link,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:50.283Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.701.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,memcached_config_mgr}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => memcached_config_mgr,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [],restart => permanent,shutdown => 1000,
                         start => {memcached_config_mgr,start_link,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:info,2025-05-15T18:46:50.289Z,ns_1@cb.local:<0.705.0>:ns_memcached_log_rotator:init:36]Starting log rotator on "/opt/couchbase/var/lib/couchbase/logs"/"memcached.log"* with an initial period of 39003ms
[error_logger:info,2025-05-15T18:46:50.289Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.705.0>},
              {id,ns_memcached_log_rotator},
              {mfargs,{ns_memcached_log_rotator,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.295Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.706.0>},
              {id,testconditions_store},
              {mfargs,{simple_store,start_link,[testconditions]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:50.298Z,ns_1@cb.local:<0.708.0>:memcached_config_mgr:memcached_port_pid:154]waiting for completion of initial ns_ports_setup round
[error_logger:info,2025-05-15T18:46:50.298Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.708.0>},
              {id,terse_cluster_info_uploader},
              {mfargs,{terse_cluster_info_uploader,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.323Z,ns_1@cb.local:ns_bucket_worker_sup<0.710.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_bucket_worker_sup}
    started: [{pid,<0.711.0>},
              {id,ns_bucket_sup},
              {mfargs,{ns_bucket_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:50.327Z,ns_1@cb.local:ns_bucket_worker_sup<0.710.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_bucket_worker_sup}
    started: [{pid,<0.712.0>},
              {id,ns_bucket_worker},
              {mfargs,{ns_bucket_worker,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.327Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.710.0>},
              {id,ns_bucket_worker_sup},
              {mfargs,{ns_bucket_worker_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:warn,2025-05-15T18:46:50.327Z,ns_1@cb.local:<0.715.0>:ns_memcached:connect:1460]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}, retrying.
[error_logger:info,2025-05-15T18:46:50.341Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.716.0>},
              {id,ns_server_stats},
              {mfargs,{ns_server_stats,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:50.346Z,ns_1@cb.local:<0.718.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ale_stats_events,<0.716.0>} exited with reason {noproc,
                                                                                {gen_statem,
                                                                                 call,
                                                                                 [mb_master,
                                                                                  master_node,
                                                                                  infinity]}}
[error_logger:info,2025-05-15T18:46:50.352Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.720.0>},
              {id,{stats_reader,"@system"}},
              {mfargs,{stats_reader,start_link,["@system"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.352Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.722.0>},
              {id,{stats_reader,"@system-processes"}},
              {mfargs,{stats_reader,start_link,["@system-processes"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.352Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.724.0>},
              {id,{stats_reader,"@query"}},
              {mfargs,{stats_reader,start_link,["@query"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.352Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.726.0>},
              {id,{stats_reader,"@global"}},
              {mfargs,{stats_reader,start_link,["@global"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.354Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.728.0>},
              {id,goxdcr_status_keeper},
              {mfargs,{goxdcr_status_keeper,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.360Z,ns_1@cb.local:services_stats_sup<0.729.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,services_stats_sup}
    started: [{pid,<0.730.0>},
              {id,service_stats_children_sup},
              {mfargs,{supervisor,start_link,
                                  [{local,service_stats_children_sup},
                                   services_stats_sup,child]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:50.364Z,ns_1@cb.local:service_status_keeper_sup<0.731.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_status_keeper_sup}
    started: [{pid,<0.732.0>},
              {id,service_status_keeper_worker},
              {mfargs,{work_queue,start_link,[service_status_keeper_worker]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.382Z,ns_1@cb.local:service_status_keeper_sup<0.731.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_status_keeper_sup}
    started: [{pid,<0.738.0>},
              {id,service_status_keeper_index},
              {mfargs,{service_index,start_keeper,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:50.385Z,ns_1@cb.local:goxdcr_status_keeper<0.728.0>:goxdcr_rest:get_from_goxdcr:139]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2025-05-15T18:46:50.385Z,ns_1@cb.local:goxdcr_status_keeper<0.728.0>:goxdcr_rest:get_from_goxdcr:139]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2025-05-15T18:46:50.385Z,ns_1@cb.local:ns_heart<0.578.0>:goxdcr_rest:get_from_goxdcr:139]Goxdcr is temporary not available. Return empty list.
[error_logger:info,2025-05-15T18:46:50.387Z,ns_1@cb.local:service_status_keeper_sup<0.731.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_status_keeper_sup}
    started: [{pid,<0.743.0>},
              {id,service_status_keeper_fts},
              {mfargs,{service_fts,start_keeper,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:50.388Z,ns_1@cb.local:ns_heart<0.578.0>:cluster_logs_collection_task:maybe_build_cluster_logs_task:40]Ignoring exception trying to read cluster_logs_collection_task_status table: error:badarg
[error_logger:info,2025-05-15T18:46:50.391Z,ns_1@cb.local:service_status_keeper_sup<0.731.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_status_keeper_sup}
    started: [{pid,<0.746.0>},
              {id,service_status_keeper_eventing},
              {mfargs,{service_eventing,start_keeper,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.391Z,ns_1@cb.local:services_stats_sup<0.729.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,services_stats_sup}
    started: [{pid,<0.731.0>},
              {id,service_status_keeper_sup},
              {mfargs,{service_status_keeper_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:50.393Z,ns_1@cb.local:services_stats_sup<0.729.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,services_stats_sup}
    started: [{pid,<0.749.0>},
              {id,service_stats_worker},
              {mfargs,{erlang,apply,[#Fun<services_stats_sup.0.35188242>,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.393Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.729.0>},
              {id,services_stats_sup},
              {mfargs,{services_stats_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:50.393Z,ns_1@cb.local:<0.756.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,
                                    compaction_daemon} with delay of 4000
[ns_server:debug,2025-05-15T18:46:50.399Z,ns_1@cb.local:<0.761.0>:new_concurrency_throttle:init:109]init concurrent throttle process, pid: <0.761.0>, type: kv_throttle# of available token: 1
[error_logger:info,2025-05-15T18:46:50.406Z,ns_1@cb.local:<0.757.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.757.0>,suppress_max_restart_intensity}
    started: [{pid,<0.759.0>},
              {id,compaction_daemon},
              {mfargs,{compaction_daemon,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,86400000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:50.406Z,ns_1@cb.local:compaction_daemon<0.759.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_kv. Rescheduling compaction.
[error_logger:info,2025-05-15T18:46:50.406Z,ns_1@cb.local:<0.755.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.755.0>,suppress_max_restart_intensity}
    started: [{pid,<0.756.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      compaction_daemon}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,compaction_daemon},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => compaction_daemon,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [compaction_daemon],
                          restart => permanent,shutdown => 86400000,
                          start => {compaction_daemon,start_link,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:50.406Z,ns_1@cb.local:compaction_daemon<0.759.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[error_logger:info,2025-05-15T18:46:50.406Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.755.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,compaction_daemon}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => compaction_daemon,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [compaction_daemon],
                         restart => permanent,shutdown => 86400000,
                         start => {compaction_daemon,start_link,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:50.406Z,ns_1@cb.local:compaction_daemon<0.759.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2025-05-15T18:46:50.407Z,ns_1@cb.local:compaction_daemon<0.759.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:46:50.407Z,ns_1@cb.local:compaction_daemon<0.759.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2025-05-15T18:46:50.407Z,ns_1@cb.local:compaction_daemon<0.759.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_master too soon. Next run will be in 3600s
[error_logger:info,2025-05-15T18:46:50.411Z,ns_1@cb.local:cluster_logs_sup<0.770.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,cluster_logs_sup}
    started: [{pid,<0.771.0>},
              {id,ets_holder},
              {mfargs,{cluster_logs_collection_task,start_link_ets_holder,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.411Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.770.0>},
              {id,cluster_logs_sup},
              {mfargs,{cluster_logs_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:50.411Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.774.0>},
              {id,collections},
              {mfargs,{collections,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.412Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.778.0>},
              {id,leader_events},
              {mfargs,{gen_event,start_link,[{local,leader_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:50.415Z,ns_1@cb.local:ns_heart_slow_status_updater<0.580.0>:goxdcr_rest:get_from_goxdcr:139]Goxdcr is temporary not available. Return empty list.
[error_logger:info,2025-05-15T18:46:50.423Z,ns_1@cb.local:leader_leases_sup<0.783.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_leases_sup}
    started: [{pid,<0.787.0>},
              {id,leader_activities},
              {mfargs,{leader_activities,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,10000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.430Z,ns_1@cb.local:leader_leases_sup<0.783.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_leases_sup}
    started: [{pid,<0.789.0>},
              {id,leader_lease_agent},
              {mfargs,{leader_lease_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.431Z,ns_1@cb.local:leader_services_sup<0.780.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_services_sup}
    started: [{pid,<0.783.0>},
              {id,leader_leases_sup},
              {mfargs,{leader_leases_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:50.447Z,ns_1@cb.local:leader_registry_sup<0.791.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_registry_sup}
    started: [{pid,<0.792.0>},
              {id,leader_registry},
              {mfargs,{leader_registry,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:50.447Z,ns_1@cb.local:leader_registry_sup<0.791.0>:mb_master:check_master_takeover_needed:183]Sending master node question to the following nodes: []
[ns_server:debug,2025-05-15T18:46:50.447Z,ns_1@cb.local:leader_registry_sup<0.791.0>:mb_master:check_master_takeover_needed:187]Got replies: []
[ns_server:debug,2025-05-15T18:46:50.447Z,ns_1@cb.local:leader_registry_sup<0.791.0>:mb_master:check_master_takeover_needed:193]Was unable to discover master, not going to force mastership takeover
[ns_server:debug,2025-05-15T18:46:50.447Z,ns_1@cb.local:mb_master<0.794.0>:mb_master:init:86]Heartbeat interval is 2000
[user:info,2025-05-15T18:46:50.447Z,ns_1@cb.local:mb_master<0.794.0>:mb_master:init:91]I'm the only node, so I'm the master.
[ns_server:debug,2025-05-15T18:46:50.447Z,ns_1@cb.local:leader_registry<0.792.0>:leader_registry:handle_new_leader:275]New leader is 'ns_1@cb.local'. Invalidating name cache.
[ns_server:debug,2025-05-15T18:46:50.447Z,ns_1@cb.local:ns_ports_setup<0.678.0>:ns_ports_setup:set_children:65]Monitor ns_child_ports_sup <16971.133.0>
[ns_server:debug,2025-05-15T18:46:50.448Z,ns_1@cb.local:<0.708.0>:memcached_config_mgr:memcached_port_pid:156]ns_ports_setup seems to be ready
[ns_server:debug,2025-05-15T18:46:50.448Z,ns_1@cb.local:memcached_config_mgr<0.704.0>:memcached_config_mgr:memcached_port_pid:156]ns_ports_setup seems to be ready
[ns_server:debug,2025-05-15T18:46:50.449Z,ns_1@cb.local:<0.708.0>:memcached_config_mgr:find_port_pid_loop:164]Found memcached port <16971.139.0>
[ns_server:debug,2025-05-15T18:46:50.449Z,ns_1@cb.local:memcached_config_mgr<0.704.0>:memcached_config_mgr:find_port_pid_loop:164]Found memcached port <16971.139.0>
[ns_server:debug,2025-05-15T18:46:50.452Z,ns_1@cb.local:memcached_config_mgr<0.704.0>:memcached_config_mgr:init:93]wrote memcached config to /opt/couchbase/var/lib/couchbase/config/memcached.json. Will activate memcached port server
[ns_server:debug,2025-05-15T18:46:50.453Z,ns_1@cb.local:memcached_config_mgr<0.704.0>:memcached_config_mgr:init:97]activated memcached port server
[ns_server:info,2025-05-15T18:46:50.454Z,ns_1@cb.local:memcached_config_mgr<0.704.0>:memcached_config_mgr:push_tls_config:233]Pushing TLS config to memcached:
{[{<<"private key">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem">>},
  {<<"certificate chain">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem">>},
  {<<"CA file">>,<<"/opt/couchbase/var/lib/couchbase/config/certs/ca.pem">>},
  {<<"minimum version">>,<<"TLS 1.2">>},
  {<<"cipher list">>,
   {[{<<"TLS 1.2">>,<<"HIGH">>},
     {<<"TLS 1.3">>,
      <<"TLS_AES_256_GCM_SHA384:TLS_AES_128_GCM_SHA256:TLS_CHACHA20_POLY1305_SHA256">>}]}},
  {<<"cipher order">>,true},
  {<<"client cert auth">>,<<"disabled">>}]}
[ns_server:warn,2025-05-15T18:46:50.454Z,ns_1@cb.local:<0.801.0>:ns_memcached:connect:1460]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}, retrying.
[ns_server:debug,2025-05-15T18:46:50.460Z,ns_1@cb.local:<0.708.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":11,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true,\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\"]}}">>
[ns_server:warn,2025-05-15T18:46:50.462Z,ns_1@cb.local:<0.803.0>:ns_memcached:connect:1460]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}, retrying.
[ns_server:debug,2025-05-15T18:46:50.470Z,ns_1@cb.local:mb_master<0.794.0>:master_activity_events:submit_cast:75]Failed to send master activity event {became_master,'ns_1@cb.local'}: {error,
                                                                       badarg}
[error_logger:info,2025-05-15T18:46:50.472Z,ns_1@cb.local:mb_master_sup<0.804.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.805.0>},
              {id,leader_lease_acquirer},
              {mfargs,{leader_lease_acquirer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,10000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:50.473Z,ns_1@cb.local:leader_quorum_nodes_manager<0.807.0>:leader_quorum_nodes_manager:pull_config:99]Attempting to pull config from nodes:
[]
[error_logger:info,2025-05-15T18:46:50.473Z,ns_1@cb.local:mb_master_sup<0.804.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.807.0>},
              {id,leader_quorum_nodes_manager},
              {mfargs,{leader_quorum_nodes_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:50.473Z,ns_1@cb.local:leader_quorum_nodes_manager<0.807.0>:leader_quorum_nodes_manager:pull_config:104]Pulled config successfully.
[ns_server:info,2025-05-15T18:46:50.479Z,ns_1@cb.local:mb_master_sup<0.804.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,ns_tick},
                                         ns_tick,[],[]]): started as <0.812.0> on 'ns_1@cb.local'

[error_logger:info,2025-05-15T18:46:50.479Z,ns_1@cb.local:mb_master_sup<0.804.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.812.0>},
              {id,ns_tick},
              {mfargs,{ns_tick,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,10},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:50.482Z,ns_1@cb.local:leader_lease_agent<0.789.0>:leader_lease_agent:do_handle_acquire_lease:140]Granting lease to {lease_holder,<<"9a39908e5882941cd97af5604964e304">>,
                                'ns_1@cb.local'} for 15000ms
[ns_server:debug,2025-05-15T18:46:50.485Z,ns_1@cb.local:<0.815.0>:chronicle_master:do_init:115]Starting with SelfRef = #Ref<0.3529285563.3650617345.4874>
[ns_server:info,2025-05-15T18:46:50.485Z,ns_1@cb.local:mb_master_sup<0.804.0>:misc:start_singleton:913]start_singleton(gen_server2, start_link, [{via,leader_registry,
                                           chronicle_master},
                                          chronicle_master,[],[]]): started as <0.815.0> on 'ns_1@cb.local'

[error_logger:info,2025-05-15T18:46:50.485Z,ns_1@cb.local:mb_master_sup<0.804.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.815.0>},
              {id,chronicle_master},
              {mfargs,{chronicle_master,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.489Z,ns_1@cb.local:ns_orchestrator_sup<0.818.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.819.0>},
              {id,compat_mode_events},
              {mfargs,{gen_event,start_link,[{local,compat_mode_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:50.492Z,ns_1@cb.local:<0.811.0>:leader_lease_acquire_worker:handle_fresh_lease_acquired:296]Acquired lease from node 'ns_1@cb.local' (lease uuid: <<"9a39908e5882941cd97af5604964e304">>)
[ns_server:info,2025-05-15T18:46:50.493Z,ns_1@cb.local:compat_mode_manager<0.820.0>:cluster_compat_mode:do_upgrades:205]Initiating rbac upgrade due to version change from [7,1] to [7,6] (target version: [7,
                                                                                    6])
[ns_server:info,2025-05-15T18:46:50.493Z,ns_1@cb.local:compat_mode_manager<0.820.0>:menelaus_users:upgrade:1057]Upgrading users database to [7,6]
[ns_server:debug,2025-05-15T18:46:50.493Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
rbac_upgrade ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554010}}]}|
 started]
[ns_server:debug,2025-05-15T18:46:50.493Z,ns_1@cb.local:ns_config_rep<0.548.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([rbac_upgrade]..)
[ns_server:debug,2025-05-15T18:46:50.494Z,ns_1@cb.local:ns_config_rep<0.548.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 3 us
[ns_server:debug,2025-05-15T18:46:50.494Z,ns_1@cb.local:ns_config_rep<0.548.0>:ns_config_rep:handle_call:149]Got full synchronization request from 'ns_1@cb.local'
[ns_server:debug,2025-05-15T18:46:50.494Z,ns_1@cb.local:ns_config_rep<0.548.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 1 us
[ns_server:debug,2025-05-15T18:46:50.494Z,ns_1@cb.local:users_storage<0.444.0>:replicated_storage:handle_call:151]Received sync_to_me with timeout = 60000, nodes = ['ns_1@cb.local']
[ns_server:debug,2025-05-15T18:46:50.494Z,ns_1@cb.local:users_replicator<0.443.0>:doc_replicator:loop:100]Received sync_to_me with timeout = 60000, nodes = ['ns_1@cb.local']
[ns_server:debug,2025-05-15T18:46:50.495Z,ns_1@cb.local:users_storage<0.444.0>:replicated_storage:handle_call:146]Received sync_token from {<0.836.0>,
                          [alias|#Ref<0.3529285563.3650682883.3790>]}
[ns_server:debug,2025-05-15T18:46:50.495Z,ns_1@cb.local:users_replicator<0.443.0>:doc_replicator:loop:95]Received sync_token from {<0.836.0>,
                          [alias|#Ref<0.3529285563.3650682883.3790>]}
[ns_server:debug,2025-05-15T18:46:50.495Z,ns_1@cb.local:<0.833.0>:replicated_storage:handle_call:157]sync_to_me reply: ok
[ns_server:debug,2025-05-15T18:46:50.495Z,ns_1@cb.local:users_storage<0.444.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,'_','_','_'},[],['$_']}],
                               100}
[ns_server:info,2025-05-15T18:46:50.495Z,ns_1@cb.local:compat_mode_manager<0.820.0>:menelaus_users:upgrade:1071]Users database was upgraded to [7,6]
[ns_server:debug,2025-05-15T18:46:50.495Z,ns_1@cb.local:users_storage<0.444.0>:replicated_storage:handle_call:151]Received sync_to_me with timeout = 60000, nodes = ['ns_1@cb.local']
[ns_server:debug,2025-05-15T18:46:50.495Z,ns_1@cb.local:users_replicator<0.443.0>:doc_replicator:loop:100]Received sync_to_me with timeout = 60000, nodes = ['ns_1@cb.local']
[ns_server:debug,2025-05-15T18:46:50.495Z,ns_1@cb.local:users_storage<0.444.0>:replicated_storage:handle_call:146]Received sync_token from {<0.840.0>,
                          [alias|#Ref<0.3529285563.3650682883.3809>]}
[ns_server:debug,2025-05-15T18:46:50.495Z,ns_1@cb.local:users_replicator<0.443.0>:doc_replicator:loop:95]Received sync_token from {<0.840.0>,
                          [alias|#Ref<0.3529285563.3650682883.3809>]}
[ns_server:debug,2025-05-15T18:46:50.495Z,ns_1@cb.local:<0.837.0>:replicated_storage:handle_call:157]sync_to_me reply: ok
[ns_server:info,2025-05-15T18:46:50.501Z,ns_1@cb.local:compat_mode_manager<0.820.0>:menelaus_users:upgrade:1073]Users database upgrade was delivered to ['ns_1@cb.local']
[ns_server:info,2025-05-15T18:46:50.501Z,ns_1@cb.local:kv<0.268.0>:chronicle_upgrade:upgrade_loop:114]Upgrading chronicle from [7,1]. Final version = [7,6]
[ns_server:info,2025-05-15T18:46:50.501Z,ns_1@cb.local:kv<0.268.0>:chronicle_upgrade:upgrade_loop:114]Upgrading chronicle from [7,2]. Final version = [7,6]
[ns_server:debug,2025-05-15T18:46:50.525Z,ns_1@cb.local:chronicle_kv_log<0.504.0>:chronicle_kv_log:log:59]update (key: cluster_compat_version, rev: {<<"ea19d13c61d8435b9ad1a5f6af1aa5eb">>,
                                           5})
[7,6]
[ns_server:debug,2025-05-15T18:46:50.526Z,ns_1@cb.local:ns_cookie_manager<0.238.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[ns_server:debug,2025-05-15T18:46:50.526Z,ns_1@cb.local:tombstone_keeper<0.277.0>:tombstone_keeper:handle_call:49]Refreshed with timestamps []
[ns_server:debug,2025-05-15T18:46:50.526Z,ns_1@cb.local:memcached_permissions<0.533.0>:memcached_permissions:producer:512]Skipping update during users upgrade
[ns_server:debug,2025-05-15T18:46:50.526Z,ns_1@cb.local:roles_cache<0.458.0>:active_cache:clean:181]Clearing the roles_cache cache
[ns_server:debug,2025-05-15T18:46:50.526Z,ns_1@cb.local:<0.842.0>:ns_node_disco:do_nodes_wanted_updated_fun:210]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"1ZVTl+fgxazsGs5XltMEJFjUhLpt0If4lYDK2laySmI=">>}
[ns_server:debug,2025-05-15T18:46:50.526Z,ns_1@cb.local:<0.842.0>:ns_node_disco:do_nodes_wanted_updated_fun:213]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"1ZVTl+fgxazsGs5XltMEJFjUhLpt0If4lYDK2laySmI=">>}
[ns_server:debug,2025-05-15T18:46:50.526Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:maybe_store_ca_certs:832]Considering to store CA certs
[ns_server:debug,2025-05-15T18:46:50.526Z,ns_1@cb.local:compiled_roles_cache<0.455.0>:versioned_cache:handle_info:89]Flushing cache compiled_roles_cache due to version change from undefined to {[7,
                                                                              6],
                                                                             {0,
                                                                              435567564},
                                                                             {0,
                                                                              435567564},
                                                                             false,
                                                                             []}
[ns_server:debug,2025-05-15T18:46:50.527Z,ns_1@cb.local:memcached_passwords<0.530.0>:memcached_cfg:write_cfg:156]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:info,2025-05-15T18:46:50.527Z,ns_1@cb.local:ns_config<0.280.0>:ns_config:do_upgrade_config:733]Upgrading config by changes:
[{set,cluster_compat_version,[7,1]}]

[ns_server:info,2025-05-15T18:46:50.527Z,ns_1@cb.local:ns_config<0.280.0>:ns_online_config_upgrader:do_upgrade_config:59]Performing online config upgrade to [7,2]
[ns_server:info,2025-05-15T18:46:50.528Z,ns_1@cb.local:ns_config<0.280.0>:ns_config:do_upgrade_config:733]Upgrading config by changes:
[{set,cluster_compat_version,[7,2]},
 {set,auto_failover_cfg,
      [{enabled,true},
       {timeout,120},
       {count,0},
       {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
       {failover_server_group,false},
       {max_count,1},
       {failed_over_server_groups,[]},
       {can_abort_rebalance,true},
       {failover_preserve_durability_majority,false}]}]

[ns_server:debug,2025-05-15T18:46:50.532Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:notify_services:1146]Going to notify following services: [capi_ssl_service,memcached]
[ns_server:info,2025-05-15T18:46:50.532Z,ns_1@cb.local:<0.850.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service memcached
[ns_server:info,2025-05-15T18:46:50.544Z,ns_1@cb.local:ns_config<0.280.0>:ns_online_config_upgrader:do_upgrade_config:59]Performing online config upgrade to [7,6]
[ns_server:info,2025-05-15T18:46:50.552Z,ns_1@cb.local:ns_config<0.280.0>:ns_config:do_upgrade_config:733]Upgrading config by changes:
[{set,cluster_compat_version,[7,6]},
 {set,audit_decriptors,
      [{8243,
        [{name,<<"mutate document">>},
         {description,<<"Document was mutated via the REST API">>},
         {enabled,true},
         {module,ns_server}]},
       {8255,
        [{name,<<"read document">>},
         {description,<<"Document was read via the REST API">>},
         {enabled,false},
         {module,ns_server}]},
       {8257,
        [{name,<<"alert email sent">>},
         {description,<<"An alert email was successfully sent">>},
         {enabled,true},
         {module,ns_server}]},
       {8265,
        [{name,<<"RBAC information retrieved">>},
         {description,<<"RBAC information was retrieved">>},
         {enabled,true},
         {module,ns_server}]},
       {16384,
        [{name,<<"remote cluster ref creation">>},
         {description,<<"created remote cluster ref">>},
         {enabled,true},
         {module,xdcr}]},
       {16385,
        [{name,<<"remote cluster ref update">>},
         {description,<<"updated remote cluster ref">>},
         {enabled,true},
         {module,xdcr}]},
       {16386,
        [{name,<<"remote cluster ref deletion">>},
         {description,<<"deleted remote cluster ref">>},
         {enabled,true},
         {module,xdcr}]},
       {16387,
        [{name,<<"replication creation">>},
         {description,<<"created replication">>},
         {enabled,true},
         {module,xdcr}]},
       {16388,
        [{name,<<"replication pause">>},
         {description,<<"paused replication">>},
         {enabled,true},
         {module,xdcr}]},
       {16389,
        [{name,<<"replication resume">>},
         {description,<<"resumed replication">>},
         {enabled,true},
         {module,xdcr}]},
       {16390,
        [{name,<<"replication cancellation">>},
         {description,<<"canceled replication">>},
         {enabled,true},
         {module,xdcr}]},
       {16391,
        [{name,<<"default replication settings update">>},
         {description,<<"updated default replication settings">>},
         {enabled,true},
         {module,xdcr}]},
       {16392,
        [{name,<<"individual replication settings update">>},
         {description,<<"updated individual replication settings">>},
         {enabled,true},
         {module,xdcr}]},
       {16393,
        [{name,<<"bucket settings update">>},
         {description,<<"updated bucket settings">>},
         {enabled,true},
         {module,xdcr}]},
       {16394,
        [{name,<<"authorization failure while adding remote cluster ref">>},
         {description,<<"failed to add remote cluster ref because of authorization failure">>},
         {enabled,true},
         {module,xdcr}]},
       {16395,
        [{name,<<"authorization failure while updating remote cluster ref">>},
         {description,<<"failed to update remote cluster ref because of authorization failure">>},
         {enabled,true},
         {module,xdcr}]},
       {16396,
        [{name,<<"access denied">>},
         {description,<<"access denied">>},
         {enabled,true},
         {module,xdcr}]},
       {20480,
        [{name,<<"opened DCP connection">>},
         {description,<<"opened DCP connection">>},
         {enabled,true},
         {module,memcached}]},
       {20482,
        [{name,<<"external memcached bucket flush">>},
         {description,<<"External user flushed the content of a memcached bucket">>},
         {enabled,true},
         {module,memcached}]},
       {20483,
        [{name,<<"invalid packet">>},
         {description,<<"Rejected an invalid packet">>},
         {enabled,true},
         {module,memcached}]},
       {20485,
        [{name,<<"authentication succeeded">>},
         {description,<<"Authentication to the cluster succeeded">>},
         {enabled,false},
         {module,memcached}]},
       {20488,
        [{name,<<"document read">>},
         {description,<<"Document was read">>},
         {enabled,false},
         {module,memcached}]},
       {20489,
        [{name,<<"document locked">>},
         {description,<<"Document was locked">>},
         {enabled,false},
         {module,memcached}]},
       {20490,
        [{name,<<"document modify">>},
         {description,<<"Document was modified">>},
         {enabled,false},
         {module,memcached}]},
       {20491,
        [{name,<<"document delete">>},
         {description,<<"Document was deleted">>},
         {enabled,false},
         {module,memcached}]},
       {20492,
        [{name,<<"select bucket">>},
         {description,<<"The specified bucket was selected">>},
         {enabled,true},
         {module,memcached}]},
       {20493,
        [{name,<<"session terminated">>},
         {description,<<"Session to the cluster has terminated">>},
         {enabled,false},
         {module,memcached}]},
       {20494,
        [{name,<<"tenant rate limited">>},
         {description,<<"The given tenant was rate limited">>},
         {enabled,true},
         {module,memcached}]},
       {24576,
        [{name,<<"Delete index">>},
         {description,<<"FTS index was deleted">>},
         {enabled,true},
         {module,fts}]},
       {24577,
        [{name,<<"Create/Update index">>},
         {description,<<"FTS index was created/Updated">>},
         {enabled,true},
         {module,fts}]},
       {24579,
        [{name,<<"Control index">>},
         {description,<<"FTS index control command was issued">>},
         {enabled,true},
         {module,fts}]},
       {24582,
        [{name,<<"GC run">>},
         {description,<<"GC run was triggered">>},
         {enabled,true},
         {module,fts}]},
       {24583,
        [{name,<<"CPU profile">>},
         {description,<<"CPU profiling was started">>},
         {enabled,true},
         {module,fts}]},
       {24584,
        [{name,<<"Memory profile">>},
         {description,<<"Memory profiling was started">>},
         {enabled,true},
         {module,fts}]},
       {28672,
        [{name,<<"SELECT statement">>},
         {description,<<"A N1QL SELECT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28673,
        [{name,<<"EXPLAIN statement">>},
         {description,<<"A N1QL EXPLAIN statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28674,
        [{name,<<"PREPARE statement">>},
         {description,<<"A N1QL PREPARE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28675,
        [{name,<<"INFER statement">>},
         {description,<<"A N1QL INFER statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28676,
        [{name,<<"INSERT statement">>},
         {description,<<"A N1QL INSERT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28677,
        [{name,<<"UPSERT statement">>},
         {description,<<"A N1QL UPSERT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28678,
        [{name,<<"DELETE statement">>},
         {description,<<"A N1QL DELETE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28679,
        [{name,<<"UPDATE statement">>},
         {description,<<"A N1QL UPDATE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28680,
        [{name,<<"MERGE statement">>},
         {description,<<"A N1QL MERGE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28681,
        [{name,<<"CREATE INDEX statement">>},
         {description,<<"A N1QL CREATE INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28682,
        [{name,<<"DROP INDEX statement">>},
         {description,<<"A N1QL DROP INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28683,
        [{name,<<"ALTER INDEX statement">>},
         {description,<<"A N1QL ALTER INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28684,
        [{name,<<"BUILD INDEX statement">>},
         {description,<<"A N1QL BUILD INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28685,
        [{name,<<"GRANT ROLE statement">>},
         {description,<<"A N1QL GRANT ROLE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28686,
        [{name,<<"REVOKE ROLE statement">>},
         {description,<<"A N1QL REVOKE ROLE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28687,
        [{name,<<"UNRECOGNIZED statement">>},
         {description,<<"An unrecognized statement was received by the N1QL query engine">>},
         {enabled,false},
         {module,n1ql}]},
       {28688,
        [{name,<<"CREATE PRIMARY INDEX statement">>},
         {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28689,
        [{name,<<"/admin/stats API request">>},
         {description,<<"An HTTP request was made to the API at /admin/stats.">>},
         {enabled,false},
         {module,n1ql}]},
       {28690,
        [{name,<<"/admin/vitals API request">>},
         {description,<<"An HTTP request was made to the API at /admin/vitals.">>},
         {enabled,false},
         {module,n1ql}]},
       {28691,
        [{name,<<"/admin/prepareds API request">>},
         {description,<<"An HTTP request was made to the API at /admin/prepareds.">>},
         {enabled,false},
         {module,n1ql}]},
       {28692,
        [{name,<<"/admin/active_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/active_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28693,
        [{name,<<"/admin/indexes/prepareds API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
         {enabled,false},
         {module,n1ql}]},
       {28694,
        [{name,<<"/admin/indexes/active_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28695,
        [{name,<<"/admin/indexes/completed_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28697,
        [{name,<<"/admin/ping API request">>},
         {description,<<"An HTTP request was made to the API at /admin/ping.">>},
         {enabled,false},
         {module,n1ql}]},
       {28698,
        [{name,<<"/admin/config API request">>},
         {description,<<"An HTTP request was made to the API at /admin/config.">>},
         {enabled,false},
         {module,n1ql}]},
       {28699,
        [{name,<<"/admin/ssl_cert API request">>},
         {description,<<"An HTTP request was made to the API at /admin/ssl_cert.">>},
         {enabled,false},
         {module,n1ql}]},
       {28700,
        [{name,<<"/admin/settings API request">>},
         {description,<<"An HTTP request was made to the API at /admin/settings.">>},
         {enabled,false},
         {module,n1ql}]},
       {28701,
        [{name,<<"/admin/clusters API request">>},
         {description,<<"An HTTP request was made to the API at /admin/clusters.">>},
         {enabled,false},
         {module,n1ql}]},
       {28702,
        [{name,<<"/admin/completed_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/completed_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28704,
        [{name,<<"/admin/functions API request">>},
         {description,<<"An HTTP request was made to the API at /admin/functions.">>},
         {enabled,false},
         {module,n1ql}]},
       {28705,
        [{name,<<"/admin/indexes/functions API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/functions.">>},
         {enabled,false},
         {module,n1ql}]},
       {28706,
        [{name,<<"CREATE FUNCTION statement">>},
         {description,<<"A N1QL CREATE FUNCTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28707,
        [{name,<<"DROP FUNCTION statement">>},
         {description,<<"A N1QL DROP FUNCTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28708,
        [{name,<<"EXECUTE FUNCTION statement">>},
         {description,<<"A N1QL EXECUTE FUNCTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28709,
        [{name,<<"/admin/tasks API request">>},
         {description,<<"An HTTP request was made to the API at /admin/tasks.">>},
         {enabled,false},
         {module,n1ql}]},
       {28710,
        [{name,<<"/admin/indexes/tasks API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/tasks.">>},
         {enabled,false},
         {module,n1ql}]},
       {28711,
        [{name,<<"/admin/dictionary_cache API request">>},
         {description,<<"An HTTP request was made to the API at /admin/dictionary_cache.">>},
         {enabled,false},
         {module,n1ql}]},
       {28712,
        [{name,<<"/admin/indexes/dictionary_cache API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/dictionary_cache.">>},
         {enabled,false},
         {module,n1ql}]},
       {28713,
        [{name,<<"CREATE SCOPE statement">>},
         {description,<<"A N1QL CREATE SCOPE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28714,
        [{name,<<"DROP SCOPE statement">>},
         {description,<<"A N1QL DROP SCOPE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28715,
        [{name,<<"CREATE COLLECTION statement">>},
         {description,<<"A N1QL CREATE COLLECTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28716,
        [{name,<<"DROP COLLECTION statement">>},
         {description,<<"A N1QL DROP COLLECTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28717,
        [{name,<<"FLUSH COLLECTION statement">>},
         {description,<<"A N1QL FLUSH COLLECTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28718,
        [{name,<<"UPDATE STATISTICS statement">>},
         {description,<<"A N1QL UPDATE STATISTICS statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28719,
        [{name,<<"ADVISE statement">>},
         {description,<<"A N1QL ADVISE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28720,
        [{name,<<"START TRANSACTION statement">>},
         {description,<<"A N1QL START TRANSACTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28721,
        [{name,<<"COMMIT TRANSACTION statement">>},
         {description,<<"A N1QL COMMIT TRANSACTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28722,
        [{name,<<"ROLLBACK TRANSACTION statement">>},
         {description,<<"A N1QL ROLLBACK TRANSACTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28723,
        [{name,<<"ROLLBACK TRANSACTION TO SAVEPOINT statement">>},
         {description,<<"A N1QL ROLLBACK TRANSACTION TO SAVEPOINT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28724,
        [{name,<<"SET TRANSACTION ISOLATION statement">>},
         {description,<<"A N1QL SET TRANSACTION ISOLATION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28725,
        [{name,<<"SAVEPOINT statement">>},
         {description,<<"A N1QL SAVEPOINT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28726,
        [{name,<<"/admin/transactions API request">>},
         {description,<<"An HTTP request was made to the API at /admin/transactions.">>},
         {enabled,false},
         {module,n1ql}]},
       {28727,
        [{name,<<"/admin/indexes/transactions API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/transactions.">>},
         {enabled,false},
         {module,n1ql}]},
       {28728,
        [{name,<<"N1QL backup / restore API request">>},
         {description,<<"An HTTP request was made to archive or restore N1QL metadata">>},
         {enabled,false},
         {module,n1ql}]},
       {28729,
        [{name,<<"/admin/shutdown API request">>},
         {description,<<"An HTTP request was made to initiate graceful shutdown">>},
         {enabled,false},
         {module,n1ql}]},
       {28730,
        [{name,<<"/admin/gc API request">>},
         {description,<<"An HTTP request was made to run garbage collection">>},
         {enabled,false},
         {module,n1ql}]},
       {28731,
        [{name,<<"/admin/ffdc API request">>},
         {description,<<"An HTTP request was made to run an FFDC collection">>},
         {enabled,false},
         {module,n1ql}]},
       {28732,
        [{name,<<"/admin/log/ API request">>},
         {description,<<"An HTTP request was made to access diagnostic logs">>},
         {enabled,false},
         {module,n1ql}]},
       {28733,
        [{name,<<"/admin/sequences_cache API request">>},
         {description,<<"An HTTP request was made to access sequences">>},
         {enabled,false},
         {module,n1ql}]},
       {28734,
        [{name,<<"CREATE SEQUENCE statement">>},
         {description,<<"A N1QL CREATE SEQUENCE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28735,
        [{name,<<"ALTER SEQUENCE statement">>},
         {description,<<"A N1QL ALTER SEQUENCE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28736,
        [{name,<<"DROP SEQUENCE statement">>},
         {description,<<"A N1QL DROP SEQUENCE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28737,
        [{name,<<"Migration abort">>},
         {description,<<"Migration was aborted">>},
         {enabled,false},
         {module,n1ql}]},
       {32768,
        [{name,<<"Create Function">>},
         {description,<<"Request to create or update eventing function definition">>},
         {enabled,true},
         {module,eventing}]},
       {32769,
        [{name,<<"Delete Function">>},
         {description,<<"Request to delete eventing function definition">>},
         {enabled,true},
         {module,eventing}]},
       {32770,
        [{name,<<"Fetch Functions">>},
         {description,<<"Request to fetch eventing function definition">>},
         {enabled,false},
         {module,eventing}]},
       {32771,
        [{name,<<"List Deployed">>},
         {description,<<"Request to fetch eventing deployed functions list">>},
         {enabled,false},
         {module,eventing}]},
       {32772,
        [{name,<<"Fetch Drafts">>},
         {description,<<"Request to fetch eventing function draft definitions">>},
         {enabled,false},
         {module,eventing}]},
       {32773,
        [{name,<<"Delete Drafts">>},
         {description,<<"Request to delete eventing function draft definitions">>},
         {enabled,true},
         {module,eventing}]},
       {32774,
        [{name,<<"Save Draft">>},
         {description,<<"Request to save a draft definition">>},
         {enabled,true},
         {module,eventing}]},
       {32775,
        [{name,<<"Start Debug">>},
         {description,<<"Request to start eventing function debugger">>},
         {enabled,true},
         {module,eventing}]},
       {32776,
        [{name,<<"Stop Debug">>},
         {description,<<"Request to stop eventing function debugger">>},
         {enabled,true},
         {module,eventing}]},
       {32777,
        [{name,<<"Start Tracing">>},
         {description,<<"Request to start tracing eventing function execution">>},
         {enabled,true},
         {module,eventing}]},
       {32778,
        [{name,<<"Stop Tracing">>},
         {description,<<"Request to stop tracing eventing function execution">>},
         {enabled,true},
         {module,eventing}]},
       {32779,
        [{name,<<"Set Settings">>},
         {description,<<"Request to save settings for an eventing function">>},
         {enabled,true},
         {module,eventing}]},
       {32780,
        [{name,<<"Fetch Config">>},
         {description,<<"Request to fetch eventing config">>},
         {enabled,false},
         {module,eventing}]},
       {32781,
        [{name,<<"Save Config">>},
         {description,<<"Request to save eventing config">>},
         {enabled,true},
         {module,eventing}]},
       {32783,
        [{name,<<"Get Settings">>},
         {description,<<"Request to fetch eventing function settings">>},
         {enabled,false},
         {module,eventing}]},
       {32784,
        [{name,<<"Import Functions">>},
         {description,<<"Request to import one or more eventing functions">>},
         {enabled,true},
         {module,eventing}]},
       {32785,
        [{name,<<"Export Functions">>},
         {description,<<"Request to export all eventing functions">>},
         {enabled,false},
         {module,eventing}]},
       {32786,
        [{name,<<"List Running">>},
         {description,<<"Request to fetch eventing running function list">>},
         {enabled,false},
         {module,eventing}]},
       {32789,
        [{name,<<"Deploy Function">>},
         {description,<<"Request to deploy eventing function">>},
         {enabled,true},
         {module,eventing}]},
       {32790,
        [{name,<<"Undeploy Function">>},
         {description,<<"Request to undeploy eventing function">>},
         {enabled,true},
         {module,eventing}]},
       {32791,
        [{name,<<"Pause Function">>},
         {description,<<"Request to pause eventing function">>},
         {enabled,true},
         {module,eventing}]},
       {32792,
        [{name,<<"Resume Function">>},
         {description,<<"Request to resume eventing function">>},
         {enabled,true},
         {module,eventing}]},
       {32793,
        [{name,<<"Backup Functions">>},
         {description,<<"Request to backup one or more eventing functions">>},
         {enabled,false},
         {module,eventing}]},
       {32794,
        [{name,<<"Restore Functions">>},
         {description,<<"Request to restore one or more eventing functions from a backup">>},
         {enabled,true},
         {module,eventing}]},
       {32795,
        [{name,<<"List Function">>},
         {description,<<"Request to fetch eventing functions">>},
         {enabled,false},
         {module,eventing}]},
       {32796,
        [{name,<<"Function Status">>},
         {description,<<"Request to fetch eventing function status">>},
         {enabled,false},
         {module,eventing}]},
       {32797,
        [{name,<<"Clear Stats">>},
         {description,<<"Request to reset eventing function stats">>},
         {enabled,true},
         {module,eventing}]},
       {32798,
        [{name,<<"Fetch Stats">>},
         {description,<<"Request to fetch eventing function stats">>},
         {enabled,false},
         {module,eventing}]},
       {32799,
        [{name,<<"Eventing Cluster Stats">>},
         {description,<<"Request to fetch eventing cluster stats">>},
         {enabled,false},
         {module,eventing}]},
       {32801,
        [{name,<<"Eventing System Event">>},
         {description,<<"Request to execute eventing node related functions">>},
         {enabled,false},
         {module,eventing}]},
       {32802,
        [{name,<<"Get User Info">>},
         {description,<<"Request to get real_userid eventing permissions">>},
         {enabled,false},
         {module,eventing}]},
       {36865,
        [{name,<<"Service configuration change">>},
         {description,<<"A successful service configuration change was made.">>},
         {enabled,true},
         {module,analytics}]},
       {36866,
        [{name,<<"Node configuration change">>},
         {description,<<"A successful node configuration change was made.">>},
         {enabled,true},
         {module,analytics}]},
       {36867,
        [{name,<<"SELECT statement">>},
         {description,<<"A N1QL SELECT statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36868,
        [{name,<<"CREATE DATAVERSE statement">>},
         {description,<<"A N1QL CREATE DATAVERSE statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36869,
        [{name,<<"DROP DATAVERSE statement">>},
         {description,<<"A N1QL DROP DATAVERSE statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36870,
        [{name,<<"CREATE DATASET statement">>},
         {description,<<"A N1QL CREATE DATASET statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36871,
        [{name,<<"DROP DATASET statement">>},
         {description,<<"A N1QL DROP DATASET statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36872,
        [{name,<<"CREATE INDEX statement">>},
         {description,<<"A N1QL CREATE INDEX statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36873,
        [{name,<<"DROP INDEX statement">>},
         {description,<<"A N1QL DROP INDEX statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36877,
        [{name,<<"CONNECT LINK statement">>},
         {description,<<"A N1QL CONNECT LINK statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36878,
        [{name,<<"DISCONNECT LINK statement">>},
         {description,<<"A N1QL DISCONNECT LINK statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36879,
        [{name,<<"UNRECOGNIZED statement">>},
         {description,<<"An UNRECOGNIZED N1QL statement was encountered">>},
         {enabled,false},
         {module,analytics}]},
       {36880,
        [{name,<<"ALTER COLLECTION statement">>},
         {description,<<"A N1QL ALTER COLLECTION statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {40960,
        [{name,<<"Create Design Doc">>},
         {description,<<"Design Doc is Created">>},
         {enabled,true},
         {module,view_engine}]},
       {40961,
        [{name,<<"Delete Design Doc">>},
         {description,<<"Design Doc is Deleted">>},
         {enabled,true},
         {module,view_engine}]},
       {40962,
        [{name,<<"Query DDoc Meta Data">>},
         {description,<<"Design Doc Meta Data Query Request">>},
         {enabled,true},
         {module,view_engine}]},
       {40963,
        [{name,<<"View Query">>},
         {description,<<"View Query Request">>},
         {enabled,false},
         {module,view_engine}]},
       {40964,
        [{name,<<"Update Design Doc">>},
         {description,<<"Design Doc is Updated">>},
         {enabled,true},
         {module,view_engine}]},
       {40966,
        [{name,<<"Access denied">>},
         {description,<<"Access denied to the REST API due to invalid permissions or credentials">>},
         {enabled,true},
         {module,view_engine}]},
       {45056,
        [{name,<<"Modify configuration">>},
         {description,<<"Backup service configuration was modified">>},
         {enabled,true},
         {module,backup}]},
       {45057,
        [{name,<<"Fetch configuration">>},
         {description,<<"Backup service configuration was retrieved">>},
         {enabled,false},
         {module,backup}]},
       {45058,
        [{name,<<"Add plan">>},
         {description,<<"A new backup plan was added">>},
         {enabled,true},
         {module,backup}]},
       {45059,
        [{name,<<"Modify plan">>},
         {description,<<"Existing backup plan was modified">>},
         {enabled,true},
         {module,backup}]},
       {45060,
        [{name,<<"Delete plan">>},
         {description,<<"A backup plan was removed">>},
         {enabled,true},
         {module,backup}]},
       {45061,
        [{name,<<"Fetch plan">>},
         {description,<<"One or more backup plans where fetched">>},
         {enabled,false},
         {module,backup}]},
       {45062,
        [{name,<<"Add repository">>},
         {description,<<"A new active backup repository was added">>},
         {enabled,true},
         {module,backup}]},
       {45063,
        [{name,<<"Archive repository">>},
         {description,<<"An active repository was archived">>},
         {enabled,true},
         {module,backup}]},
       {45064,
        [{name,<<"Pause repository">>},
         {description,<<"An active repository was paused">>},
         {enabled,true},
         {module,backup}]},
       {45065,
        [{name,<<"Resume repository">>},
         {description,<<"An active repository was resumed">>},
         {enabled,true},
         {module,backup}]},
       {45066,
        [{name,<<"Fetch repository">>},
         {description,<<"A repository was fetched">>},
         {enabled,false},
         {module,backup}]},
       {45067,
        [{name,<<"Restore repository">>},
         {description,<<"The repository data was restored">>},
         {enabled,true},
         {module,backup}]},
       {45068,
        [{name,<<"Backup repository">>},
         {description,<<"A manual backup was triggered on an active repository">>},
         {enabled,true},
         {module,backup}]},
       {45069,
        [{name,<<"Merge repository">>},
         {description,<<"A manual merge was triggered on an active repository">>},
         {enabled,true},
         {module,backup}]},
       {45070,
        [{name,<<"Info repository">>},
         {description,<<"Information about the structure and contents of the backup repository was fetched.">>},
         {enabled,false},
         {module,backup}]},
       {45071,
        [{name,<<"Examine repository">>},
         {description,<<"A document was retrieved from the repository backups">>},
         {enabled,true},
         {module,backup}]},
       {45072,
        [{name,<<"Delete repository">>},
         {description,<<"A repository was deleted">>},
         {enabled,true},
         {module,backup}]},
       {45073,
        [{name,<<"Delete backup">>},
         {description,<<"An active repository backup was deleted">>},
         {enabled,true},
         {module,backup}]},
       {45074,
        [{name,<<"Access denied">>},
         {description,<<"A user has been denied access to the REST API due to invalid permissions or credentials">>},
         {enabled,true},
         {module,backup}]}]},
 {delete,rbac_upgrade},
 {set,auto_failover_cfg,
      [{disable_max_count,false},
       {enabled,true},
       {timeout,120},
       {count,0},
       {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
       {failover_server_group,false},
       {max_count,1},
       {failed_over_server_groups,[]},
       {can_abort_rebalance,true},
       {failover_preserve_durability_majority,false}]},
 {delete,memory_alert_email},
 {delete,memory_alert_popup},
 {delete,popup_alerts_auto_failover_upgrade_70_fixed},
 {set,{metakv,<<"/indexing/settings/config">>},
      <<"{\"indexer.settings.compaction.abort_exceed_interval\":false,\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.enable_page_bloom_filter\":false,\"indexer.settings.enable_shard_affinity\":false,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.log_level\":\"info\",\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.num_replica\":0,\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.rebalance.blob_storage_bucket\":\"\",\"indexer.settings.rebalance.blob_storage_prefix\":\"\",\"indexer.settings.rebalance.blob_storage_region\":\"\",\"indexer.settings.rebalance.blob_storage_scheme\":\"\",\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.thresholds.mem_high\":70,\"indexer.settings.thresholds.mem_low\":50,\"indexer.settings.thresholds.units_high\":60,\"indexer.settings.thresholds.units_low\":40}">>},
 {set,{metakv,<<"/query/settings/config">>},
      <<"{\"cleanupclientattempts\":true,\"cleanuplostattempts\":true,\"cleanupwindow\":\"60s\",\"completed-limit\":4000,\"completed-max-plan-size\":262144,\"completed-threshold\":1000,\"loglevel\":\"info\",\"max-parallelism\":1,\"memory-quota\":0,\"n1ql-feat-ctrl\":76,\"node-quota\":0,\"node-quota-val-percent\":67,\"num-cpus\":0,\"numatrs\":1024,\"pipeline-batch\":16,\"pipeline-cap\":512,\"prepared-limit\":16384,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"query.settings.tmp_space_size\":5120,\"scan-cap\":512,\"timeout\":0,\"txtimeout\":\"0ms\",\"use-cbo\":true,\"use-replica\":\"unset\"}">>},
 {set,{metakv,<<"/analytics/settings/config">>},
      <<"{\"analytics.settings.blob_storage_bucket\":\"\",\"analytics.settings.blob_storage_prefix\":\"\",\"analytics.settings.blob_storage_region\":\"\",\"analytics.settings.blob_storage_scheme\":\"\",\"analytics.settings.num_replicas\":0}">>},
 {delete,mb33750_workaround_enabled},
 {delete,cert_and_pkey},
 {set,resource_management,
      [{bucket,[{resident_ratio,[{enabled,false},
                                 {couchstore_minimum,1},
                                 {magma_minimum,0.2}]},
                {data_size,[{enabled,false},
                            {couchstore_maximum,2},
                            {magma_maximum,16}]}]},
       {index,[]},
       {cores_per_bucket,[{enabled,false},{minimum,0.4}]},
       {disk_usage,[{enabled,false},{maximum,96}]},
       {collections_per_quota,[{enabled,false},{maximum,1}]}]}]

[ns_server:debug,2025-05-15T18:46:50.558Z,ns_1@cb.local:roles_cache<0.458.0>:active_cache:clean:181]Clearing the roles_cache cache
[ns_server:debug,2025-05-15T18:46:50.559Z,ns_1@cb.local:ns_cookie_manager<0.238.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[ns_server:debug,2025-05-15T18:46:50.559Z,ns_1@cb.local:tombstone_keeper<0.277.0>:tombstone_keeper:handle_call:49]Refreshed with timestamps []
[ns_server:debug,2025-05-15T18:46:50.559Z,ns_1@cb.local:<0.854.0>:ns_node_disco:do_nodes_wanted_updated_fun:210]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"1ZVTl+fgxazsGs5XltMEJFjUhLpt0If4lYDK2laySmI=">>}
[ns_server:debug,2025-05-15T18:46:50.559Z,ns_1@cb.local:<0.854.0>:ns_node_disco:do_nodes_wanted_updated_fun:213]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"1ZVTl+fgxazsGs5XltMEJFjUhLpt0If4lYDK2laySmI=">>}
[ns_server:debug,2025-05-15T18:46:50.560Z,ns_1@cb.local:ns_config_rep<0.548.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([audit_decriptors,auto_failover_cfg,
                               cluster_compat_version,rbac_upgrade,
                               resource_management,
                               {metakv,<<"/analytics/settings/config">>},
                               {metakv,<<"/indexing/settings/config">>},
                               {metakv,<<"/query/settings/config">>}]..)
[ns_server:debug,2025-05-15T18:46:50.561Z,ns_1@cb.local:ns_config_rep<0.548.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 3 us
[ns_server:debug,2025-05-15T18:46:50.561Z,ns_1@cb.local:memcached_permissions<0.533.0>:memcached_cfg:write_cfg:156]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2025-05-15T18:46:50.561Z,ns_1@cb.local:ns_config_rep<0.548.0>:ns_config_rep:handle_call:149]Got full synchronization request from 'ns_1@cb.local'
[ns_server:debug,2025-05-15T18:46:50.561Z,ns_1@cb.local:ns_config_rep<0.548.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 11 us
[user:warn,2025-05-15T18:46:50.561Z,ns_1@cb.local:compat_mode_manager<0.820.0>:compat_mode_manager:handle_consider_switching_compat_mode:43]Changed cluster compat mode from [7,1] to [7,6]
[error_logger:info,2025-05-15T18:46:50.561Z,ns_1@cb.local:ns_orchestrator_sup<0.818.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.820.0>},
              {id,compat_mode_manager},
              {mfargs,{compat_mode_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:50.564Z,ns_1@cb.local:memcached_permissions<0.533.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,{user,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2025-05-15T18:46:50.567Z,ns_1@cb.local:memcached_passwords<0.530.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,{auth,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2025-05-15T18:46:50.568Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:handle_call:57]File rename from "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac" is requested
[ns_server:debug,2025-05-15T18:46:50.567Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
audit_decriptors ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554010}}]},
 {8243,
  [{name,<<"mutate document">>},
   {description,<<"Document was mutated via the REST API">>},
   {enabled,true},
   {module,ns_server}]},
 {8255,
  [{name,<<"read document">>},
   {description,<<"Document was read via the REST API">>},
   {enabled,false},
   {module,ns_server}]},
 {8257,
  [{name,<<"alert email sent">>},
   {description,<<"An alert email was successfully sent">>},
   {enabled,true},
   {module,ns_server}]},
 {8265,
  [{name,<<"RBAC information retrieved">>},
   {description,<<"RBAC information was retrieved">>},
   {enabled,true},
   {module,ns_server}]},
 {16384,
  [{name,<<"remote cluster ref creation">>},
   {description,<<"created remote cluster ref">>},
   {enabled,true},
   {module,xdcr}]},
 {16385,
  [{name,<<"remote cluster ref update">>},
   {description,<<"updated remote cluster ref">>},
   {enabled,true},
   {module,xdcr}]},
 {16386,
  [{name,<<"remote cluster ref deletion">>},
   {description,<<"deleted remote cluster ref">>},
   {enabled,true},
   {module,xdcr}]},
 {16387,
  [{name,<<"replication creation">>},
   {description,<<"created replication">>},
   {enabled,true},
   {module,xdcr}]},
 {16388,
  [{name,<<"replication pause">>},
   {description,<<"paused replication">>},
   {enabled,true},
   {module,xdcr}]},
 {16389,
  [{name,<<"replication resume">>},
   {description,<<"resumed replication">>},
   {enabled,true},
   {module,xdcr}]},
 {16390,
  [{name,<<"replication cancellation">>},
   {description,<<"canceled replication">>},
   {enabled,true},
   {module,xdcr}]},
 {16391,
  [{name,<<"default replication settings update">>},
   {description,<<"updated default replication settings">>},
   {enabled,true},
   {module,xdcr}]},
 {16392,
  [{name,<<"individual replication settings update">>},
   {description,<<"updated individual replication settings">>},
   {enabled,true},
   {module,xdcr}]},
 {16393,
  [{name,<<"bucket settings update">>},
   {description,<<"updated bucket settings">>},
   {enabled,true},
   {module,xdcr}]},
 {16394,
  [{name,<<"authorization failure while adding remote cluster ref">>},
   {description,<<"failed to add remote cluster ref because of authorization failure">>},
   {enabled,true},
   {module,xdcr}]},
 {16395,
  [{name,<<"authorization failure while updating remote cluster ref">>},
   {description,<<"failed to update remote cluster ref because of authorization failure">>},
   {enabled,true},
   {module,xdcr}]},
 {16396,
  [{name,<<"access denied">>},
   {description,<<"access denied">>},
   {enabled,true},
   {module,xdcr}]},
 {20480,
  [{name,<<"opened DCP connection">>},
   {description,<<"opened DCP connection">>},
   {enabled,true},
   {module,memcached}]},
 {20482,
  [{name,<<"external memcached bucket flush">>},
   {description,<<"External user flushed the content of a memcached bucket">>},
   {enabled,true},
   {module,memcached}]},
 {20483,
  [{name,<<"invalid packet">>},
   {description,<<"Rejected an invalid packet">>},
   {enabled,true},
   {module,memcached}]},
 {20485,
  [{name,<<"authentication succeeded">>},
   {description,<<"Authentication to the cluster succeeded">>},
   {enabled,false},
   {module,memcached}]},
 {20488,
  [{name,<<"document read">>},
   {description,<<"Document was read">>},
   {enabled,false},
   {module,memcached}]},
 {20489,
  [{name,<<"document locked">>},
   {description,<<"Document was locked">>},
   {enabled,false},
   {module,memcached}]},
 {20490,
  [{name,<<"document modify">>},
   {description,<<"Document was modified">>},
   {enabled,false},
   {module,memcached}]},
 {20491,
  [{name,<<"document delete">>},
   {description,<<"Document was deleted">>},
   {enabled,false},
   {module,memcached}]},
 {20492,
  [{name,<<"select bucket">>},
   {description,<<"The specified bucket was selected">>},
   {enabled,true},
   {module,memcached}]},
 {20493,
  [{name,<<"session terminated">>},
   {description,<<"Session to the cluster has terminated">>},
   {enabled,false},
   {module,memcached}]},
 {20494,
  [{name,<<"tenant rate limited">>},
   {description,<<"The given tenant was rate limited">>},
   {enabled,true},
   {module,memcached}]},
 {24576,
  [{name,<<"Delete index">>},
   {description,<<"FTS index was deleted">>},
   {enabled,true},
   {module,fts}]},
 {24577,
  [{name,<<"Create/Update index">>},
   {description,<<"FTS index was created/Updated">>},
   {enabled,true},
   {module,fts}]},
 {24579,
  [{name,<<"Control index">>},
   {description,<<"FTS index control command was issued">>},
   {enabled,true},
   {module,fts}]},
 {24582,
  [{name,<<"GC run">>},
   {description,<<"GC run was triggered">>},
   {enabled,true},
   {module,fts}]},
 {24583,
  [{name,<<"CPU profile">>},
   {description,<<"CPU profiling was started">>},
   {enabled,true},
   {module,fts}]},
 {24584,
  [{name,<<"Memory profile">>},
   {description,<<"Memory profiling was started">>},
   {enabled,true},
   {module,fts}]},
 {28672,
  [{name,<<"SELECT statement">>},
   {description,<<"A N1QL SELECT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28673,
  [{name,<<"EXPLAIN statement">>},
   {description,<<"A N1QL EXPLAIN statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28674,
  [{name,<<"PREPARE statement">>},
   {description,<<"A N1QL PREPARE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28675,
  [{name,<<"INFER statement">>},
   {description,<<"A N1QL INFER statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28676,
  [{name,<<"INSERT statement">>},
   {description,<<"A N1QL INSERT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28677,
  [{name,<<"UPSERT statement">>},
   {description,<<"A N1QL UPSERT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28678,
  [{name,<<"DELETE statement">>},
   {description,<<"A N1QL DELETE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28679,
  [{name,<<"UPDATE statement">>},
   {description,<<"A N1QL UPDATE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28680,
  [{name,<<"MERGE statement">>},
   {description,<<"A N1QL MERGE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28681,
  [{name,<<"CREATE INDEX statement">>},
   {description,<<"A N1QL CREATE INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28682,
  [{name,<<"DROP INDEX statement">>},
   {description,<<"A N1QL DROP INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28683,
  [{name,<<"ALTER INDEX statement">>},
   {description,<<"A N1QL ALTER INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28684,
  [{name,<<"BUILD INDEX statement">>},
   {description,<<"A N1QL BUILD INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28685,
  [{name,<<"GRANT ROLE statement">>},
   {description,<<"A N1QL GRANT ROLE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28686,
  [{name,<<"REVOKE ROLE statement">>},
   {description,<<"A N1QL REVOKE ROLE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28687,
  [{name,<<"UNRECOGNIZED statement">>},
   {description,<<"An unrecognized statement was received by the N1QL query engine">>},
   {enabled,false},
   {module,n1ql}]},
 {28688,
  [{name,<<"CREATE PRIMARY INDEX statement">>},
   {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28689,
  [{name,<<"/admin/stats API request">>},
   {description,<<"An HTTP request was made to the API at /admin/stats.">>},
   {enabled,false},
   {module,n1ql}]},
 {28690,
  [{name,<<"/admin/vitals API request">>},
   {description,<<"An HTTP request was made to the API at /admin/vitals.">>},
   {enabled,false},
   {module,n1ql}]},
 {28691,
  [{name,<<"/admin/prepareds API request">>},
   {description,<<"An HTTP request was made to the API at /admin/prepareds.">>},
   {enabled,false},
   {module,n1ql}]},
 {28692,
  [{name,<<"/admin/active_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/active_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28693,
  [{name,<<"/admin/indexes/prepareds API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
   {enabled,false},
   {module,n1ql}]},
 {28694,
  [{name,<<"/admin/indexes/active_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28695,
  [{name,<<"/admin/indexes/completed_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28697,
  [{name,<<"/admin/ping API request">>},
   {description,<<"An HTTP request was made to the API at /admin/ping.">>},
   {enabled,false},
   {module,n1ql}]},
 {28698,
  [{name,<<"/admin/config API request">>},
   {description,<<"An HTTP request was made to the API at /admin/config.">>},
   {enabled,false},
   {module,n1ql}]},
 {28699,
  [{name,<<"/admin/ssl_cert API request">>},
   {description,<<"An HTTP request was made to the API at /admin/ssl_cert.">>},
   {enabled,false},
   {module,n1ql}]},
 {28700,
  [{name,<<"/admin/settings API request">>},
   {description,<<"An HTTP request was made to the API at /admin/settings.">>},
   {enabled,false},
   {module,n1ql}]},
 {28701,
  [{name,<<"/admin/clusters API request">>},
   {description,<<"An HTTP request was made to the API at /admin/clusters.">>},
   {enabled,false},
   {module,n1ql}]},
 {28702,
  [{name,<<"/admin/completed_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/completed_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28704,
  [{name,<<"/admin/functions API request">>},
   {description,<<"An HTTP request was made to the API at /admin/functions.">>},
   {enabled,false},
   {module,n1ql}]},
 {28705,
  [{name,<<"/admin/indexes/functions API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/functions.">>},
   {enabled,false},
   {module,n1ql}]},
 {28706,
  [{name,<<"CREATE FUNCTION statement">>},
   {description,<<"A N1QL CREATE FUNCTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28707,
  [{name,<<"DROP FUNCTION statement">>},
   {description,<<"A N1QL DROP FUNCTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28708,
  [{name,<<"EXECUTE FUNCTION statement">>},
   {description,<<"A N1QL EXECUTE FUNCTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28709,
  [{name,<<"/admin/tasks API request">>},
   {description,<<"An HTTP request was made to the API at /admin/tasks.">>},
   {enabled,false},
   {module,n1ql}]},
 {28710,
  [{name,<<"/admin/indexes/tasks API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/tasks.">>},
   {enabled,false},
   {module,n1ql}]},
 {28711,
  [{name,<<"/admin/dictionary_cache API request">>},
   {description,<<"An HTTP request was made to the API at /admin/dictionary_cache.">>},
   {enabled,false},
   {module,n1ql}]},
 {28712,
  [{name,<<"/admin/indexes/dictionary_cache API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/dictionary_cache.">>},
   {enabled,false},
   {module,n1ql}]},
 {28713,
  [{name,<<"CREATE SCOPE statement">>},
   {description,<<"A N1QL CREATE SCOPE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28714,
  [{name,<<"DROP SCOPE statement">>},
   {description,<<"A N1QL DROP SCOPE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28715,
  [{name,<<"CREATE COLLECTION statement">>},
   {description,<<"A N1QL CREATE COLLECTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28716,
  [{name,<<"DROP COLLECTION statement">>},
   {description,<<"A N1QL DROP COLLECTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28717,
  [{name,<<"FLUSH COLLECTION statement">>},
   {description,<<"A N1QL FLUSH COLLECTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28718,
  [{name,<<"UPDATE STATISTICS statement">>},
   {description,<<"A N1QL UPDATE STATISTICS statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28719,
  [{name,<<"ADVISE statement">>},
   {description,<<"A N1QL ADVISE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28720,
  [{name,<<"START TRANSACTION statement">>},
   {description,<<"A N1QL START TRANSACTION statement was execu"...>>},
   {enabled,false},
   {module,n1ql}]},
 {28721,
  [{name,<<"COMMIT TRANSACTION statement">>},
   {description,<<"A N1QL COMMIT TRANSACTION statement was "...>>},
   {enabled,false},
   {module,n1ql}]},
 {28722,
  [{name,<<"ROLLBACK TRANSACTION statement">>},
   {description,<<"A N1QL ROLLBACK TRANSACTION statemen"...>>},
   {enabled,false},
   {module,n1ql}]},
 {28723,
  [{name,<<"ROLLBACK TRANSACTION TO SAVEPOINT st"...>>},
   {description,<<"A N1QL ROLLBACK TRANSACTION TO S"...>>},
   {enabled,false},
   {module,n1ql}]},
 {28724,
  [{name,<<"SET TRANSACTION ISOLATION statem"...>>},
   {description,<<"A N1QL SET TRANSACTION ISOLA"...>>},
   {enabled,false},
   {module,n1ql}]},
 {28725,
  [{name,<<"SAVEPOINT statement">>},
   {description,<<"A N1QL SAVEPOINT stateme"...>>},
   {enabled,false},
   {module,n1ql}]},
 {28726,
  [{name,<<"/admin/transactions API "...>>},
   {description,<<"An HTTP request was "...>>},
   {enabled,false},
   {module,n1ql}]},
 {28727,
  [{name,<<"/admin/indexes/trans"...>>},
   {description,<<"An HTTP request "...>>},
   {enabled,false},
   {module,n1ql}]},
 {28728,
  [{name,<<"N1QL backup / re"...>>},
   {description,<<"An HTTP requ"...>>},
   {enabled,false},
   {module,n1ql}]},
 {28729,
  [{name,<<"/admin/shutd"...>>},
   {description,<<"An HTTP "...>>},
   {enabled,false},
   {module,n1ql}]},
 {28730,
  [{name,<<"/admin/g"...>>},
   {description,<<"An H"...>>},
   {enabled,false},
   {module,...}]},
 {28731,[{name,<<"/adm"...>>},{description,<<...>>},{enabled,...},{...}]},
 {28732,[{name,<<...>>},{description,...},{...}|...]},
 {28733,[{name,...},{...}|...]},
 {28734,[{...}|...]},
 {28735,[...]},
 {28736,...},
 {...}|...]
[ns_server:debug,2025-05-15T18:46:50.568Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
cluster_compat_version ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{3,63914554010}}]},7,6]
[ns_server:debug,2025-05-15T18:46:50.568Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
rbac_upgrade ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554010}}]}|
 '_deleted']
[ns_server:debug,2025-05-15T18:46:50.568Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
auto_failover_cfg ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554010}}]},
 {disable_max_count,false},
 {enabled,true},
 {timeout,120},
 {count,0},
 {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
 {failover_server_group,false},
 {max_count,1},
 {failed_over_server_groups,[]},
 {can_abort_rebalance,true},
 {failover_preserve_durability_majority,false}]
[ns_server:debug,2025-05-15T18:46:50.568Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
resource_management ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554010}}]},
 {bucket,[{resident_ratio,[{enabled,false},
                           {couchstore_minimum,1},
                           {magma_minimum,0.2}]},
          {data_size,[{enabled,false},
                      {couchstore_maximum,2},
                      {magma_maximum,16}]}]},
 {index,[]},
 {cores_per_bucket,[{enabled,false},{minimum,0.4}]},
 {disk_usage,[{enabled,false},{maximum,96}]},
 {collections_per_quota,[{enabled,false},{maximum,1}]}]
[ns_server:debug,2025-05-15T18:46:50.568Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/analytics/settings/config">>} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554010}}]}|
 <<"{\"analytics.settings.blob_storage_bucket\":\"\",\"analytics.settings.blob_storage_prefix\":\"\",\"analytics.settings.blob_storage_region\":\"\",\"analytics.settings.blob_storage_scheme\":\"\",\"analytics.settings.num_replicas\":0}">>]
[ns_server:debug,2025-05-15T18:46:50.568Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554010}}]}|
 <<"{\"indexer.settings.compaction.abort_exceed_interval\":false,\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.enable_page_bloom_filter\":false,\"indexer.settings.enable_"...>>]
[ns_server:debug,2025-05-15T18:46:50.568Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/query/settings/config">>} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554010}}]}|
 <<"{\"cleanupclientattempts\":true,\"cleanuplostattempts\":true,\"cleanupwindow\":\"60s\",\"completed-limit\":4000,\"completed-max-plan-size\":262144,\"completed-threshold\":1000,\"loglevel\":\"info\",\"max-parallelism\":1,\"memory-quota\":0,\"n1ql-feat-ctrl\":76,\"node-quota\":0,\"node-quota-val-percent\":67,\"num-cpus\":0,\"numatrs\":1024,\"pipeline-batch\":16,\"pipeline-cap\":512,\"prepared-limit\":16384,\"query.settings.cu"...>>]
[ns_server:debug,2025-05-15T18:46:50.569Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:handle_call:57]File rename from "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw" is requested
[ns_server:debug,2025-05-15T18:46:50.569Z,ns_1@cb.local:memcached_permissions<0.533.0>:memcached_cfg:rename_and_refresh:178]Successfully renamed "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2025-05-15T18:46:50.570Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:handle_cast:67]Refresh of rbac requested
[ns_server:debug,2025-05-15T18:46:50.570Z,ns_1@cb.local:memcached_passwords<0.530.0>:memcached_cfg:rename_and_refresh:178]Successfully renamed "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2025-05-15T18:46:50.570Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:handle_cast:67]Refresh of isasl requested
[ns_server:debug,2025-05-15T18:46:50.571Z,ns_1@cb.local:memcached_passwords<0.530.0>:memcached_cfg:write_cfg:156]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:info,2025-05-15T18:46:50.572Z,ns_1@cb.local:<0.849.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service capi_ssl_service
[ns_server:info,2025-05-15T18:46:50.573Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:notify_services:1162]Succesfully notified services [memcached,capi_ssl_service]
[ns_server:debug,2025-05-15T18:46:50.573Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:restart_regenerate_client_cert_timer:1447]Time left before client cert regeneration: 70588796000
[ns_server:debug,2025-05-15T18:46:50.573Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:maybe_store_ca_certs:832]Considering to store CA certs
[ns_server:debug,2025-05-15T18:46:50.574Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:restart_regenerate_client_cert_timer:1447]Time left before client cert regeneration: 70588796000
[ns_server:debug,2025-05-15T18:46:50.574Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',prometheus_auth_info} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554010}}]}|
 {"@prometheus",
  {auth,
   [{<<"hash">>,
     {[{<<"hashes">>,
        {sanitized,<<"Vmqy8YpGQ1ZVVOJNXOVKZaSx7qimZeVP/vMQ9t99FKU=">>}},
       {<<"algorithm">>,<<"argon2id">>},
       {<<"salt">>,<<"RaDchxQfbWx/9yCvS4KJhw==">>},
       {<<"parallelism">>,1},
       {<<"time">>,3},
       {<<"memory">>,524288}]}},
    {<<"scram-sha-512">>,
     {[{<<"salt">>,
        <<"WL9ZWEHJ76WYVLC0m22/0VNF6T50k/J0Z7Okpz3sW4PgL9CASwlonzChieVERPB3Sib20fCxP6nMQ/SgjVQjEg==">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"vX4Kcf9uKEqkQwiL3H4KjNM4iq2tHqkJD/vMtc9V/Xg=">>}}]}},
    {<<"scram-sha-256">>,
     {[{<<"salt">>,<<"YkWxrYn6kotQkszjlA7ouXNPvh8HqG960AycvYrvGOo=">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"gECRKBSDc8Gg85IUVUx8OK6b2QieozWWeKpDySi+GnU=">>}}]}},
    {<<"scram-sha-1">>,
     {[{<<"salt">>,<<"uv9yTXGPC84w9yF7otsSjKjc9bM=">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"SOMxs6h/s5Xzi9/iuNI3GZdbix+jQu+XMiQI1CxtTOI=">>}}]}}]}}]
[ns_server:debug,2025-05-15T18:46:50.574Z,ns_1@cb.local:ns_config_rep<0.548.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([{node,'ns_1@cb.local',prometheus_auth_info}]..)
[ns_server:debug,2025-05-15T18:46:50.574Z,ns_1@cb.local:prometheus_cfg<0.517.0>:prometheus_cfg:maybe_apply_new_settings:704]Settings didn't change, ignoring update
[ns_server:debug,2025-05-15T18:46:50.574Z,ns_1@cb.local:prometheus_cfg<0.517.0>:prometheus_cfg:maybe_apply_new_settings:704]Settings didn't change, ignoring update
[ns_server:debug,2025-05-15T18:46:50.586Z,ns_1@cb.local:memcached_passwords<0.530.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,{auth,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2025-05-15T18:46:50.587Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:handle_call:57]File rename from "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw" is requested
[ns_server:debug,2025-05-15T18:46:50.587Z,ns_1@cb.local:memcached_passwords<0.530.0>:memcached_cfg:rename_and_refresh:178]Successfully renamed "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2025-05-15T18:46:50.587Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:handle_cast:67]Refresh of isasl requested
[ns_server:debug,2025-05-15T18:46:50.588Z,ns_1@cb.local:memcached_passwords<0.530.0>:memcached_cfg:write_cfg:156]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[error_logger:info,2025-05-15T18:46:50.588Z,ns_1@cb.local:ns_orchestrator_child_sup<0.876.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.877.0>},
              {id,ns_janitor_server},
              {mfargs,{ns_janitor_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:50.600Z,ns_1@cb.local:ns_orchestrator_child_sup<0.876.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          auto_reprovision},
                                         auto_reprovision,[],[]]): started as <0.878.0> on 'ns_1@cb.local'

[error_logger:info,2025-05-15T18:46:50.600Z,ns_1@cb.local:ns_orchestrator_child_sup<0.876.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.878.0>},
              {id,auto_reprovision},
              {mfargs,{auto_reprovision,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:50.601Z,ns_1@cb.local:memcached_passwords<0.530.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,{auth,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2025-05-15T18:46:50.602Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:handle_call:57]File rename from "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw" is requested
[ns_server:debug,2025-05-15T18:46:50.602Z,ns_1@cb.local:memcached_passwords<0.530.0>:memcached_cfg:rename_and_refresh:178]Successfully renamed "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2025-05-15T18:46:50.602Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:handle_cast:67]Refresh of isasl requested
[ns_server:info,2025-05-15T18:46:50.603Z,ns_1@cb.local:ns_orchestrator_child_sup<0.876.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,auto_rebalance},
                                         auto_rebalance,[],[]]): started as <0.879.0> on 'ns_1@cb.local'

[error_logger:info,2025-05-15T18:46:50.603Z,ns_1@cb.local:ns_orchestrator_child_sup<0.876.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.879.0>},
              {id,auto_rebalance},
              {mfargs,{auto_rebalance,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:50.603Z,ns_1@cb.local:ns_orchestrator_child_sup<0.876.0>:misc:start_singleton:913]start_singleton(gen_statem, start_link, [{via,leader_registry,ns_orchestrator},
                                         ns_orchestrator,[],[]]): started as <0.880.0> on 'ns_1@cb.local'

[error_logger:info,2025-05-15T18:46:50.603Z,ns_1@cb.local:ns_orchestrator_child_sup<0.876.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.880.0>},
              {id,ns_orchestrator},
              {mfargs,{ns_orchestrator,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.603Z,ns_1@cb.local:ns_orchestrator_sup<0.818.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.876.0>},
              {id,ns_orchestrator_child_sup},
              {mfargs,{ns_orchestrator_child_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:50.603Z,ns_1@cb.local:<0.882.0>:auto_failover:init:223]init auto_failover.
[user:info,2025-05-15T18:46:50.606Z,ns_1@cb.local:<0.882.0>:auto_failover:enable_auto_failover:460]Enabled auto-failover with timeout 120 and max count 1
[ns_server:debug,2025-05-15T18:46:50.606Z,ns_1@cb.local:<0.882.0>:auto_failover:update_and_save_auto_failover_state:479]No change in timeout 120
[ns_server:debug,2025-05-15T18:46:50.606Z,ns_1@cb.local:<0.882.0>:auto_failover:update_and_save_auto_failover_state:487]No change in max count 1
[ns_server:debug,2025-05-15T18:46:50.608Z,ns_1@cb.local:<0.882.0>:auto_failover:init_logic_state:249]Using auto-failover logic state {state,[],
                                    [{service_state,kv,nil,false},
                                     {service_state,n1ql,nil,false},
                                     {service_state,index,nil,false},
                                     {service_state,fts,nil,false},
                                     {service_state,cbas,nil,false},
                                     {service_state,eventing,nil,false},
                                     {service_state,backup,nil,false}],
                                    118}
[ns_server:info,2025-05-15T18:46:50.608Z,ns_1@cb.local:ns_orchestrator_sup<0.818.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,auto_failover},
                                         auto_failover,[],[]]): started as <0.882.0> on 'ns_1@cb.local'

[ns_server:debug,2025-05-15T18:46:50.608Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
auto_failover_cfg ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{3,63914554010}}]},
 {enabled,true},
 {timeout,120},
 {count,0},
 {max_count,1},
 {disable_max_count,false},
 {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
 {failover_server_group,false},
 {failed_over_server_groups,[]},
 {can_abort_rebalance,true},
 {failover_preserve_durability_majority,false}]
[ns_server:debug,2025-05-15T18:46:50.608Z,ns_1@cb.local:ns_config_rep<0.548.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([auto_failover_cfg]..)
[error_logger:info,2025-05-15T18:46:50.608Z,ns_1@cb.local:ns_orchestrator_sup<0.818.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.882.0>},
              {id,auto_failover},
              {mfargs,{auto_failover,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.608Z,ns_1@cb.local:mb_master_sup<0.804.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.818.0>},
              {id,ns_orchestrator_sup},
              {mfargs,{ns_orchestrator_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:info,2025-05-15T18:46:50.612Z,ns_1@cb.local:mb_master_sup<0.804.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          tombstone_purger},
                                         tombstone_purger,[],[]]): started as <0.894.0> on 'ns_1@cb.local'

[error_logger:info,2025-05-15T18:46:50.612Z,ns_1@cb.local:mb_master_sup<0.804.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.894.0>},
              {id,tombstone_purger},
              {mfargs,{tombstone_purger,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.612Z,ns_1@cb.local:mb_master_sup<0.804.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.895.0>},
              {id,global_tasks},
              {mfargs,{global_tasks,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:50.615Z,ns_1@cb.local:guardrail_enforcer<0.896.0>:guardrail_enforcer:maybe_notify_services:137]Changed Statuses: #{}
[error_logger:info,2025-05-15T18:46:50.615Z,ns_1@cb.local:mb_master_sup<0.804.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.896.0>},
              {id,guardrail_enforcer},
              {mfargs,{guardrail_enforcer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:50.617Z,ns_1@cb.local:<0.898.0>:license_reporting:init:60]Starting license_reporting server
[ns_server:info,2025-05-15T18:46:50.617Z,ns_1@cb.local:mb_master_sup<0.804.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          license_reporting},
                                         license_reporting,[],[]]): started as <0.898.0> on 'ns_1@cb.local'

[error_logger:info,2025-05-15T18:46:50.617Z,ns_1@cb.local:mb_master_sup<0.804.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.898.0>},
              {id,license_reporting},
              {mfargs,{license_reporting,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.617Z,ns_1@cb.local:leader_registry_sup<0.791.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_registry_sup}
    started: [{pid,<0.794.0>},
              {id,mb_master},
              {mfargs,{mb_master,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:50.617Z,ns_1@cb.local:leader_services_sup<0.780.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_services_sup}
    started: [{pid,<0.791.0>},
              {id,leader_registry_sup},
              {mfargs,{leader_registry_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:50.617Z,ns_1@cb.local:<0.779.0>:restartable:start_child:92]Started child process <0.780.0>
  MFA: {leader_services_sup,start_link,[]}
[error_logger:info,2025-05-15T18:46:50.617Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.779.0>},
              {id,leader_services_sup},
              {mfargs,{restartable,start_link,
                                   [{leader_services_sup,start_link,[]},
                                    infinity]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:50.619Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.900.0>},
              {id,ns_tick_agent},
              {mfargs,{ns_tick_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.620Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.902.0>},
              {id,master_activity_events_ingress},
              {mfargs,{gen_event,start_link,
                                 [{local,master_activity_events_ingress}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.620Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.903.0>},
              {id,master_activity_events_timestamper},
              {mfargs,{master_activity_events,start_link_timestamper,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.621Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.904.0>},
              {id,master_activity_events_pids_watcher},
              {mfargs,{master_activity_events_pids_watcher,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.622Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.905.0>},
              {id,master_activity_events_keeper},
              {mfargs,{master_activity_events_keeper,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.624Z,ns_1@cb.local:health_monitor_sup<0.907.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.908.0>},
              {id,ns_server_monitor},
              {mfargs,{ns_server_monitor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.624Z,ns_1@cb.local:health_monitor_sup<0.907.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.910.0>},
              {id,service_monitor_children_sup},
              {mfargs,{supervisor,start_link,
                                  [{local,service_monitor_children_sup},
                                   health_monitor_sup,child]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:50.625Z,ns_1@cb.local:health_monitor_sup<0.907.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.911.0>},
              {id,service_monitor_worker},
              {mfargs,{erlang,apply,[#Fun<health_monitor_sup.0.30770475>,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.627Z,ns_1@cb.local:health_monitor_sup<0.907.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.917.0>},
              {id,node_monitor},
              {mfargs,{node_monitor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.629Z,ns_1@cb.local:health_monitor_sup<0.907.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.924.0>},
              {id,node_status_analyzer},
              {mfargs,{node_status_analyzer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.629Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.907.0>},
              {id,health_monitor_sup},
              {mfargs,{health_monitor_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:50.631Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.927.0>},
              {id,rebalance_agent},
              {mfargs,{rebalance_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.632Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.928.0>},
              {id,kv_hibernation_agent},
              {mfargs,{kv_hibernation_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.635Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.929.0>},
              {id,ns_rebalance_report_manager},
              {mfargs,{ns_rebalance_report_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:50.635Z,ns_1@cb.local:chronicle_kv_log<0.504.0>:chronicle_kv_log:log:59]update (key: tasks, rev: {<<"ea19d13c61d8435b9ad1a5f6af1aa5eb">>,6})
[]
[ns_server:debug,2025-05-15T18:46:50.637Z,ns_1@cb.local:cb_creds_rotation<0.931.0>:cb_creds_rotation:start_rotate_timer:214]Starting creds rotation timer (1800000ms)
[error_logger:info,2025-05-15T18:46:50.637Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.931.0>},
              {id,creds_rotation},
              {mfargs,{cb_creds_rotation,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:error,2025-05-15T18:46:50.637Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {local,ns_server_sup}
    errorContext: child_terminated
    reason: {noproc,{gen_statem,call,[mb_master,master_node,infinity]}}
    offender: [{pid,<0.716.0>},
               {id,ns_server_stats},
               {mfargs,{ns_server_stats,start_link,[]}},
               {restart_type,permanent},
               {significant,false},
               {shutdown,1000},
               {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.637Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.496.0>},
              {id,ns_server_sup},
              {mfargs,{ns_server_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:50.637Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:one_shot_barrier:notify:21]Notifying on barrier menelaus_barrier
[ns_server:debug,2025-05-15T18:46:50.637Z,ns_1@cb.local:menelaus_barrier<0.298.0>:one_shot_barrier:barrier_body:56]Barrier menelaus_barrier got notification from <0.291.0>
[ns_server:debug,2025-05-15T18:46:50.637Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:one_shot_barrier:notify:26]Successfuly notified on barrier menelaus_barrier
[ns_server:debug,2025-05-15T18:46:50.637Z,ns_1@cb.local:<0.290.0>:restartable:start_child:92]Started child process <0.291.0>
  MFA: {ns_server_nodes_sup,start_link,[]}
[error_logger:info,2025-05-15T18:46:50.637Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.290.0>},
              {id,ns_server_nodes_sup},
              {mfargs,{restartable,start_link,
                                   [{ns_server_nodes_sup,start_link,[]},
                                    infinity]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:50.637Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.933.0>},
              {id,ns_server_stats},
              {mfargs,{ns_server_stats,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:50.639Z,ns_1@cb.local:compiled_roles_cache<0.455.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@",admin}
[error_logger:info,2025-05-15T18:46:50.640Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.937.0>},
              {id,remote_api},
              {mfargs,{remote_api,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.647Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.938.0>},
              {id,ns_gc_runner},
              {mfargs,{ns_gc_runner,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.647Z,ns_1@cb.local:root_sup<0.214.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,root_sup}
    started: [{pid,<0.235.0>},
              {id,ns_server_cluster_sup},
              {mfargs,{ns_server_cluster_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:50.647Z,ns_1@cb.local:application_controller<0.44.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    application: ns_server
    started_at: 'ns_1@cb.local'

[ns_server:debug,2025-05-15T18:46:50.647Z,ns_1@cb.local:<0.9.0>:child_erlang:child_loop:147]118: Entered child_loop
[ns_server:debug,2025-05-15T18:46:50.648Z,ns_1@cb.local:<0.9.0>:child_erlang:child_loop:147]118: Entered child_loop
[ns_server:debug,2025-05-15T18:46:50.649Z,ns_1@cb.local:json_rpc_connection-goxdcr-cbauth<0.940.0>:json_rpc_connection:init:71]Observed revrpc connection: label "goxdcr-cbauth", handling process <0.940.0>
[ns_server:debug,2025-05-15T18:46:50.649Z,ns_1@cb.local:json_rpc_connection-saslauthd-saslauthd-port<0.941.0>:json_rpc_connection:init:71]Observed revrpc connection: label "saslauthd-saslauthd-port", handling process <0.941.0>
[ns_server:debug,2025-05-15T18:46:50.650Z,ns_1@cb.local:menelaus_cbauth<0.669.0>:menelaus_cbauth:handle_cast:203]Observed json rpc process {"goxdcr-cbauth",[{internal,true}],<0.940.0>} started
[ns_server:debug,2025-05-15T18:46:50.653Z,ns_1@cb.local:compiled_roles_cache<0.455.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@goxdcr",admin}
[ns_server:debug,2025-05-15T18:46:50.655Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/query/sequences_cache/revision">>} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554010}}]}|
 <<"0">>]
[ns_server:debug,2025-05-15T18:46:50.655Z,ns_1@cb.local:ns_config_rep<0.548.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([{metakv,<<"/query/sequences_cache/revision">>}]..)
[ns_server:debug,2025-05-15T18:46:50.709Z,ns_1@cb.local:compiled_roles_cache<0.455.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@ns_server",admin}
[ns_server:debug,2025-05-15T18:46:51.119Z,ns_1@cb.local:<0.963.0>:memcached_refresh:do_refresh:153]Successfully connected to memcached, Trying to refresh [rbac,isasl]
[ns_server:debug,2025-05-15T18:46:51.121Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:update_refresh_state:116]Refresh of [rbac,isasl] succeeded
[ns_server:info,2025-05-15T18:46:51.466Z,ns_1@cb.local:memcached_config_mgr<0.704.0>:memcached_config_mgr:push_tls_config:237]Successfully pushed TLS config to memcached
[ns_server:info,2025-05-15T18:46:51.467Z,ns_1@cb.local:memcached_config_mgr<0.704.0>:memcached_config_mgr:push_tls_config:233]Pushing TLS config to memcached:
{[{<<"private key">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem">>},
  {<<"certificate chain">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem">>},
  {<<"CA file">>,<<"/opt/couchbase/var/lib/couchbase/config/certs/ca.pem">>},
  {<<"minimum version">>,<<"TLS 1.2">>},
  {<<"cipher list">>,
   {[{<<"TLS 1.2">>,<<"HIGH">>},
     {<<"TLS 1.3">>,
      <<"TLS_AES_256_GCM_SHA384:TLS_AES_128_GCM_SHA256:TLS_CHACHA20_POLY1305_SHA256">>}]}},
  {<<"cipher order">>,true},
  {<<"client cert auth">>,<<"disabled">>}]}
[ns_server:debug,2025-05-15T18:46:51.468Z,ns_1@cb.local:<0.708.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":18,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true,\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[ns_server:info,2025-05-15T18:46:51.469Z,ns_1@cb.local:memcached_config_mgr<0.704.0>:memcached_config_mgr:push_tls_config:237]Successfully pushed TLS config to memcached
[ns_server:debug,2025-05-15T18:46:51.608Z,ns_1@cb.local:<0.882.0>:auto_failover_logic:log_master_activity:130]Transitioned node {'ns_1@cb.local',<<"e0d520cb35700b4a473cf359e3354528">>} state new -> up
[ns_server:debug,2025-05-15T18:46:51.657Z,ns_1@cb.local:ns_gc_runner<0.938.0>:ns_gc_runner:handle_info:125]GC populating new pid list of size=591, prevMaxGcDuration=0 us
[ns_server:debug,2025-05-15T18:46:53.258Z,ns_1@cb.local:cb_saml<0.429.0>:cb_saml:refresh_metadata:526]Refreshing metadata
[ns_server:debug,2025-05-15T18:46:53.258Z,ns_1@cb.local:cb_saml<0.429.0>:cb_saml:restart_refresh_timer:583]Disabling refresh timer
[ns_server:debug,2025-05-15T18:47:00.439Z,ns_1@cb.local:compiled_roles_cache<0.455.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@prometheus",stats_reader}
[ns_server:debug,2025-05-15T18:47:20.409Z,ns_1@cb.local:compaction_daemon<0.759.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2025-05-15T18:47:20.410Z,ns_1@cb.local:compaction_daemon<0.759.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:47:20.410Z,ns_1@cb.local:compaction_daemon<0.759.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2025-05-15T18:47:20.410Z,ns_1@cb.local:compaction_daemon<0.759.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:47:25.614Z,ns_1@cb.local:compiled_roles_cache<0.455.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"<ud>jaba_admin</ud>",admin}
[cluster:debug,2025-05-15T18:47:25.618Z,ns_1@cb.local:<0.1853.0>:ns_cluster:engage_cluster:97]Processing engage cluster request with [{<<"requestedTargetNodeHostname">>,
                                         <<"db3.lan">>},
                                        {<<"requestedServices">>,
                                         [<<"index">>,<<"kv">>,<<"n1ql">>]},
                                        {<<"isDeveloperPreview">>,false},
                                        {<<"availableStorage">>,
                                         {[{<<"hdd">>,
                                            [{[{<<"path">>,<<"/">>},
                                               {<<"sizeKBytes">>,1056557396},
                                               {<<"usagePercent">>,2}]},
                                             {[{<<"path">>,<<"/dev">>},
                                               {<<"sizeKBytes">>,65536},
                                               {<<"usagePercent">>,0}]},
                                             {[{<<"path">>,<<"/dev/shm">>},
                                               {<<"sizeKBytes">>,65536},
                                               {<<"usagePercent">>,0}]},
                                             {[{<<"path">>,
                                                <<"/etc/resolv.conf">>},
                                               {<<"sizeKBytes">>,1056557396},
                                               {<<"usagePercent">>,2}]},
                                             {[{<<"path">>,
                                                <<"/etc/hostname">>},
                                               {<<"sizeKBytes">>,1056557396},
                                               {<<"usagePercent">>,2}]},
                                             {[{<<"path">>,<<"/etc/hosts">>},
                                               {<<"sizeKBytes">>,1056557396},
                                               {<<"usagePercent">>,2}]},
                                             {[{<<"path">>,
                                                <<"/opt/couchbase/var">>},
                                               {<<"sizeKBytes">>,482797652},
                                               {<<"usagePercent">>,92}]},
                                             {[{<<"path">>,<<"/proc/kcore">>},
                                               {<<"sizeKBytes">>,65536},
                                               {<<"usagePercent">>,0}]},
                                             {[{<<"path">>,<<"/proc/keys">>},
                                               {<<"sizeKBytes">>,65536},
                                               {<<"usagePercent">>,0}]},
                                             {[{<<"path">>,
                                                <<"/proc/timer_list">>},
                                               {<<"sizeKBytes">>,65536},
                                               {<<"usagePercent">>,0}]},
                                             {[{<<"path">>,<<"/proc/scsi">>},
                                               {<<"sizeKBytes">>,4012680},
                                               {<<"usagePercent">>,0}]},
                                             {[{<<"path">>,
                                                <<"/sys/firmware">>},
                                               {<<"sizeKBytes">>,4012680},
                                               {<<"usagePercent">>,0}]}]}]}},
                                        {<<"storageTotals">>,
                                         {[{<<"ram">>,
                                            {[{<<"total">>,8217968640},
                                              {<<"quotaTotal">>,3221225472},
                                              {<<"quotaUsed">>,0},
                                              {<<"used">>,4251119616},
                                              {<<"usedByData">>,0},
                                              {<<"quotaUsedPerNode">>,0},
                                              {<<"quotaTotalPerNode">>,
                                               3221225472}]}},
                                           {<<"hdd">>,
                                            {[{<<"total">>,494384795648},
                                              {<<"quotaTotal">>,494384795648},
                                              {<<"used">>,454834011996},
                                              {<<"usedByData">>,0},
                                              {<<"free">>,39550783652}]}}]}},
                                        {<<"storage">>,
                                         {[{<<"ssd">>,[]},
                                           {<<"hdd">>,
                                            [{[{<<"path">>,
                                                <<"/opt/couchbase/var/lib/couchbase/data">>},
                                               {<<"index_path">>,
                                                <<"/opt/couchbase/var/lib/couchbase/data">>},
                                               {<<"cbas_dirs">>,
                                                [<<"/opt/couchbase/var/lib/couchbase/data">>]},
                                               {<<"eventing_path">>,
                                                <<"/opt/couchbase/var/lib/couchbase/data">>},
                                               {<<"java_home">>,<<>>},
                                               {<<"quotaMb">>,<<"none">>},
                                               {<<"state">>,<<"ok">>}]}]}]}},
                                        {<<"clusterMembership">>,<<"active">>},
                                        {<<"recoveryType">>,<<"none">>},
                                        {<<"status">>,<<"healthy">>},
                                        {<<"otpNode">>,<<"ns_1@172.19.0.4">>},
                                        {<<"thisNode">>,true},
                                        {<<"hostname">>,<<"172.19.0.4:8091">>},
                                        {<<"nodeUUID">>,
                                         <<"28569ac00b9c1d7c50e39741027d428c">>},
                                        {<<"clusterCompatibility">>,458758},
                                        {<<"version">>,
                                         <<"7.6.2-3721-enterprise">>},
                                        {<<"os">>,
                                         <<"aarch64-unknown-linux-gnu">>},
                                        {<<"cpuCount">>,10},
                                        {<<"ports">>,
                                         {[{<<"direct">>,11210},
                                           {<<"httpsMgmt">>,18091},
                                           {<<"httpsCAPI">>,18092},
                                           {<<"distTCP">>,21100},
                                           {<<"distTLS">>,21150}]}},
                                        {<<"services">>,
                                         [<<"index">>,<<"kv">>,<<"n1ql">>]},
                                        {<<"nodeEncryption">>,false},
                                        {<<"nodeEncryptionClientCertVerification">>,
                                         false},
                                        {<<"addressFamilyOnly">>,false},
                                        {<<"configuredHostname">>,
                                         <<"172.19.0.4:8091">>},
                                        {<<"addressFamily">>,<<"inet">>},
                                        {<<"externalListeners">>,
                                         [{[{<<"afamily">>,<<"inet">>},
                                            {<<"nodeEncryption">>,false}]}]},
                                        {<<"serverGroup">>,<<"Group 1">>},
                                        {<<"couchApiBase">>,
                                         <<"http://172.19.0.4:8092/">>},
                                        {<<"couchApiBaseHTTPS">>,
                                         <<"https://172.19.0.4:18092/">>},
                                        {<<"nodeHash">>,10838665},
                                        {<<"systemStats">>,
                                         {[{<<"cpu_utilization_rate">>,
                                            3.89883035089008},
                                           {<<"cpu_stolen_rate">>,0},
                                           {<<"swap_total">>,1073737728},
                                           {<<"swap_used">>,3956736},
                                           {<<"mem_total">>,8217968640},
                                           {<<"mem_free">>,4805804032},
                                           {<<"mem_limit">>,8217968640},
                                           {<<"cpu_cores_available">>,10},
                                           {<<"allocstall">>,17}]}},
                                        {<<"interestingStats">>,{[]}},
                                        {<<"uptime">>,<<"44">>},
                                        {<<"memoryTotal">>,8217968640},
                                        {<<"memoryFree">>,4805804032},
                                        {<<"mcdMemoryReserved">>,6269},
                                        {<<"mcdMemoryAllocated">>,6269},
                                        {<<"memoryQuota">>,3072},
                                        {<<"queryMemoryQuota">>,0},
                                        {<<"indexMemoryQuota">>,512},
                                        {<<"ftsMemoryQuota">>,512},
                                        {<<"cbasMemoryQuota">>,1024},
                                        {<<"eventingMemoryQuota">>,256},
                                        {<<"autogeneratedCA">>,
                                         <<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQCuy0dFqLPrJ6xY21RJ5ttVPR78ssxxXSe9JdTj/IsUczkwyxfZfwK\npBszlGYuHO0Utnwq1K4ruMPYFYfpLacJSfsb3KWEmZwAoNuRpjvE24AsH3tvj7cB\nB5v49HJ0axvNAFm75GWDyUr7T7UwcTJaFIFtfy+J0Umt2TjFNY0DPpt1WEzkKFKx\nOdBs+7cNc35ZG4rwVu/EJ14OXhQMXkpbUpYJxS8jzkzSI+1Eb67kH4j5QhnR9H+w\nw2S9AgMBAAGjQjBAMA4GA1UdDwEB/wQEAwIBhjAPBgNVHRMBAf8EBTADAQH/MB0G\nA1UdDgQWBBS8ASq2Mkh0UsZe4fIvzs4mCSfTmjANBgkqhkiG9w0BAQsFAAOCAQEA\nB3q1mzdJc3VY17JDCaRPKY/Veni3oZ6zGJ9r9LnWKE7IO9AxBAKYGMELHMf9httQ\nmRZrXiGQi/tYXLlFFk7IcIID6iOOZLLagDhCQJPi52dGhBB4a2EnBvGF9OJI4zW9\nJw04HNR5cF1cxWIrRdGJAX/kbs/M9jz0STlXziVNwiAYeGIxkjq/fwKk6weai8iI\nwQVu6GsbtSBL8f4FRCk2vK6a1MD54BQDasRShdi3k/DJAEtY92muu13M0jc48mCM\ndEVeXxM8rIrG6LavziTwtO+jkFyWjlRyNbSr6il+neSWJ7ZspTo+ZYFy55CAh5ws\nwk6Ljo2wlQgfGflJbKcxAw==\n-----END CERTIFICATE-----\n">>},
                                        {<<"autogeneratedClientCert">>,
                                         <<"-----BEGIN CERTIFICATE-----\nMIIDWTCCAkGgAwIBAgIIGD/HyMO1PBAwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjVaFw0y\nNzA4MTcxODQ3MjVaMC8xLTArBgNVBAMTJENvdWNoYmFzZSBJbnRlcm5hbCBDbGll\nbnQgKDMyNjYwNzI2KTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAKHe\n9VAt+X6MU19Fg7ok9H0Zxkmq5mDd0acYZWMO8WqPC7jBY2dopAKDJoF/AKgk8TLT\ncz5AYGMiY3CokE1WbBNVZZ8+VMft6H+06YxTTsLvqWttVghc4Q3UKSuDpvQrIbxj\n7PxG12wiWoQUCfnBMjrL2ILgJKAQIMjO7UoWin3LmZo8LTdR4ugoBoS3cfQJhz7R\nT7iuSbe/44TwrcYrDzrvg3UCsFrXLnKs/LJ+nfQyl0fjkS/MP33lCHkdZ6+teEaJ\ncsNTaJuYKYIQIR1ICQnsFCsXrf2D4h0v8ZUsRVoaB3TFebmEqYtNB7V3NnP/sxFf\nqQvn6gwzDmvlb0O5+/MCAwEAAaOBgzCBgDAOBgNVHQ8BAf8EBAMCBaAwEwYDVR0l\nBAwwCgYIKwYBBQUHAwIwDAYDVR0TAQH/BAIwADAfBgNVHSMEGDAWgBS8ASq2Mkh0\nUsZe4fIvzs4mCSfTmjAqBgNVHREEIzAhgR9pbnRlcm5hbEBpbnRlcm5hbC5jb3Vj\naGJhc2UuY29tMA0GCSqGSIb3DQEBCwUAA4IBAQAtkwpYapli88YqNcz52BW8oUhZ\nqHNUis2IieXUTRZSTpTQlvywLUYl+7UvOl9QZjGGowKnD7bdTGgR0qyP1Bab1eIs\nZTbE8+MMa/2zeKjsDQlv7LKoKRI6eBa7nadz2AsyuYJ6h6M8Pj8XI1fXTSdEMZyU\ny32ahUlOq2qNPC485fFLJs/6ggN+3NCZT4H/ohbv8cZvoSUuHOj/wzbuNMZg4d18\n57CxIpHFCtyqUkXfQabWuPSsJRadCqs+Z59XLqEmyVdTeX8HcPON7IXtI7lNDdWB\nS0M6ljMG3ONZVVzxAowbDDbacQ1BK+kY8qlk0KzgCydCP3z86KPAsXEFSqTr\n-----END CERTIFICATE-----\n">>},
                                        {<<"autogeneratedClientKey">>,
                                         <<"********">>},
                                        {<<"autogeneratedCert">>,
                                         <<"-----BEGIN CERTIFICATE-----\nMIIDOjCCAiKgAwIBAgIIGD/HyL2BZrYwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjVaFw0y\nNzA4MTcxODQ3MjVaMCoxKDAmBgNVBAMTH0NvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nZGIzLmxhbikwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCoBie+jXEG\nM9mqURIO0hHIXr1th5jf7TJD7f69DGF3znRjw1VTjnaMZAQOk8Gg44nwI1o1KtJj\nUmflnTGWWtZU+43sI8tDXLtymAbYoxEkodGUqqPCEPQkCWLubJvTbSZVEyegLo9P\nKw4RPwJGAkbAFq1poCecaE9pf4qeOhibUM7ivJx5nFMcTkUGKqJt6vVH4oA/FcHp\nfOLz51gjD0O/U2Ev8NqFvx6ftWkIU/+LGaHaMjoDj4x8SldegqvGtDmpfhg2+97T\nftALlkUkvvJqd4beilSDxCiurYlIKOki8GWaGFfleyV8HwD/hOIZ/wj5yz607CDj\nuny5Z11yQr7/AgMBAAGjajBoMA4GA1UdDwEB/wQEAwIFoDATBgNVHSUEDDAKBggr\nBgEFBQcDATAMBgNVHRMBAf8EAjAAMB8GA1UdIwQYMBaAFLwBKrYySHRSxl7h8i/O\nziYJJ9OaMBIGA1UdEQQLMAmCB2RiMy5sYW4wDQYJKoZIhvcNAQELBQADggEBAAPS\nNPGKGEFgoq/sasnMadhR33aGK60LnjgknxG0R9lT+/RHZKEoDMbYw4gp7UUrh+JZ\n+cneP93RcOIpvfRP7AUlGviu8HDldR5yFQBRXpDdKyWvIvPyhwZK4v65yIYUcLq4\ngxMZUzynscYxC4eN1l2eyD2Rf4FuZ7G2ehW4zcei1DD35WId1pR+YKjwKNZ8Q5Sk\nyjT9F2mftICgpSdxyZJLdNXaXDGfodNVzuCKnMnYUoRWmddt5UqqLLkPM0+7HEpU\nn5wAUJD2kSCGspqGnAZbcBEbGKbW2hmgXAKxOQVyVfi4b1ZGo/IAScLuUoMZw8hh\n+6kwlsLDy19h3nkTfxo=\n-----END CERTIFICATE-----\n">>},
                                        {<<"autogeneratedKey">>,
                                         <<"********">>}].
[ns_server:debug,2025-05-15T18:47:25.620Z,ns_1@cb.local:<0.1853.0>:ns_ssl_services_setup:set_certs:632]Setting certificates
[ns_server:debug,2025-05-15T18:47:25.643Z,ns_1@cb.local:chronicle_kv_log<0.504.0>:chronicle_kv_log:log:59]update (key: root_cert_and_pkey, rev: {<<"ea19d13c61d8435b9ad1a5f6af1aa5eb">>,
                                       8})
{sanitized,<<"oMbeHkcMXIA/ths1QH3yRdqP7VoqICiFcjKji48Mlic=">>}
[ns_server:debug,2025-05-15T18:47:25.666Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:maybe_store_ca_certs:832]Considering to store CA certs
[ns_server:debug,2025-05-15T18:47:25.666Z,ns_1@cb.local:chronicle_kv_log<0.504.0>:chronicle_kv_log:log:59]update (key: ca_certificates, rev: {<<"ea19d13c61d8435b9ad1a5f6af1aa5eb">>,9})
[[{id,1},
  {load_timestamp,63914554045},
  {subject,<<"CN=Couchbase Server c64d914e">>},
  {not_before,63524217600},
  {not_after,64691827199},
  {type,generated},
  {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl"...>>}],
 [{id,0},
  {load_timestamp,63914554006},
  {subject,<<"CN=Couchbase Server c4aff56c">>},
  {not_before,63524217600},
  {not_after,64691827199},
  {type,generated},
  {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv7ZdwOswDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNGFmZjU2YzAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzRhZmY1\nNmMwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDvE0P4l48WhjwU/MMT\njgReGICRQusZuf0g2Co1QK5tYQ69lC7J9eYYoM0snH5sM4JAx3DTHZY3ctRXc2+r"...>>}]]
[ns_server:debug,2025-05-15T18:47:25.667Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:maybe_store_ca_certs:847]Updating CA file with 2 certificates
[ns_server:info,2025-05-15T18:47:25.670Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:maybe_store_ca_certs:850]CA file updated: 2 cert(s) written
[ns_server:info,2025-05-15T18:47:25.671Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:save_certs:968]New node_cert and pkey are written to tmp file
[ns_server:info,2025-05-15T18:47:25.674Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:save_certs_phase2:995]node_cert cert and pkey files updated
[ns_server:debug,2025-05-15T18:47:25.675Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',node_cert} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]},
 {subject,<<"CN=Couchbase Server Node (db3.lan)">>},
 {not_after,63985747645},
 {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
 {load_timestamp,63914554045},
 {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQ"...>>},
 {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDOjCCAiKgAwIBAgIIGD/HyL2BZrYwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjVaFw0y\nNzA4MTcxODQ3MjVaMCoxKDAmBgNVBAMTH0NvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nZGIzLmxhbikwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCoBie+jXEG\nM9mqURIO0hHIXr1th5jf7TJD7f69DGF3znRjw1VTjnaMZAQOk8Gg44nwI1o1KtJj\nUmflnTG"...>>},
 {pkey_passphrase_settings,[]},
 {certs_epoch,0},
 {type,generated},
 {hostname,"db3.lan"}]
[ns_server:debug,2025-05-15T18:47:25.675Z,ns_1@cb.local:ns_config_rep<0.548.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([{node,'ns_1@cb.local',node_cert}]..)
[ns_server:info,2025-05-15T18:47:25.679Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:save_certs:968]New client_cert and pkey are written to tmp file
[ns_server:info,2025-05-15T18:47:25.683Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:save_certs_phase2:995]client_cert cert and pkey files updated
[ns_server:debug,2025-05-15T18:47:25.684Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',client_cert} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]},
 {subject,<<"CN=Couchbase Internal Client (32660726)">>},
 {not_after,63985747645},
 {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
 {load_timestamp,63914554045},
 {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQ"...>>},
 {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDWTCCAkGgAwIBAgIIGD/HyMO1PBAwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjVaFw0y\nNzA4MTcxODQ3MjVaMC8xLTArBgNVBAMTJENvdWNoYmFzZSBJbnRlcm5hbCBDbGll\nbnQgKDMyNjYwNzI2KTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAKHe\n9VAt+X6MU19Fg7ok9H0Zxkmq5mDd0acYZWMO8WqPC7jBY2dopAKDJoF/AKgk8TLT\ncz5AYGM"...>>},
 {pkey_passphrase_settings,[]},
 {certs_epoch,0},
 {type,generated},
 {name,"@internal"}]
[ns_server:debug,2025-05-15T18:47:25.684Z,ns_1@cb.local:ns_config_rep<0.548.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([{node,'ns_1@cb.local',client_cert}]..)
[ns_server:debug,2025-05-15T18:47:25.688Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:notify_services:1146]Going to notify following services: [capi_ssl_service,cb_dist_tls,
                                     client_cert_event,memcached,
                                     server_cert_event,ssl_service]
[ns_server:info,2025-05-15T18:47:25.688Z,ns_1@cb.local:<0.2183.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service client_cert_event
[ns_server:info,2025-05-15T18:47:25.688Z,ns_1@cb.local:<0.2184.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service memcached
[ns_server:debug,2025-05-15T18:47:25.688Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Restarting tls distribution protocols (if any)
[ns_server:info,2025-05-15T18:47:25.688Z,ns_1@cb.local:<0.2185.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service server_cert_event
[ns_server:debug,2025-05-15T18:47:25.688Z,ns_1@cb.local:<0.327.0>:restartable:loop:65]Restarting child <0.388.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
  Shutdown policy: 1000
  Caller: {<0.2186.0>,#Ref<0.3529285563.3650617345.7761>}
[ns_server:info,2025-05-15T18:47:25.689Z,ns_1@cb.local:memcached_config_mgr<0.704.0>:memcached_config_mgr:push_tls_config:233]Pushing TLS config to memcached:
{[{<<"private key">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem">>},
  {<<"certificate chain">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem">>},
  {<<"CA file">>,<<"/opt/couchbase/var/lib/couchbase/config/certs/ca.pem">>},
  {<<"minimum version">>,<<"TLS 1.2">>},
  {<<"cipher list">>,
   {[{<<"TLS 1.2">>,<<"HIGH">>},
     {<<"TLS 1.3">>,
      <<"TLS_AES_256_GCM_SHA384:TLS_AES_128_GCM_SHA256:TLS_CHACHA20_POLY1305_SHA256">>}]}},
  {<<"cipher order">>,true},
  {<<"client cert auth">>,<<"disabled">>}]}
[ns_server:debug,2025-05-15T18:47:25.689Z,ns_1@cb.local:<0.327.0>:restartable:shutdown_child:114]Successfully terminated process <0.388.0>
[ns_server:debug,2025-05-15T18:47:25.691Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Ensure config is going to change listeners. Will be stopped: [], will be started: [], will be restarted: []
[ns_server:info,2025-05-15T18:47:25.691Z,ns_1@cb.local:<0.2182.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service cb_dist_tls
[ns_server:info,2025-05-15T18:47:25.691Z,ns_1@cb.local:<0.2189.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2025-05-15T18:47:25.692Z,ns_1@cb.local:<0.2189.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2025-05-15T18:47:25.692Z,ns_1@cb.local:<0.2189.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2025-05-15T18:47:25.692Z,ns_1@cb.local:<0.2189.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2025-05-15T18:47:25.692Z,ns_1@cb.local:<0.2189.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2025-05-15T18:47:25.693Z,ns_1@cb.local:<0.2189.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"0.0.0.0"},
 [{keyfile,"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem"},
  {certfile,"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem"},
  {versions,['tlsv1.2','tlsv1.3']},
  {cacerts,[<<48,130,3,12,48,130,1,244,160,3,2,1,2,2,8,24,63,199,191,182,93,
              192,235,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,48,
              32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,101,
              114,118,101,114,32,99,52,97,102,102,53,54,99,48,30,23,13,49,51,
              48,49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,51,
              53,57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,117,99,
              104,98,97,115,101,32,83,101,114,118,101,114,32,99,52,97,102,102,
              53,54,99,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,1,5,0,3,
              130,1,15,0,48,130,1,10,2,130,1,1,0,239,19,67,248,151,143,22,134,
              60,20,252,195,19,142,4,94,24,128,145,66,235,25,185,253,32,216,42,
              53,64,174,109,97,14,189,148,46,201,245,230,24,160,205,44,156,126,
              108,51,130,64,199,112,211,29,150,55,114,212,87,115,111,171,91,
              211,209,4,80,11,4,193,158,105,110,58,22,11,11,233,217,15,75,246,
              230,250,194,222,10,140,189,52,107,169,72,160,110,117,228,32,232,
              48,150,30,69,209,54,212,98,17,225,175,64,186,132,144,132,127,38,
              143,177,100,233,245,49,196,8,82,86,55,48,149,219,162,224,146,96,
              81,40,160,101,55,23,220,164,17,136,135,153,40,106,119,177,8,162,
              177,146,159,161,247,38,51,209,50,246,203,9,187,138,173,122,229,
              135,29,19,58,163,54,97,183,240,75,136,100,238,206,152,105,50,38,
              52,63,211,25,147,70,69,3,151,146,206,165,213,48,190,208,106,173,
              123,128,168,37,158,43,24,2,18,14,74,129,208,86,132,29,51,18,243,
              241,188,209,186,130,39,39,211,90,112,68,219,123,222,17,126,105,
              63,125,240,25,157,62,125,104,250,158,67,245,97,24,43,2,3,1,0,1,
              163,66,48,64,48,14,6,3,85,29,15,1,1,255,4,4,3,2,1,134,48,15,6,3,
              85,29,19,1,1,255,4,5,48,3,1,1,255,48,29,6,3,85,29,14,4,22,4,20,
              195,20,145,17,76,20,39,101,25,197,156,185,193,171,223,93,10,165,
              210,191,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,
              120,203,72,57,92,116,122,174,212,40,240,208,113,77,217,197,208,
              114,232,156,132,111,84,12,212,179,98,155,89,207,33,189,199,14,30,
              113,136,47,23,96,202,61,115,9,135,46,202,177,176,224,133,194,238,
              37,246,69,184,170,79,48,4,28,153,121,118,29,9,128,228,245,135,65,
              24,126,23,90,111,189,19,189,130,176,154,245,134,33,87,178,238,
              156,112,206,37,154,23,125,253,153,150,189,114,6,53,194,121,143,
              243,60,171,251,95,79,137,161,179,148,50,193,0,161,1,69,205,230,
              159,122,203,205,22,158,229,163,8,38,49,224,246,14,114,59,205,240,
              98,222,168,36,160,100,180,134,22,114,187,56,241,110,48,179,195,1,
              230,193,101,186,176,175,39,182,136,205,54,99,251,226,134,82,229,
              254,48,251,1,211,72,32,51,34,200,58,88,252,44,60,81,95,87,50,39,
              254,172,242,96,187,59,51,186,47,75,33,233,8,189,159,111,26,95,
              104,52,53,170,228,89,207,253,109,140,167,146,122,230,240,11,34,
              21,166,171,118,81,188,248,27,44,157,72,200,83,85,132,118,65,231,
              178,134,240,116,169,232>>,
            <<48,130,3,12,48,130,1,244,160,3,2,1,2,2,8,24,63,199,191,156,197,
              82,17,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,48,
              32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,101,
              114,118,101,114,32,99,54,52,100,57,49,52,101,48,30,23,13,49,51,
              48,49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,51,
              53,57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,117,
              99,104,98,97,115,101,32,83,101,114,118,101,114,32,99,54,52,100,
              57,49,52,101,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,1,
              5,0,3,130,1,15,0,48,130,1,10,2,130,1,1,0,189,71,7,138,162,116,
              202,173,172,80,169,207,183,229,131,28,103,231,118,113,23,184,
              163,118,132,50,140,197,100,9,14,13,222,41,184,111,89,132,2,102,
              81,20,185,124,217,165,68,88,93,96,211,184,188,36,17,35,176,167,
              109,82,1,80,201,124,250,21,203,22,244,2,187,45,29,22,162,207,
              172,158,177,99,109,81,39,155,109,84,244,123,242,203,49,197,116,
              158,244,151,83,143,242,44,81,204,228,195,44,95,101,252,10,164,
              27,51,148,102,46,28,237,20,182,124,42,212,174,43,184,195,216,
              21,135,233,45,167,9,73,251,27,220,165,132,153,156,0,160,219,
              145,166,59,196,219,128,44,31,123,111,143,183,1,7,155,248,244,
              114,116,107,27,205,0,89,187,228,101,131,201,74,251,79,181,48,
              113,50,90,20,129,109,127,47,137,209,73,173,217,56,197,53,141,3,
              62,155,117,88,76,228,40,82,177,57,208,108,251,183,13,115,126,
              89,27,138,240,86,239,196,39,94,14,94,20,12,94,74,91,82,150,9,
              197,47,35,206,76,210,35,237,68,111,174,228,31,136,249,66,25,
              209,244,127,176,195,100,189,2,3,1,0,1,163,66,48,64,48,14,6,3,
              85,29,15,1,1,255,4,4,3,2,1,134,48,15,6,3,85,29,19,1,1,255,4,5,
              48,3,1,1,255,48,29,6,3,85,29,14,4,22,4,20,188,1,42,182,50,72,
              116,82,198,94,225,242,47,206,206,38,9,39,211,154,48,13,6,9,42,
              134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,7,122,181,155,55,73,
              115,117,88,215,178,67,9,164,79,41,143,213,122,120,183,161,158,
              179,24,159,107,244,185,214,40,78,200,59,208,49,4,2,152,24,193,
              11,28,199,253,134,219,80,153,22,107,94,33,144,139,251,88,92,
              185,69,22,78,200,112,130,3,234,35,142,100,178,218,128,56,66,64,
              147,226,231,103,70,132,16,120,107,97,39,6,241,133,244,226,72,
              227,53,189,39,13,56,28,212,121,112,93,92,197,98,43,69,209,137,
              1,127,228,110,207,204,246,60,244,73,57,87,206,37,77,194,32,24,
              120,98,49,146,58,191,127,2,164,235,7,154,139,200,136,193,5,110,
              232,107,27,181,32,75,241,254,5,68,41,54,188,174,154,212,192,
              249,224,20,3,106,196,82,133,216,183,147,240,201,0,75,88,247,
              105,174,187,93,204,210,55,56,242,96,140,116,69,94,95,19,60,172,
              138,198,232,182,175,206,36,240,180,239,163,144,92,150,142,84,
              114,53,180,171,234,41,126,157,228,150,39,182,108,165,58,62,101,
              129,114,231,144,128,135,156,44,194,78,139,142,141,176,149,8,31,
              25,249,73,108,167,49,3>>]},
  {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,
        10,118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,
        158,232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,
        66,211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,
        250,145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,
        104,159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,
        246,169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,
        110,167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,
        118,190,67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,
        74,8,205,174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,
        221,95,184,110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,
        76,187,66,211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,
        69,254,147,103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,
        202,133,173,72,6,69,167,89,112,174,40,229,171,2,1,2>>},
  {ciphers,[#{cipher => aes_256_gcm,key_exchange => any,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm_8,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_ecdsa,
              mac => aead,prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_rsa,
              mac => aead,prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_128_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf},
            #{cipher => aes_128_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf}]},
  {honor_cipher_order,true},
  {secure_renegotiate,true},
  {client_renegotiation,false},
  {password,"********"}],
 {ssl,true},
 {port,18091}]
[error_logger:info,2025-05-15T18:47:25.696Z,ns_1@cb.local:<0.2189.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2189.0>,menelaus_web}
    started: [{pid,<0.2190.0>},
              {id,menelaus_web_ipv4},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet},
                        {ssl,true},
                        {name,menelaus_web_ssl},
                        {ssl_opts_fun,#Fun<ns_ssl_services_setup.11.39330155>},
                        {port,18091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:47:25.697Z,ns_1@cb.local:<0.2181.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service capi_ssl_service
[ns_server:info,2025-05-15T18:47:25.697Z,ns_1@cb.local:memcached_config_mgr<0.704.0>:memcached_config_mgr:push_tls_config:237]Successfully pushed TLS config to memcached
[ns_server:info,2025-05-15T18:47:25.697Z,ns_1@cb.local:<0.2189.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2025-05-15T18:47:25.697Z,ns_1@cb.local:<0.2189.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2025-05-15T18:47:25.698Z,ns_1@cb.local:<0.2189.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2025-05-15T18:47:25.698Z,ns_1@cb.local:<0.2189.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2025-05-15T18:47:25.698Z,ns_1@cb.local:<0.2189.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2025-05-15T18:47:25.699Z,ns_1@cb.local:<0.2189.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"::"},
 [{keyfile,"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem"},
  {certfile,"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem"},
  {versions,['tlsv1.2','tlsv1.3']},
  {cacerts,[<<48,130,3,12,48,130,1,244,160,3,2,1,2,2,8,24,63,199,191,182,93,
              192,235,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,48,
              32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,101,
              114,118,101,114,32,99,52,97,102,102,53,54,99,48,30,23,13,49,51,
              48,49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,51,
              53,57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,117,99,
              104,98,97,115,101,32,83,101,114,118,101,114,32,99,52,97,102,102,
              53,54,99,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,1,5,0,3,
              130,1,15,0,48,130,1,10,2,130,1,1,0,239,19,67,248,151,143,22,134,
              60,20,252,195,19,142,4,94,24,128,145,66,235,25,185,253,32,216,42,
              53,64,174,109,97,14,189,148,46,201,245,230,24,160,205,44,156,126,
              108,51,130,64,199,112,211,29,150,55,114,212,87,115,111,171,91,
              211,209,4,80,11,4,193,158,105,110,58,22,11,11,233,217,15,75,246,
              230,250,194,222,10,140,189,52,107,169,72,160,110,117,228,32,232,
              48,150,30,69,209,54,212,98,17,225,175,64,186,132,144,132,127,38,
              143,177,100,233,245,49,196,8,82,86,55,48,149,219,162,224,146,96,
              81,40,160,101,55,23,220,164,17,136,135,153,40,106,119,177,8,162,
              177,146,159,161,247,38,51,209,50,246,203,9,187,138,173,122,229,
              135,29,19,58,163,54,97,183,240,75,136,100,238,206,152,105,50,38,
              52,63,211,25,147,70,69,3,151,146,206,165,213,48,190,208,106,173,
              123,128,168,37,158,43,24,2,18,14,74,129,208,86,132,29,51,18,243,
              241,188,209,186,130,39,39,211,90,112,68,219,123,222,17,126,105,
              63,125,240,25,157,62,125,104,250,158,67,245,97,24,43,2,3,1,0,1,
              163,66,48,64,48,14,6,3,85,29,15,1,1,255,4,4,3,2,1,134,48,15,6,3,
              85,29,19,1,1,255,4,5,48,3,1,1,255,48,29,6,3,85,29,14,4,22,4,20,
              195,20,145,17,76,20,39,101,25,197,156,185,193,171,223,93,10,165,
              210,191,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,
              120,203,72,57,92,116,122,174,212,40,240,208,113,77,217,197,208,
              114,232,156,132,111,84,12,212,179,98,155,89,207,33,189,199,14,30,
              113,136,47,23,96,202,61,115,9,135,46,202,177,176,224,133,194,238,
              37,246,69,184,170,79,48,4,28,153,121,118,29,9,128,228,245,135,65,
              24,126,23,90,111,189,19,189,130,176,154,245,134,33,87,178,238,
              156,112,206,37,154,23,125,253,153,150,189,114,6,53,194,121,143,
              243,60,171,251,95,79,137,161,179,148,50,193,0,161,1,69,205,230,
              159,122,203,205,22,158,229,163,8,38,49,224,246,14,114,59,205,240,
              98,222,168,36,160,100,180,134,22,114,187,56,241,110,48,179,195,1,
              230,193,101,186,176,175,39,182,136,205,54,99,251,226,134,82,229,
              254,48,251,1,211,72,32,51,34,200,58,88,252,44,60,81,95,87,50,39,
              254,172,242,96,187,59,51,186,47,75,33,233,8,189,159,111,26,95,
              104,52,53,170,228,89,207,253,109,140,167,146,122,230,240,11,34,
              21,166,171,118,81,188,248,27,44,157,72,200,83,85,132,118,65,231,
              178,134,240,116,169,232>>,
            <<48,130,3,12,48,130,1,244,160,3,2,1,2,2,8,24,63,199,191,156,197,
              82,17,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,48,
              32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,101,
              114,118,101,114,32,99,54,52,100,57,49,52,101,48,30,23,13,49,51,
              48,49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,51,
              53,57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,117,
              99,104,98,97,115,101,32,83,101,114,118,101,114,32,99,54,52,100,
              57,49,52,101,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,1,
              5,0,3,130,1,15,0,48,130,1,10,2,130,1,1,0,189,71,7,138,162,116,
              202,173,172,80,169,207,183,229,131,28,103,231,118,113,23,184,
              163,118,132,50,140,197,100,9,14,13,222,41,184,111,89,132,2,102,
              81,20,185,124,217,165,68,88,93,96,211,184,188,36,17,35,176,167,
              109,82,1,80,201,124,250,21,203,22,244,2,187,45,29,22,162,207,
              172,158,177,99,109,81,39,155,109,84,244,123,242,203,49,197,116,
              158,244,151,83,143,242,44,81,204,228,195,44,95,101,252,10,164,
              27,51,148,102,46,28,237,20,182,124,42,212,174,43,184,195,216,
              21,135,233,45,167,9,73,251,27,220,165,132,153,156,0,160,219,
              145,166,59,196,219,128,44,31,123,111,143,183,1,7,155,248,244,
              114,116,107,27,205,0,89,187,228,101,131,201,74,251,79,181,48,
              113,50,90,20,129,109,127,47,137,209,73,173,217,56,197,53,141,3,
              62,155,117,88,76,228,40,82,177,57,208,108,251,183,13,115,126,
              89,27,138,240,86,239,196,39,94,14,94,20,12,94,74,91,82,150,9,
              197,47,35,206,76,210,35,237,68,111,174,228,31,136,249,66,25,
              209,244,127,176,195,100,189,2,3,1,0,1,163,66,48,64,48,14,6,3,
              85,29,15,1,1,255,4,4,3,2,1,134,48,15,6,3,85,29,19,1,1,255,4,5,
              48,3,1,1,255,48,29,6,3,85,29,14,4,22,4,20,188,1,42,182,50,72,
              116,82,198,94,225,242,47,206,206,38,9,39,211,154,48,13,6,9,42,
              134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,7,122,181,155,55,73,
              115,117,88,215,178,67,9,164,79,41,143,213,122,120,183,161,158,
              179,24,159,107,244,185,214,40,78,200,59,208,49,4,2,152,24,193,
              11,28,199,253,134,219,80,153,22,107,94,33,144,139,251,88,92,
              185,69,22,78,200,112,130,3,234,35,142,100,178,218,128,56,66,64,
              147,226,231,103,70,132,16,120,107,97,39,6,241,133,244,226,72,
              227,53,189,39,13,56,28,212,121,112,93,92,197,98,43,69,209,137,
              1,127,228,110,207,204,246,60,244,73,57,87,206,37,77,194,32,24,
              120,98,49,146,58,191,127,2,164,235,7,154,139,200,136,193,5,110,
              232,107,27,181,32,75,241,254,5,68,41,54,188,174,154,212,192,
              249,224,20,3,106,196,82,133,216,183,147,240,201,0,75,88,247,
              105,174,187,93,204,210,55,56,242,96,140,116,69,94,95,19,60,172,
              138,198,232,182,175,206,36,240,180,239,163,144,92,150,142,84,
              114,53,180,171,234,41,126,157,228,150,39,182,108,165,58,62,101,
              129,114,231,144,128,135,156,44,194,78,139,142,141,176,149,8,31,
              25,249,73,108,167,49,3>>]},
  {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,
        10,118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,
        158,232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,
        66,211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,
        250,145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,
        104,159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,
        246,169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,
        110,167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,
        118,190,67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,
        74,8,205,174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,
        221,95,184,110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,
        76,187,66,211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,
        69,254,147,103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,
        202,133,173,72,6,69,167,89,112,174,40,229,171,2,1,2>>},
  {ciphers,[#{cipher => aes_256_gcm,key_exchange => any,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm_8,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_ecdsa,
              mac => aead,prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_rsa,
              mac => aead,prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_128_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf},
            #{cipher => aes_128_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf}]},
  {honor_cipher_order,true},
  {secure_renegotiate,true},
  {client_renegotiation,false},
  {password,"********"}],
 {ssl,true},
 {port,18091}]
[error_logger:info,2025-05-15T18:47:25.701Z,ns_1@cb.local:<0.2189.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2189.0>,menelaus_web}
    started: [{pid,<0.2210.0>},
              {id,menelaus_web_ipv6},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet6},
                        {ssl,true},
                        {name,menelaus_web_ssl},
                        {ssl_opts_fun,#Fun<ns_ssl_services_setup.11.39330155>},
                        {port,18091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:25.701Z,ns_1@cb.local:<0.327.0>:restartable:start_child:92]Started child process <0.2189.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
[ns_server:info,2025-05-15T18:47:25.701Z,ns_1@cb.local:<0.2186.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service ssl_service
[ns_server:info,2025-05-15T18:47:25.702Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:notify_services:1162]Succesfully notified services [ssl_service,server_cert_event,memcached,
                               client_cert_event,cb_dist_tls,capi_ssl_service]
[ns_server:debug,2025-05-15T18:47:25.702Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:restart_regenerate_client_cert_timer:1447]Time left before client cert regeneration: 70588800000
[ns_server:info,2025-05-15T18:47:25.702Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:handle_info:764]cert_and_pkey changed
[ns_server:info,2025-05-15T18:47:25.703Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:should_regenerate_certs:894]Should regenerate node_cert because CA or name in the certificate has changed
[ns_server:warn,2025-05-15T18:47:25.703Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:maybe_generate_certs:913]Node doesn't have private key, skipping node_cert generation
[ns_server:debug,2025-05-15T18:47:25.704Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:restart_regenerate_client_cert_timer:1447]Time left before client cert regeneration: 70588800000
[ns_server:debug,2025-05-15T18:47:25.704Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:maybe_store_ca_certs:832]Considering to store CA certs
[ns_server:debug,2025-05-15T18:47:25.704Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:restart_regenerate_client_cert_timer:1447]Time left before client cert regeneration: 70588800000
[ns_server:info,2025-05-15T18:47:25.704Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:validate_pkey:1032]Private key (node_cert) passphrase validation suceeded
[ns_server:info,2025-05-15T18:47:25.705Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:validate_pkey:1032]Private key (client_cert) passphrase validation suceeded
[ns_server:info,2025-05-15T18:47:25.705Z,ns_1@cb.local:<0.1853.0>:ns_cluster:engage_cluster_apply_certs:167]Generated certificate was loaded on the node before joining. Cert: <<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQCuy0dFqLPrJ6xY21RJ5ttVPR78ssxxXSe9JdTj/IsUczkwyxfZfwK\npBszlGYuHO0Utnwq1K4ruMPYFYfpLacJSfsb3KWEmZwAoNuRpjvE24AsH3tvj7cB\nB5v49HJ0axvNAFm75GWDyUr7T7UwcTJaFIFtfy+J0Umt2TjFNY0DPpt1WEzkKFKx\nOdBs+7cNc35ZG4rwVu/EJ14OXhQMXkpbUpYJxS8jzkzSI+1Eb67kH4j5QhnR9H+w\nw2S9AgMBAAGjQjBAMA4GA1UdDwEB/wQEAwIBhjAPBgNVHRMBAf8EBTADAQH/MB0G\nA1UdDgQWBBS8ASq2Mkh0UsZe4fIvzs4mCSfTmjANBgkqhkiG9w0BAQsFAAOCAQEA\nB3q1mzdJc3VY17JDCaRPKY/Veni3oZ6zGJ9r9LnWKE7IO9AxBAKYGMELHMf9httQ\nmRZrXiGQi/tYXLlFFk7IcIID6iOOZLLagDhCQJPi52dGhBB4a2EnBvGF9OJI4zW9\nJw04HNR5cF1cxWIrRdGJAX/kbs/M9jz0STlXziVNwiAYeGIxkjq/fwKk6weai8iI\nwQVu6GsbtSBL8f4FRCk2vK6a1MD54BQDasRShdi3k/DJAEtY92muu13M0jc48mCM\ndEVeXxM8rIrG6LavziTwtO+jkFyWjlRyNbSr6il+neSWJ7ZspTo+ZYFy55CAh5ws\nwk6Ljo2wlQgfGflJbKcxAw==\n-----END CERTIFICATE-----\n">>
[ns_server:info,2025-05-15T18:47:25.705Z,ns_1@cb.local:<0.1853.0>:ns_cluster:apply_net_config:251]Applying net config. AFamily: inet, AFamilyOnly: false, NEncryption: false, DistProtos: [{inet,
                                                                                          false}]ClientCert: false
[cluster:debug,2025-05-15T18:47:25.707Z,ns_1@cb.local:ns_cluster<0.273.0>:ns_cluster:handle_call:424]handling engage_cluster([{<<"requestedTargetNodeHostname">>,<<"db3.lan">>},
                         {<<"requestedServices">>,
                          [<<"index">>,<<"kv">>,<<"n1ql">>]},
                         {<<"isDeveloperPreview">>,false},
                         {<<"availableStorage">>,
                          {[{<<"hdd">>,
                             [{[{<<"path">>,<<"/">>},
                                {<<"sizeKBytes">>,1056557396},
                                {<<"usagePercent">>,2}]},
                              {[{<<"path">>,<<"/dev">>},
                                {<<"sizeKBytes">>,65536},
                                {<<"usagePercent">>,0}]},
                              {[{<<"path">>,<<"/dev/shm">>},
                                {<<"sizeKBytes">>,65536},
                                {<<"usagePercent">>,0}]},
                              {[{<<"path">>,<<"/etc/resolv.conf">>},
                                {<<"sizeKBytes">>,1056557396},
                                {<<"usagePercent">>,2}]},
                              {[{<<"path">>,<<"/etc/hostname">>},
                                {<<"sizeKBytes">>,1056557396},
                                {<<"usagePercent">>,2}]},
                              {[{<<"path">>,<<"/etc/hosts">>},
                                {<<"sizeKBytes">>,1056557396},
                                {<<"usagePercent">>,2}]},
                              {[{<<"path">>,<<"/opt/couchbase/var">>},
                                {<<"sizeKBytes">>,482797652},
                                {<<"usagePercent">>,92}]},
                              {[{<<"path">>,<<"/proc/kcore">>},
                                {<<"sizeKBytes">>,65536},
                                {<<"usagePercent">>,0}]},
                              {[{<<"path">>,<<"/proc/keys">>},
                                {<<"sizeKBytes">>,65536},
                                {<<"usagePercent">>,0}]},
                              {[{<<"path">>,<<"/proc/timer_list">>},
                                {<<"sizeKBytes">>,65536},
                                {<<"usagePercent">>,0}]},
                              {[{<<"path">>,<<"/proc/scsi">>},
                                {<<"sizeKBytes">>,4012680},
                                {<<"usagePercent">>,0}]},
                              {[{<<"path">>,<<"/sys/firmware">>},
                                {<<"sizeKBytes">>,4012680},
                                {<<"usagePercent">>,0}]}]}]}},
                         {<<"storageTotals">>,
                          {[{<<"ram">>,
                             {[{<<"total">>,8217968640},
                               {<<"quotaTotal">>,3221225472},
                               {<<"quotaUsed">>,0},
                               {<<"used">>,4251119616},
                               {<<"usedByData">>,0},
                               {<<"quotaUsedPerNode">>,0},
                               {<<"quotaTotalPerNode">>,3221225472}]}},
                            {<<"hdd">>,
                             {[{<<"total">>,494384795648},
                               {<<"quotaTotal">>,494384795648},
                               {<<"used">>,454834011996},
                               {<<"usedByData">>,0},
                               {<<"free">>,39550783652}]}}]}},
                         {<<"storage">>,
                          {[{<<"ssd">>,[]},
                            {<<"hdd">>,
                             [{[{<<"path">>,
                                 <<"/opt/couchbase/var/lib/couchbase/data">>},
                                {<<"index_path">>,
                                 <<"/opt/couchbase/var/lib/couchbase/data">>},
                                {<<"cbas_dirs">>,
                                 [<<"/opt/couchbase/var/lib/couchbase/data">>]},
                                {<<"eventing_path">>,
                                 <<"/opt/couchbase/var/lib/couchbase/data">>},
                                {<<"java_home">>,<<>>},
                                {<<"quotaMb">>,<<"none">>},
                                {<<"state">>,<<"ok">>}]}]}]}},
                         {<<"clusterMembership">>,<<"active">>},
                         {<<"recoveryType">>,<<"none">>},
                         {<<"status">>,<<"healthy">>},
                         {<<"otpNode">>,<<"ns_1@172.19.0.4">>},
                         {<<"thisNode">>,true},
                         {<<"hostname">>,<<"172.19.0.4:8091">>},
                         {<<"nodeUUID">>,
                          <<"28569ac00b9c1d7c50e39741027d428c">>},
                         {<<"clusterCompatibility">>,458758},
                         {<<"version">>,<<"7.6.2-3721-enterprise">>},
                         {<<"os">>,<<"aarch64-unknown-linux-gnu">>},
                         {<<"cpuCount">>,10},
                         {<<"ports">>,
                          {[{<<"direct">>,11210},
                            {<<"httpsMgmt">>,18091},
                            {<<"httpsCAPI">>,18092},
                            {<<"distTCP">>,21100},
                            {<<"distTLS">>,21150}]}},
                         {<<"services">>,[<<"index">>,<<"kv">>,<<"n1ql">>]},
                         {<<"nodeEncryption">>,false},
                         {<<"nodeEncryptionClientCertVerification">>,false},
                         {<<"addressFamilyOnly">>,false},
                         {<<"configuredHostname">>,<<"172.19.0.4:8091">>},
                         {<<"addressFamily">>,<<"inet">>},
                         {<<"externalListeners">>,
                          [{[{<<"afamily">>,<<"inet">>},
                             {<<"nodeEncryption">>,false}]}]},
                         {<<"serverGroup">>,<<"Group 1">>},
                         {<<"couchApiBase">>,<<"http://172.19.0.4:8092/">>},
                         {<<"couchApiBaseHTTPS">>,
                          <<"https://172.19.0.4:18092/">>},
                         {<<"nodeHash">>,10838665},
                         {<<"systemStats">>,
                          {[{<<"cpu_utilization_rate">>,3.89883035089008},
                            {<<"cpu_stolen_rate">>,0},
                            {<<"swap_total">>,1073737728},
                            {<<"swap_used">>,3956736},
                            {<<"mem_total">>,8217968640},
                            {<<"mem_free">>,4805804032},
                            {<<"mem_limit">>,8217968640},
                            {<<"cpu_cores_available">>,10},
                            {<<"allocstall">>,17}]}},
                         {<<"interestingStats">>,{[]}},
                         {<<"uptime">>,<<"44">>},
                         {<<"memoryTotal">>,8217968640},
                         {<<"memoryFree">>,4805804032},
                         {<<"mcdMemoryReserved">>,6269},
                         {<<"mcdMemoryAllocated">>,6269},
                         {<<"memoryQuota">>,3072},
                         {<<"queryMemoryQuota">>,0},
                         {<<"indexMemoryQuota">>,512},
                         {<<"ftsMemoryQuota">>,512},
                         {<<"cbasMemoryQuota">>,1024},
                         {<<"eventingMemoryQuota">>,256},
                         {<<"autogeneratedCA">>,
                          <<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQCuy0dFqLPrJ6xY21RJ5ttVPR78ssxxXSe9JdTj/IsUczkwyxfZfwK\npBszlGYuHO0Utnwq1K4ruMPYFYfpLacJSfsb3KWEmZwAoNuRpjvE24AsH3tvj7cB\nB5v49HJ0axvNAFm75GWDyUr7T7UwcTJaFIFtfy+J0Umt2TjFNY0DPpt1WEzkKFKx\nOdBs+7cNc35ZG4rwVu/EJ14OXhQMXkpbUpYJxS8jzkzSI+1Eb67kH4j5QhnR9H+w\nw2S9AgMBAAGjQjBAMA4GA1UdDwEB/wQEAwIBhjAPBgNVHRMBAf8EBTADAQH/MB0G\nA1UdDgQWBBS8ASq2Mkh0UsZe4fIvzs4mCSfTmjANBgkqhkiG9w0BAQsFAAOCAQEA\nB3q1mzdJc3VY17JDCaRPKY/Veni3oZ6zGJ9r9LnWKE7IO9AxBAKYGMELHMf9httQ\nmRZrXiGQi/tYXLlFFk7IcIID6iOOZLLagDhCQJPi52dGhBB4a2EnBvGF9OJI4zW9\nJw04HNR5cF1cxWIrRdGJAX/kbs/M9jz0STlXziVNwiAYeGIxkjq/fwKk6weai8iI\nwQVu6GsbtSBL8f4FRCk2vK6a1MD54BQDasRShdi3k/DJAEtY92muu13M0jc48mCM\ndEVeXxM8rIrG6LavziTwtO+jkFyWjlRyNbSr6il+neSWJ7ZspTo+ZYFy55CAh5ws\nwk6Ljo2wlQgfGflJbKcxAw==\n-----END CERTIFICATE-----\n">>},
                         {<<"autogeneratedClientCert">>,
                          <<"-----BEGIN CERTIFICATE-----\nMIIDWTCCAkGgAwIBAgIIGD/HyMO1PBAwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjVaFw0y\nNzA4MTcxODQ3MjVaMC8xLTArBgNVBAMTJENvdWNoYmFzZSBJbnRlcm5hbCBDbGll\nbnQgKDMyNjYwNzI2KTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAKHe\n9VAt+X6MU19Fg7ok9H0Zxkmq5mDd0acYZWMO8WqPC7jBY2dopAKDJoF/AKgk8TLT\ncz5AYGMiY3CokE1WbBNVZZ8+VMft6H+06YxTTsLvqWttVghc4Q3UKSuDpvQrIbxj\n7PxG12wiWoQUCfnBMjrL2ILgJKAQIMjO7UoWin3LmZo8LTdR4ugoBoS3cfQJhz7R\nT7iuSbe/44TwrcYrDzrvg3UCsFrXLnKs/LJ+nfQyl0fjkS/MP33lCHkdZ6+teEaJ\ncsNTaJuYKYIQIR1ICQnsFCsXrf2D4h0v8ZUsRVoaB3TFebmEqYtNB7V3NnP/sxFf\nqQvn6gwzDmvlb0O5+/MCAwEAAaOBgzCBgDAOBgNVHQ8BAf8EBAMCBaAwEwYDVR0l\nBAwwCgYIKwYBBQUHAwIwDAYDVR0TAQH/BAIwADAfBgNVHSMEGDAWgBS8ASq2Mkh0\nUsZe4fIvzs4mCSfTmjAqBgNVHREEIzAhgR9pbnRlcm5hbEBpbnRlcm5hbC5jb3Vj\naGJhc2UuY29tMA0GCSqGSIb3DQEBCwUAA4IBAQAtkwpYapli88YqNcz52BW8oUhZ\nqHNUis2IieXUTRZSTpTQlvywLUYl+7UvOl9QZjGGowKnD7bdTGgR0qyP1Bab1eIs\nZTbE8+MMa/2zeKjsDQlv7LKoKRI6eBa7nadz2AsyuYJ6h6M8Pj8XI1fXTSdEMZyU\ny32ahUlOq2qNPC485fFLJs/6ggN+3NCZT4H/ohbv8cZvoSUuHOj/wzbuNMZg4d18\n57CxIpHFCtyqUkXfQabWuPSsJRadCqs+Z59XLqEmyVdTeX8HcPON7IXtI7lNDdWB\nS0M6ljMG3ONZVVzxAowbDDbacQ1BK+kY8qlk0KzgCydCP3z86KPAsXEFSqTr\n-----END CERTIFICATE-----\n">>},
                         {<<"autogeneratedClientKey">>,<<"********">>},
                         {<<"autogeneratedCert">>,
                          <<"-----BEGIN CERTIFICATE-----\nMIIDOjCCAiKgAwIBAgIIGD/HyL2BZrYwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjVaFw0y\nNzA4MTcxODQ3MjVaMCoxKDAmBgNVBAMTH0NvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nZGIzLmxhbikwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCoBie+jXEG\nM9mqURIO0hHIXr1th5jf7TJD7f69DGF3znRjw1VTjnaMZAQOk8Gg44nwI1o1KtJj\nUmflnTGWWtZU+43sI8tDXLtymAbYoxEkodGUqqPCEPQkCWLubJvTbSZVEyegLo9P\nKw4RPwJGAkbAFq1poCecaE9pf4qeOhibUM7ivJx5nFMcTkUGKqJt6vVH4oA/FcHp\nfOLz51gjD0O/U2Ev8NqFvx6ftWkIU/+LGaHaMjoDj4x8SldegqvGtDmpfhg2+97T\nftALlkUkvvJqd4beilSDxCiurYlIKOki8GWaGFfleyV8HwD/hOIZ/wj5yz607CDj\nuny5Z11yQr7/AgMBAAGjajBoMA4GA1UdDwEB/wQEAwIFoDATBgNVHSUEDDAKBggr\nBgEFBQcDATAMBgNVHRMBAf8EAjAAMB8GA1UdIwQYMBaAFLwBKrYySHRSxl7h8i/O\nziYJJ9OaMBIGA1UdEQQLMAmCB2RiMy5sYW4wDQYJKoZIhvcNAQELBQADggEBAAPS\nNPGKGEFgoq/sasnMadhR33aGK60LnjgknxG0R9lT+/RHZKEoDMbYw4gp7UUrh+JZ\n+cneP93RcOIpvfRP7AUlGviu8HDldR5yFQBRXpDdKyWvIvPyhwZK4v65yIYUcLq4\ngxMZUzynscYxC4eN1l2eyD2Rf4FuZ7G2ehW4zcei1DD35WId1pR+YKjwKNZ8Q5Sk\nyjT9F2mftICgpSdxyZJLdNXaXDGfodNVzuCKnMnYUoRWmddt5UqqLLkPM0+7HEpU\nn5wAUJD2kSCGspqGnAZbcBEbGKbW2hmgXAKxOQVyVfi4b1ZGo/IAScLuUoMZw8hh\n+6kwlsLDy19h3nkTfxo=\n-----END CERTIFICATE-----\n">>},
                         {<<"autogeneratedKey">>,<<"********">>}])
[cluster:debug,2025-05-15T18:47:25.709Z,ns_1@cb.local:ns_cluster<0.273.0>:ns_cluster:get_port_from_epmd:1093]port_please('ns_1@172.19.0.4', inet, false) = {ok,21100}
[ns_server:debug,2025-05-15T18:47:25.709Z,ns_1@cb.local:ns_cluster<0.273.0>:ns_cluster:check_host_port_connectivity:656]Successfully checked TCP connectivity to "172.19.0.4":21100
[cluster:info,2025-05-15T18:47:25.709Z,ns_1@cb.local:ns_cluster<0.273.0>:ns_cluster:do_change_address:715]Change of address to "db3.lan" is requested.
[ns_server:debug,2025-05-15T18:47:25.710Z,ns_1@cb.local:ns_node_disco<0.539.0>:ns_node_disco:maybe_monitor_rename_txn:201]Monitor node renaming transaction. Pid = <0.2237.0>, MRef = #Ref<0.3529285563.3650617347.4802>
[ns_server:debug,2025-05-15T18:47:25.710Z,ns_1@cb.local:remote_monitors<0.297.0>:remote_monitors:maybe_monitor_rename_txn:159]Monitor node renaming transaction. Pid = <0.2237.0>, MRef = #Ref<0.3529285563.3650617347.4808>
[ns_server:debug,2025-05-15T18:47:25.710Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Closing listener {external,inet_tcp_dist}
[ns_server:debug,2025-05-15T18:47:25.710Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Full list of processes expected to stop: [<0.231.0>]
[ns_server:debug,2025-05-15T18:47:25.710Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Down from <0.231.0>
[error_logger:info,2025-05-15T18:47:25.710Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,959,nodedown,'ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:47:25.710Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3529285563.3650617348.3769>,
                               inet_tcp_dist,<0.492.0>,
                               #Ref<0.3529285563.3650617348.3772>}
[ns_server:debug,2025-05-15T18:47:25.710Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3529285563.3650617345.3721>,
                               inet_tcp_dist,<0.233.0>,
                               #Ref<0.3529285563.3650617345.3724>}
[chronicle:info,2025-05-15T18:47:25.711Z,nonode@nohost:chronicle_proposer<0.271.0>:chronicle_proposer:handle_nodedown:1135]Peer 'ns_1@cb.local' went down: [{nodedown_reason,net_kernel_terminated}]
[user:warn,2025-05-15T18:47:25.711Z,nonode@nohost:ns_node_disco<0.539.0>:ns_node_disco:handle_info:169]Node nonode@nohost saw that node 'ns_1@cb.local' went down. Details: [{nodedown_reason,
                                                                       net_kernel_terminated}]
[ns_server:debug,2025-05-15T18:47:25.711Z,nonode@nohost:<0.516.0>:misc:delaying_crash:1810]Delaying crash exit:{{nodedown,'babysitter_of_ns_1@cb.local'},
                     {gen_server,call,
                         [{ns_babysitter_log,'babysitter_of_ns_1@cb.local'},
                          consume,infinity]}} by 1000ms
Stacktrace: [{gen_server,call,3,[{file,"gen_server.erl"},{line,385}]},
             {ns_log,babysitter_log_consumption_loop,0,
                     [{file,"src/ns_log.erl"},{line,66}]},
             {misc,delaying_crash,2,[{file,"src/misc.erl"},{line,1808}]},
             {proc_lib,init_p,3,[{file,"proc_lib.erl"},{line,225}]}]
[error_logger:error,2025-05-15T18:47:25.711Z,nonode@nohost:cb_dist<0.227.0>:ale_error_logger_handler:do_log:101]cb_dist: terminating with reason: shutdown
[ns_server:debug,2025-05-15T18:47:25.711Z,nonode@nohost:<0.2239.0>:dist_manager:teardown:315]Got nodedown msg {nodedown,'ns_1@cb.local',
                           [{nodedown_reason,net_kernel_terminated}]} after terminating net kernel
[ns_server:info,2025-05-15T18:47:25.711Z,nonode@nohost:<0.2237.0>:dist_manager:do_adjust_address:357]Adjusted IP to "db3.lan"
[ns_server:info,2025-05-15T18:47:25.712Z,nonode@nohost:<0.2237.0>:dist_manager:bringup:246]Attempting to bring up net_kernel with name 'ns_1@db3.lan'
[error_logger:info,2025-05-15T18:47:25.712Z,nonode@nohost:ssl_dist_admin_sup<0.2242.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_admin_sup}
    started: [{pid,<0.2243.0>},
              {id,ssl_pem_cache_dist},
              {mfargs,{ssl_pem_cache,start_link_dist,[[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.713Z,nonode@nohost:ssl_dist_admin_sup<0.2242.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_admin_sup}
    started: [{pid,<0.2244.0>},
              {id,ssl_dist_manager},
              {mfargs,{ssl_manager,start_link_dist,[[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.713Z,nonode@nohost:ssl_dist_sup<0.2241.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_sup}
    started: [{pid,<0.2242.0>},
              {id,ssl_dist_admin_sup},
              {mfargs,{ssl_dist_admin_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:25.713Z,nonode@nohost:tls_dist_sup<0.2245.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_sup}
    started: [{pid,<0.2246.0>},
              {id,dist_tls_connection_sup},
              {mfargs,{tls_connection_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:25.713Z,nonode@nohost:tls_dist_server_sup<0.2247.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_server_sup}
    started: [{pid,<0.2248.0>},
              {id,dist_ssl_listen_tracker_sup},
              {mfargs,{ssl_listen_tracker_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:25.713Z,nonode@nohost:tls_dist_server_sup<0.2247.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_server_sup}
    started: [{pid,<0.2249.0>},
              {id,dist_tls_server_session_ticket},
              {mfargs,{tls_server_session_ticket_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:25.713Z,nonode@nohost:tls_dist_server_sup<0.2247.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_server_sup}
    started: [{pid,<0.2250.0>},
              {id,dist_ssl_upgrade_server_session_cache_sup},
              {mfargs,
                  {ssl_upgrade_server_session_cache_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:25.713Z,nonode@nohost:tls_dist_sup<0.2245.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_sup}
    started: [{pid,<0.2247.0>},
              {id,tls_dist_server_sup},
              {mfargs,{tls_dist_server_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:25.713Z,nonode@nohost:ssl_dist_sup<0.2241.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_sup}
    started: [{pid,<0.2245.0>},
              {id,tls_dist_sup},
              {mfargs,{tls_dist_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:25.714Z,nonode@nohost:net_sup<0.2240.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.2241.0>},
              {id,ssl_dist_sup},
              {mfargs,{ssl_dist_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:47:25.714Z,nonode@nohost:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Starting cb_dist with config [{config_vsn,2}]
[error_logger:info,2025-05-15T18:47:25.716Z,nonode@nohost:net_sup<0.2240.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.2251.0>},
              {id,cb_dist},
              {mfargs,{cb_dist,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.716Z,nonode@nohost:net_sup<0.2240.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.2252.0>},
              {id,auth},
              {mfargs,{auth,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,2000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:25.716Z,nonode@nohost:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Initial protos: [{external,inet_tcp_dist}], required protos: [{external,
                                                                        inet_tcp_dist}]
[ns_server:debug,2025-05-15T18:47:25.716Z,nonode@nohost:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Starting {external,inet_tcp_dist} listener on 21100...
[ns_server:debug,2025-05-15T18:47:25.716Z,nonode@nohost:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Started listener: inet_tcp_dist
[user:info,2025-05-15T18:47:25.717Z,ns_1@db3.lan:ns_node_disco<0.539.0>:ns_node_disco:handle_info:163]Node 'ns_1@db3.lan' saw that node 'ns_1@db3.lan' came up. Tags: []
[ns_server:debug,2025-05-15T18:47:25.717Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Updated server ssl_dist_opts (keep_secrets) - false
[ns_server:debug,2025-05-15T18:47:25.717Z,ns_1@db3.lan:<0.452.0>:doc_replicator:nodeup_monitoring_loop:145]got nodeup event. Considering ddocs replication
[chronicle:info,2025-05-15T18:47:25.717Z,ns_1@db3.lan:chronicle_proposer<0.271.0>:chronicle_proposer:handle_nodeup:1093]Peer 'ns_1@db3.lan' came up
[ns_server:debug,2025-05-15T18:47:25.717Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Started acceptor inet_tcp_dist: <0.2254.0>
[chronicle:info,2025-05-15T18:47:25.717Z,ns_1@db3.lan:chronicle_proposer<0.271.0>:chronicle_proposer:handle_nodeup:1127]Peer 'ns_1@db3.lan' is not in peers: ['ns_1@cb.local']
[ns_server:debug,2025-05-15T18:47:25.717Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Ensure config is going to change listeners. Will be stopped: [], will be started: [], will be restarted: []
[chronicle:debug,2025-05-15T18:47:25.717Z,ns_1@db3.lan:chronicle_leader<0.256.0>:chronicle_leader:handle_note_term_status:603]Ignoring stale term status {<<"ea19d13c61d8435b9ad1a5f6af1aa5eb">>,
                            {2,'ns_1@cb.local'},
                            finished}: {error,{not_a_leader,follower}}
[chronicle:info,2025-05-15T18:47:25.717Z,ns_1@db3.lan:chronicle_proposer<0.271.0>:chronicle_proposer:handle_stop:1269]Proposer for term {2,'ns_1@cb.local'} in history <<"ea19d13c61d8435b9ad1a5f6af1aa5eb">> is terminating.
[error_logger:info,2025-05-15T18:47:25.717Z,ns_1@db3.lan:net_sup<0.2240.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.2253.0>},
              {id,net_kernel},
              {mfargs,{net_kernel,start_link,
                                  [#{clean_halt => false,
                                     name => 'ns_1@db3.lan',
                                     name_domain => longnames,
                                     net_tickintensity => 4,
                                     net_ticktime => 60,
                                     supervisor => net_sup_dynamic}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,2000},
              {child_type,worker}]

[chronicle:debug,2025-05-15T18:47:25.717Z,ns_1@db3.lan:chronicle_agent<0.252.0>:chronicle_agent:handle_local_mark_committed:1997]Marked seqno 9 committed
[ns_server:debug,2025-05-15T18:47:25.717Z,ns_1@db3.lan:net_kernel<0.2253.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:25.717Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3529285563.3650617353.3611>,
                               inet_tcp_dist,undefined,undefined}
[error_logger:info,2025-05-15T18:47:25.717Z,ns_1@db3.lan:kernel_sup<0.49.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,kernel_sup}
    started: [{pid,<0.2240.0>},
              {id,net_sup_dynamic},
              {mfargs,
                  {erl_distribution,start_link,
                      [#{clean_halt => false,name => 'ns_1@db3.lan',
                         name_domain => longnames,net_tickintensity => 4,
                         net_ticktime => 60,supervisor => net_sup_dynamic}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,60000},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:47:25.717Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3529285563.3650617353.3611>,
                                  inet_tcp_dist,<0.2258.0>,
                                  #Ref<0.3529285563.3650617353.3614>}
[chronicle:info,2025-05-15T18:47:25.717Z,ns_1@db3.lan:chronicle_server<0.260.0>:chronicle_proposer:stop:136]Proposer <0.271.0> stopped: {shutdown,stop}
[ns_server:debug,2025-05-15T18:47:25.717Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Accepted new connection from <0.2254.0> DistCtrl #Port<0.81>: {con,
                                                                        #Ref<0.3529285563.3650617347.4847>,
                                                                        inet_tcp_dist,
                                                                        undefined,
                                                                        undefined}
[ns_server:debug,2025-05-15T18:47:25.717Z,ns_1@db3.lan:net_kernel<0.2253.0>:cb_dist:info_msg:1098]cb_dist: Accepting connection from <0.2254.0> using module inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:25.717Z,ns_1@db3.lan:<0.2237.0>:dist_manager:configure_net_kernel:302]Set net_kernel vebosity to 10 -> 0
[ns_server:debug,2025-05-15T18:47:25.717Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3529285563.3650617347.4847>,
                                  inet_tcp_dist,<0.2260.0>,
                                  #Ref<0.3529285563.3650617353.3621>}
[error_logger:error,2025-05-15T18:47:25.718Z,ns_1@db3.lan:net_kernel<0.2253.0>:ale_error_logger_handler:do_log:101]
=========================ERROR REPORT=========================

** Cannot get connection id for node 'ns_1@db3.lan'

[ns_server:info,2025-05-15T18:47:25.718Z,ns_1@db3.lan:<0.2237.0>:dist_manager:save_node:160]saving node name '"ns_1@db3.lan"' to "/opt/couchbase/var/lib/couchbase/couchbase-server.node"
[ns_server:debug,2025-05-15T18:47:25.718Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Accepted new connection from <0.2254.0> DistCtrl #Port<0.82>: {con,
                                                                        #Ref<0.3529285563.3650617352.3881>,
                                                                        inet_tcp_dist,
                                                                        undefined,
                                                                        undefined}
[ns_server:debug,2025-05-15T18:47:25.718Z,ns_1@db3.lan:net_kernel<0.2253.0>:cb_dist:info_msg:1098]cb_dist: Accepting connection from <0.2254.0> using module inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:25.718Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3529285563.3650617347.4847>,
                               inet_tcp_dist,<0.2260.0>,
                               #Ref<0.3529285563.3650617353.3621>}
[ns_server:debug,2025-05-15T18:47:25.718Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3529285563.3650617352.3881>,
                                  inet_tcp_dist,<0.2262.0>,
                                  #Ref<0.3529285563.3650617352.3884>}
[error_logger:info,2025-05-15T18:47:25.718Z,ns_1@db3.lan:net_kernel<0.2253.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.2260.0>,shutdown}}
[ns_server:debug,2025-05-15T18:47:25.719Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3529285563.3650617352.3881>,
                               inet_tcp_dist,<0.2262.0>,
                               #Ref<0.3529285563.3650617352.3884>}
[error_logger:info,2025-05-15T18:47:25.720Z,ns_1@db3.lan:net_kernel<0.2253.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.2262.0>,{recv_challenge_reply_failed,{error,closed}}}}
[ns_server:debug,2025-05-15T18:47:25.720Z,ns_1@db3.lan:<0.2237.0>:dist_manager:bringup:269]Attempted to save node name to disk: ok
[ns_server:debug,2025-05-15T18:47:25.720Z,ns_1@db3.lan:<0.2237.0>:dist_manager:wait_for_node:276]Waiting for connection to node 'babysitter_of_ns_1@cb.local' to be established
[error_logger:info,2025-05-15T18:47:25.720Z,ns_1@db3.lan:net_kernel<0.2253.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{connect,normal,'babysitter_of_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:47:25.720Z,ns_1@db3.lan:net_kernel<0.2253.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'babysitter_of_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:25.721Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3529285563.3650617352.3893>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:47:25.721Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3529285563.3650617352.3893>,
                                  inet_tcp_dist,<0.2264.0>,
                                  #Ref<0.3529285563.3650617352.3895>}
[ns_server:debug,2025-05-15T18:47:25.722Z,ns_1@db3.lan:<0.2237.0>:dist_manager:wait_for_node:288]Observed node 'babysitter_of_ns_1@cb.local' to come up
[ns_server:info,2025-05-15T18:47:25.722Z,ns_1@db3.lan:<0.2237.0>:dist_manager:do_adjust_address:361]Re-setting cookie {{sanitized,<<"1ZVTl+fgxazsGs5XltMEJFjUhLpt0If4lYDK2laySmI=">>},
                   'ns_1@db3.lan'}
[ns_server:info,2025-05-15T18:47:25.724Z,ns_1@db3.lan:<0.2237.0>:dist_manager:save_address_config:147]Deleting irrelevant ip file "/opt/couchbase/var/lib/couchbase/ip": ok
[ns_server:info,2025-05-15T18:47:25.724Z,ns_1@db3.lan:<0.2237.0>:dist_manager:save_address_config:148]saving ip config to "/opt/couchbase/var/lib/couchbase/ip_start"
[ns_server:info,2025-05-15T18:47:25.725Z,ns_1@db3.lan:<0.2237.0>:dist_manager:save_address_config:151]Persisted the address successfully
[ns_server:debug,2025-05-15T18:47:25.725Z,ns_1@db3.lan:<0.2237.0>:dist_manager:rename_node_in_configs:442]Renaming node from 'ns_1@cb.local' to 'ns_1@db3.lan' in config
[ns_server:debug,2025-05-15T18:47:25.725Z,ns_1@db3.lan:chronicle_local<0.239.0>:chronicle_local:handle_rename:177]Handle renaming from 'ns_1@cb.local' to 'ns_1@db3.lan'
[chronicle:debug,2025-05-15T18:47:25.725Z,ns_1@db3.lan:chronicle_agent<0.252.0>:chronicle_agent:handle_reprovision:1140]Reprovisioning peer with config:
{log_entry,<<"ea19d13c61d8435b9ad1a5f6af1aa5eb">>,
           {3,'ns_1@db3.lan'},
           10,
           {config,undefined,0,undefined,
                   #{'ns_1@db3.lan' =>
                         #{id => <<"4002160e672d4042c281f80158fca980">>,
                           role => voter}},
                   undefined,
                   #{chronicle_config_rsm =>
                         {rsm_config,chronicle_config_rsm,[]},
                     kv => {rsm_config,chronicle_kv,[]}},
                   #{},undefined,
                   [{<<"ea19d13c61d8435b9ad1a5f6af1aa5eb">>,0}]}}
[ns_server:debug,2025-05-15T18:47:25.725Z,ns_1@db3.lan:node_monitor<0.917.0>:health_monitor:handle_cast:168]Ignoring heartbeat from an unknown node 'ns_1@db3.lan'
[chronicle:info,2025-05-15T18:47:25.725Z,ns_1@db3.lan:chronicle_leader<0.256.0>:chronicle_leader:handle_reprovisioned:498]System reprovisioned.
[chronicle:info,2025-05-15T18:47:25.725Z,ns_1@db3.lan:<0.2277.0>:chronicle_leader:do_election_worker:892]Starting election.
History ID: <<"ea19d13c61d8435b9ad1a5f6af1aa5eb">>
Log position: {{3,'ns_1@db3.lan'},10}
Peers: ['ns_1@db3.lan']
Required quorum: {majority,{set,1,16,16,8,80,48,
                                {[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],
                                 []},
                                {{[],[],[],[],[],[],[],[],[],[],[],[],[],[],
                                  ['ns_1@db3.lan'],
                                  []}}}}
[chronicle:info,2025-05-15T18:47:25.725Z,ns_1@db3.lan:<0.2277.0>:chronicle_leader:do_election_worker:901]I'm the only peer, so I'm the leader.
[chronicle:info,2025-05-15T18:47:25.725Z,ns_1@db3.lan:chronicle_leader<0.256.0>:chronicle_leader:handle_election_result:691]Going to become a leader in term {4,'ns_1@db3.lan'} (history id <<"ea19d13c61d8435b9ad1a5f6af1aa5eb">>)
[chronicle:debug,2025-05-15T18:47:25.726Z,ns_1@db3.lan:chronicle_agent<0.252.0>:chronicle_agent:handle_establish_term:1529]Accepted term {4,'ns_1@db3.lan'} in history <<"ea19d13c61d8435b9ad1a5f6af1aa5eb">>
[chronicle:debug,2025-05-15T18:47:25.726Z,ns_1@db3.lan:chronicle_proposer<0.2278.0>:chronicle_proposer:establish_term_init:367]Going to establish term {4,'ns_1@db3.lan'} (history id <<"ea19d13c61d8435b9ad1a5f6af1aa5eb">>).
Quorum peers: ['ns_1@db3.lan']
Metadata:
{metadata,'ns_1@db3.lan',<<"4002160e672d4042c281f80158fca980">>,
          <<"ea19d13c61d8435b9ad1a5f6af1aa5eb">>,
          {3,'ns_1@db3.lan'},
          {3,'ns_1@db3.lan'},
          10,10,
          {log_entry,<<"ea19d13c61d8435b9ad1a5f6af1aa5eb">>,
                     {3,'ns_1@db3.lan'},
                     10,
                     {config,undefined,0,undefined,
                             #{'ns_1@db3.lan' =>
                                   #{id =>
                                         <<"4002160e672d4042c281f80158fca980">>,
                                     role => voter}},
                             undefined,
                             #{chronicle_config_rsm =>
                                   {rsm_config,chronicle_config_rsm,[]},
                               kv => {rsm_config,chronicle_kv,[]}},
                             #{},undefined,
                             [{<<"ea19d13c61d8435b9ad1a5f6af1aa5eb">>,0}]}},
          {log_entry,<<"ea19d13c61d8435b9ad1a5f6af1aa5eb">>,
                     {3,'ns_1@db3.lan'},
                     10,
                     {config,undefined,0,undefined,
                             #{'ns_1@db3.lan' =>
                                   #{id =>
                                         <<"4002160e672d4042c281f80158fca980">>,
                                     role => voter}},
                             undefined,
                             #{chronicle_config_rsm =>
                                   {rsm_config,chronicle_config_rsm,[]},
                               kv => {rsm_config,chronicle_kv,[]}},
                             #{},undefined,
                             [{<<"ea19d13c61d8435b9ad1a5f6af1aa5eb">>,0}]}},
          undefined}
[chronicle:debug,2025-05-15T18:47:25.726Z,ns_1@db3.lan:chronicle_proposer<0.2278.0>:chronicle_proposer:establish_term_maybe_transition:686]Established term {4,'ns_1@db3.lan'} (history id <<"ea19d13c61d8435b9ad1a5f6af1aa5eb">>) successfully.
Votes: ['ns_1@db3.lan']
[chronicle:debug,2025-05-15T18:47:25.726Z,ns_1@db3.lan:chronicle_proposer<0.2278.0>:chronicle_proposer:handle_state_enter:284]Starting recovery for term {4,'ns_1@db3.lan'} in history <<"ea19d13c61d8435b9ad1a5f6af1aa5eb">>
[chronicle:debug,2025-05-15T18:47:25.726Z,ns_1@db3.lan:chronicle_proposer<0.2278.0>:chronicle_proposer:handle_state_enter:296]Proposer for term {4,'ns_1@db3.lan'} in history <<"ea19d13c61d8435b9ad1a5f6af1aa5eb">> is ready. Committed seqno: 11
[chronicle:info,2025-05-15T18:47:25.726Z,ns_1@db3.lan:chronicle_leader<0.256.0>:chronicle_leader:handle_note_term_status:596]Term {4,'ns_1@db3.lan'} established.
[ns_server:debug,2025-05-15T18:47:25.751Z,ns_1@db3.lan:chronicle_kv_log<0.504.0>:chronicle_kv_log:handle_info:42]delete (key: {node,'ns_1@cb.local',membership}, rev: {<<"ea19d13c61d8435b9ad1a5f6af1aa5eb">>,
                                                      12})
[ns_server:debug,2025-05-15T18:47:25.751Z,ns_1@db3.lan:chronicle_kv_log<0.504.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@db3.lan',membership}, rev: {<<"ea19d13c61d8435b9ad1a5f6af1aa5eb">>,
                                                     12})
active
[ns_server:debug,2025-05-15T18:47:25.751Z,ns_1@db3.lan:mb_master<0.794.0>:mb_master:update_peers:543]List of peers has changed from ['ns_1@cb.local'] to ['ns_1@db3.lan']
[ns_server:debug,2025-05-15T18:47:25.751Z,ns_1@db3.lan:chronicle_kv_log<0.504.0>:chronicle_kv_log:log:59]update (key: server_groups, rev: {<<"ea19d13c61d8435b9ad1a5f6af1aa5eb">>,12})
[[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@db3.lan']}]]
[ns_server:debug,2025-05-15T18:47:25.751Z,ns_1@db3.lan:chronicle_kv_log<0.504.0>:chronicle_kv_log:log:59]update (key: nodes_wanted, rev: {<<"ea19d13c61d8435b9ad1a5f6af1aa5eb">>,12})
['ns_1@db3.lan']
[ns_server:debug,2025-05-15T18:47:25.751Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',client_cert} -> {node,
                                                          'ns_1@db3.lan',
                                                          client_cert}:
  [{subject,<<"CN=Couchbase Internal Client (32660726)">>},
   {not_after,63985747645},
   {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
   {load_timestamp,63914554045},
   {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQCuy0dFqLPrJ6xY21RJ5ttVPR78ssxxXSe9JdTj/IsUczkwyxfZfwK\npBszlGYuHO0Utnwq1K4ruMPYFYfpLacJSfsb3KWEmZwAoNuRpjvE24AsH3tvj7cB\nB5v49HJ0axvNAFm75GWDyUr7T7UwcTJaFIFtfy+J0Umt2TjFNY0DPpt1WEzkKFKx\nOdBs+7cNc35ZG4rwVu/EJ14OXhQMXkpbUpYJxS8jzkzSI+1Eb67kH4j5QhnR9H+w\nw2S9AgMBAAGjQjBAMA4GA1UdDwEB/wQEAwIBhjAPBgNVHRMBAf8EBTADAQH/MB0G\nA1UdDgQWBBS8ASq2Mkh0UsZe4fIvzs4mCSfTmjANBgkqhkiG9w0BAQsFAAOCAQEA\nB3q1mzdJc3VY17JDCaRPKY/Veni3oZ6zGJ9r9LnWKE7IO9AxBAKYGMELHMf9httQ\nmRZrXiGQi/tYXLlFFk7IcIID6iOOZLLagDhCQJPi52dGhBB4a2EnBvGF9OJI4zW9\nJw04HNR5cF1cxWIrRdGJAX/kbs/M9jz0STlXziVNwiAYeGIxkjq/fwKk6weai8iI\nwQVu6GsbtSBL8f4FRCk2vK6a1MD54BQDasRShdi3k/DJAEtY92muu13M0jc48mCM\ndEVeXxM8rIrG6LavziTwtO+jkFyWjlRyNbSr6il+neSWJ7ZspTo+ZYFy55CAh5ws\nwk6Ljo2wlQgfGflJbKcxAw==\n-----END CERTIFICATE-----\n">>},
   {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDWTCCAkGgAwIBAgIIGD/HyMO1PBAwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjVaFw0y\nNzA4MTcxODQ3MjVaMC8xLTArBgNVBAMTJENvdWNoYmFzZSBJbnRlcm5hbCBDbGll\nbnQgKDMyNjYwNzI2KTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAKHe\n9VAt+X6MU19Fg7ok9H0Zxkmq5mDd0acYZWMO8WqPC7jBY2dopAKDJoF/AKgk8TLT\ncz5AYGMiY3CokE1WbBNVZZ8+VMft6H+06YxTTsLvqWttVghc4Q3UKSuDpvQrIbxj\n7PxG12wiWoQUCfnBMjrL2ILgJKAQIMjO7UoWin3LmZo8LTdR4ugoBoS3cfQJhz7R\nT7iuSbe/44TwrcYrDzrvg3UCsFrXLnKs/LJ+nfQyl0fjkS/MP33lCHkdZ6+teEaJ\ncsNTaJuYKYIQIR1ICQnsFCsXrf2D4h0v8ZUsRVoaB3TFebmEqYtNB7V3NnP/sxFf\nqQvn6gwzDmvlb0O5+/MCAwEAAaOBgzCBgDAOBgNVHQ8BAf8EBAMCBaAwEwYDVR0l\nBAwwCgYIKwYBBQUHAwIwDAYDVR0TAQH/BAIwADAfBgNVHSMEGDAWgBS8ASq2Mkh0\nUsZe4fIvzs4mCSfTmjAqBgNVHREEIzAhgR9pbnRlcm5hbEBpbnRlcm5hbC5jb3Vj\naGJhc2UuY29tMA0GCSqGSIb3DQEBCwUAA4IBAQAtkwpYapli88YqNcz52BW8oUhZ\nqHNUis2IieXUTRZSTpTQlvywLUYl+7UvOl9QZjGGowKnD7bdTGgR0qyP1Bab1eIs\nZTbE8+MMa/2zeKjsDQlv7LKoKRI6eBa7nadz2AsyuYJ6h6M8Pj8XI1fXTSdEMZyU\ny32ahUlOq2qNPC485fFLJs/6ggN+3NCZT4H/ohbv8cZvoSUuHOj/wzbuNMZg4d18\n57CxIpHFCtyqUkXfQabWuPSsJRadCqs+Z59XLqEmyVdTeX8HcPON7IXtI7lNDdWB\nS0M6ljMG3ONZVVzxAowbDDbacQ1BK+kY8qlk0KzgCydCP3z86KPAsXEFSqTr\n-----END CERTIFICATE-----\n">>},
   {pkey_passphrase_settings,[]},
   {certs_epoch,0},
   {type,generated},
   {name,"@internal"}] ->
  [{subject,<<"CN=Couchbase Internal Client (32660726)">>},
   {not_after,63985747645},
   {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
   {load_timestamp,63914554045},
   {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQCuy0dFqLPrJ6xY21RJ5ttVPR78ssxxXSe9JdTj/IsUczkwyxfZfwK\npBszlGYuHO0Utnwq1K4ruMPYFYfpLacJSfsb3KWEmZwAoNuRpjvE24AsH3tvj7cB\nB5v49HJ0axvNAFm75GWDyUr7T7UwcTJaFIFtfy+J0Umt2TjFNY0DPpt1WEzkKFKx\nOdBs+7cNc35ZG4rwVu/EJ14OXhQMXkpbUpYJxS8jzkzSI+1Eb67kH4j5QhnR9H+w\nw2S9AgMBAAGjQjBAMA4GA1UdDwEB/wQEAwIBhjAPBgNVHRMBAf8EBTADAQH/MB0G\nA1UdDgQWBBS8ASq2Mkh0UsZe4fIvzs4mCSfTmjANBgkqhkiG9w0BAQsFAAOCAQEA\nB3q1mzdJc3VY17JDCaRPKY/Veni3oZ6zGJ9r9LnWKE7IO9AxBAKYGMELHMf9httQ\nmRZrXiGQi/tYXLlFFk7IcIID6iOOZLLagDhCQJPi52dGhBB4a2EnBvGF9OJI4zW9\nJw04HNR5cF1cxWIrRdGJAX/kbs/M9jz0STlXziVNwiAYeGIxkjq/fwKk6weai8iI\nwQVu6GsbtSBL8f4FRCk2vK6a1MD54BQDasRShdi3k/DJAEtY92muu13M0jc48mCM\ndEVeXxM8rIrG6LavziTwtO+jkFyWjlRyNbSr6il+neSWJ7ZspTo+ZYFy55CAh5ws\nwk6Ljo2wlQgfGflJbKcxAw==\n-----END CERTIFICATE-----\n">>},
   {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDWTCCAkGgAwIBAgIIGD/HyMO1PBAwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjVaFw0y\nNzA4MTcxODQ3MjVaMC8xLTArBgNVBAMTJENvdWNoYmFzZSBJbnRlcm5hbCBDbGll\nbnQgKDMyNjYwNzI2KTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAKHe\n9VAt+X6MU19Fg7ok9H0Zxkmq5mDd0acYZWMO8WqPC7jBY2dopAKDJoF/AKgk8TLT\ncz5AYGMiY3CokE1WbBNVZZ8+VMft6H+06YxTTsLvqWttVghc4Q3UKSuDpvQrIbxj\n7PxG12wiWoQUCfnBMjrL2ILgJKAQIMjO7UoWin3LmZo8LTdR4ugoBoS3cfQJhz7R\nT7iuSbe/44TwrcYrDzrvg3UCsFrXLnKs/LJ+nfQyl0fjkS/MP33lCHkdZ6+teEaJ\ncsNTaJuYKYIQIR1ICQnsFCsXrf2D4h0v8ZUsRVoaB3TFebmEqYtNB7V3NnP/sxFf\nqQvn6gwzDmvlb0O5+/MCAwEAAaOBgzCBgDAOBgNVHQ8BAf8EBAMCBaAwEwYDVR0l\nBAwwCgYIKwYBBQUHAwIwDAYDVR0TAQH/BAIwADAfBgNVHSMEGDAWgBS8ASq2Mkh0\nUsZe4fIvzs4mCSfTmjAqBgNVHREEIzAhgR9pbnRlcm5hbEBpbnRlcm5hbC5jb3Vj\naGJhc2UuY29tMA0GCSqGSIb3DQEBCwUAA4IBAQAtkwpYapli88YqNcz52BW8oUhZ\nqHNUis2IieXUTRZSTpTQlvywLUYl+7UvOl9QZjGGowKnD7bdTGgR0qyP1Bab1eIs\nZTbE8+MMa/2zeKjsDQlv7LKoKRI6eBa7nadz2AsyuYJ6h6M8Pj8XI1fXTSdEMZyU\ny32ahUlOq2qNPC485fFLJs/6ggN+3NCZT4H/ohbv8cZvoSUuHOj/wzbuNMZg4d18\n57CxIpHFCtyqUkXfQabWuPSsJRadCqs+Z59XLqEmyVdTeX8HcPON7IXtI7lNDdWB\nS0M6ljMG3ONZVVzxAowbDDbacQ1BK+kY8qlk0KzgCydCP3z86KPAsXEFSqTr\n-----END CERTIFICATE-----\n">>},
   {pkey_passphrase_settings,[]},
   {certs_epoch,0},
   {type,generated},
   {name,"@internal"}]
[ns_server:debug,2025-05-15T18:47:25.752Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',node_cert} -> {node,'ns_1@db3.lan',
                                                        node_cert}:
  [{subject,<<"CN=Couchbase Server Node (db3.lan)">>},
   {not_after,63985747645},
   {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
   {load_timestamp,63914554045},
   {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQCuy0dFqLPrJ6xY21RJ5ttVPR78ssxxXSe9JdTj/IsUczkwyxfZfwK\npBszlGYuHO0Utnwq1K4ruMPYFYfpLacJSfsb3KWEmZwAoNuRpjvE24AsH3tvj7cB\nB5v49HJ0axvNAFm75GWDyUr7T7UwcTJaFIFtfy+J0Umt2TjFNY0DPpt1WEzkKFKx\nOdBs+7cNc35ZG4rwVu/EJ14OXhQMXkpbUpYJxS8jzkzSI+1Eb67kH4j5QhnR9H+w\nw2S9AgMBAAGjQjBAMA4GA1UdDwEB/wQEAwIBhjAPBgNVHRMBAf8EBTADAQH/MB0G\nA1UdDgQWBBS8ASq2Mkh0UsZe4fIvzs4mCSfTmjANBgkqhkiG9w0BAQsFAAOCAQEA\nB3q1mzdJc3VY17JDCaRPKY/Veni3oZ6zGJ9r9LnWKE7IO9AxBAKYGMELHMf9httQ\nmRZrXiGQi/tYXLlFFk7IcIID6iOOZLLagDhCQJPi52dGhBB4a2EnBvGF9OJI4zW9\nJw04HNR5cF1cxWIrRdGJAX/kbs/M9jz0STlXziVNwiAYeGIxkjq/fwKk6weai8iI\nwQVu6GsbtSBL8f4FRCk2vK6a1MD54BQDasRShdi3k/DJAEtY92muu13M0jc48mCM\ndEVeXxM8rIrG6LavziTwtO+jkFyWjlRyNbSr6il+neSWJ7ZspTo+ZYFy55CAh5ws\nwk6Ljo2wlQgfGflJbKcxAw==\n-----END CERTIFICATE-----\n">>},
   {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDOjCCAiKgAwIBAgIIGD/HyL2BZrYwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjVaFw0y\nNzA4MTcxODQ3MjVaMCoxKDAmBgNVBAMTH0NvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nZGIzLmxhbikwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCoBie+jXEG\nM9mqURIO0hHIXr1th5jf7TJD7f69DGF3znRjw1VTjnaMZAQOk8Gg44nwI1o1KtJj\nUmflnTGWWtZU+43sI8tDXLtymAbYoxEkodGUqqPCEPQkCWLubJvTbSZVEyegLo9P\nKw4RPwJGAkbAFq1poCecaE9pf4qeOhibUM7ivJx5nFMcTkUGKqJt6vVH4oA/FcHp\nfOLz51gjD0O/U2Ev8NqFvx6ftWkIU/+LGaHaMjoDj4x8SldegqvGtDmpfhg2+97T\nftALlkUkvvJqd4beilSDxCiurYlIKOki8GWaGFfleyV8HwD/hOIZ/wj5yz607CDj\nuny5Z11yQr7/AgMBAAGjajBoMA4GA1UdDwEB/wQEAwIFoDATBgNVHSUEDDAKBggr\nBgEFBQcDATAMBgNVHRMBAf8EAjAAMB8GA1UdIwQYMBaAFLwBKrYySHRSxl7h8i/O\nziYJJ9OaMBIGA1UdEQQLMAmCB2RiMy5sYW4wDQYJKoZIhvcNAQELBQADggEBAAPS\nNPGKGEFgoq/sasnMadhR33aGK60LnjgknxG0R9lT+/RHZKEoDMbYw4gp7UUrh+JZ\n+cneP93RcOIpvfRP7AUlGviu8HDldR5yFQBRXpDdKyWvIvPyhwZK4v65yIYUcLq4\ngxMZUzynscYxC4eN1l2eyD2Rf4FuZ7G2ehW4zcei1DD35WId1pR+YKjwKNZ8Q5Sk\nyjT9F2mftICgpSdxyZJLdNXaXDGfodNVzuCKnMnYUoRWmddt5UqqLLkPM0+7HEpU\nn5wAUJD2kSCGspqGnAZbcBEbGKbW2hmgXAKxOQVyVfi4b1ZGo/IAScLuUoMZw8hh\n+6kwlsLDy19h3nkTfxo=\n-----END CERTIFICATE-----\n">>},
   {pkey_passphrase_settings,[]},
   {certs_epoch,0},
   {type,generated},
   {hostname,"db3.lan"}] ->
  [{subject,<<"CN=Couchbase Server Node (db3.lan)">>},
   {not_after,63985747645},
   {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
   {load_timestamp,63914554045},
   {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQCuy0dFqLPrJ6xY21RJ5ttVPR78ssxxXSe9JdTj/IsUczkwyxfZfwK\npBszlGYuHO0Utnwq1K4ruMPYFYfpLacJSfsb3KWEmZwAoNuRpjvE24AsH3tvj7cB\nB5v49HJ0axvNAFm75GWDyUr7T7UwcTJaFIFtfy+J0Umt2TjFNY0DPpt1WEzkKFKx\nOdBs+7cNc35ZG4rwVu/EJ14OXhQMXkpbUpYJxS8jzkzSI+1Eb67kH4j5QhnR9H+w\nw2S9AgMBAAGjQjBAMA4GA1UdDwEB/wQEAwIBhjAPBgNVHRMBAf8EBTADAQH/MB0G\nA1UdDgQWBBS8ASq2Mkh0UsZe4fIvzs4mCSfTmjANBgkqhkiG9w0BAQsFAAOCAQEA\nB3q1mzdJc3VY17JDCaRPKY/Veni3oZ6zGJ9r9LnWKE7IO9AxBAKYGMELHMf9httQ\nmRZrXiGQi/tYXLlFFk7IcIID6iOOZLLagDhCQJPi52dGhBB4a2EnBvGF9OJI4zW9\nJw04HNR5cF1cxWIrRdGJAX/kbs/M9jz0STlXziVNwiAYeGIxkjq/fwKk6weai8iI\nwQVu6GsbtSBL8f4FRCk2vK6a1MD54BQDasRShdi3k/DJAEtY92muu13M0jc48mCM\ndEVeXxM8rIrG6LavziTwtO+jkFyWjlRyNbSr6il+neSWJ7ZspTo+ZYFy55CAh5ws\nwk6Ljo2wlQgfGflJbKcxAw==\n-----END CERTIFICATE-----\n">>},
   {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDOjCCAiKgAwIBAgIIGD/HyL2BZrYwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjVaFw0y\nNzA4MTcxODQ3MjVaMCoxKDAmBgNVBAMTH0NvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nZGIzLmxhbikwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCoBie+jXEG\nM9mqURIO0hHIXr1th5jf7TJD7f69DGF3znRjw1VTjnaMZAQOk8Gg44nwI1o1KtJj\nUmflnTGWWtZU+43sI8tDXLtymAbYoxEkodGUqqPCEPQkCWLubJvTbSZVEyegLo9P\nKw4RPwJGAkbAFq1poCecaE9pf4qeOhibUM7ivJx5nFMcTkUGKqJt6vVH4oA/FcHp\nfOLz51gjD0O/U2Ev8NqFvx6ftWkIU/+LGaHaMjoDj4x8SldegqvGtDmpfhg2+97T\nftALlkUkvvJqd4beilSDxCiurYlIKOki8GWaGFfleyV8HwD/hOIZ/wj5yz607CDj\nuny5Z11yQr7/AgMBAAGjajBoMA4GA1UdDwEB/wQEAwIFoDATBgNVHSUEDDAKBggr\nBgEFBQcDATAMBgNVHRMBAf8EAjAAMB8GA1UdIwQYMBaAFLwBKrYySHRSxl7h8i/O\nziYJJ9OaMBIGA1UdEQQLMAmCB2RiMy5sYW4wDQYJKoZIhvcNAQELBQADggEBAAPS\nNPGKGEFgoq/sasnMadhR33aGK60LnjgknxG0R9lT+/RHZKEoDMbYw4gp7UUrh+JZ\n+cneP93RcOIpvfRP7AUlGviu8HDldR5yFQBRXpDdKyWvIvPyhwZK4v65yIYUcLq4\ngxMZUzynscYxC4eN1l2eyD2Rf4FuZ7G2ehW4zcei1DD35WId1pR+YKjwKNZ8Q5Sk\nyjT9F2mftICgpSdxyZJLdNXaXDGfodNVzuCKnMnYUoRWmddt5UqqLLkPM0+7HEpU\nn5wAUJD2kSCGspqGnAZbcBEbGKbW2hmgXAKxOQVyVfi4b1ZGo/IAScLuUoMZw8hh\n+6kwlsLDy19h3nkTfxo=\n-----END CERTIFICATE-----\n">>},
   {pkey_passphrase_settings,[]},
   {certs_epoch,0},
   {type,generated},
   {hostname,"db3.lan"}]
[ns_server:debug,2025-05-15T18:47:25.753Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',prometheus_auth_info} -> {node,
                                                                   'ns_1@db3.lan',
                                                                   prometheus_auth_info}:
  {"@prometheus",
   {auth,[{<<"hash">>,
           {[{<<"hashes">>,
              {sanitized,<<"Vmqy8YpGQ1ZVVOJNXOVKZaSx7qimZeVP/vMQ9t99FKU=">>}},
             {<<"algorithm">>,<<"argon2id">>},
             {<<"salt">>,<<"RaDchxQfbWx/9yCvS4KJhw==">>},
             {<<"parallelism">>,1},
             {<<"time">>,3},
             {<<"memory">>,524288}]}},
          {<<"scram-sha-512">>,
           {[{<<"salt">>,
              <<"WL9ZWEHJ76WYVLC0m22/0VNF6T50k/J0Z7Okpz3sW4PgL9CASwlonzChieVERPB3Sib20fCxP6nMQ/SgjVQjEg==">>},
             {<<"iterations">>,15000},
             {<<"hashes">>,
              {sanitized,<<"vX4Kcf9uKEqkQwiL3H4KjNM4iq2tHqkJD/vMtc9V/Xg=">>}}]}},
          {<<"scram-sha-256">>,
           {[{<<"salt">>,<<"YkWxrYn6kotQkszjlA7ouXNPvh8HqG960AycvYrvGOo=">>},
             {<<"iterations">>,15000},
             {<<"hashes">>,
              {sanitized,<<"gECRKBSDc8Gg85IUVUx8OK6b2QieozWWeKpDySi+GnU=">>}}]}},
          {<<"scram-sha-1">>,
           {[{<<"salt">>,<<"uv9yTXGPC84w9yF7otsSjKjc9bM=">>},
             {<<"iterations">>,15000},
             {<<"hashes">>,
              {sanitized,<<"SOMxs6h/s5Xzi9/iuNI3GZdbix+jQu+XMiQI1CxtTOI=">>}}]}}]}} ->
  {"@prometheus",
   {auth,[{<<"hash">>,
           {[{<<"hashes">>,
              {sanitized,<<"Vmqy8YpGQ1ZVVOJNXOVKZaSx7qimZeVP/vMQ9t99FKU=">>}},
             {<<"algorithm">>,<<"argon2id">>},
             {<<"salt">>,<<"RaDchxQfbWx/9yCvS4KJhw==">>},
             {<<"parallelism">>,1},
             {<<"time">>,3},
             {<<"memory">>,524288}]}},
          {<<"scram-sha-512">>,
           {[{<<"salt">>,
              <<"WL9ZWEHJ76WYVLC0m22/0VNF6T50k/J0Z7Okpz3sW4PgL9CASwlonzChieVERPB3Sib20fCxP6nMQ/SgjVQjEg==">>},
             {<<"iterations">>,15000},
             {<<"hashes">>,
              {sanitized,<<"vX4Kcf9uKEqkQwiL3H4KjNM4iq2tHqkJD/vMtc9V/Xg=">>}}]}},
          {<<"scram-sha-256">>,
           {[{<<"salt">>,<<"YkWxrYn6kotQkszjlA7ouXNPvh8HqG960AycvYrvGOo=">>},
             {<<"iterations">>,15000},
             {<<"hashes">>,
              {sanitized,<<"gECRKBSDc8Gg85IUVUx8OK6b2QieozWWeKpDySi+GnU=">>}}]}},
          {<<"scram-sha-1">>,
           {[{<<"salt">>,<<"uv9yTXGPC84w9yF7otsSjKjc9bM=">>},
             {<<"iterations">>,15000},
             {<<"hashes">>,
              {sanitized,<<"SOMxs6h/s5Xzi9/iuNI3GZdbix+jQu+XMiQI1CxtTOI=">>}}]}}]}}
[ns_server:debug,2025-05-15T18:47:25.753Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',eventing_dir} -> {node,
                                                           'ns_1@db3.lan',
                                                           eventing_dir}:
  "/opt/couchbase/var/lib/couchbase/data" ->
  "/opt/couchbase/var/lib/couchbase/data"
[ns_server:debug,2025-05-15T18:47:25.753Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_dirs} -> {node,'ns_1@db3.lan',
                                                        cbas_dirs}:
  ["/opt/couchbase/var/lib/couchbase/data"] ->
  ["/opt/couchbase/var/lib/couchbase/data"]
[ns_server:debug,2025-05-15T18:47:25.753Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',n2n_client_cert_auth} -> {node,
                                                                   'ns_1@db3.lan',
                                                                   n2n_client_cert_auth}:
  false ->
  false
[ns_server:debug,2025-05-15T18:47:25.753Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',erl_external_listeners} -> {node,
                                                                     'ns_1@db3.lan',
                                                                     erl_external_listeners}:
  [{inet,false}] ->
  [{inet,false}]
[ns_server:debug,2025-05-15T18:47:25.754Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',node_encryption} -> {node,
                                                              'ns_1@db3.lan',
                                                              node_encryption}:
  false ->
  false
[ns_server:debug,2025-05-15T18:47:25.754Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',address_family} -> {node,
                                                             'ns_1@db3.lan',
                                                             address_family}:
  inet ->
  inet
[ns_server:debug,2025-05-15T18:47:25.754Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf quorum_nodes -> quorum_nodes:
  ['ns_1@cb.local'] ->
  ['ns_1@db3.lan']
[ns_server:debug,2025-05-15T18:47:25.754Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',audit} -> {node,'ns_1@db3.lan',audit}:
  [] ->
  []
[ns_server:debug,2025-05-15T18:47:25.754Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',backup_grpc_port} -> {node,
                                                               'ns_1@db3.lan',
                                                               backup_grpc_port}:
  9124 ->
  9124
[ns_server:debug,2025-05-15T18:47:25.754Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',backup_http_port} -> {node,
                                                               'ns_1@db3.lan',
                                                               backup_http_port}:
  8097 ->
  8097
[ns_server:debug,2025-05-15T18:47:25.754Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',backup_https_port} -> {node,
                                                                'ns_1@db3.lan',
                                                                backup_https_port}:
  18097 ->
  18097
[ns_server:debug,2025-05-15T18:47:25.754Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',capi_port} -> {node,'ns_1@db3.lan',
                                                        capi_port}:
  8092 ->
  8092
[ns_server:debug,2025-05-15T18:47:25.754Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_admin_port} -> {node,
                                                              'ns_1@db3.lan',
                                                              cbas_admin_port}:
  9110 ->
  9110
[ns_server:debug,2025-05-15T18:47:25.754Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_cc_client_port} -> {node,
                                                                  'ns_1@db3.lan',
                                                                  cbas_cc_client_port}:
  9113 ->
  9113
[ns_server:debug,2025-05-15T18:47:25.754Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_cc_cluster_port} -> {node,
                                                                   'ns_1@db3.lan',
                                                                   cbas_cc_cluster_port}:
  9112 ->
  9112
[ns_server:debug,2025-05-15T18:47:25.754Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_cc_http_port} -> {node,
                                                                'ns_1@db3.lan',
                                                                cbas_cc_http_port}:
  9111 ->
  9111
[ns_server:debug,2025-05-15T18:47:25.754Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_cluster_port} -> {node,
                                                                'ns_1@db3.lan',
                                                                cbas_cluster_port}:
  9115 ->
  9115
[ns_server:debug,2025-05-15T18:47:25.754Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_console_port} -> {node,
                                                                'ns_1@db3.lan',
                                                                cbas_console_port}:
  9114 ->
  9114
[ns_server:debug,2025-05-15T18:47:25.754Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_data_port} -> {node,
                                                             'ns_1@db3.lan',
                                                             cbas_data_port}:
  9116 ->
  9116
[ns_server:debug,2025-05-15T18:47:25.754Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_debug_port} -> {node,
                                                              'ns_1@db3.lan',
                                                              cbas_debug_port}:
  -1 ->
  -1
[ns_server:debug,2025-05-15T18:47:25.754Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_http_port} -> {node,
                                                             'ns_1@db3.lan',
                                                             cbas_http_port}:
  8095 ->
  8095
[ns_server:debug,2025-05-15T18:47:25.754Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_messaging_port} -> {node,
                                                                  'ns_1@db3.lan',
                                                                  cbas_messaging_port}:
  9118 ->
  9118
[ns_server:debug,2025-05-15T18:47:25.754Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_metadata_callback_port} -> {node,
                                                                          'ns_1@db3.lan',
                                                                          cbas_metadata_callback_port}:
  9119 ->
  9119
[ns_server:debug,2025-05-15T18:47:25.754Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_metadata_port} -> {node,
                                                                 'ns_1@db3.lan',
                                                                 cbas_metadata_port}:
  9121 ->
  9121
[ns_server:debug,2025-05-15T18:47:25.755Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_parent_port} -> {node,
                                                               'ns_1@db3.lan',
                                                               cbas_parent_port}:
  9122 ->
  9122
[ns_server:debug,2025-05-15T18:47:25.755Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_replication_port} -> {node,
                                                                    'ns_1@db3.lan',
                                                                    cbas_replication_port}:
  9120 ->
  9120
[ns_server:debug,2025-05-15T18:47:25.755Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_result_port} -> {node,
                                                               'ns_1@db3.lan',
                                                               cbas_result_port}:
  9117 ->
  9117
[ns_server:debug,2025-05-15T18:47:25.755Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_ssl_port} -> {node,
                                                            'ns_1@db3.lan',
                                                            cbas_ssl_port}:
  18095 ->
  18095
[ns_server:debug,2025-05-15T18:47:25.755Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',compaction_daemon} -> {node,
                                                                'ns_1@db3.lan',
                                                                compaction_daemon}:
  [{check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}] ->
  [{check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}]
[ns_server:debug,2025-05-15T18:47:25.755Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',config_version} -> {node,
                                                             'ns_1@db3.lan',
                                                             config_version}:
  {7,6} ->
  {7,6}
[ns_server:debug,2025-05-15T18:47:25.755Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',database_dir} -> {node,
                                                           'ns_1@db3.lan',
                                                           database_dir}:
  "/opt/couchbase/var/lib/couchbase/data" ->
  "/opt/couchbase/var/lib/couchbase/data"
[ns_server:debug,2025-05-15T18:47:25.755Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',event_log} -> {node,'ns_1@db3.lan',
                                                        event_log}:
  [{filename,"/opt/couchbase/var/lib/couchbase/event_log"}] ->
  [{filename,"/opt/couchbase/var/lib/couchbase/event_log"}]
[ns_server:debug,2025-05-15T18:47:25.755Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',eventing_debug_port} -> {node,
                                                                  'ns_1@db3.lan',
                                                                  eventing_debug_port}:
  9140 ->
  9140
[ns_server:debug,2025-05-15T18:47:25.755Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',eventing_http_port} -> {node,
                                                                 'ns_1@db3.lan',
                                                                 eventing_http_port}:
  8096 ->
  8096
[ns_server:debug,2025-05-15T18:47:25.755Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',eventing_https_port} -> {node,
                                                                  'ns_1@db3.lan',
                                                                  eventing_https_port}:
  18096 ->
  18096
[ns_server:debug,2025-05-15T18:47:25.755Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',fts_grpc_port} -> {node,
                                                            'ns_1@db3.lan',
                                                            fts_grpc_port}:
  9130 ->
  9130
[ns_server:debug,2025-05-15T18:47:25.755Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',fts_grpc_ssl_port} -> {node,
                                                                'ns_1@db3.lan',
                                                                fts_grpc_ssl_port}:
  19130 ->
  19130
[ns_server:debug,2025-05-15T18:47:25.755Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',fts_http_port} -> {node,
                                                            'ns_1@db3.lan',
                                                            fts_http_port}:
  8094 ->
  8094
[ns_server:debug,2025-05-15T18:47:25.755Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',fts_ssl_port} -> {node,
                                                           'ns_1@db3.lan',
                                                           fts_ssl_port}:
  18094 ->
  18094
[ns_server:debug,2025-05-15T18:47:25.755Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',index_dir} -> {node,'ns_1@db3.lan',
                                                        index_dir}:
  "/opt/couchbase/var/lib/couchbase/data" ->
  "/opt/couchbase/var/lib/couchbase/data"
[ns_server:debug,2025-05-15T18:47:25.755Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',indexer_admin_port} -> {node,
                                                                 'ns_1@db3.lan',
                                                                 indexer_admin_port}:
  9100 ->
  9100
[ns_server:debug,2025-05-15T18:47:25.755Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',indexer_http_port} -> {node,
                                                                'ns_1@db3.lan',
                                                                indexer_http_port}:
  9102 ->
  9102
[ns_server:debug,2025-05-15T18:47:25.755Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',indexer_https_port} -> {node,
                                                                 'ns_1@db3.lan',
                                                                 indexer_https_port}:
  19102 ->
  19102
[ns_server:debug,2025-05-15T18:47:25.755Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',indexer_scan_port} -> {node,
                                                                'ns_1@db3.lan',
                                                                indexer_scan_port}:
  9101 ->
  9101
[ns_server:debug,2025-05-15T18:47:25.755Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',indexer_stcatchup_port} -> {node,
                                                                     'ns_1@db3.lan',
                                                                     indexer_stcatchup_port}:
  9104 ->
  9104
[ns_server:debug,2025-05-15T18:47:25.755Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',indexer_stinit_port} -> {node,
                                                                  'ns_1@db3.lan',
                                                                  indexer_stinit_port}:
  9103 ->
  9103
[ns_server:debug,2025-05-15T18:47:25.755Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',indexer_stmaint_port} -> {node,
                                                                   'ns_1@db3.lan',
                                                                   indexer_stmaint_port}:
  9105 ->
  9105
[ns_server:debug,2025-05-15T18:47:25.755Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',is_enterprise} -> {node,
                                                            'ns_1@db3.lan',
                                                            is_enterprise}:
  true ->
  true
[ns_server:debug,2025-05-15T18:47:25.756Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',isasl} -> {node,'ns_1@db3.lan',isasl}:
  [{path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}] ->
  [{path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]
[ns_server:debug,2025-05-15T18:47:25.756Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',memcached} -> {node,'ns_1@db3.lan',
                                                        memcached}:
  [{port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,11206},
   {ssl_port,11207},
   {admin_user,"@ns_server"},
   {other_users,["@cbq-engine","@projector","@goxdcr","@index","@fts",
                 "@eventing","@cbas","@backup"]},
   {admin_pass,"*****"},
   {engines,[{membase,[{engine,"/opt/couchbase/lib/memcached/ep.so"},
                       {static_config_string,"failpartialwarmup=false"}]},
             {memcached,[{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
                         {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_rotation_period,39003}] ->
  [{port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,11206},
   {ssl_port,11207},
   {admin_user,"@ns_server"},
   {other_users,["@cbq-engine","@projector","@goxdcr","@index","@fts",
                 "@eventing","@cbas","@backup"]},
   {admin_pass,"*****"},
   {engines,[{membase,[{engine,"/opt/couchbase/lib/memcached/ep.so"},
                       {static_config_string,"failpartialwarmup=false"}]},
             {memcached,[{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
                         {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_rotation_period,39003}]
[ns_server:debug,2025-05-15T18:47:25.756Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',memcached_config} -> {node,
                                                               'ns_1@db3.lan',
                                                               memcached_config}:
  {[{interfaces,{memcached_config_mgr,get_interfaces,[]}},
    {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
    {connection_idle_time,connection_idle_time},
    {privilege_debug,privilege_debug},
    {breakpad,
        {[{enabled,breakpad_enabled},
          {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
    {deployment_model,{memcached_config_mgr,get_config_profile,[]}},
    {verbosity,verbosity},
    {audit_file,{"~s",[audit_file]}},
    {rbac_file,{"~s",[rbac_file]}},
    {dedupe_nmvb_maps,dedupe_nmvb_maps},
    {tracing_enabled,tracing_enabled},
    {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
    {xattr_enabled,true},
    {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
    {collections_enabled,true},
    {max_connections,max_connections},
    {system_connections,system_connections},
    {num_reader_threads,num_reader_threads},
    {num_writer_threads,num_writer_threads},
    {num_auxio_threads,num_auxio_threads},
    {num_nonio_threads,num_nonio_threads},
    {num_storage_threads,num_storage_threads},
    {logger,
        {[{filename,{"~s/~s",[log_path,log_prefix]}},
          {cyclesize,log_cyclesize}]}},
    {external_auth_service,
        {memcached_config_mgr,get_external_auth_service,[]}},
    {active_external_users_push_interval,
        {memcached_config_mgr,get_external_users_push_interval,[]}},
    {prometheus,{memcached_config_mgr,prometheus_cfg,[]}},
    {sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
    {ssl_sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
    {tcp_keepalive_idle,tcp_keepalive_idle},
    {tcp_keepalive_interval,tcp_keepalive_interval},
    {tcp_keepalive_probes,tcp_keepalive_probes},
    {tcp_user_timeout,tcp_user_timeout},
    {always_collect_trace_info,always_collect_trace_info},
    {connection_limit_mode,connection_limit_mode},
    {free_connection_pool_size,free_connection_pool_size},
    {max_client_connection_details,max_client_connection_details}]} ->
  {[{interfaces,{memcached_config_mgr,get_interfaces,[]}},
    {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
    {connection_idle_time,connection_idle_time},
    {privilege_debug,privilege_debug},
    {breakpad,
        {[{enabled,breakpad_enabled},
          {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
    {deployment_model,{memcached_config_mgr,get_config_profile,[]}},
    {verbosity,verbosity},
    {audit_file,{"~s",[audit_file]}},
    {rbac_file,{"~s",[rbac_file]}},
    {dedupe_nmvb_maps,dedupe_nmvb_maps},
    {tracing_enabled,tracing_enabled},
    {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
    {xattr_enabled,true},
    {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
    {collections_enabled,true},
    {max_connections,max_connections},
    {system_connections,system_connections},
    {num_reader_threads,num_reader_threads},
    {num_writer_threads,num_writer_threads},
    {num_auxio_threads,num_auxio_threads},
    {num_nonio_threads,num_nonio_threads},
    {num_storage_threads,num_storage_threads},
    {logger,
        {[{filename,{"~s/~s",[log_path,log_prefix]}},
          {cyclesize,log_cyclesize}]}},
    {external_auth_service,
        {memcached_config_mgr,get_external_auth_service,[]}},
    {active_external_users_push_interval,
        {memcached_config_mgr,get_external_users_push_interval,[]}},
    {prometheus,{memcached_config_mgr,prometheus_cfg,[]}},
    {sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
    {ssl_sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
    {tcp_keepalive_idle,tcp_keepalive_idle},
    {tcp_keepalive_interval,tcp_keepalive_interval},
    {tcp_keepalive_probes,tcp_keepalive_probes},
    {tcp_user_timeout,tcp_user_timeout},
    {always_collect_trace_info,always_collect_trace_info},
    {connection_limit_mode,connection_limit_mode},
    {free_connection_pool_size,free_connection_pool_size},
    {max_client_connection_details,max_client_connection_details}]}
[ns_server:debug,2025-05-15T18:47:25.756Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',memcached_dedicated_ssl_port} -> {node,
                                                                           'ns_1@db3.lan',
                                                                           memcached_dedicated_ssl_port}:
  11206 ->
  11206
[ns_server:debug,2025-05-15T18:47:25.756Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',memcached_defaults} -> {node,
                                                                 'ns_1@db3.lan',
                                                                 memcached_defaults}:
  [{max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {je_malloc_conf,undefined},
   {tracing_enabled,true},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>},
   {num_auxio_threads,<<"default">>},
   {num_nonio_threads,<<"default">>},
   {num_storage_threads,<<"default">>},
   {tcp_keepalive_idle,360},
   {tcp_keepalive_interval,10},
   {tcp_keepalive_probes,3},
   {tcp_user_timeout,30},
   {always_collect_trace_info,true},
   {connection_limit_mode,<<"disconnect">>},
   {free_connection_pool_size,0},
   {max_client_connection_details,0}] ->
  [{max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {je_malloc_conf,undefined},
   {tracing_enabled,true},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>},
   {num_auxio_threads,<<"default">>},
   {num_nonio_threads,<<"default">>},
   {num_storage_threads,<<"default">>},
   {tcp_keepalive_idle,360},
   {tcp_keepalive_interval,10},
   {tcp_keepalive_probes,3},
   {tcp_user_timeout,30},
   {always_collect_trace_info,true},
   {connection_limit_mode,<<"disconnect">>},
   {free_connection_pool_size,0},
   {max_client_connection_details,0}]
[ns_server:debug,2025-05-15T18:47:25.756Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',memcached_prometheus} -> {node,
                                                                   'ns_1@db3.lan',
                                                                   memcached_prometheus}:
  11280 ->
  11280
[ns_server:debug,2025-05-15T18:47:25.756Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',ns_log} -> {node,'ns_1@db3.lan',
                                                     ns_log}:
  [{filename,"/opt/couchbase/var/lib/couchbase/ns_log"}] ->
  [{filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]
[ns_server:debug,2025-05-15T18:47:25.757Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',port_servers} -> {node,
                                                           'ns_1@db3.lan',
                                                           port_servers}:
  [] ->
  []
[ns_server:debug,2025-05-15T18:47:25.757Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',projector_port} -> {node,
                                                             'ns_1@db3.lan',
                                                             projector_port}:
  9999 ->
  9999
[ns_server:debug,2025-05-15T18:47:25.757Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',projector_ssl_port} -> {node,
                                                                 'ns_1@db3.lan',
                                                                 projector_ssl_port}:
  9999 ->
  9999
[ns_server:debug,2025-05-15T18:47:25.757Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',prometheus_http_port} -> {node,
                                                                   'ns_1@db3.lan',
                                                                   prometheus_http_port}:
  9123 ->
  9123
[ns_server:debug,2025-05-15T18:47:25.757Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',query_port} -> {node,'ns_1@db3.lan',
                                                         query_port}:
  8093 ->
  8093
[ns_server:debug,2025-05-15T18:47:25.757Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',rest} -> {node,'ns_1@db3.lan',rest}:
  [{port,8091},{port_meta,global}] ->
  [{port,8091},{port_meta,global}]
[ns_server:debug,2025-05-15T18:47:25.757Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',saslauthd_enabled} -> {node,
                                                                'ns_1@db3.lan',
                                                                saslauthd_enabled}:
  true ->
  true
[ns_server:debug,2025-05-15T18:47:25.757Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',ssl_capi_port} -> {node,
                                                            'ns_1@db3.lan',
                                                            ssl_capi_port}:
  18092 ->
  18092
[ns_server:debug,2025-05-15T18:47:25.757Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',ssl_query_port} -> {node,
                                                             'ns_1@db3.lan',
                                                             ssl_query_port}:
  18093 ->
  18093
[ns_server:debug,2025-05-15T18:47:25.757Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',ssl_rest_port} -> {node,
                                                            'ns_1@db3.lan',
                                                            ssl_rest_port}:
  18091 ->
  18091
[ns_server:debug,2025-05-15T18:47:25.757Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',uuid} -> {node,'ns_1@db3.lan',uuid}:
  <<"e0d520cb35700b4a473cf359e3354528">> ->
  <<"e0d520cb35700b4a473cf359e3354528">>
[ns_server:debug,2025-05-15T18:47:25.757Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',xdcr_rest_port} -> {node,
                                                             'ns_1@db3.lan',
                                                             xdcr_rest_port}:
  9998 ->
  9998
[ns_server:debug,2025-05-15T18:47:25.757Z,ns_1@db3.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',{project_intact,is_vulnerable}} -> {node,
                                                                             'ns_1@db3.lan',
                                                                             {project_intact,
                                                                              is_vulnerable}}:
  false ->
  false
[ns_server:debug,2025-05-15T18:47:25.757Z,ns_1@db3.lan:<0.2237.0>:dist_manager:wait_for_node:276]Waiting for connection to node 'couchdb_ns_1@cb.local' to be established
[error_logger:info,2025-05-15T18:47:25.757Z,ns_1@db3.lan:net_kernel<0.2253.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:info,2025-05-15T18:47:25.757Z,ns_1@db3.lan:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:handle_info:764]cert_and_pkey changed
[ns_server:debug,2025-05-15T18:47:25.757Z,ns_1@db3.lan:net_kernel<0.2253.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:warn,2025-05-15T18:47:25.758Z,ns_1@db3.lan:leader_quorum_nodes_manager<0.807.0>:leader_quorum_nodes_manager:handle_quorum_nodes_updated:155]Somebody else updated the quorum nodes when we are the master node.
Our quorum nodes: ['ns_1@cb.local']
Their quorum nodes: ['ns_1@db3.lan']
[ns_server:debug,2025-05-15T18:47:25.758Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3529285563.3650617348.5455>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:47:25.758Z,ns_1@db3.lan:<0.708.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":27,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true,\"hostname\":\"db3.lan\",\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[ns_server:debug,2025-05-15T18:47:25.758Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3529285563.3650617348.5455>,
                                  inet_tcp_dist,<0.2281.0>,
                                  #Ref<0.3529285563.3650617348.5458>}
[ns_server:debug,2025-05-15T18:47:25.758Z,ns_1@db3.lan:leader_activities<0.787.0>:leader_activities:handle_internal_process_down:440]Process {quorum_nodes_manager,<0.807.0>} terminated with reason {shutdown,
                                                                 {quorum_nodes_update_conflict,
                                                                  ['ns_1@cb.local'],
                                                                  ['ns_1@db3.lan']}}
[ns_server:debug,2025-05-15T18:47:25.758Z,ns_1@db3.lan:prometheus_cfg<0.517.0>:prometheus_cfg:maybe_apply_new_settings:704]Settings didn't change, ignoring update
[ns_server:debug,2025-05-15T18:47:25.758Z,ns_1@db3.lan:<0.810.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.807.0>} exited with reason {shutdown,
                                                                                {quorum_nodes_update_conflict,
                                                                                 ['ns_1@cb.local'],
                                                                                 ['ns_1@db3.lan']}}
[ns_server:debug,2025-05-15T18:47:25.758Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',{project_intact,is_vulnerable}} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|false]
[ns_server:debug,2025-05-15T18:47:25.758Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',xdcr_rest_port} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|9998]
[ns_server:debug,2025-05-15T18:47:25.758Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',uuid} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|
 <<"e0d520cb35700b4a473cf359e3354528">>]
[error_logger:error,2025-05-15T18:47:25.758Z,ns_1@db3.lan:mb_master_sup<0.804.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {local,mb_master_sup}
    errorContext: child_terminated
    reason: {shutdown,
                {quorum_nodes_update_conflict,
                    ['ns_1@cb.local'],
                    ['ns_1@db3.lan']}}
    offender: [{pid,<0.807.0>},
               {id,leader_quorum_nodes_manager},
               {mfargs,{leader_quorum_nodes_manager,start_link,[]}},
               {restart_type,permanent},
               {significant,false},
               {shutdown,1000},
               {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:25.758Z,ns_1@db3.lan:prometheus_cfg<0.517.0>:prometheus_cfg:maybe_apply_new_settings:704]Settings didn't change, ignoring update
[ns_server:debug,2025-05-15T18:47:25.758Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',ssl_rest_port} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|18091]
[ns_server:debug,2025-05-15T18:47:25.758Z,ns_1@db3.lan:leader_quorum_nodes_manager<0.2285.0>:leader_quorum_nodes_manager:pull_config:99]Attempting to pull config from nodes:
[]
[ns_server:debug,2025-05-15T18:47:25.757Z,ns_1@db3.lan:ns_config_rep<0.548.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([quorum_nodes,
                               {node,'ns_1@db3.lan',address_family},
                               {node,'ns_1@db3.lan',audit},
                               {node,'ns_1@db3.lan',backup_grpc_port},
                               {node,'ns_1@db3.lan',backup_http_port},
                               {node,'ns_1@db3.lan',backup_https_port},
                               {node,'ns_1@db3.lan',capi_port},
                               {node,'ns_1@db3.lan',cbas_admin_port},
                               {node,'ns_1@db3.lan',cbas_cc_client_port},
                               {node,'ns_1@db3.lan',cbas_cc_cluster_port},
                               {node,'ns_1@db3.lan',cbas_cc_http_port},
                               {node,'ns_1@db3.lan',cbas_cluster_port},
                               {node,'ns_1@db3.lan',cbas_console_port},
                               {node,'ns_1@db3.lan',cbas_data_port},
                               {node,'ns_1@db3.lan',cbas_debug_port},
                               {node,'ns_1@db3.lan',cbas_dirs},
                               {node,'ns_1@db3.lan',cbas_http_port},
                               {node,'ns_1@db3.lan',cbas_messaging_port},
                               {node,'ns_1@db3.lan',
                                     cbas_metadata_callback_port},
                               {node,'ns_1@db3.lan',cbas_metadata_port},
                               {node,'ns_1@db3.lan',cbas_parent_port},
                               {node,'ns_1@db3.lan',cbas_replication_port},
                               {node,'ns_1@db3.lan',cbas_result_port},
                               {node,'ns_1@db3.lan',cbas_ssl_port},
                               {node,'ns_1@db3.lan',client_cert},
                               {node,'ns_1@db3.lan',compaction_daemon},
                               {node,'ns_1@db3.lan',config_version},
                               {node,'ns_1@db3.lan',database_dir},
                               {node,'ns_1@db3.lan',erl_external_listeners},
                               {node,'ns_1@db3.lan',event_log},
                               {node,'ns_1@db3.lan',eventing_debug_port},
                               {node,'ns_1@db3.lan',eventing_dir},
                               {node,'ns_1@db3.lan',eventing_http_port},
                               {node,'ns_1@db3.lan',eventing_https_port},
                               {node,'ns_1@db3.lan',fts_grpc_port},
                               {node,'ns_1@db3.lan',fts_grpc_ssl_port},
                               {node,'ns_1@db3.lan',fts_http_port},
                               {node,'ns_1@db3.lan',fts_ssl_port},
                               {node,'ns_1@db3.lan',index_dir},
                               {node,'ns_1@db3.lan',indexer_admin_port},
                               {node,'ns_1@db3.lan',indexer_http_port},
                               {node,'ns_1@db3.lan',indexer_https_port},
                               {node,'ns_1@db3.lan',indexer_scan_port},
                               {node,'ns_1@db3.lan',indexer_stcatchup_port},
                               {node,'ns_1@db3.lan',indexer_stinit_port},
                               {node,'ns_1@db3.lan',indexer_stmaint_port},
                               {node,'ns_1@db3.lan',is_enterprise},
                               {node,'ns_1@db3.lan',isasl},
                               {node,'ns_1@db3.lan',memcached},
                               {node,'ns_1@db3.lan',memcached_config},
                               {node,'ns_1@db3.lan',
                                     memcached_dedicated_ssl_port},
                               {node,'ns_1@db3.lan',memcached_defaults},
                               {node,'ns_1@db3.lan',memcached_prometheus},
                               {node,'ns_1@db3.lan',n2n_client_cert_auth},
                               {node,'ns_1@db3.lan',node_cert},
                               {node,'ns_1@db3.lan',node_encryption},
                               {node,'ns_1@db3.lan',ns_log},
                               {node,'ns_1@db3.lan',port_servers},
                               {node,'ns_1@db3.lan',projector_port},
                               {node,'ns_1@db3.lan',projector_ssl_port},
                               {node,'ns_1@db3.lan',prometheus_auth_info},
                               {node,'ns_1@db3.lan',prometheus_http_port},
                               {node,'ns_1@db3.lan',query_port}]..)
[ns_server:debug,2025-05-15T18:47:25.758Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',ssl_query_port} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|18093]
[ns_server:debug,2025-05-15T18:47:25.758Z,ns_1@db3.lan:prometheus_cfg<0.517.0>:prometheus_cfg:maybe_apply_new_settings:704]Settings didn't change, ignoring update
[ns_server:debug,2025-05-15T18:47:25.758Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',ssl_capi_port} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|18092]
[error_logger:info,2025-05-15T18:47:25.758Z,ns_1@db3.lan:mb_master_sup<0.804.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.2285.0>},
              {id,leader_quorum_nodes_manager},
              {mfargs,{leader_quorum_nodes_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:25.758Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',saslauthd_enabled} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|true]
[ns_server:debug,2025-05-15T18:47:25.758Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',rest} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]},
 {port,8091},
 {port_meta,global}]
[ns_server:debug,2025-05-15T18:47:25.758Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',query_port} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|8093]
[ns_server:debug,2025-05-15T18:47:25.759Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',prometheus_http_port} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|9123]
[ns_server:debug,2025-05-15T18:47:25.759Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',projector_ssl_port} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|9999]
[ns_server:debug,2025-05-15T18:47:25.759Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',projector_port} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|9999]
[ns_server:debug,2025-05-15T18:47:25.759Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',port_servers} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}]
[ns_server:debug,2025-05-15T18:47:25.759Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',ns_log} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]},
 {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]
[ns_server:debug,2025-05-15T18:47:25.759Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',memcached_prometheus} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|11280]
[ns_server:debug,2025-05-15T18:47:25.759Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',memcached_defaults} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]},
 {max_connections,65000},
 {system_connections,5000},
 {connection_idle_time,0},
 {verbosity,0},
 {privilege_debug,false},
 {breakpad_enabled,true},
 {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
 {dedupe_nmvb_maps,false},
 {je_malloc_conf,undefined},
 {tracing_enabled,true},
 {datatype_snappy,true},
 {num_reader_threads,<<"default">>},
 {num_writer_threads,<<"default">>},
 {num_auxio_threads,<<"default">>},
 {num_nonio_threads,<<"default">>},
 {num_storage_threads,<<"default">>},
 {tcp_keepalive_idle,360},
 {tcp_keepalive_interval,10},
 {tcp_keepalive_probes,3},
 {tcp_user_timeout,30},
 {always_collect_trace_info,true},
 {connection_limit_mode,<<"disconnect">>},
 {free_connection_pool_size,0},
 {max_client_connection_details,0}]
[ns_server:debug,2025-05-15T18:47:25.759Z,ns_1@db3.lan:<0.708.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":27,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true,\"hostname\":\"db3.lan\",\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[ns_server:debug,2025-05-15T18:47:25.759Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',memcached_dedicated_ssl_port} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|11206]
[ns_server:debug,2025-05-15T18:47:25.759Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',memcached_config} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|
 {[{interfaces,{memcached_config_mgr,get_interfaces,[]}},
   {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
   {connection_idle_time,connection_idle_time},
   {privilege_debug,privilege_debug},
   {breakpad,
    {[{enabled,breakpad_enabled},
      {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
   {deployment_model,{memcached_config_mgr,get_config_profile,[]}},
   {verbosity,verbosity},
   {audit_file,{"~s",[audit_file]}},
   {rbac_file,{"~s",[rbac_file]}},
   {dedupe_nmvb_maps,dedupe_nmvb_maps},
   {tracing_enabled,tracing_enabled},
   {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
   {xattr_enabled,true},
   {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
   {collections_enabled,true},
   {max_connections,max_connections},
   {system_connections,system_connections},
   {num_reader_threads,num_reader_threads},
   {num_writer_threads,num_writer_threads},
   {num_auxio_threads,num_auxio_threads},
   {num_nonio_threads,num_nonio_threads},
   {num_storage_threads,num_storage_threads},
   {logger,
    {[{filename,{"~s/~s",[log_path,log_prefix]}},{cyclesize,log_cyclesize}]}},
   {external_auth_service,{memcached_config_mgr,get_external_auth_service,[]}},
   {active_external_users_push_interval,
    {memcached_config_mgr,get_external_users_push_interval,[]}},
   {prometheus,{memcached_config_mgr,prometheus_cfg,[]}},
   {sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
   {ssl_sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
   {tcp_keepalive_idle,tcp_keepalive_idle},
   {tcp_keepalive_interval,tcp_keepalive_interval},
   {tcp_keepalive_probes,tcp_keepalive_probes},
   {tcp_user_timeout,tcp_user_timeout},
   {always_collect_trace_info,always_collect_trace_info},
   {connection_limit_mode,connection_limit_mode},
   {free_connection_pool_size,free_connection_pool_size},
   {max_client_connection_details,max_client_connection_details}]}]
[ns_server:debug,2025-05-15T18:47:25.759Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',memcached} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]},
 {port,11210},
 {dedicated_port,11209},
 {dedicated_ssl_port,11206},
 {ssl_port,11207},
 {admin_user,"@ns_server"},
 {other_users,["@cbq-engine","@projector","@goxdcr","@index","@fts",
               "@eventing","@cbas","@backup"]},
 {admin_pass,"*****"},
 {engines,[{membase,[{engine,"/opt/couchbase/lib/memcached/ep.so"},
                     {static_config_string,"failpartialwarmup=false"}]},
           {memcached,[{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
                       {static_config_string,"vb0=true"}]}]},
 {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
 {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
 {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
 {log_prefix,"memcached.log"},
 {log_generations,20},
 {log_cyclesize,10485760},
 {log_rotation_period,39003}]
[ns_server:debug,2025-05-15T18:47:25.760Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',isasl} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]},
 {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]
[ns_server:debug,2025-05-15T18:47:25.760Z,ns_1@db3.lan:leader_quorum_nodes_manager<0.2285.0>:leader_quorum_nodes_manager:pull_config:104]Pulled config successfully.
[ns_server:debug,2025-05-15T18:47:25.760Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',is_enterprise} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|true]
[ns_server:debug,2025-05-15T18:47:25.760Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',indexer_stmaint_port} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|9105]
[ns_server:debug,2025-05-15T18:47:25.760Z,ns_1@db3.lan:<0.2237.0>:dist_manager:wait_for_node:288]Observed node 'couchdb_ns_1@cb.local' to come up
[ns_server:debug,2025-05-15T18:47:25.760Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',indexer_stinit_port} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|9103]
[ns_server:debug,2025-05-15T18:47:25.760Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',indexer_stcatchup_port} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|9104]
[ns_server:debug,2025-05-15T18:47:25.760Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',indexer_scan_port} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|9101]
[ns_server:debug,2025-05-15T18:47:25.760Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',indexer_https_port} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|19102]
[ns_server:debug,2025-05-15T18:47:25.760Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',indexer_http_port} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|9102]
[ns_server:debug,2025-05-15T18:47:25.760Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',indexer_admin_port} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|9100]
[ns_server:debug,2025-05-15T18:47:25.760Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',index_dir} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2025-05-15T18:47:25.760Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',fts_ssl_port} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|18094]
[ns_server:debug,2025-05-15T18:47:25.760Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',fts_http_port} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|8094]
[ns_server:debug,2025-05-15T18:47:25.760Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',fts_grpc_ssl_port} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|19130]
[ns_server:debug,2025-05-15T18:47:25.760Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',fts_grpc_port} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|9130]
[ns_server:debug,2025-05-15T18:47:25.760Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',eventing_https_port} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|18096]
[ns_server:debug,2025-05-15T18:47:25.760Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',eventing_http_port} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|8096]
[ns_server:debug,2025-05-15T18:47:25.760Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',eventing_debug_port} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|9140]
[ns_server:debug,2025-05-15T18:47:25.761Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',event_log} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]},
 {filename,"/opt/couchbase/var/lib/couchbase/event_log"}]
[ns_server:debug,2025-05-15T18:47:25.761Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',database_dir} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2025-05-15T18:47:25.761Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',config_version} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|{7,6}]
[ns_server:debug,2025-05-15T18:47:25.761Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',compaction_daemon} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]},
 {check_interval,30},
 {min_db_file_size,131072},
 {min_view_file_size,20971520}]
[ns_server:debug,2025-05-15T18:47:25.761Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_ssl_port} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|18095]
[ns_server:debug,2025-05-15T18:47:25.761Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_result_port} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|9117]
[ns_server:debug,2025-05-15T18:47:25.761Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_replication_port} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|9120]
[ns_server:debug,2025-05-15T18:47:25.761Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_parent_port} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|9122]
[ns_server:debug,2025-05-15T18:47:25.761Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_metadata_port} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|9121]
[ns_server:debug,2025-05-15T18:47:25.761Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_metadata_callback_port} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|9119]
[ns_server:debug,2025-05-15T18:47:25.761Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_messaging_port} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|9118]
[ns_server:debug,2025-05-15T18:47:25.761Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_http_port} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|8095]
[ns_server:debug,2025-05-15T18:47:25.761Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_debug_port} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|-1]
[ns_server:debug,2025-05-15T18:47:25.761Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_data_port} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|9116]
[ns_server:debug,2025-05-15T18:47:25.761Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_console_port} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|9114]
[ns_server:debug,2025-05-15T18:47:25.761Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_cluster_port} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|9115]
[ns_server:debug,2025-05-15T18:47:25.761Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_cc_http_port} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|9111]
[ns_server:debug,2025-05-15T18:47:25.761Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_cc_cluster_port} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|9112]
[ns_server:debug,2025-05-15T18:47:25.761Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_cc_client_port} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|9113]
[ns_server:debug,2025-05-15T18:47:25.761Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_admin_port} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|9110]
[ns_server:debug,2025-05-15T18:47:25.762Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',capi_port} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|8092]
[ns_server:debug,2025-05-15T18:47:25.762Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',backup_https_port} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|18097]
[ns_server:debug,2025-05-15T18:47:25.762Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',backup_http_port} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|8097]
[ns_server:debug,2025-05-15T18:47:25.762Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',backup_grpc_port} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|9124]
[ns_server:debug,2025-05-15T18:47:25.762Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',audit} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}]
[ns_server:debug,2025-05-15T18:47:25.762Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
quorum_nodes ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554045}}]},
 'ns_1@db3.lan']
[ns_server:debug,2025-05-15T18:47:25.762Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',address_family} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|inet]
[ns_server:debug,2025-05-15T18:47:25.762Z,ns_1@db3.lan:ns_config_rep<0.548.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 31 us
[ns_server:debug,2025-05-15T18:47:25.762Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',node_encryption} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|false]
[ns_server:debug,2025-05-15T18:47:25.762Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',erl_external_listeners} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]},
 {inet,false}]
[ns_server:debug,2025-05-15T18:47:25.762Z,ns_1@db3.lan:<0.2237.0>:dist_manager:complete_rename:411]Node 'ns_1@cb.local' has been renamed to 'ns_1@db3.lan'.
[ns_server:debug,2025-05-15T18:47:25.762Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',n2n_client_cert_auth} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]}|false]
[ns_server:debug,2025-05-15T18:47:25.762Z,ns_1@db3.lan:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:restart_regenerate_client_cert_timer:1447]Time left before client cert regeneration: 70588800000
[ns_server:debug,2025-05-15T18:47:25.762Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_dirs} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]},
 "/opt/couchbase/var/lib/couchbase/data"]
[ns_server:debug,2025-05-15T18:47:25.762Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',eventing_dir} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2025-05-15T18:47:25.762Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Ensure config is going to change listeners. Will be stopped: [], will be started: [], will be restarted: []
[ns_server:debug,2025-05-15T18:47:25.762Z,ns_1@db3.lan:<0.582.0>:restartable:loop:65]Restarting child <0.590.0>
  MFA: {ns_doctor_sup,start_link,[]}
  Shutdown policy: infinity
  Caller: {<0.2237.0>,#Ref<0.3529285563.3650617345.8016>}
[ns_server:debug,2025-05-15T18:47:25.762Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',prometheus_auth_info} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{3,63914554045}}]}|
 {"@prometheus",
  {auth,
   [{<<"hash">>,
     {[{<<"hashes">>,
        {sanitized,<<"Vmqy8YpGQ1ZVVOJNXOVKZaSx7qimZeVP/vMQ9t99FKU=">>}},
       {<<"algorithm">>,<<"argon2id">>},
       {<<"salt">>,<<"RaDchxQfbWx/9yCvS4KJhw==">>},
       {<<"parallelism">>,1},
       {<<"time">>,3},
       {<<"memory">>,524288}]}},
    {<<"scram-sha-512">>,
     {[{<<"salt">>,
        <<"WL9ZWEHJ76WYVLC0m22/0VNF6T50k/J0Z7Okpz3sW4PgL9CASwlonzChieVERPB3Sib20fCxP6nMQ/SgjVQjEg==">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"vX4Kcf9uKEqkQwiL3H4KjNM4iq2tHqkJD/vMtc9V/Xg=">>}}]}},
    {<<"scram-sha-256">>,
     {[{<<"salt">>,<<"YkWxrYn6kotQkszjlA7ouXNPvh8HqG960AycvYrvGOo=">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"gECRKBSDc8Gg85IUVUx8OK6b2QieozWWeKpDySi+GnU=">>}}]}},
    {<<"scram-sha-1">>,
     {[{<<"salt">>,<<"uv9yTXGPC84w9yF7otsSjKjc9bM=">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"SOMxs6h/s5Xzi9/iuNI3GZdbix+jQu+XMiQI1CxtTOI=">>}}]}}]}}]
[ns_server:debug,2025-05-15T18:47:25.763Z,ns_1@db3.lan:<0.595.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.594.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.763Z,ns_1@db3.lan:<0.582.0>:restartable:shutdown_child:114]Successfully terminated process <0.590.0>
[ns_server:debug,2025-05-15T18:47:25.763Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',node_cert} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{3,63914554045}}]},
 {subject,<<"CN=Couchbase Server Node (db3.lan)">>},
 {not_after,63985747645},
 {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
 {load_timestamp,63914554045},
 {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQ"...>>},
 {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDOjCCAiKgAwIBAgIIGD/HyL2BZrYwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjVaFw0y\nNzA4MTcxODQ3MjVaMCoxKDAmBgNVBAMTH0NvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nZGIzLmxhbikwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCoBie+jXEG\nM9mqURIO0hHIXr1th5jf7TJD7f69DGF3znRjw1VTjnaMZAQOk8Gg44nwI1o1KtJj\nUmflnTG"...>>},
 {pkey_passphrase_settings,[]},
 {certs_epoch,0},
 {type,generated},
 {hostname,"db3.lan"}]
[error_logger:info,2025-05-15T18:47:25.763Z,ns_1@db3.lan:ns_doctor_sup<0.2302.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_doctor_sup}
    started: [{pid,<0.2303.0>},
              {id,ns_doctor_events},
              {mfargs,{gen_event,start_link,[{local,ns_doctor_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:25.763Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',client_cert} ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{3,63914554045}}]},
 {subject,<<"CN=Couchbase Internal Client (32660726)">>},
 {not_after,63985747645},
 {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
 {load_timestamp,63914554045},
 {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQ"...>>},
 {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDWTCCAkGgAwIBAgIIGD/HyMO1PBAwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjVaFw0y\nNzA4MTcxODQ3MjVaMC8xLTArBgNVBAMTJENvdWNoYmFzZSBJbnRlcm5hbCBDbGll\nbnQgKDMyNjYwNzI2KTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAKHe\n9VAt+X6MU19Fg7ok9H0Zxkmq5mDd0acYZWMO8WqPC7jBY2dopAKDJoF/AKgk8TLT\ncz5AYGM"...>>},
 {pkey_passphrase_settings,[]},
 {certs_epoch,0},
 {type,generated},
 {name,"@internal"}]
[error_logger:info,2025-05-15T18:47:25.763Z,ns_1@db3.lan:ns_doctor_sup<0.2302.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_doctor_sup}
    started: [{pid,<0.2304.0>},
              {id,ns_doctor},
              {mfargs,{ns_doctor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:25.763Z,ns_1@db3.lan:<0.582.0>:restartable:start_child:92]Started child process <0.2302.0>
  MFA: {ns_doctor_sup,start_link,[]}
[ns_server:debug,2025-05-15T18:47:25.763Z,ns_1@db3.lan:<0.779.0>:restartable:loop:65]Restarting child <0.780.0>
  MFA: {leader_services_sup,start_link,[]}
  Shutdown policy: infinity
  Caller: {<0.2237.0>,#Ref<0.3529285563.3650617353.3658>}
[ns_server:info,2025-05-15T18:47:25.763Z,ns_1@db3.lan:mb_master<0.794.0>:mb_master:terminate:247]Synchronously shutting down child mb_master_sup
[ns_server:debug,2025-05-15T18:47:25.763Z,ns_1@db3.lan:<0.899.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.898.0>} exited with reason shutdown
[ns_server:info,2025-05-15T18:47:25.763Z,ns_1@db3.lan:leader_registry<0.792.0>:leader_registry:handle_down:286]Process <0.898.0> registered as 'license_reporting' terminated.
[ns_server:debug,2025-05-15T18:47:25.763Z,ns_1@db3.lan:<0.897.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.896.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.763Z,ns_1@db3.lan:<0.884.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.882.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.763Z,ns_1@db3.lan:<0.883.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {compat_mode_events,<0.882.0>} exited with reason shutdown
[ns_server:info,2025-05-15T18:47:25.763Z,ns_1@db3.lan:leader_registry<0.792.0>:leader_registry:handle_down:286]Process <0.894.0> registered as 'tombstone_purger' terminated.
[ns_server:info,2025-05-15T18:47:25.763Z,ns_1@db3.lan:leader_registry<0.792.0>:leader_registry:handle_down:286]Process <0.882.0> registered as 'auto_failover' terminated.
[ns_server:info,2025-05-15T18:47:25.763Z,ns_1@db3.lan:leader_registry<0.792.0>:leader_registry:handle_down:286]Process <0.880.0> registered as 'ns_orchestrator' terminated.
[ns_server:info,2025-05-15T18:47:25.763Z,ns_1@db3.lan:leader_registry<0.792.0>:leader_registry:handle_down:286]Process <0.879.0> registered as 'auto_rebalance' terminated.
[ns_server:debug,2025-05-15T18:47:25.763Z,ns_1@db3.lan:<0.816.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {'kv-events',<0.815.0>} exited with reason shutdown
[ns_server:info,2025-05-15T18:47:25.763Z,ns_1@db3.lan:leader_registry<0.792.0>:leader_registry:handle_down:286]Process <0.878.0> registered as 'auto_reprovision' terminated.
[ns_server:debug,2025-05-15T18:47:25.763Z,ns_1@db3.lan:<0.2295.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.2285.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.763Z,ns_1@db3.lan:leader_activities<0.787.0>:leader_activities:handle_internal_process_down:440]Process {quorum_nodes_manager,<0.2285.0>} terminated with reason shutdown
[ns_server:info,2025-05-15T18:47:25.763Z,ns_1@db3.lan:leader_registry<0.792.0>:leader_registry:handle_down:286]Process <0.815.0> registered as 'chronicle_master' terminated.
[ns_server:info,2025-05-15T18:47:25.764Z,ns_1@db3.lan:leader_registry<0.792.0>:leader_registry:handle_down:286]Process <0.812.0> registered as 'ns_tick' terminated.
[ns_server:debug,2025-05-15T18:47:25.764Z,ns_1@db3.lan:<0.806.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_node_disco_events,<0.805.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.764Z,ns_1@db3.lan:leader_activities<0.787.0>:leader_activities:handle_internal_process_down:440]Process {acquirer,<0.805.0>} terminated with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.764Z,ns_1@db3.lan:leader_registry<0.792.0>:leader_registry:handle_new_leader:275]New leader is undefined. Invalidating name cache.
[ns_server:debug,2025-05-15T18:47:25.764Z,ns_1@db3.lan:<0.795.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.794.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.764Z,ns_1@db3.lan:<0.793.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {leader_events,<0.792.0>} exited with reason shutdown
[ns_server:warn,2025-05-15T18:47:25.764Z,ns_1@db3.lan:leader_lease_agent<0.789.0>:leader_lease_agent:handle_terminate:302]Terminating with reason shutdown when we own an active lease:
{lease,
    {lease_holder,<<"9a39908e5882941cd97af5604964e304">>,'ns_1@cb.local'},
    -576460707924796438,-576460692924796438,
    {timer,#Ref<0.3529285563.3650617352.3781>,
        {lease_expired,
            {lease_holder,<<"9a39908e5882941cd97af5604964e304">>,
                'ns_1@cb.local'}}},
    active}
Persisting updated lease.
[ns_server:debug,2025-05-15T18:47:25.766Z,ns_1@db3.lan:leader_activities<0.787.0>:leader_activities:handle_internal_process_down:440]Process {agent,<0.789.0>} terminated with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.767Z,ns_1@db3.lan:<0.779.0>:restartable:shutdown_child:114]Successfully terminated process <0.780.0>
[error_logger:info,2025-05-15T18:47:25.767Z,ns_1@db3.lan:leader_leases_sup<0.2316.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_leases_sup}
    started: [{pid,<0.2317.0>},
              {id,leader_activities},
              {mfargs,{leader_activities,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,10000},
              {child_type,worker}]

[ns_server:warn,2025-05-15T18:47:25.768Z,ns_1@db3.lan:leader_lease_agent<0.2318.0>:leader_lease_agent:maybe_recover_persisted_lease:393]Found persisted lease [{node,'ns_1@cb.local'},
                       {uuid,<<"9a39908e5882941cd97af5604964e304">>},
                       {time_left,13803},
                       {status,active}]
[error_logger:info,2025-05-15T18:47:25.768Z,ns_1@db3.lan:leader_leases_sup<0.2316.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_leases_sup}
    started: [{pid,<0.2318.0>},
              {id,leader_lease_agent},
              {mfargs,{leader_lease_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.768Z,ns_1@db3.lan:leader_services_sup<0.2315.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_services_sup}
    started: [{pid,<0.2316.0>},
              {id,leader_leases_sup},
              {mfargs,{leader_leases_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:25.768Z,ns_1@db3.lan:leader_registry_sup<0.2320.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_registry_sup}
    started: [{pid,<0.2321.0>},
              {id,leader_registry},
              {mfargs,{leader_registry,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:25.768Z,ns_1@db3.lan:leader_registry_sup<0.2320.0>:mb_master:check_master_takeover_needed:183]Sending master node question to the following nodes: []
[ns_server:debug,2025-05-15T18:47:25.768Z,ns_1@db3.lan:leader_registry_sup<0.2320.0>:mb_master:check_master_takeover_needed:187]Got replies: []
[ns_server:debug,2025-05-15T18:47:25.768Z,ns_1@db3.lan:leader_registry_sup<0.2320.0>:mb_master:check_master_takeover_needed:193]Was unable to discover master, not going to force mastership takeover
[ns_server:debug,2025-05-15T18:47:25.769Z,ns_1@db3.lan:mb_master<0.2323.0>:mb_master:init:86]Heartbeat interval is 2000
[user:info,2025-05-15T18:47:25.769Z,ns_1@db3.lan:mb_master<0.2323.0>:mb_master:init:91]I'm the only node, so I'm the master.
[ns_server:info,2025-05-15T18:47:25.769Z,ns_1@db3.lan:ns_log<0.506.0>:ns_log:is_duplicate_log:156]suppressing duplicate log mb_master:0([<<"I'm the only node, so I'm the master.">>]) because it's been seen 1 times in the past 35.321782 secs (last seen 35.321782 secs ago
[ns_server:debug,2025-05-15T18:47:25.769Z,ns_1@db3.lan:leader_registry<0.2321.0>:leader_registry:handle_new_leader:275]New leader is 'ns_1@db3.lan'. Invalidating name cache.
[error_logger:info,2025-05-15T18:47:25.769Z,ns_1@db3.lan:mb_master_sup<0.2325.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.2326.0>},
              {id,leader_lease_acquirer},
              {mfargs,{leader_lease_acquirer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,10000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:25.769Z,ns_1@db3.lan:leader_quorum_nodes_manager<0.2328.0>:leader_quorum_nodes_manager:pull_config:99]Attempting to pull config from nodes:
[]
[ns_server:debug,2025-05-15T18:47:25.769Z,ns_1@db3.lan:leader_quorum_nodes_manager<0.2328.0>:leader_quorum_nodes_manager:pull_config:104]Pulled config successfully.
[error_logger:info,2025-05-15T18:47:25.769Z,ns_1@db3.lan:mb_master_sup<0.2325.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.2328.0>},
              {id,leader_quorum_nodes_manager},
              {mfargs,{leader_quorum_nodes_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:47:25.769Z,ns_1@db3.lan:mb_master_sup<0.2325.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,ns_tick},
                                         ns_tick,[],[]]): started as <0.2333.0> on 'ns_1@db3.lan'

[error_logger:info,2025-05-15T18:47:25.769Z,ns_1@db3.lan:mb_master_sup<0.2325.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.2333.0>},
              {id,ns_tick},
              {mfargs,{ns_tick,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,10},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:25.769Z,ns_1@db3.lan:<0.2334.0>:chronicle_master:do_init:115]Starting with SelfRef = #Ref<0.3529285563.3650617346.4479>
[ns_server:info,2025-05-15T18:47:25.769Z,ns_1@db3.lan:mb_master_sup<0.2325.0>:misc:start_singleton:913]start_singleton(gen_server2, start_link, [{via,leader_registry,
                                           chronicle_master},
                                          chronicle_master,[],[]]): started as <0.2334.0> on 'ns_1@db3.lan'

[error_logger:info,2025-05-15T18:47:25.769Z,ns_1@db3.lan:mb_master_sup<0.2325.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.2334.0>},
              {id,chronicle_master},
              {mfargs,{chronicle_master,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:warn,2025-05-15T18:47:25.769Z,ns_1@db3.lan:<0.2331.0>:leader_lease_acquire_worker:handle_lease_already_acquired:226]Failed to acquire lease from 'ns_1@db3.lan' because its already taken by {'ns_1@cb.local',
                                                                          <<"9a39908e5882941cd97af5604964e304">>} (valid for 13801ms)
[error_logger:info,2025-05-15T18:47:25.770Z,ns_1@db3.lan:ns_orchestrator_sup<0.2338.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.2339.0>},
              {id,compat_mode_events},
              {mfargs,{gen_event,start_link,[{local,compat_mode_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.770Z,ns_1@db3.lan:ns_orchestrator_sup<0.2338.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.2341.0>},
              {id,compat_mode_manager},
              {mfargs,{compat_mode_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.770Z,ns_1@db3.lan:ns_orchestrator_child_sup<0.2343.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.2344.0>},
              {id,ns_janitor_server},
              {mfargs,{ns_janitor_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:47:25.770Z,ns_1@db3.lan:ns_orchestrator_child_sup<0.2343.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          auto_reprovision},
                                         auto_reprovision,[],[]]): started as <0.2346.0> on 'ns_1@db3.lan'

[error_logger:info,2025-05-15T18:47:25.770Z,ns_1@db3.lan:ns_orchestrator_child_sup<0.2343.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.2346.0>},
              {id,auto_reprovision},
              {mfargs,{auto_reprovision,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:47:25.770Z,ns_1@db3.lan:ns_orchestrator_child_sup<0.2343.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,auto_rebalance},
                                         auto_rebalance,[],[]]): started as <0.2348.0> on 'ns_1@db3.lan'

[error_logger:info,2025-05-15T18:47:25.770Z,ns_1@db3.lan:ns_orchestrator_child_sup<0.2343.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.2348.0>},
              {id,auto_rebalance},
              {mfargs,{auto_rebalance,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:47:25.770Z,ns_1@db3.lan:ns_orchestrator_child_sup<0.2343.0>:misc:start_singleton:913]start_singleton(gen_statem, start_link, [{via,leader_registry,ns_orchestrator},
                                         ns_orchestrator,[],[]]): started as <0.2350.0> on 'ns_1@db3.lan'

[error_logger:info,2025-05-15T18:47:25.770Z,ns_1@db3.lan:ns_orchestrator_child_sup<0.2343.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.2350.0>},
              {id,ns_orchestrator},
              {mfargs,{ns_orchestrator,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.771Z,ns_1@db3.lan:ns_orchestrator_sup<0.2338.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.2343.0>},
              {id,ns_orchestrator_child_sup},
              {mfargs,{ns_orchestrator_child_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:47:25.771Z,ns_1@db3.lan:<0.2352.0>:auto_failover:init:223]init auto_failover.
[user:info,2025-05-15T18:47:25.771Z,ns_1@db3.lan:<0.2352.0>:auto_failover:enable_auto_failover:460]Enabled auto-failover with timeout 120 and max count 1
[ns_server:debug,2025-05-15T18:47:25.771Z,ns_1@db3.lan:<0.2352.0>:auto_failover:update_and_save_auto_failover_state:479]No change in timeout 120
[ns_server:info,2025-05-15T18:47:25.771Z,ns_1@db3.lan:ns_log<0.506.0>:ns_log:is_duplicate_log:156]suppressing duplicate log auto_failover:0([<<"Enabled auto-failover with timeout 120 and max count 1">>]) because it's been seen 1 times in the past 35.16474 secs (last seen 35.16474 secs ago
[ns_server:debug,2025-05-15T18:47:25.771Z,ns_1@db3.lan:<0.2352.0>:auto_failover:update_and_save_auto_failover_state:487]No change in max count 1
[ns_server:debug,2025-05-15T18:47:25.771Z,ns_1@db3.lan:<0.2352.0>:auto_failover:init_logic_state:249]Using auto-failover logic state {state,[],
                                    [{service_state,kv,nil,false},
                                     {service_state,n1ql,nil,false},
                                     {service_state,index,nil,false},
                                     {service_state,fts,nil,false},
                                     {service_state,cbas,nil,false},
                                     {service_state,eventing,nil,false},
                                     {service_state,backup,nil,false}],
                                    118}
[ns_server:info,2025-05-15T18:47:25.771Z,ns_1@db3.lan:ns_orchestrator_sup<0.2338.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,auto_failover},
                                         auto_failover,[],[]]): started as <0.2352.0> on 'ns_1@db3.lan'

[error_logger:info,2025-05-15T18:47:25.771Z,ns_1@db3.lan:ns_orchestrator_sup<0.2338.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.2352.0>},
              {id,auto_failover},
              {mfargs,{auto_failover,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.771Z,ns_1@db3.lan:mb_master_sup<0.2325.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.2338.0>},
              {id,ns_orchestrator_sup},
              {mfargs,{ns_orchestrator_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:info,2025-05-15T18:47:25.771Z,ns_1@db3.lan:mb_master_sup<0.2325.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          tombstone_purger},
                                         tombstone_purger,[],[]]): started as <0.2357.0> on 'ns_1@db3.lan'

[error_logger:info,2025-05-15T18:47:25.771Z,ns_1@db3.lan:mb_master_sup<0.2325.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.2357.0>},
              {id,tombstone_purger},
              {mfargs,{tombstone_purger,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.771Z,ns_1@db3.lan:mb_master_sup<0.2325.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.2358.0>},
              {id,global_tasks},
              {mfargs,{global_tasks,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.771Z,ns_1@db3.lan:mb_master_sup<0.2325.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.2359.0>},
              {id,guardrail_enforcer},
              {mfargs,{guardrail_enforcer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:25.771Z,ns_1@db3.lan:guardrail_enforcer<0.2359.0>:guardrail_enforcer:maybe_notify_services:137]Changed Statuses: #{}
[ns_server:debug,2025-05-15T18:47:25.772Z,ns_1@db3.lan:<0.2361.0>:license_reporting:init:60]Starting license_reporting server
[ns_server:info,2025-05-15T18:47:25.772Z,ns_1@db3.lan:mb_master_sup<0.2325.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          license_reporting},
                                         license_reporting,[],[]]): started as <0.2361.0> on 'ns_1@db3.lan'

[error_logger:info,2025-05-15T18:47:25.772Z,ns_1@db3.lan:mb_master_sup<0.2325.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.2361.0>},
              {id,license_reporting},
              {mfargs,{license_reporting,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.772Z,ns_1@db3.lan:leader_registry_sup<0.2320.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_registry_sup}
    started: [{pid,<0.2323.0>},
              {id,mb_master},
              {mfargs,{mb_master,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:25.772Z,ns_1@db3.lan:leader_services_sup<0.2315.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_services_sup}
    started: [{pid,<0.2320.0>},
              {id,leader_registry_sup},
              {mfargs,{leader_registry_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:47:25.772Z,ns_1@db3.lan:<0.779.0>:restartable:start_child:92]Started child process <0.2315.0>
  MFA: {leader_services_sup,start_link,[]}
[ns_server:debug,2025-05-15T18:47:25.772Z,ns_1@db3.lan:ns_node_disco<0.539.0>:ns_node_disco:handle_info:160]Node renaming transaction ended. MRef = #Ref<0.3529285563.3650617347.4802>
[ns_server:debug,2025-05-15T18:47:25.772Z,ns_1@db3.lan:remote_monitors<0.297.0>:remote_monitors:handle_info:86]Node renaming transaction ended. MRef = #Ref<0.3529285563.3650617347.4808>
[ns_server:debug,2025-05-15T18:47:25.772Z,ns_1@db3.lan:ns_cookie_manager<0.238.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[ns_server:debug,2025-05-15T18:47:25.772Z,ns_1@db3.lan:wait_link_to_couchdb_node<0.472.0>:ns_server_nodes_sup:wait_link_to_couchdb_node_loop:202]Link to couchdb node was unpaused.
[ns_server:debug,2025-05-15T18:47:25.772Z,ns_1@db3.lan:memcached_config_mgr<0.704.0>:memcached_config_mgr:handle_info:198]Got DOWN with reason: unpaused from memcached port server: <16971.139.0>. Shutting down
[ns_server:debug,2025-05-15T18:47:25.772Z,ns_1@db3.lan:ns_node_disco_events<0.538.0>:ns_config_rep:handle_node_disco_event:513]Detected new nodes (['ns_1@db3.lan']).  Moving config around.
[ns_server:debug,2025-05-15T18:47:25.772Z,ns_1@db3.lan:wait_link_to_couchdb_node<0.472.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:158]Waiting for ns_couchdb node to start
[cluster:debug,2025-05-15T18:47:25.772Z,ns_1@db3.lan:ns_cluster<0.273.0>:ns_cluster:maybe_rename:751]Renamed node from 'ns_1@cb.local' to 'ns_1@db3.lan'.
[ns_server:debug,2025-05-15T18:47:25.772Z,ns_1@db3.lan:<0.708.0>:terse_cluster_info_uploader:handle_info:64]Got DOWN with reason: unpaused from memcached port server: <16971.139.0>. Shutting down
[ns_server:info,2025-05-15T18:47:25.772Z,ns_1@db3.lan:ns_node_disco_events<0.538.0>:ns_node_disco_log:handle_event:40]ns_node_disco_log: nodes changed: ['ns_1@db3.lan']
[cluster:info,2025-05-15T18:47:25.772Z,ns_1@db3.lan:ns_cluster<0.273.0>:ns_cluster:do_change_address:720]Renamed node. New name is 'ns_1@db3.lan'.
[ns_server:debug,2025-05-15T18:47:25.772Z,ns_1@db3.lan:<0.799.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.704.0>} exited with reason {shutdown,
                                                                                {memcached_port_server_down,
                                                                                 <16971.139.0>,
                                                                                 unpaused}}
[ns_server:debug,2025-05-15T18:47:25.772Z,ns_1@db3.lan:<0.709.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {bucket_info_cache_invalidations,<0.708.0>} exited with reason {shutdown,
                                                                                               {memcached_port_server_down,
                                                                                                <16971.139.0>,
                                                                                                unpaused}}
[error_logger:error,2025-05-15T18:47:25.772Z,ns_1@db3.lan:<0.703.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {<0.703.0>,suppress_max_restart_intensity}
    errorContext: child_terminated
    reason: {shutdown,{memcached_port_server_down,<16971.139.0>,unpaused}}
    offender: [{pid,<0.704.0>},
               {id,memcached_config_mgr},
               {mfargs,{memcached_config_mgr,start_link,[]}},
               {restart_type,permanent},
               {significant,false},
               {shutdown,1000},
               {child_type,worker}]

[error_logger:error,2025-05-15T18:47:25.772Z,ns_1@db3.lan:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {local,ns_server_sup}
    errorContext: child_terminated
    reason: {shutdown,{memcached_port_server_down,<16971.139.0>,unpaused}}
    offender: [{pid,<0.708.0>},
               {id,terse_cluster_info_uploader},
               {mfargs,{terse_cluster_info_uploader,start_link,[]}},
               {restart_type,permanent},
               {significant,false},
               {shutdown,1000},
               {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:25.773Z,ns_1@db3.lan:memcached_config_mgr<0.2365.0>:memcached_config_mgr:memcached_port_pid:154]waiting for completion of initial ns_ports_setup round
[ns_server:debug,2025-05-15T18:47:25.772Z,ns_1@db3.lan:<0.2363.0>:ns_node_disco:do_nodes_wanted_updated_fun:210]ns_node_disco: nodes_wanted updated: ['ns_1@db3.lan'], with cookie: {sanitized,
                                                                     <<"1ZVTl+fgxazsGs5XltMEJFjUhLpt0If4lYDK2laySmI=">>}
[error_logger:info,2025-05-15T18:47:25.773Z,ns_1@db3.lan:<0.703.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.703.0>,suppress_max_restart_intensity}
    started: [{pid,<0.2365.0>},
              {id,memcached_config_mgr},
              {mfargs,{memcached_config_mgr,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:25.773Z,ns_1@db3.lan:<0.2366.0>:memcached_config_mgr:memcached_port_pid:154]waiting for completion of initial ns_ports_setup round
[ns_server:debug,2025-05-15T18:47:25.773Z,ns_1@db3.lan:<0.2363.0>:ns_node_disco:do_nodes_wanted_updated_fun:213]ns_node_disco: nodes_wanted pong: ['ns_1@db3.lan'], with cookie: {sanitized,
                                                                  <<"1ZVTl+fgxazsGs5XltMEJFjUhLpt0If4lYDK2laySmI=">>}
[error_logger:info,2025-05-15T18:47:25.773Z,ns_1@db3.lan:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2366.0>},
              {id,terse_cluster_info_uploader},
              {mfargs,{terse_cluster_info_uploader,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:25.773Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
otp ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{2,63914554045}}]},
 {cookie,{sanitized,<<"GrXN7d09MHJy5EYJlsotX+rLKNDa9jz1ZbcgSjsngeg=">>}}]
[user:info,2025-05-15T18:47:25.773Z,ns_1@db3.lan:ns_cookie_manager<0.238.0>:ns_cookie_manager:do_cookie_init:78]Initial otp cookie generated: {sanitized,
                                  <<"GrXN7d09MHJy5EYJlsotX+rLKNDa9jz1ZbcgSjsngeg=">>}
[ns_server:debug,2025-05-15T18:47:25.773Z,ns_1@db3.lan:ns_cookie_manager<0.238.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[cluster:debug,2025-05-15T18:47:25.773Z,ns_1@db3.lan:ns_cluster<0.273.0>:ns_cluster:handle_call:426]engage_cluster(..) -> {ok,ok}
[ns_server:debug,2025-05-15T18:47:25.773Z,ns_1@db3.lan:<0.2370.0>:ns_node_disco:do_nodes_wanted_updated_fun:210]ns_node_disco: nodes_wanted updated: ['ns_1@db3.lan'], with cookie: {sanitized,
                                                                     <<"GrXN7d09MHJy5EYJlsotX+rLKNDa9jz1ZbcgSjsngeg=">>}
[ns_server:debug,2025-05-15T18:47:25.773Z,ns_1@db3.lan:<0.2370.0>:ns_node_disco:do_nodes_wanted_updated_fun:213]ns_node_disco: nodes_wanted pong: ['ns_1@db3.lan'], with cookie: {sanitized,
                                                                  <<"GrXN7d09MHJy5EYJlsotX+rLKNDa9jz1ZbcgSjsngeg=">>}
[ns_server:debug,2025-05-15T18:47:25.773Z,ns_1@db3.lan:ns_config_rep<0.548.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([otp]..)
[ns_server:debug,2025-05-15T18:47:25.776Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Accepted new connection from <0.2254.0> DistCtrl #Port<0.85>: {con,
                                                                        #Ref<0.3529285563.3650617347.5005>,
                                                                        inet_tcp_dist,
                                                                        undefined,
                                                                        undefined}
[ns_server:debug,2025-05-15T18:47:25.776Z,ns_1@db3.lan:net_kernel<0.2253.0>:cb_dist:info_msg:1098]cb_dist: Accepting connection from <0.2254.0> using module inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:25.777Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3529285563.3650617347.5005>,
                                  inet_tcp_dist,<0.2377.0>,
                                  #Ref<0.3529285563.3650617346.4514>}
[error_logger:info,2025-05-15T18:47:25.777Z,ns_1@db3.lan:net_kernel<0.2253.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.2377.0>,shutdown}}
[ns_server:debug,2025-05-15T18:47:25.777Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3529285563.3650617347.5005>,
                               inet_tcp_dist,<0.2377.0>,
                               #Ref<0.3529285563.3650617346.4514>}
[ns_server:debug,2025-05-15T18:47:25.802Z,ns_1@db3.lan:ns_ports_setup<0.678.0>:ns_ports_setup:children_loop_continue:98]Remote monitor <16971.133.0> was unpaused after node name change. Restart loop.
[ns_server:debug,2025-05-15T18:47:25.803Z,ns_1@db3.lan:ns_ports_setup<0.678.0>:ns_ports_manager:set_dynamic_children:48]Setting children [memcached,saslauthd_port,goxdcr]
[ns_server:debug,2025-05-15T18:47:25.803Z,ns_1@db3.lan:ns_ports_setup<0.678.0>:ns_ports_setup:set_children:65]Monitor ns_child_ports_sup <16971.133.0>
[ns_server:debug,2025-05-15T18:47:25.803Z,ns_1@db3.lan:<0.2366.0>:memcached_config_mgr:memcached_port_pid:156]ns_ports_setup seems to be ready
[ns_server:debug,2025-05-15T18:47:25.803Z,ns_1@db3.lan:memcached_config_mgr<0.2365.0>:memcached_config_mgr:memcached_port_pid:156]ns_ports_setup seems to be ready
[ns_server:debug,2025-05-15T18:47:25.804Z,ns_1@db3.lan:<0.2366.0>:memcached_config_mgr:find_port_pid_loop:164]Found memcached port <16971.139.0>
[ns_server:debug,2025-05-15T18:47:25.804Z,ns_1@db3.lan:memcached_config_mgr<0.2365.0>:memcached_config_mgr:find_port_pid_loop:164]Found memcached port <16971.139.0>
[ns_server:debug,2025-05-15T18:47:25.804Z,ns_1@db3.lan:<0.2366.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":27,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true,\"hostname\":\"db3.lan\",\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[ns_server:debug,2025-05-15T18:47:25.804Z,ns_1@db3.lan:memcached_config_mgr<0.2365.0>:memcached_config_mgr:do_read_current_memcached_config:371]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2025-05-15T18:47:25.805Z,ns_1@db3.lan:memcached_config_mgr<0.2365.0>:memcached_config_mgr:init:100]found memcached port to be already active
[ns_server:info,2025-05-15T18:47:25.805Z,ns_1@db3.lan:memcached_config_mgr<0.2365.0>:memcached_config_mgr:push_tls_config:233]Pushing TLS config to memcached:
{[{<<"private key">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem">>},
  {<<"certificate chain">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem">>},
  {<<"CA file">>,<<"/opt/couchbase/var/lib/couchbase/config/certs/ca.pem">>},
  {<<"minimum version">>,<<"TLS 1.2">>},
  {<<"cipher list">>,
   {[{<<"TLS 1.2">>,<<"HIGH">>},
     {<<"TLS 1.3">>,
      <<"TLS_AES_256_GCM_SHA384:TLS_AES_128_GCM_SHA256:TLS_CHACHA20_POLY1305_SHA256">>}]}},
  {<<"cipher order">>,true},
  {<<"client cert auth">>,<<"disabled">>}]}
[ns_server:info,2025-05-15T18:47:25.807Z,ns_1@db3.lan:memcached_config_mgr<0.2365.0>:memcached_config_mgr:push_tls_config:237]Successfully pushed TLS config to memcached
[ns_server:debug,2025-05-15T18:47:25.831Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Accepted new connection from <0.2254.0> DistCtrl #Port<0.86>: {con,
                                                                        #Ref<0.3529285563.3650617352.3955>,
                                                                        inet_tcp_dist,
                                                                        undefined,
                                                                        undefined}
[ns_server:debug,2025-05-15T18:47:25.831Z,ns_1@db3.lan:net_kernel<0.2253.0>:cb_dist:info_msg:1098]cb_dist: Accepting connection from <0.2254.0> using module inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:25.831Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3529285563.3650617352.3955>,
                                  inet_tcp_dist,<0.2387.0>,
                                  #Ref<0.3529285563.3650617346.4542>}
[error_logger:error,2025-05-15T18:47:25.832Z,ns_1@db3.lan:<0.2387.0>:ale_error_logger_handler:do_log:101]
=========================ERROR REPORT=========================
** Connection attempt from node 'ns_1@172.19.0.4' rejected. Invalid challenge reply. **

[ns_server:debug,2025-05-15T18:47:25.832Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3529285563.3650617352.3955>,
                               inet_tcp_dist,<0.2387.0>,
                               #Ref<0.3529285563.3650617346.4542>}
[error_logger:info,2025-05-15T18:47:25.832Z,ns_1@db3.lan:net_kernel<0.2253.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.2387.0>,{recv_challenge_reply_failed,bad_cookie}}}
[error_logger:info,2025-05-15T18:47:25.832Z,ns_1@db3.lan:net_kernel<0.2253.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'ns_1@172.19.0.4'}}
[ns_server:debug,2025-05-15T18:47:25.834Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Accepted new connection from <0.2254.0> DistCtrl #Port<0.87>: {con,
                                                                        #Ref<0.3529285563.3650617352.3962>,
                                                                        inet_tcp_dist,
                                                                        undefined,
                                                                        undefined}
[ns_server:debug,2025-05-15T18:47:25.834Z,ns_1@db3.lan:net_kernel<0.2253.0>:cb_dist:info_msg:1098]cb_dist: Accepting connection from <0.2254.0> using module inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:25.834Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3529285563.3650617352.3962>,
                                  inet_tcp_dist,<0.2389.0>,
                                  #Ref<0.3529285563.3650617352.3965>}
[error_logger:error,2025-05-15T18:47:25.835Z,ns_1@db3.lan:<0.2389.0>:ale_error_logger_handler:do_log:101]
=========================ERROR REPORT=========================
** Connection attempt from node 'ns_1@db2.lan' rejected. Invalid challenge reply. **

[ns_server:debug,2025-05-15T18:47:25.835Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3529285563.3650617352.3962>,
                               inet_tcp_dist,<0.2389.0>,
                               #Ref<0.3529285563.3650617352.3965>}
[error_logger:info,2025-05-15T18:47:25.835Z,ns_1@db3.lan:net_kernel<0.2253.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.2389.0>,{recv_challenge_reply_failed,bad_cookie}}}
[error_logger:info,2025-05-15T18:47:25.835Z,ns_1@db3.lan:net_kernel<0.2253.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'ns_1@db2.lan'}}
[ns_server:debug,2025-05-15T18:47:25.851Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Accepted new connection from <0.2254.0> DistCtrl #Port<0.88>: {con,
                                                                        #Ref<0.3529285563.3650617352.3970>,
                                                                        inet_tcp_dist,
                                                                        undefined,
                                                                        undefined}
[ns_server:debug,2025-05-15T18:47:25.851Z,ns_1@db3.lan:net_kernel<0.2253.0>:cb_dist:info_msg:1098]cb_dist: Accepting connection from <0.2254.0> using module inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:25.851Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3529285563.3650617352.3970>,
                                  inet_tcp_dist,<0.2391.0>,
                                  #Ref<0.3529285563.3650617352.3973>}
[error_logger:error,2025-05-15T18:47:25.852Z,ns_1@db3.lan:<0.2391.0>:ale_error_logger_handler:do_log:101]
=========================ERROR REPORT=========================
** Connection attempt from node 'ns_1@172.19.0.4' rejected. Invalid challenge reply. **

[ns_server:debug,2025-05-15T18:47:25.852Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3529285563.3650617352.3970>,
                               inet_tcp_dist,<0.2391.0>,
                               #Ref<0.3529285563.3650617352.3973>}
[error_logger:info,2025-05-15T18:47:25.852Z,ns_1@db3.lan:net_kernel<0.2253.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.2391.0>,{recv_challenge_reply_failed,bad_cookie}}}
[error_logger:info,2025-05-15T18:47:25.852Z,ns_1@db3.lan:net_kernel<0.2253.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'ns_1@172.19.0.4'}}
[cluster:debug,2025-05-15T18:47:25.892Z,ns_1@db3.lan:ns_cluster<0.273.0>:ns_cluster:handle_call:431]handling complete_join([{<<"targetNode">>,<<"ns_1@db3.lan">>},
                        {<<"requestedServices">>,
                         [<<"index">>,<<"kv">>,<<"n1ql">>]},
                        {<<"chronicleInfo">>,
                         <<"g3QAAAAEZAAPY29tbWl0dGVkX3NlcW5vYRhkAA5jb21wYXRfdmVyc2lvbmEAZAAGY29uZmlnaAVkAAlsb2dfZW50cnltAAAAIDVhYWIwM2RiZWNhZDA2YjUwZmZiNDc0ZGE1OWEwMGM5aAJhBGQAD25zXzFAMTcyLjE5LjAuNGEYaApkAAZjb25maWdoA20AAAAgMWRkNTk5MTQ2M2Q1MTllMjM3MjlkZGEyYzgwYzM3NmJhAGEDYQBtAAAAIDQ1NzI0OTE1ZGFhMmE0ODljMGE2ODE2ZDIwMTk4NTg1dAAAAANkAA9uc18xQDE3Mi4xOS4wLjR0AAAAAmQAAmlkbQAAACAxZGQ1OTkxNDYzZDUxOWUyMzcyOWRkYTJjODBjMzc2YmQABHJvbGVkAAV2b3RlcmQADG5zXzFAZGIyLmxhbnQAAAACZAACaWRtAAAAIDlmNjg5NGI1MWZmYTQ5N2NkZmM4Yjc5MmQ5ZTY3MjZiZAAEcm9sZWQAB3JlcGxpY2FkAAxuc18xQGRiMy5sYW50AAAAAmQAAmlkbQAAACA2NTllYzAzYzgxMmY1Y2M5NDU2YmQ3MWZlM2MxYTA0N2QABHJvbGVkAAdyZXBsaWNhZAAJdW5kZWZpbmVkdAAAAAJkABRjaHJvbmljbGVfY29uZmlnX3JzbWgDZAAKcnNtX2NvbmZpZ2QAFGNocm9uaWNsZV9jb25maWdfcnNtamQAAmt2aANkAApyc21fY29uZmlnZAAMY2hyb25pY2xlX2t2anQAAAAAZAAJdW5kZWZpbmVkbAAAAAFoAm0AAAAgNWFhYjAzZGJlY2FkMDZiNTBmZmI0NzRkYTU5YTAwYzlhAGpkAApoaXN0b3J5X2lkbQAAACA1YWFiMDNkYmVjYWQwNmI1MGZmYjQ3NGRhNTlhMDBjOQ==">>},
                        {<<"availableStorage">>,
                         {[{<<"hdd">>,
                            [{[{<<"path">>,<<"/">>},
                               {<<"sizeKBytes">>,1056557396},
                               {<<"usagePercent">>,2}]},
                             {[{<<"path">>,<<"/dev">>},
                               {<<"sizeKBytes">>,65536},
                               {<<"usagePercent">>,0}]},
                             {[{<<"path">>,<<"/dev/shm">>},
                               {<<"sizeKBytes">>,65536},
                               {<<"usagePercent">>,0}]},
                             {[{<<"path">>,<<"/etc/resolv.conf">>},
                               {<<"sizeKBytes">>,1056557396},
                               {<<"usagePercent">>,2}]},
                             {[{<<"path">>,<<"/etc/hostname">>},
                               {<<"sizeKBytes">>,1056557396},
                               {<<"usagePercent">>,2}]},
                             {[{<<"path">>,<<"/etc/hosts">>},
                               {<<"sizeKBytes">>,1056557396},
                               {<<"usagePercent">>,2}]},
                             {[{<<"path">>,<<"/opt/couchbase/var">>},
                               {<<"sizeKBytes">>,482797652},
                               {<<"usagePercent">>,92}]},
                             {[{<<"path">>,<<"/proc/kcore">>},
                               {<<"sizeKBytes">>,65536},
                               {<<"usagePercent">>,0}]},
                             {[{<<"path">>,<<"/proc/keys">>},
                               {<<"sizeKBytes">>,65536},
                               {<<"usagePercent">>,0}]},
                             {[{<<"path">>,<<"/proc/timer_list">>},
                               {<<"sizeKBytes">>,65536},
                               {<<"usagePercent">>,0}]},
                             {[{<<"path">>,<<"/proc/scsi">>},
                               {<<"sizeKBytes">>,4012680},
                               {<<"usagePercent">>,0}]},
                             {[{<<"path">>,<<"/sys/firmware">>},
                               {<<"sizeKBytes">>,4012680},
                               {<<"usagePercent">>,0}]}]}]}},
                        {<<"storageTotals">>,
                         {[{<<"ram">>,
                            {[{<<"total">>,8217968640},
                              {<<"quotaTotal">>,3221225472},
                              {<<"quotaUsed">>,0},
                              {<<"used">>,4251119616},
                              {<<"usedByData">>,0},
                              {<<"quotaUsedPerNode">>,0},
                              {<<"quotaTotalPerNode">>,3221225472}]}},
                           {<<"hdd">>,
                            {[{<<"total">>,494384795648},
                              {<<"quotaTotal">>,494384795648},
                              {<<"used">>,454834011996},
                              {<<"usedByData">>,0},
                              {<<"free">>,39550783652}]}}]}},
                        {<<"storage">>,
                         {[{<<"ssd">>,[]},
                           {<<"hdd">>,
                            [{[{<<"path">>,
                                <<"/opt/couchbase/var/lib/couchbase/data">>},
                               {<<"index_path">>,
                                <<"/opt/couchbase/var/lib/couchbase/data">>},
                               {<<"cbas_dirs">>,
                                [<<"/opt/couchbase/var/lib/couchbase/data">>]},
                               {<<"eventing_path">>,
                                <<"/opt/couchbase/var/lib/couchbase/data">>},
                               {<<"java_home">>,<<>>},
                               {<<"quotaMb">>,<<"none">>},
                               {<<"state">>,<<"ok">>}]}]}]}},
                        {<<"clusterMembership">>,<<"active">>},
                        {<<"recoveryType">>,<<"none">>},
                        {<<"status">>,<<"healthy">>},
                        {<<"otpNode">>,<<"ns_1@172.19.0.4">>},
                        {<<"thisNode">>,true},
                        {<<"hostname">>,<<"172.19.0.4:8091">>},
                        {<<"nodeUUID">>,
                         <<"28569ac00b9c1d7c50e39741027d428c">>},
                        {<<"clusterCompatibility">>,458758},
                        {<<"version">>,<<"7.6.2-3721-enterprise">>},
                        {<<"os">>,<<"aarch64-unknown-linux-gnu">>},
                        {<<"cpuCount">>,10},
                        {<<"ports">>,
                         {[{<<"direct">>,11210},
                           {<<"httpsMgmt">>,18091},
                           {<<"httpsCAPI">>,18092},
                           {<<"distTCP">>,21100},
                           {<<"distTLS">>,21150}]}},
                        {<<"services">>,[<<"index">>,<<"kv">>,<<"n1ql">>]},
                        {<<"nodeEncryption">>,false},
                        {<<"nodeEncryptionClientCertVerification">>,false},
                        {<<"addressFamilyOnly">>,false},
                        {<<"configuredHostname">>,<<"172.19.0.4:8091">>},
                        {<<"addressFamily">>,<<"inet">>},
                        {<<"externalListeners">>,
                         [{[{<<"afamily">>,<<"inet">>},
                            {<<"nodeEncryption">>,false}]}]},
                        {<<"serverGroup">>,<<"Group 1">>},
                        {<<"otpCookie">>,
                         {sanitized,<<"biSYTaQUFDVndTs/b+IcDmD2L0woktv9naRdv3GCK6A=">>}},
                        {<<"couchApiBase">>,<<"http://172.19.0.4:8092/">>},
                        {<<"couchApiBaseHTTPS">>,
                         <<"https://172.19.0.4:18092/">>},
                        {<<"nodeHash">>,38045879},
                        {<<"systemStats">>,
                         {[{<<"cpu_utilization_rate">>,3.89883035089008},
                           {<<"cpu_stolen_rate">>,0},
                           {<<"swap_total">>,1073737728},
                           {<<"swap_used">>,3956736},
                           {<<"mem_total">>,8217968640},
                           {<<"mem_free">>,4805804032},
                           {<<"mem_limit">>,8217968640},
                           {<<"cpu_cores_available">>,10},
                           {<<"allocstall">>,17}]}},
                        {<<"interestingStats">>,{[]}},
                        {<<"uptime">>,<<"44">>},
                        {<<"memoryTotal">>,8217968640},
                        {<<"memoryFree">>,4805804032},
                        {<<"mcdMemoryReserved">>,6269},
                        {<<"mcdMemoryAllocated">>,6269},
                        {<<"memoryQuota">>,3072},
                        {<<"queryMemoryQuota">>,0},
                        {<<"indexMemoryQuota">>,512},
                        {<<"ftsMemoryQuota">>,512},
                        {<<"cbasMemoryQuota">>,1024},
                        {<<"eventingMemoryQuota">>,256}])
[user:info,2025-05-15T18:47:25.893Z,ns_1@db3.lan:ns_cluster<0.273.0>:ns_cluster:perform_actual_join:1590]Node 'ns_1@db3.lan' is joining cluster via node 'ns_1@172.19.0.4'.
[ns_server:debug,2025-05-15T18:47:25.893Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
i_am_a_dead_man ->
[{'_vclock',[{<<"e0d520cb35700b4a473cf359e3354528">>,{1,63914554045}}]}|true]
[ns_server:debug,2025-05-15T18:47:25.893Z,ns_1@db3.lan:ns_config_rep<0.548.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([i_am_a_dead_man]..)
[ns_server:debug,2025-05-15T18:47:25.928Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Accepted new connection from <0.2254.0> DistCtrl #Port<0.90>: {con,
                                                                        #Ref<0.3529285563.3650617352.3995>,
                                                                        inet_tcp_dist,
                                                                        undefined,
                                                                        undefined}
[ns_server:debug,2025-05-15T18:47:25.928Z,ns_1@db3.lan:net_kernel<0.2253.0>:cb_dist:info_msg:1098]cb_dist: Accepting connection from <0.2254.0> using module inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:25.928Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3529285563.3650617352.3995>,
                                  inet_tcp_dist,<0.2404.0>,
                                  #Ref<0.3529285563.3650617347.5032>}
[error_logger:error,2025-05-15T18:47:25.929Z,ns_1@db3.lan:<0.2404.0>:ale_error_logger_handler:do_log:101]
=========================ERROR REPORT=========================
** Connection attempt from node 'ns_1@172.19.0.4' rejected. Invalid challenge reply. **

[ns_server:debug,2025-05-15T18:47:25.929Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3529285563.3650617352.3995>,
                               inet_tcp_dist,<0.2404.0>,
                               #Ref<0.3529285563.3650617347.5032>}
[error_logger:info,2025-05-15T18:47:25.929Z,ns_1@db3.lan:net_kernel<0.2253.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.2404.0>,{recv_challenge_reply_failed,bad_cookie}}}
[error_logger:info,2025-05-15T18:47:25.929Z,ns_1@db3.lan:net_kernel<0.2253.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'ns_1@172.19.0.4'}}
[ns_server:debug,2025-05-15T18:47:25.946Z,ns_1@db3.lan:ns_ports_setup<0.678.0>:ns_ports_setup:children_loop_continue:86]Send shutdown to all go ports
[ns_server:debug,2025-05-15T18:47:25.947Z,ns_1@db3.lan:ns_ports_setup<0.678.0>:ns_ports_manager:set_dynamic_children:48]Setting children [memcached,saslauthd_port]
[ns_server:debug,2025-05-15T18:47:25.950Z,ns_1@db3.lan:json_rpc_connection-goxdcr-cbauth<0.940.0>:json_rpc_connection:handle_info:142]Socket closed
[ns_server:debug,2025-05-15T18:47:25.950Z,ns_1@db3.lan:<0.983.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.643.0>} exited with reason normal
[ns_server:debug,2025-05-15T18:47:25.950Z,ns_1@db3.lan:<0.965.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.642.0>} exited with reason normal
[ns_server:debug,2025-05-15T18:47:25.950Z,ns_1@db3.lan:<0.961.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.640.0>} exited with reason normal
[ns_server:debug,2025-05-15T18:47:25.950Z,ns_1@db3.lan:<0.944.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.940.0>} exited with reason shutdown
[ns_server:info,2025-05-15T18:47:25.950Z,ns_1@db3.lan:menelaus_cbauth_worker-goxdcr-cbauth<0.948.0>:menelaus_cbauth_worker:handle_info:93]Observed json rpc process <0.940.0> died with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.950Z,ns_1@db3.lan:<0.985.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.644.0>} exited with reason normal
[ns_server:debug,2025-05-15T18:47:25.950Z,ns_1@db3.lan:menelaus_cbauth<0.669.0>:menelaus_cbauth:handle_info:258]Observed worker process {worker,<0.948.0>,"goxdcr-cbauth",internal,
                                #Ref<0.3529285563.3650617346.3908>,<0.940.0>} died with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.950Z,ns_1@db3.lan:<0.957.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.635.0>} exited with reason normal
[ns_server:debug,2025-05-15T18:47:25.954Z,ns_1@db3.lan:<0.932.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.931.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.954Z,ns_1@db3.lan:<0.930.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.929.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.954Z,ns_1@db3.lan:<0.926.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.924.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.954Z,ns_1@db3.lan:<0.925.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.924.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.954Z,ns_1@db3.lan:<0.919.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.917.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.955Z,ns_1@db3.lan:<0.912.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.911.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.955Z,ns_1@db3.lan:<0.918.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.917.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.955Z,ns_1@db3.lan:<0.909.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.908.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.955Z,ns_1@db3.lan:<0.906.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {master_activity_events,<0.905.0>} exited with reason killed
[ns_server:debug,2025-05-15T18:47:25.955Z,ns_1@db3.lan:<0.901.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {leader_events,<0.900.0>} exited with reason shutdown
[ns_server:info,2025-05-15T18:47:25.955Z,ns_1@db3.lan:mb_master<0.2323.0>:mb_master:terminate:247]Synchronously shutting down child mb_master_sup
[ns_server:info,2025-05-15T18:47:25.955Z,ns_1@db3.lan:leader_registry<0.2321.0>:leader_registry:handle_down:286]Process <0.2361.0> registered as 'license_reporting' terminated.
[ns_server:debug,2025-05-15T18:47:25.955Z,ns_1@db3.lan:<0.2360.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.2359.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.955Z,ns_1@db3.lan:<0.2362.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.2361.0>} exited with reason shutdown
[ns_server:info,2025-05-15T18:47:25.955Z,ns_1@db3.lan:leader_registry<0.2321.0>:leader_registry:handle_down:286]Process <0.2357.0> registered as 'tombstone_purger' terminated.
[ns_server:debug,2025-05-15T18:47:25.955Z,ns_1@db3.lan:<0.2353.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {compat_mode_events,<0.2352.0>} exited with reason shutdown
[ns_server:info,2025-05-15T18:47:25.955Z,ns_1@db3.lan:leader_registry<0.2321.0>:leader_registry:handle_down:286]Process <0.2352.0> registered as 'auto_failover' terminated.
[ns_server:debug,2025-05-15T18:47:25.955Z,ns_1@db3.lan:<0.2354.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.2352.0>} exited with reason shutdown
[ns_server:info,2025-05-15T18:47:25.955Z,ns_1@db3.lan:leader_registry<0.2321.0>:leader_registry:handle_down:286]Process <0.2350.0> registered as 'ns_orchestrator' terminated.
[ns_server:info,2025-05-15T18:47:25.955Z,ns_1@db3.lan:leader_registry<0.2321.0>:leader_registry:handle_down:286]Process <0.2348.0> registered as 'auto_rebalance' terminated.
[ns_server:info,2025-05-15T18:47:25.955Z,ns_1@db3.lan:leader_registry<0.2321.0>:leader_registry:handle_down:286]Process <0.2346.0> registered as 'auto_reprovision' terminated.
[ns_server:debug,2025-05-15T18:47:25.955Z,ns_1@db3.lan:<0.2335.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {'kv-events',<0.2334.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.955Z,ns_1@db3.lan:<0.2332.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.2328.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.955Z,ns_1@db3.lan:leader_activities<0.2317.0>:leader_activities:handle_internal_process_down:440]Process {quorum_nodes_manager,<0.2328.0>} terminated with reason shutdown
[ns_server:info,2025-05-15T18:47:25.955Z,ns_1@db3.lan:leader_registry<0.2321.0>:leader_registry:handle_down:286]Process <0.2334.0> registered as 'chronicle_master' terminated.
[ns_server:debug,2025-05-15T18:47:25.955Z,ns_1@db3.lan:leader_activities<0.2317.0>:leader_activities:handle_internal_process_down:440]Process {acquirer,<0.2326.0>} terminated with reason shutdown
[ns_server:info,2025-05-15T18:47:25.955Z,ns_1@db3.lan:leader_registry<0.2321.0>:leader_registry:handle_down:286]Process <0.2333.0> registered as 'ns_tick' terminated.
[ns_server:debug,2025-05-15T18:47:25.955Z,ns_1@db3.lan:<0.2324.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.2323.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.955Z,ns_1@db3.lan:leader_lease_agent<0.2318.0>:leader_lease_agent:handle_abolish_lease:246]Received abolish lease request from {lease_holder,
                                     <<"41ba5f3c412d2f63aca66e746cdec880">>,
                                     'ns_1@db3.lan'} when lease is {lease,
                                                                    {lease_holder,
                                                                     <<"9a39908e5882941cd97af5604964e304">>,
                                                                     'ns_1@cb.local'},
                                                                    undefined,
                                                                    -576460692921486396,
                                                                    {timer,
                                                                     #Ref<0.3529285563.3650617347.4972>,
                                                                     {lease_expired,
                                                                      {lease_holder,
                                                                       <<"9a39908e5882941cd97af5604964e304">>,
                                                                       'ns_1@cb.local'}}},
                                                                    active}
[ns_server:debug,2025-05-15T18:47:25.955Z,ns_1@db3.lan:<0.2327.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_node_disco_events,<0.2326.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.955Z,ns_1@db3.lan:<0.2322.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {leader_events,<0.2321.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.955Z,ns_1@db3.lan:leader_lease_agent<0.2318.0>:leader_lease_agent:handle_abolish_lease:258]Ignoring stale abolish request
[ns_server:warn,2025-05-15T18:47:25.955Z,ns_1@db3.lan:leader_lease_agent<0.2318.0>:leader_lease_agent:handle_terminate:302]Terminating with reason shutdown when we own an active lease:
{lease,
    {lease_holder,<<"9a39908e5882941cd97af5604964e304">>,'ns_1@cb.local'},
    undefined,-576460692921486396,
    {timer,#Ref<0.3529285563.3650617347.4972>,
        {lease_expired,
            {lease_holder,<<"9a39908e5882941cd97af5604964e304">>,
                'ns_1@cb.local'}}},
    active}
Persisting updated lease.
[ns_server:debug,2025-05-15T18:47:25.957Z,ns_1@db3.lan:leader_activities<0.2317.0>:leader_activities:handle_internal_process_down:440]Process {agent,<0.2318.0>} terminated with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.957Z,ns_1@db3.lan:<0.779.0>:restartable:shutdown_child:114]Successfully terminated process <0.2315.0>
[ns_server:debug,2025-05-15T18:47:25.958Z,ns_1@db3.lan:<0.775.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.774.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.958Z,ns_1@db3.lan:<0.760.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.759.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.958Z,ns_1@db3.lan:<0.750.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.749.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.958Z,ns_1@db3.lan:<0.748.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_node_disco_events,<0.746.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.958Z,ns_1@db3.lan:<0.745.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_node_disco_events,<0.743.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.958Z,ns_1@db3.lan:<0.747.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.746.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.958Z,ns_1@db3.lan:<0.739.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.738.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.958Z,ns_1@db3.lan:<0.740.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_node_disco_events,<0.738.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.958Z,ns_1@db3.lan:<0.727.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_tick_event,<0.726.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.958Z,ns_1@db3.lan:<0.725.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_tick_event,<0.724.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.958Z,ns_1@db3.lan:<0.744.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.743.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.958Z,ns_1@db3.lan:<0.721.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_tick_event,<0.720.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.958Z,ns_1@db3.lan:<0.723.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_tick_event,<0.722.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.958Z,ns_1@db3.lan:<0.935.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ale_stats_events,<0.933.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.958Z,ns_1@db3.lan:<0.713.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.712.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.958Z,ns_1@db3.lan:<0.2367.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {bucket_info_cache_invalidations,<0.2366.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.958Z,ns_1@db3.lan:<0.2381.0>:remote_monitors:handle_down:151]Caller of remote monitor <0.2366.0> died with shutdown. Exiting
[ns_server:debug,2025-05-15T18:47:25.958Z,ns_1@db3.lan:<0.2384.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.2365.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.958Z,ns_1@db3.lan:<0.2380.0>:remote_monitors:handle_down:151]Caller of remote monitor <0.2365.0> died with shutdown. Exiting
[ns_server:debug,2025-05-15T18:47:25.958Z,ns_1@db3.lan:<0.692.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.691.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.958Z,ns_1@db3.lan:<0.687.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.686.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.958Z,ns_1@db3.lan:<0.684.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.683.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.959Z,ns_1@db3.lan:<0.680.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {user_storage_events,<0.678.0>} exited with reason killed
[ns_server:debug,2025-05-15T18:47:25.959Z,ns_1@db3.lan:<0.2379.0>:remote_monitors:handle_down:151]Caller of remote monitor <0.678.0> died with killed. Exiting
[ns_server:debug,2025-05-15T18:47:25.959Z,ns_1@db3.lan:<0.679.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.678.0>} exited with reason killed
[ns_server:info,2025-05-15T18:47:25.959Z,ns_1@db3.lan:menelaus_cbauth<0.669.0>:menelaus_cbauth:terminate_external_connections:159]External connections to be terminated: []
[ns_server:debug,2025-05-15T18:47:25.959Z,ns_1@db3.lan:<0.670.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {json_rpc_events,<0.669.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.959Z,ns_1@db3.lan:<0.674.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ssl_service_events,<0.669.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.959Z,ns_1@db3.lan:<0.667.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.666.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.959Z,ns_1@db3.lan:<0.671.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_node_disco_events,<0.669.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.959Z,ns_1@db3.lan:<0.673.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {user_storage_events,<0.669.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.959Z,ns_1@db3.lan:<0.672.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.669.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.959Z,ns_1@db3.lan:<0.628.0>:restartable:shutdown_child:114]Successfully terminated process <0.629.0>
[ns_server:debug,2025-05-15T18:47:25.959Z,ns_1@db3.lan:<0.619.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.618.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.959Z,ns_1@db3.lan:<0.610.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.609.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.959Z,ns_1@db3.lan:<0.608.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.607.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.959Z,ns_1@db3.lan:<0.606.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.605.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.959Z,ns_1@db3.lan:<0.604.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.603.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.960Z,ns_1@db3.lan:<0.2305.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.2304.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.960Z,ns_1@db3.lan:<0.582.0>:restartable:shutdown_child:114]Successfully terminated process <0.2302.0>
[ns_server:debug,2025-05-15T18:47:25.960Z,ns_1@db3.lan:<0.579.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {buckets_events,<0.578.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.982Z,ns_1@db3.lan:chronicle_kv_log<0.504.0>:chronicle_kv_log:log:59]update (key: tasks, rev: {<<"ea19d13c61d8435b9ad1a5f6af1aa5eb">>,14})
[]
[ns_server:debug,2025-05-15T18:47:25.983Z,ns_1@db3.lan:<0.569.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.568.0>} exited with reason killed
[ns_server:debug,2025-05-15T18:47:25.983Z,ns_1@db3.lan:<0.572.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.570.0>} exited with reason killed
[ns_server:debug,2025-05-15T18:47:25.983Z,ns_1@db3.lan:<0.566.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.565.0>} exited with reason killed
[ns_server:debug,2025-05-15T18:47:25.983Z,ns_1@db3.lan:<0.559.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.558.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.983Z,ns_1@db3.lan:<0.549.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events_local,<0.548.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.983Z,ns_1@db3.lan:<0.551.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_node_disco_events,<0.548.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.983Z,ns_1@db3.lan:<0.540.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.539.0>} exited with reason shutdown
[ns_server:info,2025-05-15T18:47:25.983Z,ns_1@db3.lan:prometheus_cfg<0.517.0>:prometheus_cfg:terminate:609]Terminate: shutdown
[ns_server:debug,2025-05-15T18:47:25.983Z,ns_1@db3.lan:<0.535.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {user_storage_events,<0.533.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.983Z,ns_1@db3.lan:<0.534.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.533.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.983Z,ns_1@db3.lan:<0.531.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.530.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.983Z,ns_1@db3.lan:<0.532.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {user_storage_events,<0.530.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.983Z,ns_1@db3.lan:prometheus_cfg<0.517.0>:prometheus_cfg:terminate_prometheus:773]Terminating Prometheus gracefully
[ns_server:debug,2025-05-15T18:47:25.983Z,ns_1@db3.lan:<0.550.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.548.0>} exited with reason shutdown
[error_logger:error,2025-05-15T18:47:25.985Z,ns_1@db3.lan:bucket_info_cache_invalidations<0.571.0>:ale_error_logger_handler:do_log:101]
=========================CRASH REPORT=========================
  crasher:
    initial call: gen_event:init_it/6
    pid: <0.571.0>
    registered_name: bucket_info_cache_invalidations
    exception exit: killed
      in function  gen_event:terminate_server/4 (gen_event.erl, line 580)
    ancestors: [bucket_info_cache,ns_server_sup,ns_server_nodes_sup,
                  <0.290.0>,ns_server_cluster_sup,root_sup,<0.154.0>]
    message_queue_len: 0
    messages: []
    links: []
    dictionary: []
    trap_exit: true
    status: running
    heap_size: 376
    stack_size: 28
    reductions: 973
  neighbours:

[ns_server:debug,2025-05-15T18:47:25.986Z,ns_1@db3.lan:prometheus-goport<0.529.0>:goport:handle_eof:592]Stream 'stdout' closed
[ns_server:debug,2025-05-15T18:47:25.986Z,ns_1@db3.lan:prometheus-goport<0.529.0>:goport:handle_eof:592]Stream 'stderr' closed
[ns_server:info,2025-05-15T18:47:25.986Z,ns_1@db3.lan:prometheus-goport<0.529.0>:goport:handle_process_exit:573]Port exited with status 0.
[ns_server:info,2025-05-15T18:47:25.986Z,ns_1@db3.lan:<0.523.0>:ns_port_server:handle_info:149]Got {exit_status,0} from port prometheus. Exiting normally
[ns_server:debug,2025-05-15T18:47:25.986Z,ns_1@db3.lan:prometheus_cfg<0.517.0>:prometheus_cfg:terminate_prometheus:794]Prometheus port server stopped successfully
[ns_server:debug,2025-05-15T18:47:25.987Z,ns_1@db3.lan:<0.518.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.517.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.987Z,ns_1@db3.lan:<0.511.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.510.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.988Z,ns_1@db3.lan:<0.509.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.508.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.990Z,ns_1@db3.lan:<0.505.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_kv_event_manager,<0.504.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.990Z,ns_1@db3.lan:<0.2369.0>:remote_monitors:handle_down:151]Caller of remote monitor <0.472.0> died with shutdown. Exiting
[ns_server:debug,2025-05-15T18:47:25.990Z,ns_1@db3.lan:ns_couchdb_port<0.471.0>:ns_port_server:terminate:198]Shutting down port ns_couchdb
[ns_server:debug,2025-05-15T18:47:25.990Z,ns_1@db3.lan:ns_couchdb_port<0.471.0>:ns_port_server:port_shutdown:348]Shutdown command: "shutdown"
[ns_server:debug,2025-05-15T18:47:25.996Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3529285563.3650617348.5455>,
                               inet_tcp_dist,<0.2281.0>,
                               #Ref<0.3529285563.3650617348.5458>}
[error_logger:info,2025-05-15T18:47:25.996Z,ns_1@db3.lan:net_kernel<0.2253.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',#Port<0.84>,normal}}
[error_logger:info,2025-05-15T18:47:25.996Z,ns_1@db3.lan:net_kernel<0.2253.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.2281.0>,connection_closed}}
[error_logger:info,2025-05-15T18:47:25.996Z,ns_1@db3.lan:net_kernel<0.2253.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {15946800,#Ref<0.3529285563.3650748419.3237>}}}
[ns_server:debug,2025-05-15T18:47:25.996Z,ns_1@db3.lan:net_kernel<0.2253.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:25.996Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3529285563.3650617350.3322>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:47:25.996Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3529285563.3650617350.3322>,
                                  inet_tcp_dist,<0.2413.0>,
                                  #Ref<0.3529285563.3650617347.5081>}
[ns_server:debug,2025-05-15T18:47:25.997Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3529285563.3650617350.3322>,
                               inet_tcp_dist,<0.2413.0>,
                               #Ref<0.3529285563.3650617347.5081>}
[error_logger:info,2025-05-15T18:47:25.997Z,ns_1@db3.lan:net_kernel<0.2253.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.2413.0>,shutdown}}
[ns_server:info,2025-05-15T18:47:25.997Z,ns_1@db3.lan:ns_couchdb_port<0.471.0>:ns_port_server:handle_info:149]Got {exit_status,0} from port ns_couchdb. Exiting normally
[error_logger:info,2025-05-15T18:47:25.997Z,ns_1@db3.lan:net_kernel<0.2253.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:47:25.997Z,ns_1@db3.lan:ns_couchdb_port<0.471.0>:ns_port_server:terminate:201]ns_couchdb has exited
[ns_server:info,2025-05-15T18:47:25.997Z,ns_1@db3.lan:ns_couchdb_port<0.471.0>:ns_port_server:log:226]ns_couchdb<0.471.0>: 221: got shutdown request. Exiting
ns_couchdb<0.471.0>: [os_mon] memory supervisor port (memsup): Erlang has closed
ns_couchdb<0.471.0>: [os_mon] cpu supervisor port (cpu_sup): Erlang has closed

[ns_server:debug,2025-05-15T18:47:25.997Z,ns_1@db3.lan:<0.461.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.458.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.997Z,ns_1@db3.lan:<0.460.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {user_storage_events,<0.458.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.997Z,ns_1@db3.lan:<0.457.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.455.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.997Z,ns_1@db3.lan:<0.456.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {user_storage_events,<0.455.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.997Z,ns_1@db3.lan:<0.459.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.458.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.997Z,ns_1@db3.lan:<0.445.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.444.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.997Z,ns_1@db3.lan:<0.430.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.429.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.997Z,ns_1@db3.lan:<0.409.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.408.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.998Z,ns_1@db3.lan:<0.327.0>:restartable:shutdown_child:114]Successfully terminated process <0.2189.0>
[ns_server:debug,2025-05-15T18:47:25.998Z,ns_1@db3.lan:<0.309.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.308.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.998Z,ns_1@db3.lan:<0.295.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.292.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.998Z,ns_1@db3.lan:<0.296.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {'kv-events',<0.292.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:25.998Z,ns_1@db3.lan:<0.290.0>:restartable:shutdown_child:114]Successfully terminated process <0.291.0>
[ns_server:debug,2025-05-15T18:47:25.998Z,ns_1@db3.lan:<0.945.0>:ns_pubsub:do_subscribe_link_continue:142]gen_event chronicle_compat_event_manager is shutting down. Propagating to the subscriber <0.941.0>.
[cluster:debug,2025-05-15T18:47:25.999Z,ns_1@db3.lan:ns_cluster<0.273.0>:ns_cluster:perform_actual_join:1600]ns_cluster: joining cluster. Child has exited.
[error_logger:error,2025-05-15T18:47:25.998Z,ns_1@db3.lan:<0.945.0>:ale_error_logger_handler:do_log:101]
=========================CRASH REPORT=========================
  crasher:
    initial call: ns_pubsub:do_subscribe_link/4
    pid: <0.945.0>
    registered_name: []
    exception exit: {gen_event_shutdown,chronicle_compat_event_manager}
      in function  ns_pubsub:do_subscribe_link_continue/3 (src/ns_pubsub.erl, line 168)
    ancestors: ['json_rpc_connection-saslauthd-saslauthd-port',
                  json_rpc_connection_sup,ns_server_cluster_sup,root_sup,
                  <0.154.0>]
    message_queue_len: 0
    messages: []
    links: [<0.941.0>]
    dictionary: []
    trap_exit: true
    status: running
    heap_size: 1598
    stack_size: 28
    reductions: 2418
  neighbours:
    neighbour:
      pid: <0.943.0>
      registered_name: []
      initial call: erlang:apply/2
      current_function: {prim_inet,recv0,3}
      ancestors: ['json_rpc_connection-saslauthd-saslauthd-port',
                  json_rpc_connection_sup,ns_server_cluster_sup,root_sup,
                  <0.154.0>]
      message_queue_len: 0
      links: [<0.941.0>]
      trap_exit: false
      status: waiting
      heap_size: 233
      stack_size: 11
      reductions: 145
      current_stacktrace: [{prim_inet,recv0,3,[]},
                  {json_rpc_connection,receiver_loop,3,
                      [{file,"src/json_rpc_connection.erl"},{line,195}]},
                  {proc_lib,init_p_do_apply,3,
                      [{file,"proc_lib.erl"},{line,240}]}]
    neighbour:
      pid: <0.941.0>
      registered_name: 'json_rpc_connection-saslauthd-saslauthd-port'
      initial call: json_rpc_connection:init/1
      current_function: {gen_server,loop,7}
      ancestors: [json_rpc_connection_sup,ns_server_cluster_sup,root_sup,
                  <0.154.0>]
      message_queue_len: 0
      links: [<0.943.0>,#Port<0.42>,<0.945.0>,<0.289.0>]
      trap_exit: false
      status: waiting
      heap_size: 4185
      stack_size: 11
      reductions: 9576
      current_stacktrace: [{gen_server,loop,7,[{file,"gen_server.erl"},{line,871}]},
                  {proc_lib,init_p_do_apply,3,
                            [{file,"proc_lib.erl"},{line,240}]}]
[error_logger:error,2025-05-15T18:47:25.999Z,ns_1@db3.lan:json_rpc_connection_sup<0.289.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {local,json_rpc_connection_sup}
    errorContext: child_terminated
    reason: {gen_event_shutdown,chronicle_compat_event_manager}
    offender: [{pid,<0.941.0>},
               {id,json_rpc_connection},
               {mfargs,{json_rpc_connection,start_link,undefined}},
               {restart_type,temporary},
               {significant,false},
               {shutdown,brutal_kill},
               {child_type,worker}]

[ns_server:info,2025-05-15T18:47:26.000Z,ns_1@db3.lan:ns_cluster<0.273.0>:menelaus_users:delete_storage_offline:175]User storage "/opt/couchbase/var/lib/couchbase/config/users.dets" was deleted
[ns_server:debug,2025-05-15T18:47:26.000Z,ns_1@db3.lan:ns_config<0.280.0>:ns_config:handle_call:920]Regenerated node UUID: <<"80960cc3730024bccd4fc49444efdc14">> 

[ns_server:debug,2025-05-15T18:47:26.001Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',uuid} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554045}}]}|
 <<"80960cc3730024bccd4fc49444efdc14">>]
[ns_server:debug,2025-05-15T18:47:26.001Z,ns_1@db3.lan:chronicle_local<0.239.0>:chronicle_local:handle_call:77]Wiping chronicle before prepare join.
[ns_server:debug,2025-05-15T18:47:26.001Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',{project_intact,is_vulnerable}} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|false]
[chronicle:info,2025-05-15T18:47:26.001Z,ns_1@db3.lan:chronicle_agent<0.252.0>:chronicle_agent:handle_wipe:1261]Wipe requested.
[ns_server:debug,2025-05-15T18:47:26.001Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',xdcr_rest_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9998]
[ns_server:debug,2025-05-15T18:47:26.001Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',ssl_rest_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|18091]
[ns_server:debug,2025-05-15T18:47:26.001Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',ssl_query_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|18093]
[chronicle:info,2025-05-15T18:47:26.001Z,ns_1@db3.lan:chronicle_proposer<0.2278.0>:chronicle_proposer:handle_stop:1269]Proposer for term {4,'ns_1@db3.lan'} in history <<"ea19d13c61d8435b9ad1a5f6af1aa5eb">> is terminating.
[ns_server:debug,2025-05-15T18:47:26.001Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',ssl_capi_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|18092]
[chronicle:debug,2025-05-15T18:47:26.001Z,ns_1@db3.lan:chronicle_agent<0.252.0>:chronicle_agent:handle_local_mark_committed:1997]Marked seqno 14 committed
[ns_server:debug,2025-05-15T18:47:26.001Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',saslauthd_enabled} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|true]
[ns_server:debug,2025-05-15T18:47:26.001Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',rest} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
 {port,8091},
 {port_meta,global}]
[chronicle:info,2025-05-15T18:47:26.001Z,ns_1@db3.lan:chronicle_server<0.260.0>:chronicle_proposer:stop:136]Proposer <0.2278.0> stopped: {shutdown,stop}
[ns_server:debug,2025-05-15T18:47:26.001Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',query_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|8093]
[ns_server:debug,2025-05-15T18:47:26.001Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',prometheus_http_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9123]
[ns_server:debug,2025-05-15T18:47:26.001Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',projector_ssl_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9999]
[ns_server:debug,2025-05-15T18:47:26.002Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',projector_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9999]
[ns_server:debug,2025-05-15T18:47:26.002Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',port_servers} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}]
[chronicle:info,2025-05-15T18:47:26.002Z,ns_1@db3.lan:chronicle_agent<0.252.0>:chronicle_agent:handle_prepare_wipe_done:1295]All secondary processes have terminated.
[chronicle:info,2025-05-15T18:47:26.002Z,ns_1@db3.lan:chronicle_agent<0.252.0>:chronicle_agent:handle_prepare_wipe_done:1305]Wiping
[chronicle:debug,2025-05-15T18:47:26.002Z,ns_1@db3.lan:chronicle_storage_writer<0.253.0>:chronicle_storage:writer_loop:1489]Got exit from <0.252.0> with reason shutdown
[ns_server:debug,2025-05-15T18:47:26.002Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',ns_log} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
 {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]
[ns_server:debug,2025-05-15T18:47:26.002Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',memcached_prometheus} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|11280]
[ns_server:debug,2025-05-15T18:47:26.002Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',memcached_defaults} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
 {max_connections,65000},
 {system_connections,5000},
 {connection_idle_time,0},
 {verbosity,0},
 {privilege_debug,false},
 {breakpad_enabled,true},
 {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
 {dedupe_nmvb_maps,false},
 {je_malloc_conf,undefined},
 {tracing_enabled,true},
 {datatype_snappy,true},
 {num_reader_threads,<<"default">>},
 {num_writer_threads,<<"default">>},
 {num_auxio_threads,<<"default">>},
 {num_nonio_threads,<<"default">>},
 {num_storage_threads,<<"default">>},
 {tcp_keepalive_idle,360},
 {tcp_keepalive_interval,10},
 {tcp_keepalive_probes,3},
 {tcp_user_timeout,30},
 {always_collect_trace_info,true},
 {connection_limit_mode,<<"disconnect">>},
 {free_connection_pool_size,0},
 {max_client_connection_details,0}]
[ns_server:debug,2025-05-15T18:47:26.002Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',memcached_dedicated_ssl_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|11206]
[ns_server:debug,2025-05-15T18:47:26.002Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',memcached_config} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
 {[{interfaces,{memcached_config_mgr,get_interfaces,[]}},
   {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
   {connection_idle_time,connection_idle_time},
   {privilege_debug,privilege_debug},
   {breakpad,
    {[{enabled,breakpad_enabled},
      {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
   {deployment_model,{memcached_config_mgr,get_config_profile,[]}},
   {verbosity,verbosity},
   {audit_file,{"~s",[audit_file]}},
   {rbac_file,{"~s",[rbac_file]}},
   {dedupe_nmvb_maps,dedupe_nmvb_maps},
   {tracing_enabled,tracing_enabled},
   {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
   {xattr_enabled,true},
   {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
   {collections_enabled,true},
   {max_connections,max_connections},
   {system_connections,system_connections},
   {num_reader_threads,num_reader_threads},
   {num_writer_threads,num_writer_threads},
   {num_auxio_threads,num_auxio_threads},
   {num_nonio_threads,num_nonio_threads},
   {num_storage_threads,num_storage_threads},
   {logger,
    {[{filename,{"~s/~s",[log_path,log_prefix]}},{cyclesize,log_cyclesize}]}},
   {external_auth_service,{memcached_config_mgr,get_external_auth_service,[]}},
   {active_external_users_push_interval,
    {memcached_config_mgr,get_external_users_push_interval,[]}},
   {prometheus,{memcached_config_mgr,prometheus_cfg,[]}},
   {sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
   {ssl_sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
   {tcp_keepalive_idle,tcp_keepalive_idle},
   {tcp_keepalive_interval,tcp_keepalive_interval},
   {tcp_keepalive_probes,tcp_keepalive_probes},
   {tcp_user_timeout,tcp_user_timeout},
   {always_collect_trace_info,always_collect_trace_info},
   {connection_limit_mode,connection_limit_mode},
   {free_connection_pool_size,free_connection_pool_size},
   {max_client_connection_details,max_client_connection_details}]}]
[ns_server:debug,2025-05-15T18:47:26.003Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',memcached} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
 {port,11210},
 {dedicated_port,11209},
 {dedicated_ssl_port,11206},
 {ssl_port,11207},
 {admin_user,"@ns_server"},
 {other_users,["@cbq-engine","@projector","@goxdcr","@index","@fts",
               "@eventing","@cbas","@backup"]},
 {admin_pass,"*****"},
 {engines,[{membase,[{engine,"/opt/couchbase/lib/memcached/ep.so"},
                     {static_config_string,"failpartialwarmup=false"}]},
           {memcached,[{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
                       {static_config_string,"vb0=true"}]}]},
 {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
 {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
 {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
 {log_prefix,"memcached.log"},
 {log_generations,20},
 {log_cyclesize,10485760},
 {log_rotation_period,39003}]
[ns_server:debug,2025-05-15T18:47:26.003Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',isasl} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
 {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]
[ns_server:debug,2025-05-15T18:47:26.003Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',is_enterprise} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|true]
[ns_server:debug,2025-05-15T18:47:26.003Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',indexer_stmaint_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9105]
[ns_server:debug,2025-05-15T18:47:26.003Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',indexer_stinit_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9103]
[ns_server:debug,2025-05-15T18:47:26.003Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',indexer_stcatchup_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9104]
[ns_server:debug,2025-05-15T18:47:26.003Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',indexer_scan_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9101]
[ns_server:debug,2025-05-15T18:47:26.003Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',indexer_https_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|19102]
[ns_server:debug,2025-05-15T18:47:26.003Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',indexer_http_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9102]
[ns_server:debug,2025-05-15T18:47:26.003Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',indexer_admin_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9100]
[ns_server:debug,2025-05-15T18:47:26.003Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',index_dir} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2025-05-15T18:47:26.004Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',fts_ssl_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|18094]
[ns_server:debug,2025-05-15T18:47:26.004Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',fts_http_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|8094]
[ns_server:debug,2025-05-15T18:47:26.004Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',fts_grpc_ssl_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|19130]
[ns_server:debug,2025-05-15T18:47:26.004Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',fts_grpc_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9130]
[ns_server:debug,2025-05-15T18:47:26.004Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',eventing_https_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|18096]
[ns_server:debug,2025-05-15T18:47:26.004Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',eventing_http_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|8096]
[ns_server:debug,2025-05-15T18:47:26.004Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',eventing_debug_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9140]
[ns_server:debug,2025-05-15T18:47:26.004Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',event_log} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
 {filename,"/opt/couchbase/var/lib/couchbase/event_log"}]
[ns_server:debug,2025-05-15T18:47:26.004Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',database_dir} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2025-05-15T18:47:26.004Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',config_version} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|{7,6}]
[ns_server:debug,2025-05-15T18:47:26.004Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',compaction_daemon} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
 {check_interval,30},
 {min_db_file_size,131072},
 {min_view_file_size,20971520}]
[ns_server:debug,2025-05-15T18:47:26.004Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_ssl_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|18095]
[ns_server:debug,2025-05-15T18:47:26.004Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_result_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9117]
[ns_server:debug,2025-05-15T18:47:26.004Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_replication_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9120]
[ns_server:debug,2025-05-15T18:47:26.004Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_parent_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9122]
[ns_server:debug,2025-05-15T18:47:26.004Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_metadata_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9121]
[ns_server:debug,2025-05-15T18:47:26.004Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_metadata_callback_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9119]
[ns_server:debug,2025-05-15T18:47:26.004Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_messaging_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9118]
[ns_server:debug,2025-05-15T18:47:26.004Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_http_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|8095]
[ns_server:debug,2025-05-15T18:47:26.004Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_debug_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|-1]
[ns_server:debug,2025-05-15T18:47:26.005Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_data_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9116]
[ns_server:debug,2025-05-15T18:47:26.005Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_console_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9114]
[ns_server:debug,2025-05-15T18:47:26.005Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_cluster_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9115]
[ns_server:debug,2025-05-15T18:47:26.005Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_cc_http_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9111]
[ns_server:debug,2025-05-15T18:47:26.005Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_cc_cluster_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9112]
[ns_server:debug,2025-05-15T18:47:26.005Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_cc_client_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9113]
[ns_server:debug,2025-05-15T18:47:26.005Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_admin_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9110]
[ns_server:debug,2025-05-15T18:47:26.005Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',capi_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|8092]
[ns_server:debug,2025-05-15T18:47:26.005Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',backup_https_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|18097]
[ns_server:debug,2025-05-15T18:47:26.005Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',backup_http_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|8097]
[ns_server:debug,2025-05-15T18:47:26.005Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',backup_grpc_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9124]
[ns_server:debug,2025-05-15T18:47:26.005Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',audit} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}]
[ns_server:debug,2025-05-15T18:47:26.005Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
nodes_wanted ->
['ns_1@db3.lan','ns_1@172.19.0.4']
[ns_server:debug,2025-05-15T18:47:26.005Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',address_family} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|inet]
[ns_server:debug,2025-05-15T18:47:26.005Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',node_encryption} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|false]
[ns_server:debug,2025-05-15T18:47:26.005Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',erl_external_listeners} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
 {inet,false}]
[ns_server:debug,2025-05-15T18:47:26.005Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',n2n_client_cert_auth} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|false]
[ns_server:debug,2025-05-15T18:47:26.005Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_dirs} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
 "/opt/couchbase/var/lib/couchbase/data"]
[ns_server:debug,2025-05-15T18:47:26.005Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',eventing_dir} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2025-05-15T18:47:26.006Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',prometheus_auth_info} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
 {"@prometheus",
  {auth,
   [{<<"hash">>,
     {[{<<"hashes">>,
        {sanitized,<<"Vmqy8YpGQ1ZVVOJNXOVKZaSx7qimZeVP/vMQ9t99FKU=">>}},
       {<<"algorithm">>,<<"argon2id">>},
       {<<"salt">>,<<"RaDchxQfbWx/9yCvS4KJhw==">>},
       {<<"parallelism">>,1},
       {<<"time">>,3},
       {<<"memory">>,524288}]}},
    {<<"scram-sha-512">>,
     {[{<<"salt">>,
        <<"WL9ZWEHJ76WYVLC0m22/0VNF6T50k/J0Z7Okpz3sW4PgL9CASwlonzChieVERPB3Sib20fCxP6nMQ/SgjVQjEg==">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"vX4Kcf9uKEqkQwiL3H4KjNM4iq2tHqkJD/vMtc9V/Xg=">>}}]}},
    {<<"scram-sha-256">>,
     {[{<<"salt">>,<<"YkWxrYn6kotQkszjlA7ouXNPvh8HqG960AycvYrvGOo=">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"gECRKBSDc8Gg85IUVUx8OK6b2QieozWWeKpDySi+GnU=">>}}]}},
    {<<"scram-sha-1">>,
     {[{<<"salt">>,<<"uv9yTXGPC84w9yF7otsSjKjc9bM=">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"SOMxs6h/s5Xzi9/iuNI3GZdbix+jQu+XMiQI1CxtTOI=">>}}]}}]}}]
[ns_server:debug,2025-05-15T18:47:26.006Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',node_cert} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
 {subject,<<"CN=Couchbase Server Node (db3.lan)">>},
 {not_after,63985747645},
 {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
 {load_timestamp,63914554045},
 {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQ"...>>},
 {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDOjCCAiKgAwIBAgIIGD/HyL2BZrYwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjVaFw0y\nNzA4MTcxODQ3MjVaMCoxKDAmBgNVBAMTH0NvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nZGIzLmxhbikwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCoBie+jXEG\nM9mqURIO0hHIXr1th5jf7TJD7f69DGF3znRjw1VTjnaMZAQOk8Gg44nwI1o1KtJj\nUmflnTG"...>>},
 {pkey_passphrase_settings,[]},
 {certs_epoch,0},
 {type,generated},
 {hostname,"db3.lan"}]
[ns_server:debug,2025-05-15T18:47:26.006Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',client_cert} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
 {subject,<<"CN=Couchbase Internal Client (32660726)">>},
 {not_after,63985747645},
 {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
 {load_timestamp,63914554045},
 {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQ"...>>},
 {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDWTCCAkGgAwIBAgIIGD/HyMO1PBAwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjVaFw0y\nNzA4MTcxODQ3MjVaMC8xLTArBgNVBAMTJENvdWNoYmFzZSBJbnRlcm5hbCBDbGll\nbnQgKDMyNjYwNzI2KTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAKHe\n9VAt+X6MU19Fg7ok9H0Zxkmq5mDd0acYZWMO8WqPC7jBY2dopAKDJoF/AKgk8TLT\ncz5AYGM"...>>},
 {pkey_passphrase_settings,[]},
 {certs_epoch,0},
 {type,generated},
 {name,"@internal"}]
[ns_server:debug,2025-05-15T18:47:26.006Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
otp ->
[{cookie,{sanitized,<<"biSYTaQUFDVndTs/b+IcDmD2L0woktv9naRdv3GCK6A=">>}}]
[ns_server:debug,2025-05-15T18:47:26.006Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',uuid} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
 <<"80960cc3730024bccd4fc49444efdc14">>]
[ns_server:debug,2025-05-15T18:47:26.006Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
otp ->
[{cookie,{sanitized,<<"biSYTaQUFDVndTs/b+IcDmD2L0woktv9naRdv3GCK6A=">>}}]
[ns_server:debug,2025-05-15T18:47:26.006Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
nodes_wanted ->
['ns_1@db3.lan','ns_1@172.19.0.4']
[ns_server:debug,2025-05-15T18:47:26.006Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
cluster_compat_mode ->
undefined
[ns_server:debug,2025-05-15T18:47:26.006Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',membership} ->
inactiveAdded
[chronicle:info,2025-05-15T18:47:26.010Z,ns_1@db3.lan:chronicle_agent<0.252.0>:chronicle_agent:handle_prepare_wipe_done:1308]Wiped successfully
[ns_server:debug,2025-05-15T18:47:26.010Z,ns_1@db3.lan:chronicle_local<0.239.0>:chronicle_local:handle_call:84]Prepare join. Info: #{committed_seqno => 24,compat_version => 0,
                      config =>
                          {log_entry,<<"5aab03dbecad06b50ffb474da59a00c9">>,
                              {4,'ns_1@172.19.0.4'},
                              24,
                              {config,
                                  {<<"1dd5991463d519e23729dda2c80c376b">>,0,3},
                                  0,<<"45724915daa2a489c0a6816d20198585">>,
                                  #{'ns_1@172.19.0.4' =>
                                        #{id =>
                                              <<"1dd5991463d519e23729dda2c80c376b">>,
                                          role => voter},
                                    'ns_1@db2.lan' =>
                                        #{id =>
                                              <<"9f6894b51ffa497cdfc8b792d9e6726b">>,
                                          role => replica},
                                    'ns_1@db3.lan' =>
                                        #{id =>
                                              <<"659ec03c812f5cc9456bd71fe3c1a047">>,
                                          role => replica}},
                                  undefined,
                                  #{chronicle_config_rsm =>
                                        {rsm_config,chronicle_config_rsm,[]},
                                    kv => {rsm_config,chronicle_kv,[]}},
                                  #{},undefined,
                                  [{<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                    0}]}},
                      history_id => <<"5aab03dbecad06b50ffb474da59a00c9">>}
[chronicle:info,2025-05-15T18:47:26.014Z,ns_1@db3.lan:chronicle_agent<0.252.0>:chronicle_storage:open_logs:180]Reading log files [{0,
                    "/opt/couchbase/var/lib/couchbase/config/chronicle/logs/0.log"}]
[chronicle:info,2025-05-15T18:47:26.014Z,ns_1@db3.lan:chronicle_agent<0.252.0>:chronicle_storage:open_current_log:232]Error while opening log file "/opt/couchbase/var/lib/couchbase/config/chronicle/logs/0.log": no_header
[chronicle:info,2025-05-15T18:47:26.014Z,ns_1@db3.lan:chronicle_agent<0.252.0>:chronicle_storage:create_log:280]Creating log file /opt/couchbase/var/lib/couchbase/config/chronicle/logs/0.log (high seqno = 0)
[chronicle:info,2025-05-15T18:47:26.015Z,ns_1@db3.lan:chronicle_agent<0.252.0>:chronicle_agent:maybe_seed_storage:2444]Found empty storage. Seeding it with default metadata:
#{committed_seqno => 0,history_id => <<"no-history">>,peer => nonode@nohost,
  peer_id => <<>>,pending_branch => undefined,state => not_provisioned,
  term => {0,nonode@nohost}}
[ns_server:debug,2025-05-15T18:47:26.016Z,ns_1@db3.lan:ns_config<0.280.0>:ns_config:handle_call:895]Reload config
[ns_server:debug,2025-05-15T18:47:26.016Z,ns_1@db3.lan:ns_config<0.280.0>:ns_config:wait_saver:844]Done waiting for saver.
[error_logger:info,2025-05-15T18:47:26.016Z,ns_1@db3.lan:<0.255.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.255.0>,dynamic_supervisor}
    started: [{pid,<0.2420.0>},
              {id,chronicle_leader},
              {mfargs,{chronicle_leader,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:47:26.019Z,ns_1@db3.lan:ns_config<0.280.0>:ns_config:load_config:1113]Loading static config from "/opt/couchbase/etc/couchbase/config"
[ns_server:info,2025-05-15T18:47:26.020Z,ns_1@db3.lan:ns_config<0.280.0>:ns_config:load_config:1127]Loading dynamic config from "/opt/couchbase/var/lib/couchbase/config/config.dat"
[ns_server:debug,2025-05-15T18:47:26.022Z,ns_1@db3.lan:ns_config<0.280.0>:ns_config:load_config:1135]Here's full dynamic config we loaded:
[[{{node,'ns_1@db3.lan',membership},inactiveAdded},
  {cluster_compat_mode,undefined},
  {nodes_wanted,['ns_1@db3.lan','ns_1@172.19.0.4']},
  {otp,
   [{cookie,{sanitized,<<"biSYTaQUFDVndTs/b+IcDmD2L0woktv9naRdv3GCK6A=">>}}]},
  {{node,'ns_1@db3.lan',uuid},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    <<"80960cc3730024bccd4fc49444efdc14">>]},
  {{node,'ns_1@db3.lan',client_cert},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
    {subject,<<"CN=Couchbase Internal Client (32660726)">>},
    {not_after,63985747645},
    {verified_with,
     <<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
    {load_timestamp,63914554045},
    {ca,
     <<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQCuy0dFqLPrJ6xY21RJ5ttVPR78ssxxXSe9JdTj/IsUczkwyxfZfwK\npBszlGYuHO0Utnwq1K4ruMPYFYfpLacJSfsb3KWEmZwAoNuRpjvE24AsH3tvj7cB\nB5v49HJ0axvNAFm75GWDyUr7T7UwcTJaFIFtfy+J0Umt2TjFNY0DPpt1WEzkKFKx\nOdBs+7cNc35ZG4rwVu/EJ14OXhQMXkpbUpYJxS8jzkzSI+1Eb67kH4j5QhnR9H+w\nw2S9AgMBAAGjQjBAMA4GA1UdDwEB/wQEAwIBhjAPBgNVHRMBAf8EBTADAQH/MB0G\nA1UdDgQWBBS8ASq2Mkh0UsZe4fIvzs4mCSfTmjANBgkqhkiG9w0BAQsFAAOCAQEA\nB3q1mzdJc3VY17JDCaRPKY/Veni3oZ6zGJ9r9LnWKE7IO9AxBAKYGMELHMf9httQ\nmRZrXiGQi/tYXLlFFk7IcIID6iOOZLLagDhCQJPi52dGhBB4a2EnBvGF9OJI4zW9\nJw04HNR5cF1cxWIrRdGJAX/kbs/M9jz0STlXziVNwiAYeGIxkjq/fwKk6weai8iI\nwQVu6GsbtSBL8f4FRCk2vK6a1MD54BQDasRShdi3k/DJAEtY92muu13M0jc48mCM\ndEVeXxM8rIrG6LavziTwtO+jkFyWjlRyNbSr6il+neSWJ7ZspTo+ZYFy55CAh5ws\nwk6Ljo2wlQgfGflJbKcxAw==\n-----END CERTIFICATE-----\n">>},
    {pem,
     <<"-----BEGIN CERTIFICATE-----\nMIIDWTCCAkGgAwIBAgIIGD/HyMO1PBAwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjVaFw0y\nNzA4MTcxODQ3MjVaMC8xLTArBgNVBAMTJENvdWNoYmFzZSBJbnRlcm5hbCBDbGll\nbnQgKDMyNjYwNzI2KTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAKHe\n9VAt+X6MU19Fg7ok9H0Zxkmq5mDd0acYZWMO8WqPC7jBY2dopAKDJoF/AKgk8TLT\ncz5AYGMiY3CokE1WbBNVZZ8+VMft6H+06YxTTsLvqWttVghc4Q3UKSuDpvQrIbxj\n7PxG12wiWoQUCfnBMjrL2ILgJKAQIMjO7UoWin3LmZo8LTdR4ugoBoS3cfQJhz7R\nT7iuSbe/44TwrcYrDzrvg3UCsFrXLnKs/LJ+nfQyl0fjkS/MP33lCHkdZ6+teEaJ\ncsNTaJuYKYIQIR1ICQnsFCsXrf2D4h0v8ZUsRVoaB3TFebmEqYtNB7V3NnP/sxFf\nqQvn6gwzDmvlb0O5+/MCAwEAAaOBgzCBgDAOBgNVHQ8BAf8EBAMCBaAwEwYDVR0l\nBAwwCgYIKwYBBQUHAwIwDAYDVR0TAQH/BAIwADAfBgNVHSMEGDAWgBS8ASq2Mkh0\nUsZe4fIvzs4mCSfTmjAqBgNVHREEIzAhgR9pbnRlcm5hbEBpbnRlcm5hbC5jb3Vj\naGJhc2UuY29tMA0GCSqGSIb3DQEBCwUAA4IBAQAtkwpYapli88YqNcz52BW8oUhZ\nqHNUis2IieXUTRZSTpTQlvywLUYl+7UvOl9QZjGGowKnD7bdTGgR0qyP1Bab1eIs\nZTbE8+MMa/2zeKjsDQlv7LKoKRI6eBa7nadz2AsyuYJ6h6M8Pj8XI1fXTSdEMZyU\ny32ahUlOq2qNPC485fFLJs/6ggN+3NCZT4H/ohbv8cZvoSUuHOj/wzbuNMZg4d18\n57CxIpHFCtyqUkXfQabWuPSsJRadCqs+Z59XLqEmyVdTeX8HcPON7IXtI7lNDdWB\nS0M6ljMG3ONZVVzxAowbDDbacQ1BK+kY8qlk0KzgCydCP3z86KPAsXEFSqTr\n-----END CERTIFICATE-----\n">>},
    {pkey_passphrase_settings,[]},
    {certs_epoch,0},
    {type,generated},
    {name,"@internal"}]},
  {{node,'ns_1@db3.lan',node_cert},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
    {subject,<<"CN=Couchbase Server Node (db3.lan)">>},
    {not_after,63985747645},
    {verified_with,
     <<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
    {load_timestamp,63914554045},
    {ca,
     <<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQCuy0dFqLPrJ6xY21RJ5ttVPR78ssxxXSe9JdTj/IsUczkwyxfZfwK\npBszlGYuHO0Utnwq1K4ruMPYFYfpLacJSfsb3KWEmZwAoNuRpjvE24AsH3tvj7cB\nB5v49HJ0axvNAFm75GWDyUr7T7UwcTJaFIFtfy+J0Umt2TjFNY0DPpt1WEzkKFKx\nOdBs+7cNc35ZG4rwVu/EJ14OXhQMXkpbUpYJxS8jzkzSI+1Eb67kH4j5QhnR9H+w\nw2S9AgMBAAGjQjBAMA4GA1UdDwEB/wQEAwIBhjAPBgNVHRMBAf8EBTADAQH/MB0G\nA1UdDgQWBBS8ASq2Mkh0UsZe4fIvzs4mCSfTmjANBgkqhkiG9w0BAQsFAAOCAQEA\nB3q1mzdJc3VY17JDCaRPKY/Veni3oZ6zGJ9r9LnWKE7IO9AxBAKYGMELHMf9httQ\nmRZrXiGQi/tYXLlFFk7IcIID6iOOZLLagDhCQJPi52dGhBB4a2EnBvGF9OJI4zW9\nJw04HNR5cF1cxWIrRdGJAX/kbs/M9jz0STlXziVNwiAYeGIxkjq/fwKk6weai8iI\nwQVu6GsbtSBL8f4FRCk2vK6a1MD54BQDasRShdi3k/DJAEtY92muu13M0jc48mCM\ndEVeXxM8rIrG6LavziTwtO+jkFyWjlRyNbSr6il+neSWJ7ZspTo+ZYFy55CAh5ws\nwk6Ljo2wlQgfGflJbKcxAw==\n-----END CERTIFICATE-----\n">>},
    {pem,
     <<"-----BEGIN CERTIFICATE-----\nMIIDOjCCAiKgAwIBAgIIGD/HyL2BZrYwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjVaFw0y\nNzA4MTcxODQ3MjVaMCoxKDAmBgNVBAMTH0NvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nZGIzLmxhbikwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCoBie+jXEG\nM9mqURIO0hHIXr1th5jf7TJD7f69DGF3znRjw1VTjnaMZAQOk8Gg44nwI1o1KtJj\nUmflnTGWWtZU+43sI8tDXLtymAbYoxEkodGUqqPCEPQkCWLubJvTbSZVEyegLo9P\nKw4RPwJGAkbAFq1poCecaE9pf4qeOhibUM7ivJx5nFMcTkUGKqJt6vVH4oA/FcHp\nfOLz51gjD0O/U2Ev8NqFvx6ftWkIU/+LGaHaMjoDj4x8SldegqvGtDmpfhg2+97T\nftALlkUkvvJqd4beilSDxCiurYlIKOki8GWaGFfleyV8HwD/hOIZ/wj5yz607CDj\nuny5Z11yQr7/AgMBAAGjajBoMA4GA1UdDwEB/wQEAwIFoDATBgNVHSUEDDAKBggr\nBgEFBQcDATAMBgNVHRMBAf8EAjAAMB8GA1UdIwQYMBaAFLwBKrYySHRSxl7h8i/O\nziYJJ9OaMBIGA1UdEQQLMAmCB2RiMy5sYW4wDQYJKoZIhvcNAQELBQADggEBAAPS\nNPGKGEFgoq/sasnMadhR33aGK60LnjgknxG0R9lT+/RHZKEoDMbYw4gp7UUrh+JZ\n+cneP93RcOIpvfRP7AUlGviu8HDldR5yFQBRXpDdKyWvIvPyhwZK4v65yIYUcLq4\ngxMZUzynscYxC4eN1l2eyD2Rf4FuZ7G2ehW4zcei1DD35WId1pR+YKjwKNZ8Q5Sk\nyjT9F2mftICgpSdxyZJLdNXaXDGfodNVzuCKnMnYUoRWmddt5UqqLLkPM0+7HEpU\nn5wAUJD2kSCGspqGnAZbcBEbGKbW2hmgXAKxOQVyVfi4b1ZGo/IAScLuUoMZw8hh\n+6kwlsLDy19h3nkTfxo=\n-----END CERTIFICATE-----\n">>},
    {pkey_passphrase_settings,[]},
    {certs_epoch,0},
    {type,generated},
    {hostname,"db3.lan"}]},
  {{node,'ns_1@db3.lan',prometheus_auth_info},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    {"@prometheus",
     {auth,
      [{<<"hash">>,
        {[{<<"hashes">>,
           {sanitized,<<"Vmqy8YpGQ1ZVVOJNXOVKZaSx7qimZeVP/vMQ9t99FKU=">>}},
          {<<"algorithm">>,<<"argon2id">>},
          {<<"salt">>,<<"RaDchxQfbWx/9yCvS4KJhw==">>},
          {<<"parallelism">>,1},
          {<<"time">>,3},
          {<<"memory">>,524288}]}},
       {<<"scram-sha-512">>,
        {[{<<"salt">>,
           <<"WL9ZWEHJ76WYVLC0m22/0VNF6T50k/J0Z7Okpz3sW4PgL9CASwlonzChieVERPB3Sib20fCxP6nMQ/SgjVQjEg==">>},
          {<<"iterations">>,15000},
          {<<"hashes">>,
           {sanitized,<<"vX4Kcf9uKEqkQwiL3H4KjNM4iq2tHqkJD/vMtc9V/Xg=">>}}]}},
       {<<"scram-sha-256">>,
        {[{<<"salt">>,<<"YkWxrYn6kotQkszjlA7ouXNPvh8HqG960AycvYrvGOo=">>},
          {<<"iterations">>,15000},
          {<<"hashes">>,
           {sanitized,<<"gECRKBSDc8Gg85IUVUx8OK6b2QieozWWeKpDySi+GnU=">>}}]}},
       {<<"scram-sha-1">>,
        {[{<<"salt">>,<<"uv9yTXGPC84w9yF7otsSjKjc9bM=">>},
          {<<"iterations">>,15000},
          {<<"hashes">>,
           {sanitized,
            <<"SOMxs6h/s5Xzi9/iuNI3GZdbix+jQu+XMiQI1CxtTOI=">>}}]}}]}}]},
  {{node,'ns_1@db3.lan',eventing_dir},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
    47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,
    105,98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
  {{node,'ns_1@db3.lan',cbas_dirs},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
    "/opt/couchbase/var/lib/couchbase/data"]},
  {{node,'ns_1@db3.lan',n2n_client_cert_auth},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    false]},
  {{node,'ns_1@db3.lan',erl_external_listeners},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
    {inet,false}]},
  {{node,'ns_1@db3.lan',node_encryption},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    false]},
  {{node,'ns_1@db3.lan',address_family},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    inet]},
  {{node,'ns_1@db3.lan',audit},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}]},
  {{node,'ns_1@db3.lan',backup_grpc_port},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    9124]},
  {{node,'ns_1@db3.lan',backup_http_port},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    8097]},
  {{node,'ns_1@db3.lan',backup_https_port},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    18097]},
  {{node,'ns_1@db3.lan',capi_port},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    8092]},
  {{node,'ns_1@db3.lan',cbas_admin_port},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    9110]},
  {{node,'ns_1@db3.lan',cbas_cc_client_port},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    9113]},
  {{node,'ns_1@db3.lan',cbas_cc_cluster_port},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    9112]},
  {{node,'ns_1@db3.lan',cbas_cc_http_port},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    9111]},
  {{node,'ns_1@db3.lan',cbas_cluster_port},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    9115]},
  {{node,'ns_1@db3.lan',cbas_console_port},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    9114]},
  {{node,'ns_1@db3.lan',cbas_data_port},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    9116]},
  {{node,'ns_1@db3.lan',cbas_debug_port},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    -1]},
  {{node,'ns_1@db3.lan',cbas_http_port},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    8095]},
  {{node,'ns_1@db3.lan',cbas_messaging_port},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    9118]},
  {{node,'ns_1@db3.lan',cbas_metadata_callback_port},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    9119]},
  {{node,'ns_1@db3.lan',cbas_metadata_port},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    9121]},
  {{node,'ns_1@db3.lan',cbas_parent_port},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    9122]},
  {{node,'ns_1@db3.lan',cbas_replication_port},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    9120]},
  {{node,'ns_1@db3.lan',cbas_result_port},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    9117]},
  {{node,'ns_1@db3.lan',cbas_ssl_port},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    18095]},
  {{node,'ns_1@db3.lan',compaction_daemon},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
    {check_interval,30},
    {min_db_file_size,131072},
    {min_view_file_size,20971520}]},
  {{node,'ns_1@db3.lan',config_version},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    {7,6}]},
  {{node,'ns_1@db3.lan',database_dir},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
    47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,
    105,98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
  {{node,'ns_1@db3.lan',event_log},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
    {filename,"/opt/couchbase/var/lib/couchbase/event_log"}]},
  {{node,'ns_1@db3.lan',eventing_debug_port},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    9140]},
  {{node,'ns_1@db3.lan',eventing_http_port},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    8096]},
  {{node,'ns_1@db3.lan',eventing_https_port},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    18096]},
  {{node,'ns_1@db3.lan',fts_grpc_port},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    9130]},
  {{node,'ns_1@db3.lan',fts_grpc_ssl_port},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    19130]},
  {{node,'ns_1@db3.lan',fts_http_port},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    8094]},
  {{node,'ns_1@db3.lan',fts_ssl_port},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    18094]},
  {{node,'ns_1@db3.lan',index_dir},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
    47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,
    105,98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
  {{node,'ns_1@db3.lan',indexer_admin_port},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    9100]},
  {{node,'ns_1@db3.lan',indexer_http_port},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    9102]},
  {{node,'ns_1@db3.lan',indexer_https_port},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    19102]},
  {{node,'ns_1@db3.lan',indexer_scan_port},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    9101]},
  {{node,'ns_1@db3.lan',indexer_stcatchup_port},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    9104]},
  {{node,'ns_1@db3.lan',indexer_stinit_port},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    9103]},
  {{node,'ns_1@db3.lan',indexer_stmaint_port},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    9105]},
  {{node,'ns_1@db3.lan',is_enterprise},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    true]},
  {{node,'ns_1@db3.lan',isasl},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
    {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
  {{node,'ns_1@db3.lan',memcached},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
    {port,11210},
    {dedicated_port,11209},
    {dedicated_ssl_port,11206},
    {ssl_port,11207},
    {admin_user,"@ns_server"},
    {other_users,
     ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
      "@cbas","@backup"]},
    {admin_pass,"*****"},
    {engines,
     [{membase,
       [{engine,"/opt/couchbase/lib/memcached/ep.so"},
        {static_config_string,"failpartialwarmup=false"}]},
      {memcached,
       [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
        {static_config_string,"vb0=true"}]}]},
    {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
    {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
    {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
    {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
    {log_prefix,"memcached.log"},
    {log_generations,20},
    {log_cyclesize,10485760},
    {log_rotation_period,39003}]},
  {{node,'ns_1@db3.lan',memcached_config},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    {[{interfaces,{memcached_config_mgr,get_interfaces,[]}},
      {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
      {connection_idle_time,connection_idle_time},
      {privilege_debug,privilege_debug},
      {breakpad,
       {[{enabled,breakpad_enabled},
         {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
      {deployment_model,{memcached_config_mgr,get_config_profile,[]}},
      {verbosity,verbosity},
      {audit_file,{"~s",[audit_file]}},
      {rbac_file,{"~s",[rbac_file]}},
      {dedupe_nmvb_maps,dedupe_nmvb_maps},
      {tracing_enabled,tracing_enabled},
      {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
      {xattr_enabled,true},
      {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
      {collections_enabled,true},
      {max_connections,max_connections},
      {system_connections,system_connections},
      {num_reader_threads,num_reader_threads},
      {num_writer_threads,num_writer_threads},
      {num_auxio_threads,num_auxio_threads},
      {num_nonio_threads,num_nonio_threads},
      {num_storage_threads,num_storage_threads},
      {logger,
       {[{filename,{"~s/~s",[log_path,log_prefix]}},
         {cyclesize,log_cyclesize}]}},
      {external_auth_service,
       {memcached_config_mgr,get_external_auth_service,[]}},
      {active_external_users_push_interval,
       {memcached_config_mgr,get_external_users_push_interval,[]}},
      {prometheus,{memcached_config_mgr,prometheus_cfg,[]}},
      {sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
      {ssl_sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
      {tcp_keepalive_idle,tcp_keepalive_idle},
      {tcp_keepalive_interval,tcp_keepalive_interval},
      {tcp_keepalive_probes,tcp_keepalive_probes},
      {tcp_user_timeout,tcp_user_timeout},
      {always_collect_trace_info,always_collect_trace_info},
      {connection_limit_mode,connection_limit_mode},
      {free_connection_pool_size,free_connection_pool_size},
      {max_client_connection_details,max_client_connection_details}]}]},
  {{node,'ns_1@db3.lan',memcached_dedicated_ssl_port},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    11206]},
  {{node,'ns_1@db3.lan',memcached_defaults},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
    {max_connections,65000},
    {system_connections,5000},
    {connection_idle_time,0},
    {verbosity,0},
    {privilege_debug,false},
    {breakpad_enabled,true},
    {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
    {dedupe_nmvb_maps,false},
    {je_malloc_conf,undefined},
    {tracing_enabled,true},
    {datatype_snappy,true},
    {num_reader_threads,<<"default">>},
    {num_writer_threads,<<"default">>},
    {num_auxio_threads,<<"default">>},
    {num_nonio_threads,<<"default">>},
    {num_storage_threads,<<"default">>},
    {tcp_keepalive_idle,360},
    {tcp_keepalive_interval,10},
    {tcp_keepalive_probes,3},
    {tcp_user_timeout,30},
    {always_collect_trace_info,true},
    {connection_limit_mode,<<"disconnect">>},
    {free_connection_pool_size,0},
    {max_client_connection_details,0}]},
  {{node,'ns_1@db3.lan',memcached_prometheus},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    11280]},
  {{node,'ns_1@db3.lan',ns_log},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
    {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
  {{node,'ns_1@db3.lan',port_servers},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}]},
  {{node,'ns_1@db3.lan',projector_port},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    9999]},
  {{node,'ns_1@db3.lan',projector_ssl_port},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    9999]},
  {{node,'ns_1@db3.lan',prometheus_http_port},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    9123]},
  {{node,'ns_1@db3.lan',query_port},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    8093]},
  {{node,'ns_1@db3.lan',rest},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
    {port,8091},
    {port_meta,global}]},
  {{node,'ns_1@db3.lan',saslauthd_enabled},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    true]},
  {{node,'ns_1@db3.lan',ssl_capi_port},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    18092]},
  {{node,'ns_1@db3.lan',ssl_query_port},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    18093]},
  {{node,'ns_1@db3.lan',ssl_rest_port},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    18091]},
  {{node,'ns_1@db3.lan',xdcr_rest_port},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    9998]},
  {{node,'ns_1@db3.lan',{project_intact,is_vulnerable}},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
    false]},
  {{local_changes_count,<<"80960cc3730024bccd4fc49444efdc14">>},
   [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{5,63914554046}}]}]}]]
[ns_server:info,2025-05-15T18:47:26.027Z,ns_1@db3.lan:ns_config<0.280.0>:ns_config:load_config:1157]Here's full dynamic config we loaded + static & default config:
[{resource_management,
  [{bucket,
    [{resident_ratio,
      [{enabled,false},{couchstore_minimum,1},{magma_minimum,0.2}]},
     {data_size,[{enabled,false},{couchstore_maximum,2},{magma_maximum,16}]}]},
   {index,[]},
   {cores_per_bucket,[{enabled,false},{minimum,0.4}]},
   {disk_usage,[{enabled,false},{maximum,96}]},
   {collections_per_quota,[{enabled,false},{maximum,1}]}]},
 {auto_failover_cfg,
  [{enabled,true},
   {timeout,120},
   {count,0},
   {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
   {failover_server_group,false},
   {max_count,1},
   {failed_over_server_groups,[]},
   {can_abort_rebalance,true}]},
 {retry_rebalance,[{enabled,false},{after_time_period,300},{max_attempts,1}]},
 {rest,[{port,8091}]},
 {password_policy,[{min_length,6},{must_present,[]}]},
 {health_monitor_refresh_interval,[]},
 {service_orchestrator_weight,
  [{kv,10000},
   {index,1000},
   {fts,1000},
   {cbas,1000},
   {n1ql,100},
   {eventing,100},
   {backup,10}]},
 {log_redaction_default_cfg,[{redact_level,none}]},
 {replication,[{enabled,true}]},
 {alert_limits,
  [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
 {email_alerts,
  [{recipients,["root@localhost"]},
   {sender,"couchbase@localhost"},
   {enabled,false},
   {email_server,
    [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
   {alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     indexer_low_resident_percentage,ep_clock_cas_drift_threshold_exceeded,
     communication_issue,time_out_of_sync,disk_usage_analyzer_stuck,
     cert_expires_soon,cert_expired,memory_threshold,history_size_warning,
     stuck_rebalance,memcached_connections]},
   {pop_up_alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     indexer_low_resident_percentage,ep_clock_cas_drift_threshold_exceeded,
     communication_issue,time_out_of_sync,disk_usage_analyzer_stuck,
     cert_expires_soon,cert_expired,memory_threshold,history_size_warning,
     stuck_rebalance,memcached_connections]}]},
 {secure_headers,[]},
 {buckets,[{configs,[]}]},
 {cbas_memory_quota,1549},
 {fts_memory_quota,512},
 {memory_quota,3406},
 {memcached,[]},
 {audit,
  [{auditd_enabled,false},
   {rotate_interval,86400},
   {rotate_size,20971520},
   {prune_age,0},
   {disabled,[]},
   {enabled,[]},
   {disabled_users,[]},
   {sync,[]},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
 {remote_clusters,[]},
 {scramsha_fallback_salt,<<213,116,154,49,54,134,239,240,55,23,111,63>>},
 {client_cert_auth,[{state,"disable"},{prefixes,[]}]},
 {rest_creds,null},
 {{metakv,<<"/analytics/settings/config">>},
  <<"{\"analytics.settings.num_replicas\":0}">>},
 {{metakv,<<"/query/settings/config">>},
  <<"{\"cleanupclientattempts\":true,\"cleanuplostattempts\":true,\"cleanupwindow\":\"60s\",\"completed-limit\":4000,\"completed-threshold\":1000,\"loglevel\":\"info\",\"max-parallelism\":1,\"memory-quota\":0,\"n1ql-feat-ctrl\":76,\"numatrs\":1024,\"pipeline-batch\":16,\"pipeline-cap\":512,\"prepared-limit\":16384,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"query.settings.tmp_space_size\":5120,\"scan-cap\":512,\"timeout\":0,\"txtimeout\":\"0ms\",\"use-cbo\":true}">>},
 {{metakv,<<"/eventing/settings/config">>},<<"{\"ram_quota\":256}">>},
 {{metakv,<<"/indexing/settings/config">>},
  <<"{\"indexer.settings.compaction.abort_exceed_interval\":false,\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.enable_page_bloom_filter\":false,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.log_level\":\"info\",\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.num_replica\":0,\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.storage_mode\":\"\"}">>},
 {{couchdb,max_parallel_replica_indexers},2},
 {{couchdb,max_parallel_indexers},4},
 {quorum_nodes,['ns_1@db3.lan']},
 {set_view_update_daemon,
  [{update_interval,5000},
   {update_min_changes,5000},
   {replica_update_min_changes,5000}]},
 {max_bucket_count,30},
 {index_aware_rebalance_disabled,false},
 {{local_changes_count,<<"80960cc3730024bccd4fc49444efdc14">>},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{5,63914554046}}]}]},
 {{node,'ns_1@db3.lan',{project_intact,is_vulnerable}},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   false]},
 {{node,'ns_1@db3.lan',xdcr_rest_port},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   9998]},
 {{node,'ns_1@db3.lan',ssl_rest_port},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   18091]},
 {{node,'ns_1@db3.lan',ssl_query_port},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   18093]},
 {{node,'ns_1@db3.lan',ssl_capi_port},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   18092]},
 {{node,'ns_1@db3.lan',saslauthd_enabled},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   true]},
 {{node,'ns_1@db3.lan',rest},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
   {port,8091},
   {port_meta,global}]},
 {{node,'ns_1@db3.lan',query_port},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   8093]},
 {{node,'ns_1@db3.lan',prometheus_http_port},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   9123]},
 {{node,'ns_1@db3.lan',projector_ssl_port},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   9999]},
 {{node,'ns_1@db3.lan',projector_port},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   9999]},
 {{node,'ns_1@db3.lan',port_servers},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}]},
 {{node,'ns_1@db3.lan',ns_log},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
 {{node,'ns_1@db3.lan',memcached_prometheus},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   11280]},
 {{node,'ns_1@db3.lan',memcached_defaults},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
   {max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {je_malloc_conf,undefined},
   {tracing_enabled,true},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>},
   {num_auxio_threads,<<"default">>},
   {num_nonio_threads,<<"default">>},
   {num_storage_threads,<<"default">>},
   {tcp_keepalive_idle,360},
   {tcp_keepalive_interval,10},
   {tcp_keepalive_probes,3},
   {tcp_user_timeout,30},
   {always_collect_trace_info,true},
   {connection_limit_mode,<<"disconnect">>},
   {free_connection_pool_size,0},
   {max_client_connection_details,0}]},
 {{node,'ns_1@db3.lan',memcached_dedicated_ssl_port},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   11206]},
 {{node,'ns_1@db3.lan',memcached_config},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   {[{interfaces,{memcached_config_mgr,get_interfaces,[]}},
     {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
     {connection_idle_time,connection_idle_time},
     {privilege_debug,privilege_debug},
     {breakpad,
      {[{enabled,breakpad_enabled},
        {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
     {deployment_model,{memcached_config_mgr,get_config_profile,[]}},
     {verbosity,verbosity},
     {audit_file,{"~s",[audit_file]}},
     {rbac_file,{"~s",[rbac_file]}},
     {dedupe_nmvb_maps,dedupe_nmvb_maps},
     {tracing_enabled,tracing_enabled},
     {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
     {xattr_enabled,true},
     {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
     {collections_enabled,true},
     {max_connections,max_connections},
     {system_connections,system_connections},
     {num_reader_threads,num_reader_threads},
     {num_writer_threads,num_writer_threads},
     {num_auxio_threads,num_auxio_threads},
     {num_nonio_threads,num_nonio_threads},
     {num_storage_threads,num_storage_threads},
     {logger,
      {[{filename,{"~s/~s",[log_path,log_prefix]}},
        {cyclesize,log_cyclesize}]}},
     {external_auth_service,
      {memcached_config_mgr,get_external_auth_service,[]}},
     {active_external_users_push_interval,
      {memcached_config_mgr,get_external_users_push_interval,[]}},
     {prometheus,{memcached_config_mgr,prometheus_cfg,[]}},
     {sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
     {ssl_sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
     {tcp_keepalive_idle,tcp_keepalive_idle},
     {tcp_keepalive_interval,tcp_keepalive_interval},
     {tcp_keepalive_probes,tcp_keepalive_probes},
     {tcp_user_timeout,tcp_user_timeout},
     {always_collect_trace_info,always_collect_trace_info},
     {connection_limit_mode,connection_limit_mode},
     {free_connection_pool_size,free_connection_pool_size},
     {max_client_connection_details,max_client_connection_details}]}]},
 {{node,'ns_1@db3.lan',memcached},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
   {port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,11206},
   {ssl_port,11207},
   {admin_user,"@ns_server"},
   {other_users,
    ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing","@cbas",
     "@backup"]},
   {admin_pass,"*****"},
   {engines,
    [{membase,
      [{engine,"/opt/couchbase/lib/memcached/ep.so"},
       {static_config_string,"failpartialwarmup=false"}]},
     {memcached,
      [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
       {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_rotation_period,39003}]},
 {{node,'ns_1@db3.lan',isasl},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
   {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
 {{node,'ns_1@db3.lan',is_enterprise},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   true]},
 {{node,'ns_1@db3.lan',indexer_stmaint_port},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   9105]},
 {{node,'ns_1@db3.lan',indexer_stinit_port},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   9103]},
 {{node,'ns_1@db3.lan',indexer_stcatchup_port},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   9104]},
 {{node,'ns_1@db3.lan',indexer_scan_port},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   9101]},
 {{node,'ns_1@db3.lan',indexer_https_port},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   19102]},
 {{node,'ns_1@db3.lan',indexer_http_port},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   9102]},
 {{node,'ns_1@db3.lan',indexer_admin_port},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   9100]},
 {{node,'ns_1@db3.lan',index_dir},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
   47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
   98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
 {{node,'ns_1@db3.lan',fts_ssl_port},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   18094]},
 {{node,'ns_1@db3.lan',fts_http_port},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   8094]},
 {{node,'ns_1@db3.lan',fts_grpc_ssl_port},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   19130]},
 {{node,'ns_1@db3.lan',fts_grpc_port},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   9130]},
 {{node,'ns_1@db3.lan',eventing_https_port},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   18096]},
 {{node,'ns_1@db3.lan',eventing_http_port},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   8096]},
 {{node,'ns_1@db3.lan',eventing_debug_port},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   9140]},
 {{node,'ns_1@db3.lan',event_log},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/event_log"}]},
 {{node,'ns_1@db3.lan',database_dir},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
   47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
   98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
 {{node,'ns_1@db3.lan',config_version},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   {7,6}]},
 {{node,'ns_1@db3.lan',compaction_daemon},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
   {check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}]},
 {{node,'ns_1@db3.lan',cbas_ssl_port},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   18095]},
 {{node,'ns_1@db3.lan',cbas_result_port},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   9117]},
 {{node,'ns_1@db3.lan',cbas_replication_port},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   9120]},
 {{node,'ns_1@db3.lan',cbas_parent_port},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   9122]},
 {{node,'ns_1@db3.lan',cbas_metadata_port},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   9121]},
 {{node,'ns_1@db3.lan',cbas_metadata_callback_port},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   9119]},
 {{node,'ns_1@db3.lan',cbas_messaging_port},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   9118]},
 {{node,'ns_1@db3.lan',cbas_http_port},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   8095]},
 {{node,'ns_1@db3.lan',cbas_debug_port},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|-1]},
 {{node,'ns_1@db3.lan',cbas_data_port},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   9116]},
 {{node,'ns_1@db3.lan',cbas_console_port},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   9114]},
 {{node,'ns_1@db3.lan',cbas_cluster_port},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   9115]},
 {{node,'ns_1@db3.lan',cbas_cc_http_port},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   9111]},
 {{node,'ns_1@db3.lan',cbas_cc_cluster_port},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   9112]},
 {{node,'ns_1@db3.lan',cbas_cc_client_port},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   9113]},
 {{node,'ns_1@db3.lan',cbas_admin_port},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   9110]},
 {{node,'ns_1@db3.lan',capi_port},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   8092]},
 {{node,'ns_1@db3.lan',backup_https_port},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   18097]},
 {{node,'ns_1@db3.lan',backup_http_port},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   8097]},
 {{node,'ns_1@db3.lan',backup_grpc_port},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   9124]},
 {{node,'ns_1@db3.lan',audit},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}]},
 {{node,'ns_1@db3.lan',address_family},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   inet]},
 {{node,'ns_1@db3.lan',node_encryption},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   false]},
 {{node,'ns_1@db3.lan',erl_external_listeners},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
   {inet,false}]},
 {{node,'ns_1@db3.lan',n2n_client_cert_auth},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   false]},
 {{node,'ns_1@db3.lan',cbas_dirs},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
   "/opt/couchbase/var/lib/couchbase/data"]},
 {{node,'ns_1@db3.lan',eventing_dir},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
   47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
   98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
 {{node,'ns_1@db3.lan',prometheus_auth_info},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   {"@prometheus",
    {auth,
     [{<<"hash">>,
       {[{<<"hashes">>,
          {sanitized,<<"Vmqy8YpGQ1ZVVOJNXOVKZaSx7qimZeVP/vMQ9t99FKU=">>}},
         {<<"algorithm">>,<<"argon2id">>},
         {<<"salt">>,<<"RaDchxQfbWx/9yCvS4KJhw==">>},
         {<<"parallelism">>,1},
         {<<"time">>,3},
         {<<"memory">>,524288}]}},
      {<<"scram-sha-512">>,
       {[{<<"salt">>,
          <<"WL9ZWEHJ76WYVLC0m22/0VNF6T50k/J0Z7Okpz3sW4PgL9CASwlonzChieVERPB3Sib20fCxP6nMQ/SgjVQjEg==">>},
         {<<"iterations">>,15000},
         {<<"hashes">>,
          {sanitized,<<"vX4Kcf9uKEqkQwiL3H4KjNM4iq2tHqkJD/vMtc9V/Xg=">>}}]}},
      {<<"scram-sha-256">>,
       {[{<<"salt">>,<<"YkWxrYn6kotQkszjlA7ouXNPvh8HqG960AycvYrvGOo=">>},
         {<<"iterations">>,15000},
         {<<"hashes">>,
          {sanitized,<<"gECRKBSDc8Gg85IUVUx8OK6b2QieozWWeKpDySi+GnU=">>}}]}},
      {<<"scram-sha-1">>,
       {[{<<"salt">>,<<"uv9yTXGPC84w9yF7otsSjKjc9bM=">>},
         {<<"iterations">>,15000},
         {<<"hashes">>,
          {sanitized,
           <<"SOMxs6h/s5Xzi9/iuNI3GZdbix+jQu+XMiQI1CxtTOI=">>}}]}}]}}]},
 {{node,'ns_1@db3.lan',node_cert},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
   {subject,<<"CN=Couchbase Server Node (db3.lan)">>},
   {not_after,63985747645},
   {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
   {load_timestamp,63914554045},
   {ca,
    <<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQCuy0dFqLPrJ6xY21RJ5ttVPR78ssxxXSe9JdTj/IsUczkwyxfZfwK\npBszlGYuHO0Utnwq1K4ruMPYFYfpLacJSfsb3KWEmZwAoNuRpjvE24AsH3tvj7cB\nB5v49HJ0axvNAFm75GWDyUr7T7UwcTJaFIFtfy+J0Umt2TjFNY0DPpt1WEzkKFKx\nOdBs+7cNc35ZG4rwVu/EJ14OXhQMXkpbUpYJxS8jzkzSI+1Eb67kH4j5QhnR9H+w\nw2S9AgMBAAGjQjBAMA4GA1UdDwEB/wQEAwIBhjAPBgNVHRMBAf8EBTADAQH/MB0G\nA1UdDgQWBBS8ASq2Mkh0UsZe4fIvzs4mCSfTmjANBgkqhkiG9w0BAQsFAAOCAQEA\nB3q1mzdJc3VY17JDCaRPKY/Veni3oZ6zGJ9r9LnWKE7IO9AxBAKYGMELHMf9httQ\nmRZrXiGQi/tYXLlFFk7IcIID6iOOZLLagDhCQJPi52dGhBB4a2EnBvGF9OJI4zW9\nJw04HNR5cF1cxWIrRdGJAX/kbs/M9jz0STlXziVNwiAYeGIxkjq/fwKk6weai8iI\nwQVu6GsbtSBL8f4FRCk2vK6a1MD54BQDasRShdi3k/DJAEtY92muu13M0jc48mCM\ndEVeXxM8rIrG6LavziTwtO+jkFyWjlRyNbSr6il+neSWJ7ZspTo+ZYFy55CAh5ws\nwk6Ljo2wlQgfGflJbKcxAw==\n-----END CERTIFICATE-----\n">>},
   {pem,
    <<"-----BEGIN CERTIFICATE-----\nMIIDOjCCAiKgAwIBAgIIGD/HyL2BZrYwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjVaFw0y\nNzA4MTcxODQ3MjVaMCoxKDAmBgNVBAMTH0NvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nZGIzLmxhbikwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCoBie+jXEG\nM9mqURIO0hHIXr1th5jf7TJD7f69DGF3znRjw1VTjnaMZAQOk8Gg44nwI1o1KtJj\nUmflnTGWWtZU+43sI8tDXLtymAbYoxEkodGUqqPCEPQkCWLubJvTbSZVEyegLo9P\nKw4RPwJGAkbAFq1poCecaE9pf4qeOhibUM7ivJx5nFMcTkUGKqJt6vVH4oA/FcHp\nfOLz51gjD0O/U2Ev8NqFvx6ftWkIU/+LGaHaMjoDj4x8SldegqvGtDmpfhg2+97T\nftALlkUkvvJqd4beilSDxCiurYlIKOki8GWaGFfleyV8HwD/hOIZ/wj5yz607CDj\nuny5Z11yQr7/AgMBAAGjajBoMA4GA1UdDwEB/wQEAwIFoDATBgNVHSUEDDAKBggr\nBgEFBQcDATAMBgNVHRMBAf8EAjAAMB8GA1UdIwQYMBaAFLwBKrYySHRSxl7h8i/O\nziYJJ9OaMBIGA1UdEQQLMAmCB2RiMy5sYW4wDQYJKoZIhvcNAQELBQADggEBAAPS\nNPGKGEFgoq/sasnMadhR33aGK60LnjgknxG0R9lT+/RHZKEoDMbYw4gp7UUrh+JZ\n+cneP93RcOIpvfRP7AUlGviu8HDldR5yFQBRXpDdKyWvIvPyhwZK4v65yIYUcLq4\ngxMZUzynscYxC4eN1l2eyD2Rf4FuZ7G2ehW4zcei1DD35WId1pR+YKjwKNZ8Q5Sk\nyjT9F2mftICgpSdxyZJLdNXaXDGfodNVzuCKnMnYUoRWmddt5UqqLLkPM0+7HEpU\nn5wAUJD2kSCGspqGnAZbcBEbGKbW2hmgXAKxOQVyVfi4b1ZGo/IAScLuUoMZw8hh\n+6kwlsLDy19h3nkTfxo=\n-----END CERTIFICATE-----\n">>},
   {pkey_passphrase_settings,[]},
   {certs_epoch,0},
   {type,generated},
   {hostname,"db3.lan"}]},
 {{node,'ns_1@db3.lan',client_cert},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
   {subject,<<"CN=Couchbase Internal Client (32660726)">>},
   {not_after,63985747645},
   {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
   {load_timestamp,63914554045},
   {ca,
    <<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQCuy0dFqLPrJ6xY21RJ5ttVPR78ssxxXSe9JdTj/IsUczkwyxfZfwK\npBszlGYuHO0Utnwq1K4ruMPYFYfpLacJSfsb3KWEmZwAoNuRpjvE24AsH3tvj7cB\nB5v49HJ0axvNAFm75GWDyUr7T7UwcTJaFIFtfy+J0Umt2TjFNY0DPpt1WEzkKFKx\nOdBs+7cNc35ZG4rwVu/EJ14OXhQMXkpbUpYJxS8jzkzSI+1Eb67kH4j5QhnR9H+w\nw2S9AgMBAAGjQjBAMA4GA1UdDwEB/wQEAwIBhjAPBgNVHRMBAf8EBTADAQH/MB0G\nA1UdDgQWBBS8ASq2Mkh0UsZe4fIvzs4mCSfTmjANBgkqhkiG9w0BAQsFAAOCAQEA\nB3q1mzdJc3VY17JDCaRPKY/Veni3oZ6zGJ9r9LnWKE7IO9AxBAKYGMELHMf9httQ\nmRZrXiGQi/tYXLlFFk7IcIID6iOOZLLagDhCQJPi52dGhBB4a2EnBvGF9OJI4zW9\nJw04HNR5cF1cxWIrRdGJAX/kbs/M9jz0STlXziVNwiAYeGIxkjq/fwKk6weai8iI\nwQVu6GsbtSBL8f4FRCk2vK6a1MD54BQDasRShdi3k/DJAEtY92muu13M0jc48mCM\ndEVeXxM8rIrG6LavziTwtO+jkFyWjlRyNbSr6il+neSWJ7ZspTo+ZYFy55CAh5ws\nwk6Ljo2wlQgfGflJbKcxAw==\n-----END CERTIFICATE-----\n">>},
   {pem,
    <<"-----BEGIN CERTIFICATE-----\nMIIDWTCCAkGgAwIBAgIIGD/HyMO1PBAwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjVaFw0y\nNzA4MTcxODQ3MjVaMC8xLTArBgNVBAMTJENvdWNoYmFzZSBJbnRlcm5hbCBDbGll\nbnQgKDMyNjYwNzI2KTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAKHe\n9VAt+X6MU19Fg7ok9H0Zxkmq5mDd0acYZWMO8WqPC7jBY2dopAKDJoF/AKgk8TLT\ncz5AYGMiY3CokE1WbBNVZZ8+VMft6H+06YxTTsLvqWttVghc4Q3UKSuDpvQrIbxj\n7PxG12wiWoQUCfnBMjrL2ILgJKAQIMjO7UoWin3LmZo8LTdR4ugoBoS3cfQJhz7R\nT7iuSbe/44TwrcYrDzrvg3UCsFrXLnKs/LJ+nfQyl0fjkS/MP33lCHkdZ6+teEaJ\ncsNTaJuYKYIQIR1ICQnsFCsXrf2D4h0v8ZUsRVoaB3TFebmEqYtNB7V3NnP/sxFf\nqQvn6gwzDmvlb0O5+/MCAwEAAaOBgzCBgDAOBgNVHQ8BAf8EBAMCBaAwEwYDVR0l\nBAwwCgYIKwYBBQUHAwIwDAYDVR0TAQH/BAIwADAfBgNVHSMEGDAWgBS8ASq2Mkh0\nUsZe4fIvzs4mCSfTmjAqBgNVHREEIzAhgR9pbnRlcm5hbEBpbnRlcm5hbC5jb3Vj\naGJhc2UuY29tMA0GCSqGSIb3DQEBCwUAA4IBAQAtkwpYapli88YqNcz52BW8oUhZ\nqHNUis2IieXUTRZSTpTQlvywLUYl+7UvOl9QZjGGowKnD7bdTGgR0qyP1Bab1eIs\nZTbE8+MMa/2zeKjsDQlv7LKoKRI6eBa7nadz2AsyuYJ6h6M8Pj8XI1fXTSdEMZyU\ny32ahUlOq2qNPC485fFLJs/6ggN+3NCZT4H/ohbv8cZvoSUuHOj/wzbuNMZg4d18\n57CxIpHFCtyqUkXfQabWuPSsJRadCqs+Z59XLqEmyVdTeX8HcPON7IXtI7lNDdWB\nS0M6ljMG3ONZVVzxAowbDDbacQ1BK+kY8qlk0KzgCydCP3z86KPAsXEFSqTr\n-----END CERTIFICATE-----\n">>},
   {pkey_passphrase_settings,[]},
   {certs_epoch,0},
   {type,generated},
   {name,"@internal"}]},
 {{node,'ns_1@db3.lan',uuid},
  [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
   <<"80960cc3730024bccd4fc49444efdc14">>]},
 {otp,
  [{cookie,{sanitized,<<"biSYTaQUFDVndTs/b+IcDmD2L0woktv9naRdv3GCK6A=">>}}]},
 {nodes_wanted,['ns_1@db3.lan','ns_1@172.19.0.4']},
 {cluster_compat_mode,undefined},
 {{node,'ns_1@db3.lan',membership},inactiveAdded}]
[ns_server:debug,2025-05-15T18:47:26.031Z,ns_1@db3.lan:tombstone_keeper<0.277.0>:tombstone_keeper:handle_call:52]Refresh skipped due to chronicle being in state joining_cluster
[cluster:debug,2025-05-15T18:47:26.033Z,ns_1@db3.lan:ns_cluster<0.273.0>:ns_cluster:perform_actual_join:1613]pre-join cleaned config is:
{config,
 {full,"/opt/couchbase/etc/couchbase/config",undefined,ns_config_default},
 [[],
  [{{node,'ns_1@db3.lan',config_version},{7,6}},
   {directory,"/opt/couchbase/var/lib/couchbase/config"},
   {{node,'ns_1@db3.lan',is_enterprise},true},
   {{node,'ns_1@db3.lan',saslauthd_enabled},true},
   {index_aware_rebalance_disabled,false},
   {max_bucket_count,30},
   {set_view_update_daemon,
    [{update_interval,5000},
     {update_min_changes,5000},
     {replica_update_min_changes,5000}]},
   {{node,'ns_1@db3.lan',compaction_daemon},
    [{check_interval,30},
     {min_db_file_size,131072},
     {min_view_file_size,20971520}]},
   {nodes_wanted,[]},
   {quorum_nodes,['ns_1@db3.lan']},
   {{couchdb,max_parallel_indexers},4},
   {{couchdb,max_parallel_replica_indexers},2},
   {{metakv,<<"/indexing/settings/config">>},
    <<"{\"indexer.settings.compaction.abort_exceed_interval\":false,\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.enable_page_bloom_filter\":false,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.log_level\":\"info\",\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.num_replica\":0,\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.storage_mode\":\"\"}">>},
   {{metakv,<<"/eventing/settings/config">>},<<"{\"ram_quota\":256}">>},
   {{metakv,<<"/query/settings/config">>},
    <<"{\"cleanupclientattempts\":true,\"cleanuplostattempts\":true,\"cleanupwindow\":\"60s\",\"completed-limit\":4000,\"completed-threshold\":1000,\"loglevel\":\"info\",\"max-parallelism\":1,\"memory-quota\":0,\"n1ql-feat-ctrl\":76,\"numatrs\":1024,\"pipeline-batch\":16,\"pipeline-cap\":512,\"prepared-limit\":16384,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"query.settings.tmp_space_size\":5120,\"scan-cap\":512,\"timeout\":0,\"txtimeout\":\"0ms\",\"use-cbo\":true}">>},
   {{metakv,<<"/analytics/settings/config">>},
    <<"{\"analytics.settings.num_replicas\":0}">>},
   {rest_creds,null},
   {client_cert_auth,[{state,"disable"},{prefixes,[]}]},
   {scramsha_fallback_salt,<<213,116,154,49,54,134,239,240,55,23,111,63>>},
   {remote_clusters,[]},
   {{node,'ns_1@db3.lan',isasl},
    [{path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
   {audit,
    [{auditd_enabled,false},
     {rotate_interval,86400},
     {rotate_size,20971520},
     {prune_age,0},
     {disabled,[]},
     {enabled,[]},
     {disabled_users,[]},
     {sync,[]},
     {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
   {{node,'ns_1@db3.lan',audit},[]},
   {memcached,[]},
   {{node,'ns_1@db3.lan',memcached_defaults},
    [{max_connections,65000},
     {system_connections,5000},
     {connection_idle_time,0},
     {verbosity,0},
     {privilege_debug,false},
     {breakpad_enabled,true},
     {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
     {dedupe_nmvb_maps,false},
     {je_malloc_conf,undefined},
     {tracing_enabled,true},
     {datatype_snappy,true},
     {num_reader_threads,<<"default">>},
     {num_writer_threads,<<"default">>},
     {num_auxio_threads,<<"default">>},
     {num_nonio_threads,<<"default">>},
     {num_storage_threads,<<"default">>},
     {tcp_keepalive_idle,360},
     {tcp_keepalive_interval,10},
     {tcp_keepalive_probes,3},
     {tcp_user_timeout,30},
     {always_collect_trace_info,true},
     {connection_limit_mode,<<"disconnect">>},
     {free_connection_pool_size,0},
     {max_client_connection_details,0}]},
   {{node,'ns_1@db3.lan',memcached},
    [{port,11210},
     {dedicated_port,11209},
     {dedicated_ssl_port,11206},
     {ssl_port,11207},
     {admin_user,"@ns_server"},
     {other_users,
      ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
       "@cbas","@backup"]},
     {admin_pass,"*****"},
     {engines,
      [{membase,
        [{engine,"/opt/couchbase/lib/memcached/ep.so"},
         {static_config_string,"failpartialwarmup=false"}]},
       {memcached,
        [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
         {static_config_string,"vb0=true"}]}]},
     {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
     {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
     {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
     {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
     {log_prefix,"memcached.log"},
     {log_generations,20},
     {log_cyclesize,10485760},
     {log_rotation_period,39003}]},
   {{node,'ns_1@db3.lan',memcached_config},
    {[{interfaces,{memcached_config_mgr,get_interfaces,[]}},
      {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
      {connection_idle_time,connection_idle_time},
      {privilege_debug,privilege_debug},
      {breakpad,
       {[{enabled,breakpad_enabled},
         {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
      {deployment_model,{memcached_config_mgr,get_config_profile,[]}},
      {verbosity,verbosity},
      {audit_file,{"~s",[audit_file]}},
      {rbac_file,{"~s",[rbac_file]}},
      {dedupe_nmvb_maps,dedupe_nmvb_maps},
      {tracing_enabled,tracing_enabled},
      {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
      {xattr_enabled,true},
      {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
      {collections_enabled,true},
      {max_connections,max_connections},
      {system_connections,system_connections},
      {num_reader_threads,num_reader_threads},
      {num_writer_threads,num_writer_threads},
      {num_auxio_threads,num_auxio_threads},
      {num_nonio_threads,num_nonio_threads},
      {num_storage_threads,num_storage_threads},
      {logger,
       {[{filename,{"~s/~s",[log_path,log_prefix]}},
         {cyclesize,log_cyclesize}]}},
      {external_auth_service,
       {memcached_config_mgr,get_external_auth_service,[]}},
      {active_external_users_push_interval,
       {memcached_config_mgr,get_external_users_push_interval,[]}},
      {prometheus,{memcached_config_mgr,prometheus_cfg,[]}},
      {sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
      {ssl_sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
      {tcp_keepalive_idle,tcp_keepalive_idle},
      {tcp_keepalive_interval,tcp_keepalive_interval},
      {tcp_keepalive_probes,tcp_keepalive_probes},
      {tcp_user_timeout,tcp_user_timeout},
      {always_collect_trace_info,always_collect_trace_info},
      {connection_limit_mode,connection_limit_mode},
      {free_connection_pool_size,free_connection_pool_size},
      {max_client_connection_details,max_client_connection_details}]}},
   {memory_quota,3406},
   {fts_memory_quota,512},
   {cbas_memory_quota,1549},
   {buckets,[{configs,[]}]},
   {secure_headers,[]},
   {{node,'ns_1@db3.lan',port_servers},[]},
   {{node,'ns_1@db3.lan',ns_log},
    [{filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
   {{node,'ns_1@db3.lan',event_log},
    [{filename,"/opt/couchbase/var/lib/couchbase/event_log"}]},
   {email_alerts,
    [{recipients,["root@localhost"]},
     {sender,"couchbase@localhost"},
     {enabled,false},
     {email_server,
      [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
     {alerts,
      [auto_failover_node,auto_failover_maximum_reached,
       auto_failover_other_nodes_down,auto_failover_cluster_too_small,
       auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
       ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
       indexer_low_resident_percentage,ep_clock_cas_drift_threshold_exceeded,
       communication_issue,time_out_of_sync,disk_usage_analyzer_stuck,
       cert_expires_soon,cert_expired,memory_threshold,history_size_warning,
       stuck_rebalance,memcached_connections]},
     {pop_up_alerts,
      [auto_failover_node,auto_failover_maximum_reached,
       auto_failover_other_nodes_down,auto_failover_cluster_too_small,
       auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
       ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
       indexer_low_resident_percentage,ep_clock_cas_drift_threshold_exceeded,
       communication_issue,time_out_of_sync,disk_usage_analyzer_stuck,
       cert_expires_soon,cert_expired,memory_threshold,history_size_warning,
       stuck_rebalance,memcached_connections]}]},
   {alert_limits,
    [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
   {replication,[{enabled,true}]},
   {log_redaction_default_cfg,[{redact_level,none}]},
   {service_orchestrator_weight,
    [{kv,10000},
     {index,1000},
     {fts,1000},
     {cbas,1000},
     {n1ql,100},
     {eventing,100},
     {backup,10}]},
   {health_monitor_refresh_interval,[]},
   {password_policy,[{min_length,6},{must_present,[]}]},
   {rest,[{port,8091}]},
   {{node,'ns_1@db3.lan',rest},[{port,8091},{port_meta,global}]},
   {{node,'ns_1@db3.lan',ssl_rest_port},18091},
   {{node,'ns_1@db3.lan',xdcr_rest_port},9998},
   {{node,'ns_1@db3.lan',memcached_dedicated_ssl_port},11206},
   {{node,'ns_1@db3.lan',memcached_prometheus},11280},
   {{node,'ns_1@db3.lan',projector_port},9999},
   {{node,'ns_1@db3.lan',projector_ssl_port},9999},
   {{node,'ns_1@db3.lan',query_port},8093},
   {{node,'ns_1@db3.lan',ssl_query_port},18093},
   {{node,'ns_1@db3.lan',indexer_admin_port},9100},
   {{node,'ns_1@db3.lan',indexer_scan_port},9101},
   {{node,'ns_1@db3.lan',indexer_http_port},9102},
   {{node,'ns_1@db3.lan',indexer_stinit_port},9103},
   {{node,'ns_1@db3.lan',indexer_stcatchup_port},9104},
   {{node,'ns_1@db3.lan',indexer_stmaint_port},9105},
   {{node,'ns_1@db3.lan',indexer_https_port},19102},
   {{node,'ns_1@db3.lan',fts_http_port},8094},
   {{node,'ns_1@db3.lan',fts_ssl_port},18094},
   {{node,'ns_1@db3.lan',fts_grpc_port},9130},
   {{node,'ns_1@db3.lan',fts_grpc_ssl_port},19130},
   {{node,'ns_1@db3.lan',eventing_http_port},8096},
   {{node,'ns_1@db3.lan',eventing_debug_port},9140},
   {{node,'ns_1@db3.lan',eventing_https_port},18096},
   {{node,'ns_1@db3.lan',cbas_http_port},8095},
   {{node,'ns_1@db3.lan',cbas_ssl_port},18095},
   {{node,'ns_1@db3.lan',cbas_admin_port},9110},
   {{node,'ns_1@db3.lan',cbas_cc_http_port},9111},
   {{node,'ns_1@db3.lan',cbas_cc_cluster_port},9112},
   {{node,'ns_1@db3.lan',cbas_cc_client_port},9113},
   {{node,'ns_1@db3.lan',cbas_console_port},9114},
   {{node,'ns_1@db3.lan',cbas_cluster_port},9115},
   {{node,'ns_1@db3.lan',cbas_data_port},9116},
   {{node,'ns_1@db3.lan',cbas_result_port},9117},
   {{node,'ns_1@db3.lan',cbas_messaging_port},9118},
   {{node,'ns_1@db3.lan',cbas_metadata_callback_port},9119},
   {{node,'ns_1@db3.lan',cbas_replication_port},9120},
   {{node,'ns_1@db3.lan',cbas_metadata_port},9121},
   {{node,'ns_1@db3.lan',cbas_parent_port},9122},
   {{node,'ns_1@db3.lan',cbas_debug_port},-1},
   {{node,'ns_1@db3.lan',prometheus_http_port},9123},
   {{node,'ns_1@db3.lan',backup_http_port},8097},
   {{node,'ns_1@db3.lan',backup_https_port},18097},
   {{node,'ns_1@db3.lan',backup_grpc_port},9124},
   {{node,'ns_1@db3.lan',capi_port},8092},
   {{node,'ns_1@db3.lan',ssl_capi_port},18092},
   {{node,'ns_1@db3.lan',{project_intact,is_vulnerable}},false},
   {retry_rebalance,
    [{enabled,false},{after_time_period,300},{max_attempts,1}]},
   {auto_failover_cfg,
    [{enabled,true},
     {timeout,120},
     {count,0},
     {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
     {failover_server_group,false},
     {max_count,1},
     {failed_over_server_groups,[]},
     {can_abort_rebalance,true}]},
   {{node,'ns_1@db3.lan',database_dir},
    "/opt/couchbase/var/lib/couchbase/data"},
   {{node,'ns_1@db3.lan',index_dir},"/opt/couchbase/var/lib/couchbase/data"},
   {resource_management,
    [{bucket,
      [{resident_ratio,
        [{enabled,false},{couchstore_minimum,1},{magma_minimum,0.2}]},
       {data_size,
        [{enabled,false},{couchstore_maximum,2},{magma_maximum,16}]}]},
     {index,[]},
     {cores_per_bucket,[{enabled,false},{minimum,0.4}]},
     {disk_usage,[{enabled,false},{maximum,96}]},
     {collections_per_quota,[{enabled,false},{maximum,1}]}]}]],
 [[{{node,'ns_1@db3.lan',{project_intact,is_vulnerable}},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     false]},
   {{node,'ns_1@db3.lan',xdcr_rest_port},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     9998]},
   {{node,'ns_1@db3.lan',uuid},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     <<"80960cc3730024bccd4fc49444efdc14">>]},
   {{node,'ns_1@db3.lan',ssl_rest_port},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     18091]},
   {{node,'ns_1@db3.lan',ssl_query_port},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     18093]},
   {{node,'ns_1@db3.lan',ssl_capi_port},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     18092]},
   {{node,'ns_1@db3.lan',saslauthd_enabled},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     true]},
   {{node,'ns_1@db3.lan',rest},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
     {port,8091},
     {port_meta,global}]},
   {{node,'ns_1@db3.lan',query_port},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     8093]},
   {{node,'ns_1@db3.lan',prometheus_http_port},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     9123]},
   {{node,'ns_1@db3.lan',prometheus_auth_info},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     {"@prometheus",
      {auth,
       [{<<"hash">>,
         {[{<<"hashes">>,
            {sanitized,<<"Vmqy8YpGQ1ZVVOJNXOVKZaSx7qimZeVP/vMQ9t99FKU=">>}},
           {<<"algorithm">>,<<"argon2id">>},
           {<<"salt">>,<<"RaDchxQfbWx/9yCvS4KJhw==">>},
           {<<"parallelism">>,1},
           {<<"time">>,3},
           {<<"memory">>,524288}]}},
        {<<"scram-sha-512">>,
         {[{<<"salt">>,
            <<"WL9ZWEHJ76WYVLC0m22/0VNF6T50k/J0Z7Okpz3sW4PgL9CASwlonzChieVERPB3Sib20fCxP6nMQ/SgjVQjEg==">>},
           {<<"iterations">>,15000},
           {<<"hashes">>,
            {sanitized,<<"vX4Kcf9uKEqkQwiL3H4KjNM4iq2tHqkJD/vMtc9V/Xg=">>}}]}},
        {<<"scram-sha-256">>,
         {[{<<"salt">>,<<"YkWxrYn6kotQkszjlA7ouXNPvh8HqG960AycvYrvGOo=">>},
           {<<"iterations">>,15000},
           {<<"hashes">>,
            {sanitized,<<"gECRKBSDc8Gg85IUVUx8OK6b2QieozWWeKpDySi+GnU=">>}}]}},
        {<<"scram-sha-1">>,
         {[{<<"salt">>,<<"uv9yTXGPC84w9yF7otsSjKjc9bM=">>},
           {<<"iterations">>,15000},
           {<<"hashes">>,
            {sanitized,
             <<"SOMxs6h/s5Xzi9/iuNI3GZdbix+jQu+XMiQI1CxtTOI=">>}}]}}]}}]},
   {{node,'ns_1@db3.lan',projector_ssl_port},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     9999]},
   {{node,'ns_1@db3.lan',projector_port},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     9999]},
   {{node,'ns_1@db3.lan',port_servers},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}]},
   {{node,'ns_1@db3.lan',ns_log},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
     {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
   {{node,'ns_1@db3.lan',node_encryption},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     false]},
   {{node,'ns_1@db3.lan',node_cert},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
     {subject,<<"CN=Couchbase Server Node (db3.lan)">>},
     {not_after,63985747645},
     {verified_with,
      <<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
     {load_timestamp,63914554045},
     {ca,
      <<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQCuy0dFqLPrJ6xY21RJ5ttVPR78ssxxXSe9JdTj/IsUczkwyxfZfwK\npBszlGYuHO0Utnwq1K4ruMPYFYfpLacJSfsb3KWEmZwAoNuRpjvE24AsH3tvj7cB\nB5v49HJ0axvNAFm75GWDyUr7T7UwcTJaFIFtfy+J0Umt2TjFNY0DPpt1WEzkKFKx\nOdBs+7cNc35ZG4rwVu/EJ14OXhQMXkpbUpYJxS8jzkzSI+1Eb67kH4j5QhnR9H+w\nw2S9AgMBAAGjQjBAMA4GA1UdDwEB/wQEAwIBhjAPBgNVHRMBAf8EBTADAQH/MB0G\nA1UdDgQWBBS8ASq2Mkh0UsZe4fIvzs4mCSfTmjANBgkqhkiG9w0BAQsFAAOCAQEA\nB3q1mzdJc3VY17JDCaRPKY/Veni3oZ6zGJ9r9LnWKE7IO9AxBAKYGMELHMf9httQ\nmRZrXiGQi/tYXLlFFk7IcIID6iOOZLLagDhCQJPi52dGhBB4a2EnBvGF9OJI4zW9\nJw04HNR5cF1cxWIrRdGJAX/kbs/M9jz0STlXziVNwiAYeGIxkjq/fwKk6weai8iI\nwQVu6GsbtSBL8f4FRCk2vK6a1MD54BQDasRShdi3k/DJAEtY92muu13M0jc48mCM\ndEVeXxM8rIrG6LavziTwtO+jkFyWjlRyNbSr6il+neSWJ7ZspTo+ZYFy55CAh5ws\nwk6Ljo2wlQgfGflJbKcxAw==\n-----END CERTIFICATE-----\n">>},
     {pem,
      <<"-----BEGIN CERTIFICATE-----\nMIIDOjCCAiKgAwIBAgIIGD/HyL2BZrYwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjVaFw0y\nNzA4MTcxODQ3MjVaMCoxKDAmBgNVBAMTH0NvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nZGIzLmxhbikwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCoBie+jXEG\nM9mqURIO0hHIXr1th5jf7TJD7f69DGF3znRjw1VTjnaMZAQOk8Gg44nwI1o1KtJj\nUmflnTGWWtZU+43sI8tDXLtymAbYoxEkodGUqqPCEPQkCWLubJvTbSZVEyegLo9P\nKw4RPwJGAkbAFq1poCecaE9pf4qeOhibUM7ivJx5nFMcTkUGKqJt6vVH4oA/FcHp\nfOLz51gjD0O/U2Ev8NqFvx6ftWkIU/+LGaHaMjoDj4x8SldegqvGtDmpfhg2+97T\nftALlkUkvvJqd4beilSDxCiurYlIKOki8GWaGFfleyV8HwD/hOIZ/wj5yz607CDj\nuny5Z11yQr7/AgMBAAGjajBoMA4GA1UdDwEB/wQEAwIFoDATBgNVHSUEDDAKBggr\nBgEFBQcDATAMBgNVHRMBAf8EAjAAMB8GA1UdIwQYMBaAFLwBKrYySHRSxl7h8i/O\nziYJJ9OaMBIGA1UdEQQLMAmCB2RiMy5sYW4wDQYJKoZIhvcNAQELBQADggEBAAPS\nNPGKGEFgoq/sasnMadhR33aGK60LnjgknxG0R9lT+/RHZKEoDMbYw4gp7UUrh+JZ\n+cneP93RcOIpvfRP7AUlGviu8HDldR5yFQBRXpDdKyWvIvPyhwZK4v65yIYUcLq4\ngxMZUzynscYxC4eN1l2eyD2Rf4FuZ7G2ehW4zcei1DD35WId1pR+YKjwKNZ8Q5Sk\nyjT9F2mftICgpSdxyZJLdNXaXDGfodNVzuCKnMnYUoRWmddt5UqqLLkPM0+7HEpU\nn5wAUJD2kSCGspqGnAZbcBEbGKbW2hmgXAKxOQVyVfi4b1ZGo/IAScLuUoMZw8hh\n+6kwlsLDy19h3nkTfxo=\n-----END CERTIFICATE-----\n">>},
     {pkey_passphrase_settings,[]},
     {certs_epoch,0},
     {type,generated},
     {hostname,"db3.lan"}]},
   {{node,'ns_1@db3.lan',n2n_client_cert_auth},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     false]},
   {{node,'ns_1@db3.lan',memcached_prometheus},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     11280]},
   {{node,'ns_1@db3.lan',memcached_defaults},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
     {max_connections,65000},
     {system_connections,5000},
     {connection_idle_time,0},
     {verbosity,0},
     {privilege_debug,false},
     {breakpad_enabled,true},
     {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
     {dedupe_nmvb_maps,false},
     {je_malloc_conf,undefined},
     {tracing_enabled,true},
     {datatype_snappy,true},
     {num_reader_threads,<<"default">>},
     {num_writer_threads,<<"default">>},
     {num_auxio_threads,<<"default">>},
     {num_nonio_threads,<<"default">>},
     {num_storage_threads,<<"default">>},
     {tcp_keepalive_idle,360},
     {tcp_keepalive_interval,10},
     {tcp_keepalive_probes,3},
     {tcp_user_timeout,30},
     {always_collect_trace_info,true},
     {connection_limit_mode,<<"disconnect">>},
     {free_connection_pool_size,0},
     {max_client_connection_details,0}]},
   {{node,'ns_1@db3.lan',memcached_dedicated_ssl_port},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     11206]},
   {{node,'ns_1@db3.lan',memcached_config},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     {[{interfaces,{memcached_config_mgr,get_interfaces,[]}},
       {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
       {connection_idle_time,connection_idle_time},
       {privilege_debug,privilege_debug},
       {breakpad,
        {[{enabled,breakpad_enabled},
          {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
       {deployment_model,{memcached_config_mgr,get_config_profile,[]}},
       {verbosity,verbosity},
       {audit_file,{"~s",[audit_file]}},
       {rbac_file,{"~s",[rbac_file]}},
       {dedupe_nmvb_maps,dedupe_nmvb_maps},
       {tracing_enabled,tracing_enabled},
       {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
       {xattr_enabled,true},
       {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
       {collections_enabled,true},
       {max_connections,max_connections},
       {system_connections,system_connections},
       {num_reader_threads,num_reader_threads},
       {num_writer_threads,num_writer_threads},
       {num_auxio_threads,num_auxio_threads},
       {num_nonio_threads,num_nonio_threads},
       {num_storage_threads,num_storage_threads},
       {logger,
        {[{filename,{"~s/~s",[log_path,log_prefix]}},
          {cyclesize,log_cyclesize}]}},
       {external_auth_service,
        {memcached_config_mgr,get_external_auth_service,[]}},
       {active_external_users_push_interval,
        {memcached_config_mgr,get_external_users_push_interval,[]}},
       {prometheus,{memcached_config_mgr,prometheus_cfg,[]}},
       {sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
       {ssl_sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
       {tcp_keepalive_idle,tcp_keepalive_idle},
       {tcp_keepalive_interval,tcp_keepalive_interval},
       {tcp_keepalive_probes,tcp_keepalive_probes},
       {tcp_user_timeout,tcp_user_timeout},
       {always_collect_trace_info,always_collect_trace_info},
       {connection_limit_mode,connection_limit_mode},
       {free_connection_pool_size,free_connection_pool_size},
       {max_client_connection_details,max_client_connection_details}]}]},
   {{node,'ns_1@db3.lan',memcached},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
     {port,11210},
     {dedicated_port,11209},
     {dedicated_ssl_port,11206},
     {ssl_port,11207},
     {admin_user,"@ns_server"},
     {other_users,
      ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
       "@cbas","@backup"]},
     {admin_pass,"*****"},
     {engines,
      [{membase,
        [{engine,"/opt/couchbase/lib/memcached/ep.so"},
         {static_config_string,"failpartialwarmup=false"}]},
       {memcached,
        [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
         {static_config_string,"vb0=true"}]}]},
     {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
     {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
     {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
     {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
     {log_prefix,"memcached.log"},
     {log_generations,20},
     {log_cyclesize,10485760},
     {log_rotation_period,39003}]},
   {{node,'ns_1@db3.lan',membership},inactiveAdded},
   {{node,'ns_1@db3.lan',isasl},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
     {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
   {{node,'ns_1@db3.lan',is_enterprise},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     true]},
   {{node,'ns_1@db3.lan',indexer_stmaint_port},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     9105]},
   {{node,'ns_1@db3.lan',indexer_stinit_port},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     9103]},
   {{node,'ns_1@db3.lan',indexer_stcatchup_port},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     9104]},
   {{node,'ns_1@db3.lan',indexer_scan_port},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     9101]},
   {{node,'ns_1@db3.lan',indexer_https_port},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     19102]},
   {{node,'ns_1@db3.lan',indexer_http_port},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     9102]},
   {{node,'ns_1@db3.lan',indexer_admin_port},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     9100]},
   {{node,'ns_1@db3.lan',index_dir},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
     47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,
     105,98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
   {{node,'ns_1@db3.lan',fts_ssl_port},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     18094]},
   {{node,'ns_1@db3.lan',fts_http_port},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     8094]},
   {{node,'ns_1@db3.lan',fts_grpc_ssl_port},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     19130]},
   {{node,'ns_1@db3.lan',fts_grpc_port},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     9130]},
   {{node,'ns_1@db3.lan',eventing_https_port},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     18096]},
   {{node,'ns_1@db3.lan',eventing_http_port},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     8096]},
   {{node,'ns_1@db3.lan',eventing_dir},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
     47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,
     105,98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
   {{node,'ns_1@db3.lan',eventing_debug_port},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     9140]},
   {{node,'ns_1@db3.lan',event_log},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
     {filename,"/opt/couchbase/var/lib/couchbase/event_log"}]},
   {{node,'ns_1@db3.lan',erl_external_listeners},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
     {inet,false}]},
   {{node,'ns_1@db3.lan',database_dir},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
     47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,
     105,98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
   {{node,'ns_1@db3.lan',config_version},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     {7,6}]},
   {{node,'ns_1@db3.lan',compaction_daemon},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
     {check_interval,30},
     {min_db_file_size,131072},
     {min_view_file_size,20971520}]},
   {{node,'ns_1@db3.lan',client_cert},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
     {subject,<<"CN=Couchbase Internal Client (32660726)">>},
     {not_after,63985747645},
     {verified_with,
      <<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
     {load_timestamp,63914554045},
     {ca,
      <<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQCuy0dFqLPrJ6xY21RJ5ttVPR78ssxxXSe9JdTj/IsUczkwyxfZfwK\npBszlGYuHO0Utnwq1K4ruMPYFYfpLacJSfsb3KWEmZwAoNuRpjvE24AsH3tvj7cB\nB5v49HJ0axvNAFm75GWDyUr7T7UwcTJaFIFtfy+J0Umt2TjFNY0DPpt1WEzkKFKx\nOdBs+7cNc35ZG4rwVu/EJ14OXhQMXkpbUpYJxS8jzkzSI+1Eb67kH4j5QhnR9H+w\nw2S9AgMBAAGjQjBAMA4GA1UdDwEB/wQEAwIBhjAPBgNVHRMBAf8EBTADAQH/MB0G\nA1UdDgQWBBS8ASq2Mkh0UsZe4fIvzs4mCSfTmjANBgkqhkiG9w0BAQsFAAOCAQEA\nB3q1mzdJc3VY17JDCaRPKY/Veni3oZ6zGJ9r9LnWKE7IO9AxBAKYGMELHMf9httQ\nmRZrXiGQi/tYXLlFFk7IcIID6iOOZLLagDhCQJPi52dGhBB4a2EnBvGF9OJI4zW9\nJw04HNR5cF1cxWIrRdGJAX/kbs/M9jz0STlXziVNwiAYeGIxkjq/fwKk6weai8iI\nwQVu6GsbtSBL8f4FRCk2vK6a1MD54BQDasRShdi3k/DJAEtY92muu13M0jc48mCM\ndEVeXxM8rIrG6LavziTwtO+jkFyWjlRyNbSr6il+neSWJ7ZspTo+ZYFy55CAh5ws\nwk6Ljo2wlQgfGflJbKcxAw==\n-----END CERTIFICATE-----\n">>},
     {pem,
      <<"-----BEGIN CERTIFICATE-----\nMIIDWTCCAkGgAwIBAgIIGD/HyMO1PBAwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjVaFw0y\nNzA4MTcxODQ3MjVaMC8xLTArBgNVBAMTJENvdWNoYmFzZSBJbnRlcm5hbCBDbGll\nbnQgKDMyNjYwNzI2KTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAKHe\n9VAt+X6MU19Fg7ok9H0Zxkmq5mDd0acYZWMO8WqPC7jBY2dopAKDJoF/AKgk8TLT\ncz5AYGMiY3CokE1WbBNVZZ8+VMft6H+06YxTTsLvqWttVghc4Q3UKSuDpvQrIbxj\n7PxG12wiWoQUCfnBMjrL2ILgJKAQIMjO7UoWin3LmZo8LTdR4ugoBoS3cfQJhz7R\nT7iuSbe/44TwrcYrDzrvg3UCsFrXLnKs/LJ+nfQyl0fjkS/MP33lCHkdZ6+teEaJ\ncsNTaJuYKYIQIR1ICQnsFCsXrf2D4h0v8ZUsRVoaB3TFebmEqYtNB7V3NnP/sxFf\nqQvn6gwzDmvlb0O5+/MCAwEAAaOBgzCBgDAOBgNVHQ8BAf8EBAMCBaAwEwYDVR0l\nBAwwCgYIKwYBBQUHAwIwDAYDVR0TAQH/BAIwADAfBgNVHSMEGDAWgBS8ASq2Mkh0\nUsZe4fIvzs4mCSfTmjAqBgNVHREEIzAhgR9pbnRlcm5hbEBpbnRlcm5hbC5jb3Vj\naGJhc2UuY29tMA0GCSqGSIb3DQEBCwUAA4IBAQAtkwpYapli88YqNcz52BW8oUhZ\nqHNUis2IieXUTRZSTpTQlvywLUYl+7UvOl9QZjGGowKnD7bdTGgR0qyP1Bab1eIs\nZTbE8+MMa/2zeKjsDQlv7LKoKRI6eBa7nadz2AsyuYJ6h6M8Pj8XI1fXTSdEMZyU\ny32ahUlOq2qNPC485fFLJs/6ggN+3NCZT4H/ohbv8cZvoSUuHOj/wzbuNMZg4d18\n57CxIpHFCtyqUkXfQabWuPSsJRadCqs+Z59XLqEmyVdTeX8HcPON7IXtI7lNDdWB\nS0M6ljMG3ONZVVzxAowbDDbacQ1BK+kY8qlk0KzgCydCP3z86KPAsXEFSqTr\n-----END CERTIFICATE-----\n">>},
     {pkey_passphrase_settings,[]},
     {certs_epoch,0},
     {type,generated},
     {name,"@internal"}]},
   {{node,'ns_1@db3.lan',cbas_ssl_port},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     18095]},
   {{node,'ns_1@db3.lan',cbas_result_port},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     9117]},
   {{node,'ns_1@db3.lan',cbas_replication_port},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     9120]},
   {{node,'ns_1@db3.lan',cbas_parent_port},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     9122]},
   {{node,'ns_1@db3.lan',cbas_metadata_port},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     9121]},
   {{node,'ns_1@db3.lan',cbas_metadata_callback_port},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     9119]},
   {{node,'ns_1@db3.lan',cbas_messaging_port},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     9118]},
   {{node,'ns_1@db3.lan',cbas_http_port},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     8095]},
   {{node,'ns_1@db3.lan',cbas_dirs},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
     "/opt/couchbase/var/lib/couchbase/data"]},
   {{node,'ns_1@db3.lan',cbas_debug_port},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     -1]},
   {{node,'ns_1@db3.lan',cbas_data_port},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     9116]},
   {{node,'ns_1@db3.lan',cbas_console_port},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     9114]},
   {{node,'ns_1@db3.lan',cbas_cluster_port},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     9115]},
   {{node,'ns_1@db3.lan',cbas_cc_http_port},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     9111]},
   {{node,'ns_1@db3.lan',cbas_cc_cluster_port},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     9112]},
   {{node,'ns_1@db3.lan',cbas_cc_client_port},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     9113]},
   {{node,'ns_1@db3.lan',cbas_admin_port},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     9110]},
   {{node,'ns_1@db3.lan',capi_port},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     8092]},
   {{node,'ns_1@db3.lan',backup_https_port},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     18097]},
   {{node,'ns_1@db3.lan',backup_http_port},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     8097]},
   {{node,'ns_1@db3.lan',backup_grpc_port},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     9124]},
   {{node,'ns_1@db3.lan',audit},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}]},
   {{node,'ns_1@db3.lan',address_family},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
     inet]},
   {{metakv,<<"/query/settings/config">>},
    <<"{\"cleanupclientattempts\":true,\"cleanuplostattempts\":true,\"cleanupwindow\":\"60s\",\"completed-limit\":4000,\"completed-threshold\":1000,\"loglevel\":\"info\",\"max-parallelism\":1,\"memory-quota\":0,\"n1ql-feat-ctrl\":76,\"numatrs\":1024,\"pipeline-batch\":16,\"pipeline-cap\":512,\"prepared-limit\":16384,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"query.settings.tmp_space_size\":5120,\"scan-cap\":512,\"timeout\":0,\"txtimeout\":\"0ms\",\"use-cbo\":true}">>},
   {{metakv,<<"/indexing/settings/config">>},
    <<"{\"indexer.settings.compaction.abort_exceed_interval\":false,\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.enable_page_bloom_filter\":false,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.log_level\":\"info\",\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.num_replica\":0,\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.storage_mode\":\"\"}">>},
   {{metakv,<<"/eventing/settings/config">>},<<"{\"ram_quota\":256}">>},
   {{metakv,<<"/analytics/settings/config">>},
    <<"{\"analytics.settings.num_replicas\":0}">>},
   {{local_changes_count,<<"80960cc3730024bccd4fc49444efdc14">>},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{5,63914554046}}]}]},
   {{couchdb,max_parallel_replica_indexers},2},
   {{couchdb,max_parallel_indexers},4},
   {set_view_update_daemon,
    [{update_interval,5000},
     {update_min_changes,5000},
     {replica_update_min_changes,5000}]},
   {service_orchestrator_weight,
    [{kv,10000},
     {index,1000},
     {fts,1000},
     {cbas,1000},
     {n1ql,100},
     {eventing,100},
     {backup,10}]},
   {secure_headers,[]},
   {scramsha_fallback_salt,<<213,116,154,49,54,134,239,240,55,23,111,63>>},
   {retry_rebalance,
    [{enabled,false},{after_time_period,300},{max_attempts,1}]},
   {rest_creds,null},
   {rest,[{port,8091}]},
   {resource_management,
    [{bucket,
      [{resident_ratio,
        [{enabled,false},{couchstore_minimum,1},{magma_minimum,0.2}]},
       {data_size,
        [{enabled,false},{couchstore_maximum,2},{magma_maximum,16}]}]},
     {index,[]},
     {cores_per_bucket,[{enabled,false},{minimum,0.4}]},
     {disk_usage,[{enabled,false},{maximum,96}]},
     {collections_per_quota,[{enabled,false},{maximum,1}]}]},
   {replication,[{enabled,true}]},
   {remote_clusters,[]},
   {quorum_nodes,['ns_1@db3.lan']},
   {password_policy,[{min_length,6},{must_present,[]}]},
   {otp,
    [{cookie,{sanitized,<<"biSYTaQUFDVndTs/b+IcDmD2L0woktv9naRdv3GCK6A=">>}}]},
   {nodes_wanted,['ns_1@db3.lan','ns_1@172.19.0.4']},
   {memory_quota,3406},
   {memcached,[]},
   {max_bucket_count,30},
   {log_redaction_default_cfg,[{redact_level,none}]},
   {index_aware_rebalance_disabled,false},
   {health_monitor_refresh_interval,[]},
   {fts_memory_quota,512},
   {email_alerts,
    [{recipients,["root@localhost"]},
     {sender,"couchbase@localhost"},
     {enabled,false},
     {email_server,
      [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
     {alerts,
      [auto_failover_node,auto_failover_maximum_reached,
       auto_failover_other_nodes_down,auto_failover_cluster_too_small,
       auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
       ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
       indexer_low_resident_percentage,ep_clock_cas_drift_threshold_exceeded,
       communication_issue,time_out_of_sync,disk_usage_analyzer_stuck,
       cert_expires_soon,cert_expired,memory_threshold,history_size_warning,
       stuck_rebalance,memcached_connections]},
     {pop_up_alerts,
      [auto_failover_node,auto_failover_maximum_reached,
       auto_failover_other_nodes_down,auto_failover_cluster_too_small,
       auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
       ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
       indexer_low_resident_percentage,ep_clock_cas_drift_threshold_exceeded,
       communication_issue,time_out_of_sync,disk_usage_analyzer_stuck,
       cert_expires_soon,cert_expired,memory_threshold,history_size_warning,
       stuck_rebalance,memcached_connections]}]},
   {cluster_compat_mode,undefined},
   {client_cert_auth,[{state,"disable"},{prefixes,[]}]},
   {cbas_memory_quota,1549},
   {buckets,[{configs,[]}]},
   {auto_failover_cfg,
    [{enabled,true},
     {timeout,120},
     {count,0},
     {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
     {failover_server_group,false},
     {max_count,1},
     {failed_over_server_groups,[]},
     {can_abort_rebalance,true}]},
   {audit,
    [{auditd_enabled,false},
     {rotate_interval,86400},
     {rotate_size,20971520},
     {prune_age,0},
     {disabled,[]},
     {enabled,[]},
     {disabled_users,[]},
     {sync,[]},
     {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
   {alert_limits,
    [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]}]],
 ns_config_default,
 {ns_config_default,encrypt_and_save,[]},
 <0.2422.0>,false,<<"80960cc3730024bccd4fc49444efdc14">>,
 #Fun<ns_config.18.54949477>}
[ns_server:debug,2025-05-15T18:47:26.040Z,ns_1@db3.lan:ns_cookie_manager<0.238.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[user:info,2025-05-15T18:47:26.040Z,ns_1@db3.lan:ns_cookie_manager<0.238.0>:ns_cookie_manager:do_cookie_sync:122]Node 'ns_1@db3.lan' synchronized otp cookie {sanitized,
                                             <<"biSYTaQUFDVndTs/b+IcDmD2L0woktv9naRdv3GCK6A=">>} from cluster
[ns_server:debug,2025-05-15T18:47:26.040Z,ns_1@db3.lan:ns_cluster<0.273.0>:ns_cluster:perform_actual_join:1622]Trying to connect to node 'ns_1@172.19.0.4'...
[error_logger:info,2025-05-15T18:47:26.041Z,ns_1@db3.lan:net_kernel<0.2253.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{connect,normal,'ns_1@172.19.0.4'}}
[ns_server:debug,2025-05-15T18:47:26.042Z,ns_1@db3.lan:net_kernel<0.2253.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'ns_1@172.19.0.4' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:26.042Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3529285563.3650617354.3122>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:47:26.042Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3529285563.3650617354.3122>,
                                  inet_tcp_dist,<0.2423.0>,
                                  #Ref<0.3529285563.3650617354.3125>}
[cluster:debug,2025-05-15T18:47:26.043Z,ns_1@db3.lan:ns_cluster<0.273.0>:ns_cluster:perform_actual_join:1632]Connection from 'ns_1@db3.lan' to 'ns_1@172.19.0.4':  true
[ns_server:debug,2025-05-15T18:47:26.043Z,ns_1@db3.lan:chronicle_local<0.239.0>:chronicle_local:handle_call:89]Joining cluster. Info: #{committed_seqno => 24,compat_version => 0,
                         config =>
                          {log_entry,<<"5aab03dbecad06b50ffb474da59a00c9">>,
                           {4,'ns_1@172.19.0.4'},
                           24,
                           {config,
                            {<<"1dd5991463d519e23729dda2c80c376b">>,0,3},
                            0,<<"45724915daa2a489c0a6816d20198585">>,
                            #{'ns_1@172.19.0.4' =>
                               #{id => <<"1dd5991463d519e23729dda2c80c376b">>,
                                 role => voter},
                              'ns_1@db2.lan' =>
                               #{id => <<"9f6894b51ffa497cdfc8b792d9e6726b">>,
                                 role => replica},
                              'ns_1@db3.lan' =>
                               #{id => <<"659ec03c812f5cc9456bd71fe3c1a047">>,
                                 role => replica}},
                            undefined,
                            #{chronicle_config_rsm =>
                               {rsm_config,chronicle_config_rsm,[]},
                              kv => {rsm_config,chronicle_kv,[]}},
                            #{},undefined,
                            [{<<"5aab03dbecad06b50ffb474da59a00c9">>,0}]}},
                         history_id => <<"5aab03dbecad06b50ffb474da59a00c9">>}
[chronicle:info,2025-05-15T18:47:26.046Z,ns_1@db3.lan:chronicle_leader<0.2420.0>:chronicle_leader:handle_provisioned:475]System became provisioned.
[error_logger:info,2025-05-15T18:47:26.046Z,ns_1@db3.lan:chronicle_secondary_restartable_sup<0.2426.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_secondary_restartable_sup}
    started: [{pid,<0.2427.0>},
              {id,chronicle_status},
              {mfargs,{chronicle_status,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:26.046Z,ns_1@db3.lan:chronicle_secondary_restartable_sup<0.2426.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_secondary_restartable_sup}
    started: [{pid,<0.2428.0>},
              {id,chronicle_failover},
              {mfargs,{chronicle_failover,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:26.046Z,ns_1@db3.lan:<0.255.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.255.0>,dynamic_supervisor}
    started: [{pid,<0.2426.0>},
              {id,chronicle_secondary_restartable_sup},
              {mfargs,{chronicle_secondary_restartable_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:26.046Z,ns_1@db3.lan:<0.255.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.255.0>,dynamic_supervisor}
    started: [{pid,<0.2429.0>},
              {id,chronicle_server},
              {mfargs,{chronicle_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[chronicle:info,2025-05-15T18:47:26.047Z,ns_1@db3.lan:chronicle_config_rsm<0.2433.0>:chronicle_rsm:get_incarnation:1594]No incarnation file found at '/opt/couchbase/var/lib/couchbase/config/chronicle/rsms/chronicle_config_rsm/incarnation'
[chronicle:debug,2025-05-15T18:47:26.048Z,ns_1@db3.lan:chronicle_server<0.2429.0>:chronicle_server:handle_register_rsm:361]Registering RSM chronicle_config_rsm with pid <0.2433.0>
[error_logger:info,2025-05-15T18:47:26.048Z,ns_1@db3.lan:<0.2432.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2432.0>,chronicle_single_rsm_sup}
    started: [{pid,<0.2433.0>},
              {id,chronicle_config_rsm},
              {mfargs,{chronicle_rsm,start_link,
                                     [chronicle_config_rsm,
                                      <<"659ec03c812f5cc9456bd71fe3c1a047">>,
                                      chronicle_config_rsm,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:26.048Z,ns_1@db3.lan:<0.2431.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2431.0>,dynamic_supervisor}
    started: [{pid,<0.2432.0>},
              {id,chronicle_config_rsm},
              {mfargs,
                  {chronicle_single_rsm_sup,start_link,
                      [chronicle_config_rsm,
                       <<"659ec03c812f5cc9456bd71fe3c1a047">>,
                       chronicle_config_rsm,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:26.049Z,ns_1@db3.lan:<0.2436.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2436.0>,chronicle_single_rsm_sup}
    started: [{pid,<0.2437.0>},
              {id,'kv-events'},
              {mfargs,{gen_event,start_link,[{local,'kv-events'}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[chronicle:info,2025-05-15T18:47:26.049Z,ns_1@db3.lan:kv<0.2438.0>:chronicle_rsm:get_incarnation:1594]No incarnation file found at '/opt/couchbase/var/lib/couchbase/config/chronicle/rsms/kv/incarnation'
[error_logger:info,2025-05-15T18:47:26.050Z,ns_1@db3.lan:<0.2436.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2436.0>,chronicle_single_rsm_sup}
    started: [{pid,<0.2438.0>},
              {id,kv},
              {mfargs,{chronicle_rsm,start_link,
                                     [kv,
                                      <<"659ec03c812f5cc9456bd71fe3c1a047">>,
                                      chronicle_kv,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:26.050Z,ns_1@db3.lan:<0.2431.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2431.0>,dynamic_supervisor}
    started: [{pid,<0.2436.0>},
              {id,kv},
              {mfargs,
                  {chronicle_single_rsm_sup,start_link,
                      [kv,<<"659ec03c812f5cc9456bd71fe3c1a047">>,chronicle_kv,
                       []]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:26.050Z,ns_1@db3.lan:<0.255.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.255.0>,dynamic_supervisor}
    started: [{pid,<0.2430.0>},
              {id,chronicle_rsm_sup},
              {mfargs,{chronicle_rsm_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[chronicle:debug,2025-05-15T18:47:26.050Z,ns_1@db3.lan:chronicle_server<0.2429.0>:chronicle_server:handle_register_rsm:361]Registering RSM kv with pid <0.2438.0>
[ns_server:debug,2025-05-15T18:47:26.050Z,ns_1@db3.lan:tombstone_keeper<0.277.0>:tombstone_keeper:handle_call:49]Refreshed with timestamps []
[ns_server:info,2025-05-15T18:47:26.051Z,ns_1@db3.lan:ns_cluster<0.273.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@172.19.0.4'
[ns_server:debug,2025-05-15T18:47:26.053Z,ns_1@db3.lan:ns_cluster<0.273.0>:ns_config:log_conflict:1423]Conflicting configuration changes to field nodes_wanted:
[] and
['ns_1@db3.lan','ns_1@172.19.0.4'], choosing the former.
[ns_server:debug,2025-05-15T18:47:26.053Z,ns_1@db3.lan:ns_cluster<0.273.0>:ns_config:log_conflict:1423]Conflicting configuration changes to field scramsha_fallback_salt:
<<55,187,62,55,248,32,6,50,18,123,218,116>> and
<<213,116,154,49,54,134,239,240,55,23,111,63>>, choosing the former.
[ns_server:debug,2025-05-15T18:47:26.053Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
nodes_wanted ->
[]
[ns_server:debug,2025-05-15T18:47:26.053Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
scramsha_fallback_salt ->
<<55,187,62,55,248,32,6,50,18,123,218,116>>
[ns_server:debug,2025-05-15T18:47:26.055Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
audit_decriptors ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554009}}]},
 {8243,
  [{name,<<"mutate document">>},
   {description,<<"Document was mutated via the REST API">>},
   {enabled,true},
   {module,ns_server}]},
 {8255,
  [{name,<<"read document">>},
   {description,<<"Document was read via the REST API">>},
   {enabled,false},
   {module,ns_server}]},
 {8257,
  [{name,<<"alert email sent">>},
   {description,<<"An alert email was successfully sent">>},
   {enabled,true},
   {module,ns_server}]},
 {8265,
  [{name,<<"RBAC information retrieved">>},
   {description,<<"RBAC information was retrieved">>},
   {enabled,true},
   {module,ns_server}]},
 {16384,
  [{name,<<"remote cluster ref creation">>},
   {description,<<"created remote cluster ref">>},
   {enabled,true},
   {module,xdcr}]},
 {16385,
  [{name,<<"remote cluster ref update">>},
   {description,<<"updated remote cluster ref">>},
   {enabled,true},
   {module,xdcr}]},
 {16386,
  [{name,<<"remote cluster ref deletion">>},
   {description,<<"deleted remote cluster ref">>},
   {enabled,true},
   {module,xdcr}]},
 {16387,
  [{name,<<"replication creation">>},
   {description,<<"created replication">>},
   {enabled,true},
   {module,xdcr}]},
 {16388,
  [{name,<<"replication pause">>},
   {description,<<"paused replication">>},
   {enabled,true},
   {module,xdcr}]},
 {16389,
  [{name,<<"replication resume">>},
   {description,<<"resumed replication">>},
   {enabled,true},
   {module,xdcr}]},
 {16390,
  [{name,<<"replication cancellation">>},
   {description,<<"canceled replication">>},
   {enabled,true},
   {module,xdcr}]},
 {16391,
  [{name,<<"default replication settings update">>},
   {description,<<"updated default replication settings">>},
   {enabled,true},
   {module,xdcr}]},
 {16392,
  [{name,<<"individual replication settings update">>},
   {description,<<"updated individual replication settings">>},
   {enabled,true},
   {module,xdcr}]},
 {16393,
  [{name,<<"bucket settings update">>},
   {description,<<"updated bucket settings">>},
   {enabled,true},
   {module,xdcr}]},
 {16394,
  [{name,<<"authorization failure while adding remote cluster ref">>},
   {description,<<"failed to add remote cluster ref because of authorization failure">>},
   {enabled,true},
   {module,xdcr}]},
 {16395,
  [{name,<<"authorization failure while updating remote cluster ref">>},
   {description,<<"failed to update remote cluster ref because of authorization failure">>},
   {enabled,true},
   {module,xdcr}]},
 {16396,
  [{name,<<"access denied">>},
   {description,<<"access denied">>},
   {enabled,true},
   {module,xdcr}]},
 {20480,
  [{name,<<"opened DCP connection">>},
   {description,<<"opened DCP connection">>},
   {enabled,true},
   {module,memcached}]},
 {20482,
  [{name,<<"external memcached bucket flush">>},
   {description,<<"External user flushed the content of a memcached bucket">>},
   {enabled,true},
   {module,memcached}]},
 {20483,
  [{name,<<"invalid packet">>},
   {description,<<"Rejected an invalid packet">>},
   {enabled,true},
   {module,memcached}]},
 {20485,
  [{name,<<"authentication succeeded">>},
   {description,<<"Authentication to the cluster succeeded">>},
   {enabled,false},
   {module,memcached}]},
 {20488,
  [{name,<<"document read">>},
   {description,<<"Document was read">>},
   {enabled,false},
   {module,memcached}]},
 {20489,
  [{name,<<"document locked">>},
   {description,<<"Document was locked">>},
   {enabled,false},
   {module,memcached}]},
 {20490,
  [{name,<<"document modify">>},
   {description,<<"Document was modified">>},
   {enabled,false},
   {module,memcached}]},
 {20491,
  [{name,<<"document delete">>},
   {description,<<"Document was deleted">>},
   {enabled,false},
   {module,memcached}]},
 {20492,
  [{name,<<"select bucket">>},
   {description,<<"The specified bucket was selected">>},
   {enabled,true},
   {module,memcached}]},
 {20493,
  [{name,<<"session terminated">>},
   {description,<<"Session to the cluster has terminated">>},
   {enabled,false},
   {module,memcached}]},
 {20494,
  [{name,<<"tenant rate limited">>},
   {description,<<"The given tenant was rate limited">>},
   {enabled,true},
   {module,memcached}]},
 {24576,
  [{name,<<"Delete index">>},
   {description,<<"FTS index was deleted">>},
   {enabled,true},
   {module,fts}]},
 {24577,
  [{name,<<"Create/Update index">>},
   {description,<<"FTS index was created/Updated">>},
   {enabled,true},
   {module,fts}]},
 {24579,
  [{name,<<"Control index">>},
   {description,<<"FTS index control command was issued">>},
   {enabled,true},
   {module,fts}]},
 {24582,
  [{name,<<"GC run">>},
   {description,<<"GC run was triggered">>},
   {enabled,true},
   {module,fts}]},
 {24583,
  [{name,<<"CPU profile">>},
   {description,<<"CPU profiling was started">>},
   {enabled,true},
   {module,fts}]},
 {24584,
  [{name,<<"Memory profile">>},
   {description,<<"Memory profiling was started">>},
   {enabled,true},
   {module,fts}]},
 {28672,
  [{name,<<"SELECT statement">>},
   {description,<<"A N1QL SELECT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28673,
  [{name,<<"EXPLAIN statement">>},
   {description,<<"A N1QL EXPLAIN statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28674,
  [{name,<<"PREPARE statement">>},
   {description,<<"A N1QL PREPARE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28675,
  [{name,<<"INFER statement">>},
   {description,<<"A N1QL INFER statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28676,
  [{name,<<"INSERT statement">>},
   {description,<<"A N1QL INSERT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28677,
  [{name,<<"UPSERT statement">>},
   {description,<<"A N1QL UPSERT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28678,
  [{name,<<"DELETE statement">>},
   {description,<<"A N1QL DELETE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28679,
  [{name,<<"UPDATE statement">>},
   {description,<<"A N1QL UPDATE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28680,
  [{name,<<"MERGE statement">>},
   {description,<<"A N1QL MERGE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28681,
  [{name,<<"CREATE INDEX statement">>},
   {description,<<"A N1QL CREATE INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28682,
  [{name,<<"DROP INDEX statement">>},
   {description,<<"A N1QL DROP INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28683,
  [{name,<<"ALTER INDEX statement">>},
   {description,<<"A N1QL ALTER INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28684,
  [{name,<<"BUILD INDEX statement">>},
   {description,<<"A N1QL BUILD INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28685,
  [{name,<<"GRANT ROLE statement">>},
   {description,<<"A N1QL GRANT ROLE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28686,
  [{name,<<"REVOKE ROLE statement">>},
   {description,<<"A N1QL REVOKE ROLE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28687,
  [{name,<<"UNRECOGNIZED statement">>},
   {description,<<"An unrecognized statement was received by the N1QL query engine">>},
   {enabled,false},
   {module,n1ql}]},
 {28688,
  [{name,<<"CREATE PRIMARY INDEX statement">>},
   {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28689,
  [{name,<<"/admin/stats API request">>},
   {description,<<"An HTTP request was made to the API at /admin/stats.">>},
   {enabled,false},
   {module,n1ql}]},
 {28690,
  [{name,<<"/admin/vitals API request">>},
   {description,<<"An HTTP request was made to the API at /admin/vitals.">>},
   {enabled,false},
   {module,n1ql}]},
 {28691,
  [{name,<<"/admin/prepareds API request">>},
   {description,<<"An HTTP request was made to the API at /admin/prepareds.">>},
   {enabled,false},
   {module,n1ql}]},
 {28692,
  [{name,<<"/admin/active_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/active_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28693,
  [{name,<<"/admin/indexes/prepareds API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
   {enabled,false},
   {module,n1ql}]},
 {28694,
  [{name,<<"/admin/indexes/active_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28695,
  [{name,<<"/admin/indexes/completed_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28697,
  [{name,<<"/admin/ping API request">>},
   {description,<<"An HTTP request was made to the API at /admin/ping.">>},
   {enabled,false},
   {module,n1ql}]},
 {28698,
  [{name,<<"/admin/config API request">>},
   {description,<<"An HTTP request was made to the API at /admin/config.">>},
   {enabled,false},
   {module,n1ql}]},
 {28699,
  [{name,<<"/admin/ssl_cert API request">>},
   {description,<<"An HTTP request was made to the API at /admin/ssl_cert.">>},
   {enabled,false},
   {module,n1ql}]},
 {28700,
  [{name,<<"/admin/settings API request">>},
   {description,<<"An HTTP request was made to the API at /admin/settings.">>},
   {enabled,false},
   {module,n1ql}]},
 {28701,
  [{name,<<"/admin/clusters API request">>},
   {description,<<"An HTTP request was made to the API at /admin/clusters.">>},
   {enabled,false},
   {module,n1ql}]},
 {28702,
  [{name,<<"/admin/completed_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/completed_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28704,
  [{name,<<"/admin/functions API request">>},
   {description,<<"An HTTP request was made to the API at /admin/functions.">>},
   {enabled,false},
   {module,n1ql}]},
 {28705,
  [{name,<<"/admin/indexes/functions API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/functions.">>},
   {enabled,false},
   {module,n1ql}]},
 {28706,
  [{name,<<"CREATE FUNCTION statement">>},
   {description,<<"A N1QL CREATE FUNCTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28707,
  [{name,<<"DROP FUNCTION statement">>},
   {description,<<"A N1QL DROP FUNCTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28708,
  [{name,<<"EXECUTE FUNCTION statement">>},
   {description,<<"A N1QL EXECUTE FUNCTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28709,
  [{name,<<"/admin/tasks API request">>},
   {description,<<"An HTTP request was made to the API at /admin/tasks.">>},
   {enabled,false},
   {module,n1ql}]},
 {28710,
  [{name,<<"/admin/indexes/tasks API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/tasks.">>},
   {enabled,false},
   {module,n1ql}]},
 {28711,
  [{name,<<"/admin/dictionary_cache API request">>},
   {description,<<"An HTTP request was made to the API at /admin/dictionary_cache.">>},
   {enabled,false},
   {module,n1ql}]},
 {28712,
  [{name,<<"/admin/indexes/dictionary_cache API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/dictionary_cache.">>},
   {enabled,false},
   {module,n1ql}]},
 {28713,
  [{name,<<"CREATE SCOPE statement">>},
   {description,<<"A N1QL CREATE SCOPE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28714,
  [{name,<<"DROP SCOPE statement">>},
   {description,<<"A N1QL DROP SCOPE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28715,
  [{name,<<"CREATE COLLECTION statement">>},
   {description,<<"A N1QL CREATE COLLECTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28716,
  [{name,<<"DROP COLLECTION statement">>},
   {description,<<"A N1QL DROP COLLECTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28717,
  [{name,<<"FLUSH COLLECTION statement">>},
   {description,<<"A N1QL FLUSH COLLECTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28718,
  [{name,<<"UPDATE STATISTICS statement">>},
   {description,<<"A N1QL UPDATE STATISTICS statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28719,
  [{name,<<"ADVISE statement">>},
   {description,<<"A N1QL ADVISE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28720,
  [{name,<<"START TRANSACTION statement">>},
   {description,<<"A N1QL START TRANSACTION statement was execu"...>>},
   {enabled,false},
   {module,n1ql}]},
 {28721,
  [{name,<<"COMMIT TRANSACTION statement">>},
   {description,<<"A N1QL COMMIT TRANSACTION statement was "...>>},
   {enabled,false},
   {module,n1ql}]},
 {28722,
  [{name,<<"ROLLBACK TRANSACTION statement">>},
   {description,<<"A N1QL ROLLBACK TRANSACTION statemen"...>>},
   {enabled,false},
   {module,n1ql}]},
 {28723,
  [{name,<<"ROLLBACK TRANSACTION TO SAVEPOINT st"...>>},
   {description,<<"A N1QL ROLLBACK TRANSACTION TO S"...>>},
   {enabled,false},
   {module,n1ql}]},
 {28724,
  [{name,<<"SET TRANSACTION ISOLATION statem"...>>},
   {description,<<"A N1QL SET TRANSACTION ISOLA"...>>},
   {enabled,false},
   {module,n1ql}]},
 {28725,
  [{name,<<"SAVEPOINT statement">>},
   {description,<<"A N1QL SAVEPOINT stateme"...>>},
   {enabled,false},
   {module,n1ql}]},
 {28726,
  [{name,<<"/admin/transactions API "...>>},
   {description,<<"An HTTP request was "...>>},
   {enabled,false},
   {module,n1ql}]},
 {28727,
  [{name,<<"/admin/indexes/trans"...>>},
   {description,<<"An HTTP request "...>>},
   {enabled,false},
   {module,n1ql}]},
 {28728,
  [{name,<<"N1QL backup / re"...>>},
   {description,<<"An HTTP requ"...>>},
   {enabled,false},
   {module,n1ql}]},
 {28729,
  [{name,<<"/admin/shutd"...>>},
   {description,<<"An HTTP "...>>},
   {enabled,false},
   {module,n1ql}]},
 {28730,
  [{name,<<"/admin/g"...>>},
   {description,<<"An H"...>>},
   {enabled,false},
   {module,...}]},
 {28731,[{name,<<"/adm"...>>},{description,<<...>>},{enabled,...},{...}]},
 {28732,[{name,<<...>>},{description,...},{...}|...]},
 {28733,[{name,...},{...}|...]},
 {28734,[{...}|...]},
 {28735,[...]},
 {28736,...},
 {...}|...]
[ns_server:debug,2025-05-15T18:47:26.055Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
auto_failover_cfg ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{3,63914554009}}]},
 {enabled,true},
 {timeout,120},
 {count,0},
 {max_count,1},
 {disable_max_count,false},
 {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
 {failover_server_group,false},
 {failed_over_server_groups,[]},
 {can_abort_rebalance,true},
 {failover_preserve_durability_majority,false}]
[ns_server:debug,2025-05-15T18:47:26.055Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
cbas_memory_quota ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554041}}]}|1024]
[ns_server:debug,2025-05-15T18:47:26.056Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
cluster_compat_version ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{3,63914554009}}]},7,6]
[ns_server:debug,2025-05-15T18:47:26.056Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
memory_quota ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554041}}]}|3072]
[ns_server:debug,2025-05-15T18:47:26.056Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
otp ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554009}}]},
 {cookie,{sanitized,<<"biSYTaQUFDVndTs/b+IcDmD2L0woktv9naRdv3GCK6A=">>}}]
[ns_server:debug,2025-05-15T18:47:26.056Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
quorum_nodes ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554042}}]},
 'ns_1@172.19.0.4']
[ns_server:debug,2025-05-15T18:47:26.056Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
rbac_upgrade ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554009}}]}|
 '_deleted']
[ns_server:debug,2025-05-15T18:47:26.056Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
resource_management ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554009}}]},
 {bucket,[{resident_ratio,[{enabled,false},
                           {couchstore_minimum,1},
                           {magma_minimum,0.2}]},
          {data_size,[{enabled,false},
                      {couchstore_maximum,2},
                      {magma_maximum,16}]}]},
 {index,[]},
 {cores_per_bucket,[{enabled,false},{minimum,0.4}]},
 {disk_usage,[{enabled,false},{maximum,96}]},
 {collections_per_quota,[{enabled,false},{maximum,1}]}]
[ns_server:debug,2025-05-15T18:47:26.056Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
rest_creds ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554041}}]}|
 {"<ud>jaba_admin</ud>",
  {auth,
   [{<<"hash">>,
     {[{<<"hashes">>,
        {sanitized,<<"3I14lVC6WTo2HkE1Ma2OkSoP1R5UwTP501YfMTKM7ak=">>}},
       {<<"algorithm">>,<<"argon2id">>},
       {<<"salt">>,<<"5CFoS4MqJh4VclPVzeJxBw==">>},
       {<<"parallelism">>,1},
       {<<"time">>,3},
       {<<"memory">>,524288}]}},
    {<<"scram-sha-512">>,
     {[{<<"salt">>,
        <<"pulx/Jw0I8WxLIxMcCD8CqAxiVNfHNaHWpBI6wjnArfbB+WyxXwBuk5SnOGA7W5LgTZZWGevGaY3ih1tGH4jeg==">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"+8aFWL6Fwfs8HKehUbj7ZkLi5LW9/xRhbkbg/BwGA9w=">>}}]}},
    {<<"scram-sha-256">>,
     {[{<<"salt">>,<<"6mOH6zwaMRxGOfTsFTCuTxfMVctxE7w9qyktmzrJtVg=">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"t9IO+WS5kijXIacgFn1gB5R0bN61IAKLCL/JWibrJSA=">>}}]}},
    {<<"scram-sha-1">>,
     {[{<<"salt">>,<<"pEmVjw1zOFSsjZc/8lZKrEstBkA=">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"tJt6Wglz80zHvML1mQcTdrXmQggip4HdkH+YfamhrEc=">>}}]}}]}}]
[ns_server:debug,2025-05-15T18:47:26.056Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
settings ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554041}}]},
 {stats,[{send_stats,true}]}]
[ns_server:debug,2025-05-15T18:47:26.056Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
uuid ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554041}}]}|
 <<"61933a8028482692b0278a91060e0d90">>]
[ns_server:debug,2025-05-15T18:47:26.056Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/analytics/settings/config">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554009}}]}|
 <<"{\"analytics.settings.blob_storage_bucket\":\"\",\"analytics.settings.blob_storage_prefix\":\"\",\"analytics.settings.blob_storage_region\":\"\",\"analytics.settings.blob_storage_scheme\":\"\",\"analytics.settings.num_replicas\":0}">>]
[ns_server:debug,2025-05-15T18:47:26.056Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/indexing/rebalance/RebalanceToken">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554045}}]}|
 <<"{\"MasterId\":\"28569ac00b9c1d7c50e39741027d428c\",\"RebalId\":\"35d452100b72fa9639f18b2cc28124e2\",\"Source\":0,\"Error\":\"\",\"MasterIP\":\"172.19.0.4\",\"Version\":0,\"RebalPhase\":0,\"ActiveRebalancer\":1}">>]
[ns_server:debug,2025-05-15T18:47:26.056Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{3,63914554045}}]}|
 <<"{\"indexer.plasma.backIndex.enablePageBloomFilter\":false,\"indexer.settings.compaction.abort_exceed_interval\":false,\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.en"...>>]
[ns_server:debug,2025-05-15T18:47:26.056Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/query/functions_cache/counter">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554042}}]}|
 <<"[172.19.0.4:8091]0">>]
[ns_server:debug,2025-05-15T18:47:26.056Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/query/sequences_cache/revision">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554009}}]}|
 <<"0">>]
[ns_server:debug,2025-05-15T18:47:26.056Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/query/settings/config">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554009}}]}|
 <<"{\"cleanupclientattempts\":true,\"cleanuplostattempts\":true,\"cleanupwindow\":\"60s\",\"completed-limit\":4000,\"completed-max-plan-size\":262144,\"completed-threshold\":1000,\"loglevel\":\"info\",\"max-parallelism\":1,\"memory-quota\":0,\"n1ql-feat-ctrl\":76,\"node-quota\":0,\"node-quota-val-percent\":67,\"num-cpus\":0,\"numatrs\":1024,\"pipeline-batch\":16,\"pipeline-cap\":512,\"prepared-limit\":16384,\"query.settings.cu"...>>]
[ns_server:debug,2025-05-15T18:47:26.057Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',address_family} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|inet]
[ns_server:debug,2025-05-15T18:47:26.057Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',audit} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}]
[ns_server:debug,2025-05-15T18:47:26.057Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',backup_grpc_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9124]
[ns_server:debug,2025-05-15T18:47:26.057Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',backup_http_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|8097]
[ns_server:debug,2025-05-15T18:47:26.057Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',backup_https_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|18097]
[ns_server:debug,2025-05-15T18:47:26.057Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',capi_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|8092]
[ns_server:debug,2025-05-15T18:47:26.057Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_admin_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9110]
[ns_server:debug,2025-05-15T18:47:26.057Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_cc_client_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9113]
[ns_server:debug,2025-05-15T18:47:26.057Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_cc_cluster_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9112]
[ns_server:debug,2025-05-15T18:47:26.057Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_cc_http_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9111]
[ns_server:debug,2025-05-15T18:47:26.057Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_cluster_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9115]
[ns_server:debug,2025-05-15T18:47:26.057Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_console_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9114]
[ns_server:debug,2025-05-15T18:47:26.057Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_data_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9116]
[ns_server:debug,2025-05-15T18:47:26.057Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_debug_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|-1]
[ns_server:debug,2025-05-15T18:47:26.057Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_dirs} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
 "/opt/couchbase/var/lib/couchbase/data"]
[ns_server:debug,2025-05-15T18:47:26.057Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_http_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|8095]
[ns_server:debug,2025-05-15T18:47:26.057Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_messaging_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9118]
[ns_server:debug,2025-05-15T18:47:26.057Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_metadata_callback_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9119]
[ns_server:debug,2025-05-15T18:47:26.057Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_metadata_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9121]
[ns_server:debug,2025-05-15T18:47:26.057Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_parent_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9122]
[ns_server:debug,2025-05-15T18:47:26.057Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_replication_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9120]
[ns_server:debug,2025-05-15T18:47:26.057Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_result_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9117]
[ns_server:debug,2025-05-15T18:47:26.057Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_ssl_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|18095]
[ns_server:debug,2025-05-15T18:47:26.058Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',client_cert} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
 {subject,<<"CN=Couchbase Internal Client (70858842)">>},
 {not_after,63985747606},
 {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
 {load_timestamp,63914554006},
 {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQ"...>>},
 {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDWTCCAkGgAwIBAgIIGD/Hv7XHejcwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ2NDZaFw0y\nNzA4MTcxODQ2NDZaMC8xLTArBgNVBAMTJENvdWNoYmFzZSBJbnRlcm5hbCBDbGll\nbnQgKDcwODU4ODQyKTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMSK\nvXvBcS/ChHK2Kb84PvDZyFlVdyCnMwkD8aug5L/C9zj5626M2/KO3H+iam8QErOa\nUj7TQ/A"...>>},
 {pkey_passphrase_settings,[]},
 {certs_epoch,0},
 {type,generated},
 {name,"@internal"}]
[ns_server:debug,2025-05-15T18:47:26.058Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',compaction_daemon} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
 {check_interval,30},
 {min_db_file_size,131072},
 {min_view_file_size,20971520}]
[ns_server:debug,2025-05-15T18:47:26.058Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',config_version} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|{7,6}]
[ns_server:debug,2025-05-15T18:47:26.058Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',database_dir} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2025-05-15T18:47:26.058Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',erl_external_listeners} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
 {inet,false}]
[ns_server:debug,2025-05-15T18:47:26.058Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',event_log} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
 {filename,"/opt/couchbase/var/lib/couchbase/event_log"}]
[ns_server:debug,2025-05-15T18:47:26.058Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',eventing_debug_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9140]
[ns_server:debug,2025-05-15T18:47:26.058Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',eventing_dir} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2025-05-15T18:47:26.058Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',eventing_http_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|8096]
[ns_server:debug,2025-05-15T18:47:26.058Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',eventing_https_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|18096]
[ns_server:debug,2025-05-15T18:47:26.058Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',fts_grpc_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9130]
[ns_server:debug,2025-05-15T18:47:26.058Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',fts_grpc_ssl_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|19130]
[ns_server:debug,2025-05-15T18:47:26.058Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',fts_http_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|8094]
[ns_server:debug,2025-05-15T18:47:26.058Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',fts_ssl_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|18094]
[ns_server:debug,2025-05-15T18:47:26.058Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',index_dir} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2025-05-15T18:47:26.059Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',indexer_admin_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9100]
[ns_server:debug,2025-05-15T18:47:26.059Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',indexer_http_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9102]
[ns_server:debug,2025-05-15T18:47:26.059Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',indexer_https_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|19102]
[ns_server:debug,2025-05-15T18:47:26.059Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',indexer_scan_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9101]
[ns_server:debug,2025-05-15T18:47:26.059Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',indexer_stcatchup_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9104]
[ns_server:debug,2025-05-15T18:47:26.059Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',indexer_stinit_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9103]
[ns_server:debug,2025-05-15T18:47:26.059Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',indexer_stmaint_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9105]
[ns_server:debug,2025-05-15T18:47:26.059Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',is_enterprise} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|true]
[ns_server:debug,2025-05-15T18:47:26.059Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',isasl} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
 {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]
[ns_server:debug,2025-05-15T18:47:26.060Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',memcached} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
 {port,11210},
 {dedicated_port,11209},
 {dedicated_ssl_port,11206},
 {ssl_port,11207},
 {admin_user,"@ns_server"},
 {other_users,["@cbq-engine","@projector","@goxdcr","@index","@fts",
               "@eventing","@cbas","@backup"]},
 {admin_pass,"*****"},
 {engines,[{membase,[{engine,"/opt/couchbase/lib/memcached/ep.so"},
                     {static_config_string,"failpartialwarmup=false"}]},
           {memcached,[{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
                       {static_config_string,"vb0=true"}]}]},
 {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
 {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
 {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
 {log_prefix,"memcached.log"},
 {log_generations,20},
 {log_cyclesize,10485760},
 {log_rotation_period,39003}]
[ns_server:debug,2025-05-15T18:47:26.060Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',memcached_config} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
 {[{interfaces,{memcached_config_mgr,get_interfaces,[]}},
   {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
   {connection_idle_time,connection_idle_time},
   {privilege_debug,privilege_debug},
   {breakpad,
    {[{enabled,breakpad_enabled},
      {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
   {deployment_model,{memcached_config_mgr,get_config_profile,[]}},
   {verbosity,verbosity},
   {audit_file,{"~s",[audit_file]}},
   {rbac_file,{"~s",[rbac_file]}},
   {dedupe_nmvb_maps,dedupe_nmvb_maps},
   {tracing_enabled,tracing_enabled},
   {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
   {xattr_enabled,true},
   {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
   {collections_enabled,true},
   {max_connections,max_connections},
   {system_connections,system_connections},
   {num_reader_threads,num_reader_threads},
   {num_writer_threads,num_writer_threads},
   {num_auxio_threads,num_auxio_threads},
   {num_nonio_threads,num_nonio_threads},
   {num_storage_threads,num_storage_threads},
   {logger,
    {[{filename,{"~s/~s",[log_path,log_prefix]}},{cyclesize,log_cyclesize}]}},
   {external_auth_service,{memcached_config_mgr,get_external_auth_service,[]}},
   {active_external_users_push_interval,
    {memcached_config_mgr,get_external_users_push_interval,[]}},
   {prometheus,{memcached_config_mgr,prometheus_cfg,[]}},
   {sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
   {ssl_sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
   {tcp_keepalive_idle,tcp_keepalive_idle},
   {tcp_keepalive_interval,tcp_keepalive_interval},
   {tcp_keepalive_probes,tcp_keepalive_probes},
   {tcp_user_timeout,tcp_user_timeout},
   {always_collect_trace_info,always_collect_trace_info},
   {connection_limit_mode,connection_limit_mode},
   {free_connection_pool_size,free_connection_pool_size},
   {max_client_connection_details,max_client_connection_details}]}]
[ns_server:debug,2025-05-15T18:47:26.060Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',memcached_dedicated_ssl_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|11206]
[ns_server:debug,2025-05-15T18:47:26.061Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',memcached_defaults} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
 {max_connections,65000},
 {system_connections,5000},
 {connection_idle_time,0},
 {verbosity,0},
 {privilege_debug,false},
 {breakpad_enabled,true},
 {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
 {dedupe_nmvb_maps,false},
 {je_malloc_conf,undefined},
 {tracing_enabled,true},
 {datatype_snappy,true},
 {num_reader_threads,<<"default">>},
 {num_writer_threads,<<"default">>},
 {num_auxio_threads,<<"default">>},
 {num_nonio_threads,<<"default">>},
 {num_storage_threads,<<"default">>},
 {tcp_keepalive_idle,360},
 {tcp_keepalive_interval,10},
 {tcp_keepalive_probes,3},
 {tcp_user_timeout,30},
 {always_collect_trace_info,true},
 {connection_limit_mode,<<"disconnect">>},
 {free_connection_pool_size,0},
 {max_client_connection_details,0}]
[ns_server:debug,2025-05-15T18:47:26.061Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',memcached_prometheus} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|11280]
[ns_server:debug,2025-05-15T18:47:26.061Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',n2n_client_cert_auth} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|false]
[ns_server:debug,2025-05-15T18:47:26.061Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',node_cert} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{3,63914554042}}]},
 {subject,<<"CN=Couchbase Server Node (172.19.0.4)">>},
 {not_after,63985747642},
 {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
 {load_timestamp,63914554042},
 {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQ"...>>},
 {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDOjCCAiKgAwIBAgIIGD/Hx/dYFPowDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjJaFw0y\nNzA4MTcxODQ3MjJaMC0xKzApBgNVBAMTIkNvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nMTcyLjE5LjAuNCkwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQD6Fik5\nZTKKG0O216KNVrKyjS2SrdB4IermP3HNcmWnhkrwz0Ppm9expYhmviu+eAmFSeOK\n3LRZtjU"...>>},
 {pkey_passphrase_settings,[]},
 {certs_epoch,0},
 {type,generated},
 {hostname,"172.19.0.4"}]
[ns_server:debug,2025-05-15T18:47:26.061Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',node_encryption} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|false]
[ns_server:debug,2025-05-15T18:47:26.061Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',ns_log} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
 {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]
[ns_server:debug,2025-05-15T18:47:26.061Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',port_servers} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}]
[ns_server:debug,2025-05-15T18:47:26.061Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',projector_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9999]
[ns_server:debug,2025-05-15T18:47:26.061Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',projector_ssl_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9999]
[ns_server:debug,2025-05-15T18:47:26.061Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',prometheus_auth_info} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{3,63914554042}}]}|
 {"@prometheus",
  {auth,
   [{<<"hash">>,
     {[{<<"hashes">>,
        {sanitized,<<"ocCjmLXobw413zJaq2v5hbk6F9JaoNKMHAEhwOXAF/k=">>}},
       {<<"algorithm">>,<<"argon2id">>},
       {<<"salt">>,<<"FlBs6+3flsH1DlD+dLEwjg==">>},
       {<<"parallelism">>,1},
       {<<"time">>,3},
       {<<"memory">>,524288}]}},
    {<<"scram-sha-512">>,
     {[{<<"salt">>,
        <<"F/Zqupd2IseMgxcuAf9fe4gpX0DBCFm1/fWg3iuaAuQdnTMBGYBMeqXKzW13cDp+Q9KGShsd3l23qMAKTVwiVA==">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"fNgWati8g2faSgrPjZUuj7XSR1xhSOtSh9nzY5vi8O4=">>}}]}},
    {<<"scram-sha-256">>,
     {[{<<"salt">>,<<"OKVHe2p1Uu/nuWHnr4hDmY1YmTEi+LTh2QSQmLrl6JU=">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"wJZdBxOuuYIylriwbbRV9JdvCNKKT1kZtZR+FOCJUQg=">>}}]}},
    {<<"scram-sha-1">>,
     {[{<<"salt">>,<<"FmlE6xOIeEZ8k+MADCxX6iijEW8=">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"gfRT+N7lHhRndbdyfaO+d/dOfRbaCQl3VgGgatuBZck=">>}}]}}]}}]
[ns_server:debug,2025-05-15T18:47:26.061Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',prometheus_http_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9123]
[ns_server:debug,2025-05-15T18:47:26.061Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',query_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|8093]
[ns_server:debug,2025-05-15T18:47:26.061Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',rest} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
 {port,8091},
 {port_meta,global}]
[ns_server:debug,2025-05-15T18:47:26.062Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',saslauthd_enabled} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|true]
[ns_server:debug,2025-05-15T18:47:26.062Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',ssl_capi_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|18092]
[ns_server:debug,2025-05-15T18:47:26.062Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',ssl_query_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|18093]
[ns_server:debug,2025-05-15T18:47:26.062Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',ssl_rest_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|18091]
[ns_server:debug,2025-05-15T18:47:26.062Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',uuid} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
 <<"28569ac00b9c1d7c50e39741027d428c">>]
[ns_server:debug,2025-05-15T18:47:26.062Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',xdcr_rest_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9998]
[ns_server:debug,2025-05-15T18:47:26.062Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',{project_intact,is_vulnerable}} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|false]
[ns_server:debug,2025-05-15T18:47:26.062Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',address_family} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|inet]
[ns_server:debug,2025-05-15T18:47:26.062Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',audit} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}]
[ns_server:debug,2025-05-15T18:47:26.062Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',backup_grpc_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9124]
[ns_server:debug,2025-05-15T18:47:26.062Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',backup_http_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|8097]
[ns_server:debug,2025-05-15T18:47:26.062Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',backup_https_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|18097]
[ns_server:debug,2025-05-15T18:47:26.062Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',capi_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|8092]
[ns_server:debug,2025-05-15T18:47:26.062Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_admin_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9110]
[ns_server:debug,2025-05-15T18:47:26.062Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_cc_client_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9113]
[ns_server:debug,2025-05-15T18:47:26.062Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_cc_cluster_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9112]
[ns_server:debug,2025-05-15T18:47:26.062Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_cc_http_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9111]
[ns_server:debug,2025-05-15T18:47:26.062Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_cluster_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9115]
[ns_server:debug,2025-05-15T18:47:26.062Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_console_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9114]
[ns_server:debug,2025-05-15T18:47:26.062Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_data_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9116]
[ns_server:debug,2025-05-15T18:47:26.062Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_debug_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|-1]
[ns_server:debug,2025-05-15T18:47:26.062Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_dirs} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
 "/opt/couchbase/var/lib/couchbase/data"]
[ns_server:debug,2025-05-15T18:47:26.062Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_http_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|8095]
[ns_server:debug,2025-05-15T18:47:26.062Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_messaging_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9118]
[ns_server:debug,2025-05-15T18:47:26.062Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_metadata_callback_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9119]
[ns_server:debug,2025-05-15T18:47:26.062Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_metadata_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9121]
[ns_server:debug,2025-05-15T18:47:26.062Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_parent_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9122]
[ns_server:debug,2025-05-15T18:47:26.063Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_replication_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9120]
[ns_server:debug,2025-05-15T18:47:26.063Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_result_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9117]
[ns_server:debug,2025-05-15T18:47:26.063Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_ssl_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|18095]
[ns_server:debug,2025-05-15T18:47:26.063Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',client_cert} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{2,63914554043}}]},
 {certs_epoch,0},
 {subject,<<"CN=Couchbase Internal Client (116550952)">>},
 {not_after,63985747642},
 {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
 {load_timestamp,63914554042},
 {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hX"...>>},
 {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDWjCCAkKgAwIBAgIIGD/HyA7ClpEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjJaFw0y\nNzA4MTcxODQ3MjJaMDAxLjAsBgNVBAMTJUNvdWNoYmFzZSBJbnRlcm5hbCBDbGll\nbnQgKDExNjU1MDk1MikwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDg\nAaBAyJnrm54fDG0xlsEQH9E51WTLpkwtleiz980c498XKRPsgRWnjY920KTUIH2/\ndQc"...>>},
 {pkey_passphrase_settings,[]},
 {type,generated},
 {name,"@internal"}]
[ns_server:debug,2025-05-15T18:47:26.063Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',compaction_daemon} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
 {check_interval,30},
 {min_db_file_size,131072},
 {min_view_file_size,20971520}]
[ns_server:debug,2025-05-15T18:47:26.063Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',config_version} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|{7,6}]
[ns_server:debug,2025-05-15T18:47:26.063Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',database_dir} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2025-05-15T18:47:26.063Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',erl_external_listeners} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
 {inet,false}]
[ns_server:debug,2025-05-15T18:47:26.063Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',event_log} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
 {filename,"/opt/couchbase/var/lib/couchbase/event_log"}]
[ns_server:debug,2025-05-15T18:47:26.063Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',eventing_debug_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9140]
[ns_server:debug,2025-05-15T18:47:26.063Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',eventing_dir} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2025-05-15T18:47:26.063Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',eventing_http_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|8096]
[ns_server:debug,2025-05-15T18:47:26.063Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',eventing_https_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|18096]
[ns_server:debug,2025-05-15T18:47:26.063Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',fts_grpc_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9130]
[ns_server:debug,2025-05-15T18:47:26.063Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',fts_grpc_ssl_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|19130]
[ns_server:debug,2025-05-15T18:47:26.063Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',fts_http_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|8094]
[ns_server:debug,2025-05-15T18:47:26.063Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',fts_ssl_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|18094]
[ns_server:debug,2025-05-15T18:47:26.063Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',index_dir} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2025-05-15T18:47:26.063Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',indexer_admin_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9100]
[ns_server:debug,2025-05-15T18:47:26.063Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',indexer_http_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9102]
[ns_server:debug,2025-05-15T18:47:26.063Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',indexer_https_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|19102]
[ns_server:debug,2025-05-15T18:47:26.063Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',indexer_scan_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9101]
[ns_server:debug,2025-05-15T18:47:26.063Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',indexer_stcatchup_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9104]
[ns_server:debug,2025-05-15T18:47:26.063Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',indexer_stinit_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9103]
[ns_server:debug,2025-05-15T18:47:26.064Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',indexer_stmaint_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9105]
[ns_server:debug,2025-05-15T18:47:26.064Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',is_enterprise} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|true]
[ns_server:debug,2025-05-15T18:47:26.064Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',isasl} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
 {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]
[ns_server:debug,2025-05-15T18:47:26.064Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',membership} ->
inactiveAdded
[ns_server:debug,2025-05-15T18:47:26.064Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',memcached} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
 {port,11210},
 {dedicated_port,11209},
 {dedicated_ssl_port,11206},
 {ssl_port,11207},
 {admin_user,"@ns_server"},
 {other_users,["@cbq-engine","@projector","@goxdcr","@index","@fts",
               "@eventing","@cbas","@backup"]},
 {admin_pass,"*****"},
 {engines,[{membase,[{engine,"/opt/couchbase/lib/memcached/ep.so"},
                     {static_config_string,"failpartialwarmup=false"}]},
           {memcached,[{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
                       {static_config_string,"vb0=true"}]}]},
 {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
 {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
 {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
 {log_prefix,"memcached.log"},
 {log_generations,20},
 {log_cyclesize,10485760},
 {log_rotation_period,39003}]
[ns_server:debug,2025-05-15T18:47:26.064Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',memcached_config} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
 {[{interfaces,{memcached_config_mgr,get_interfaces,[]}},
   {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
   {connection_idle_time,connection_idle_time},
   {privilege_debug,privilege_debug},
   {breakpad,
    {[{enabled,breakpad_enabled},
      {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
   {deployment_model,{memcached_config_mgr,get_config_profile,[]}},
   {verbosity,verbosity},
   {audit_file,{"~s",[audit_file]}},
   {rbac_file,{"~s",[rbac_file]}},
   {dedupe_nmvb_maps,dedupe_nmvb_maps},
   {tracing_enabled,tracing_enabled},
   {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
   {xattr_enabled,true},
   {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
   {collections_enabled,true},
   {max_connections,max_connections},
   {system_connections,system_connections},
   {num_reader_threads,num_reader_threads},
   {num_writer_threads,num_writer_threads},
   {num_auxio_threads,num_auxio_threads},
   {num_nonio_threads,num_nonio_threads},
   {num_storage_threads,num_storage_threads},
   {logger,
    {[{filename,{"~s/~s",[log_path,log_prefix]}},{cyclesize,log_cyclesize}]}},
   {external_auth_service,{memcached_config_mgr,get_external_auth_service,[]}},
   {active_external_users_push_interval,
    {memcached_config_mgr,get_external_users_push_interval,[]}},
   {prometheus,{memcached_config_mgr,prometheus_cfg,[]}},
   {sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
   {ssl_sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
   {tcp_keepalive_idle,tcp_keepalive_idle},
   {tcp_keepalive_interval,tcp_keepalive_interval},
   {tcp_keepalive_probes,tcp_keepalive_probes},
   {tcp_user_timeout,tcp_user_timeout},
   {always_collect_trace_info,always_collect_trace_info},
   {connection_limit_mode,connection_limit_mode},
   {free_connection_pool_size,free_connection_pool_size},
   {max_client_connection_details,max_client_connection_details}]}]
[ns_server:debug,2025-05-15T18:47:26.064Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',memcached_dedicated_ssl_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|11206]
[ns_server:debug,2025-05-15T18:47:26.064Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',memcached_defaults} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
 {max_connections,65000},
 {system_connections,5000},
 {connection_idle_time,0},
 {verbosity,0},
 {privilege_debug,false},
 {breakpad_enabled,true},
 {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
 {dedupe_nmvb_maps,false},
 {je_malloc_conf,undefined},
 {tracing_enabled,true},
 {datatype_snappy,true},
 {num_reader_threads,<<"default">>},
 {num_writer_threads,<<"default">>},
 {num_auxio_threads,<<"default">>},
 {num_nonio_threads,<<"default">>},
 {num_storage_threads,<<"default">>},
 {tcp_keepalive_idle,360},
 {tcp_keepalive_interval,10},
 {tcp_keepalive_probes,3},
 {tcp_user_timeout,30},
 {always_collect_trace_info,true},
 {connection_limit_mode,<<"disconnect">>},
 {free_connection_pool_size,0},
 {max_client_connection_details,0}]
[ns_server:debug,2025-05-15T18:47:26.064Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',memcached_prometheus} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|11280]
[ns_server:debug,2025-05-15T18:47:26.064Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',n2n_client_cert_auth} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|false]
[ns_server:debug,2025-05-15T18:47:26.064Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',node_cert} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{2,63914554043}}]},
 {certs_epoch,0},
 {subject,<<"CN=Couchbase Server Node (db2.lan)">>},
 {not_after,63985747642},
 {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
 {load_timestamp,63914554042},
 {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hX"...>>},
 {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDOjCCAiKgAwIBAgIIGD/HyAMALgMwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjJaFw0y\nNzA4MTcxODQ3MjJaMCoxKDAmBgNVBAMTH0NvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nZGIyLmxhbikwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC63r7EIhIk\nOyMwE9mw7bzVJfstC9FfdZvk0bP8vxZkuhNt/bVcND8p00ihMP8FCbfsa5d8VgWC\nDoa"...>>},
 {pkey_passphrase_settings,[]},
 {type,generated},
 {hostname,"db2.lan"}]
[ns_server:debug,2025-05-15T18:47:26.064Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',node_encryption} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|false]
[ns_server:debug,2025-05-15T18:47:26.064Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',ns_log} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
 {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]
[ns_server:debug,2025-05-15T18:47:26.064Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',port_servers} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}]
[ns_server:debug,2025-05-15T18:47:26.064Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',projector_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9999]
[ns_server:debug,2025-05-15T18:47:26.064Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',projector_ssl_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9999]
[ns_server:debug,2025-05-15T18:47:26.064Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',prometheus_auth_info} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{2,63914554045}}]}|
 {"@prometheus",
  {auth,
   [{<<"hash">>,
     {[{<<"hashes">>,
        {sanitized,<<"E4iwPDExOnOjGXs0qP6la711Y5qTaST+1nzp3WWzVqw=">>}},
       {<<"algorithm">>,<<"argon2id">>},
       {<<"salt">>,<<"61T+47ADFbRDEeE8WMrAGw==">>},
       {<<"parallelism">>,1},
       {<<"time">>,3},
       {<<"memory">>,524288}]}},
    {<<"scram-sha-512">>,
     {[{<<"salt">>,
        <<"kGjfTLY3stMxG3p1BzztyO2J8qheD+hlb8euMQoyGg1cr+r9pUQyPsr32qJ3H9T4JpSJnBUEyXbjZ/tnoEGXMw==">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"KuADGEOAaLhVMSkwIKxrztvRUhXdeMaZA8c8MPeK2qw=">>}}]}},
    {<<"scram-sha-256">>,
     {[{<<"salt">>,<<"ga97SSKrWv6mIdojxSSVO+hBOALInVDadwYP0X8uqjU=">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"QaFsRlNDNHpIcZSkMUdzIYnBSJznlCdgo3U/gZzgIW8=">>}}]}},
    {<<"scram-sha-1">>,
     {[{<<"salt">>,<<"kN6RJArUekQYl2C69D1t3KNt2Zc=">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"JyPn7MPQQ8xQtBYdBohgEg3u+BTyMU7ioflt+xVGsak=">>}}]}}]}}]
[ns_server:debug,2025-05-15T18:47:26.064Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',prometheus_http_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9123]
[ns_server:debug,2025-05-15T18:47:26.064Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',query_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|8093]
[ns_server:debug,2025-05-15T18:47:26.064Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',rest} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
 {port,8091},
 {port_meta,global}]
[ns_server:debug,2025-05-15T18:47:26.064Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',saslauthd_enabled} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|true]
[ns_server:debug,2025-05-15T18:47:26.064Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',ssl_capi_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|18092]
[ns_server:debug,2025-05-15T18:47:26.064Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',ssl_query_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|18093]
[ns_server:debug,2025-05-15T18:47:26.064Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',ssl_rest_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|18091]
[ns_server:debug,2025-05-15T18:47:26.065Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',uuid} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
 <<"7a33d46fd1976e65466c5efc8b45ef2e">>]
[ns_server:debug,2025-05-15T18:47:26.065Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',xdcr_rest_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9998]
[ns_server:debug,2025-05-15T18:47:26.065Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',{project_intact,is_vulnerable}} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|false]
[cluster:debug,2025-05-15T18:47:26.062Z,ns_1@db3.lan:ns_cluster<0.273.0>:ns_cluster:perform_actual_join:1641]pre-join merged config is:
{config,
 {full,"/opt/couchbase/etc/couchbase/config",undefined,ns_config_default},
 [[],
  [{{node,'ns_1@db3.lan',config_version},{7,6}},
   {directory,"/opt/couchbase/var/lib/couchbase/config"},
   {{node,'ns_1@db3.lan',is_enterprise},true},
   {{node,'ns_1@db3.lan',saslauthd_enabled},true},
   {index_aware_rebalance_disabled,false},
   {max_bucket_count,30},
   {set_view_update_daemon,
    [{update_interval,5000},
     {update_min_changes,5000},
     {replica_update_min_changes,5000}]},
   {{node,'ns_1@db3.lan',compaction_daemon},
    [{check_interval,30},
     {min_db_file_size,131072},
     {min_view_file_size,20971520}]},
   {nodes_wanted,[]},
   {quorum_nodes,['ns_1@db3.lan']},
   {{couchdb,max_parallel_indexers},4},
   {{couchdb,max_parallel_replica_indexers},2},
   {{metakv,<<"/indexing/settings/config">>},
    <<"{\"indexer.settings.compaction.abort_exceed_interval\":false,\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.enable_page_bloom_filter\":false,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.log_level\":\"info\",\"indexer.settin"...>>},
   {{metakv,<<"/eventing/settings/config">>},<<"{\"ram_quota\":256}">>},
   {{metakv,<<"/query/settings/config">>},
    <<"{\"cleanupclientattempts\":true,\"cleanuplostattempts\":true,\"cleanupwindow\":\"60s\",\"completed-limit\":4000,\"completed-threshold\":1000,\"loglevel\":\"info\",\"max-parallelism\":1,\"memory-quota\":0,\"n1ql-feat-ctrl\":76,\"numatrs\":1024,\"pipeline-batch\":16,\"pipeline-cap\":512,\"prepared-limit\":16384,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"query.settin"...>>},
   {{metakv,<<"/analytics/settings/config">>},
    <<"{\"analytics.settings.num_replicas\":0}">>},
   {rest_creds,null},
   {client_cert_auth,[{state,"disable"},{prefixes,[]}]},
   {scramsha_fallback_salt,<<213,116,154,49,54,134,239,240,55,23,111,63>>},
   {remote_clusters,[]},
   {{node,'ns_1@db3.lan',isasl},
    [{path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
   {audit,
    [{auditd_enabled,false},
     {rotate_interval,86400},
     {rotate_size,20971520},
     {prune_age,0},
     {disabled,[]},
     {enabled,[]},
     {disabled_users,[]},
     {sync,[]},
     {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
   {{node,'ns_1@db3.lan',audit},[]},
   {memcached,[]},
   {{node,'ns_1@db3.lan',memcached_defaults},
    [{max_connections,65000},
     {system_connections,5000},
     {connection_idle_time,0},
     {verbosity,0},
     {privilege_debug,false},
     {breakpad_enabled,true},
     {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
     {dedupe_nmvb_maps,false},
     {je_malloc_conf,undefined},
     {tracing_enabled,true},
     {datatype_snappy,true},
     {num_reader_threads,<<"default">>},
     {num_writer_threads,<<"default">>},
     {num_auxio_threads,<<"default">>},
     {num_nonio_threads,<<"default">>},
     {num_storage_threads,<<"default">>},
     {tcp_keepalive_idle,360},
     {tcp_keepalive_interval,10},
     {tcp_keepalive_probes,3},
     {tcp_user_timeout,30},
     {always_collect_trace_info,true},
     {connection_limit_mode,<<"disconnect">>},
     {free_connection_pool_size,0},
     {max_client_connection_details,0}]},
   {{node,'ns_1@db3.lan',memcached},
    [{port,11210},
     {dedicated_port,11209},
     {dedicated_ssl_port,11206},
     {ssl_port,11207},
     {admin_user,"@ns_server"},
     {other_users,
      ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
       "@cbas","@backup"]},
     {admin_pass,"*****"},
     {engines,
      [{membase,
        [{engine,"/opt/couchbase/lib/memcached/ep.so"},
         {static_config_string,"failpartialwarmup=false"}]},
       {memcached,
        [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
         {static_config_string,"vb0=true"}]}]},
     {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
     {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
     {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
     {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
     {log_prefix,"memcached.log"},
     {log_generations,20},
     {log_cyclesize,10485760},
     {log_rotation_period,39003}]},
   {{node,'ns_1@db3.lan',memcached_config},
    {[{interfaces,{memcached_config_mgr,get_interfaces,[]}},
      {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
      {connection_idle_time,connection_idle_time},
      {privilege_debug,privilege_debug},
      {breakpad,
       {[{enabled,breakpad_enabled},
         {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
      {deployment_model,{memcached_config_mgr,get_config_profile,[]}},
      {verbosity,verbosity},
      {audit_file,{"~s",[audit_file]}},
      {rbac_file,{"~s",[rbac_file]}},
      {dedupe_nmvb_maps,dedupe_nmvb_maps},
      {tracing_enabled,tracing_enabled},
      {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
      {xattr_enabled,true},
      {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
      {collections_enabled,true},
      {max_connections,max_connections},
      {system_connections,system_connections},
      {num_reader_threads,num_reader_threads},
      {num_writer_threads,num_writer_threads},
      {num_auxio_threads,num_auxio_threads},
      {num_nonio_threads,num_nonio_threads},
      {num_storage_threads,num_storage_threads},
      {logger,
       {[{filename,{"~s/~s",[log_path,log_prefix]}},
         {cyclesize,log_cyclesize}]}},
      {external_auth_service,
       {memcached_config_mgr,get_external_auth_service,[]}},
      {active_external_users_push_interval,
       {memcached_config_mgr,get_external_users_push_interval,[]}},
      {prometheus,{memcached_config_mgr,prometheus_cfg,[]}},
      {sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
      {ssl_sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
      {tcp_keepalive_idle,tcp_keepalive_idle},
      {tcp_keepalive_interval,tcp_keepalive_interval},
      {tcp_keepalive_probes,tcp_keepalive_probes},
      {tcp_user_timeout,tcp_user_timeout},
      {always_collect_trace_info,always_collect_trace_info},
      {connection_limit_mode,connection_limit_mode},
      {free_connection_pool_size,free_connection_pool_size},
      {max_client_connection_details,max_client_connection_details}]}},
   {memory_quota,3406},
   {fts_memory_quota,512},
   {cbas_memory_quota,1549},
   {buckets,[{configs,[]}]},
   {secure_headers,[]},
   {{node,'ns_1@db3.lan',port_servers},[]},
   {{node,'ns_1@db3.lan',ns_log},
    [{filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
   {{node,'ns_1@db3.lan',event_log},
    [{filename,"/opt/couchbase/var/lib/couchbase/event_log"}]},
   {email_alerts,
    [{recipients,["root@localhost"]},
     {sender,"couchbase@localhost"},
     {enabled,false},
     {email_server,
      [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
     {alerts,
      [auto_failover_node,auto_failover_maximum_reached,
       auto_failover_other_nodes_down,auto_failover_cluster_too_small,
       auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
       ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
       indexer_low_resident_percentage,ep_clock_cas_drift_threshold_exceeded,
       communication_issue,time_out_of_sync,disk_usage_analyzer_stuck,
       cert_expires_soon,cert_expired,memory_threshold,history_size_warning,
       stuck_rebalance,memcached_connections]},
     {pop_up_alerts,
      [auto_failover_node,auto_failover_maximum_reached,
       auto_failover_other_nodes_down,auto_failover_cluster_too_small,
       auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
       ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
       indexer_low_resident_percentage,ep_clock_cas_drift_threshold_exceeded,
       communication_issue,time_out_of_sync,disk_usage_analyzer_stuck,
       cert_expires_soon,cert_expired,memory_threshold,history_size_warning,
       stuck_rebalance,memcached_connections]}]},
   {alert_limits,
    [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
   {replication,[{enabled,true}]},
   {log_redaction_default_cfg,[{redact_level,none}]},
   {service_orchestrator_weight,
    [{kv,10000},
     {index,1000},
     {fts,1000},
     {cbas,1000},
     {n1ql,100},
     {eventing,100},
     {backup,10}]},
   {health_monitor_refresh_interval,[]},
   {password_policy,[{min_length,6},{must_present,[]}]},
   {rest,[{port,8091}]},
   {{node,'ns_1@db3.lan',rest},[{port,8091},{port_meta,global}]},
   {{node,'ns_1@db3.lan',ssl_rest_port},18091},
   {{node,'ns_1@db3.lan',xdcr_rest_port},9998},
   {{node,'ns_1@db3.lan',memcached_dedicated_ssl_port},11206},
   {{node,'ns_1@db3.lan',memcached_prometheus},11280},
   {{node,'ns_1@db3.lan',projector_port},9999},
   {{node,'ns_1@db3.lan',projector_ssl_port},9999},
   {{node,'ns_1@db3.lan',query_port},8093},
   {{node,'ns_1@db3.lan',ssl_query_port},18093},
   {{node,'ns_1@db3.lan',indexer_admin_port},9100},
   {{node,'ns_1@db3.lan',indexer_scan_port},9101},
   {{node,'ns_1@db3.lan',indexer_http_port},9102},
   {{node,'ns_1@db3.lan',indexer_stinit_port},9103},
   {{node,'ns_1@db3.lan',indexer_stcatchup_port},9104},
   {{node,'ns_1@db3.lan',indexer_stmaint_port},9105},
   {{node,'ns_1@db3.lan',indexer_https_port},19102},
   {{node,'ns_1@db3.lan',fts_http_port},8094},
   {{node,'ns_1@db3.lan',fts_ssl_port},18094},
   {{node,'ns_1@db3.lan',fts_grpc_port},9130},
   {{node,'ns_1@db3.lan',fts_grpc_ssl_port},19130},
   {{node,'ns_1@db3.lan',eventing_http_port},8096},
   {{node,'ns_1@db3.lan',eventing_debug_port},9140},
   {{node,'ns_1@db3.lan',eventing_https_port},18096},
   {{node,'ns_1@db3.lan',cbas_http_port},8095},
   {{node,'ns_1@db3.lan',cbas_ssl_port},18095},
   {{node,'ns_1@db3.lan',cbas_admin_port},9110},
   {{node,'ns_1@db3.lan',cbas_cc_http_port},9111},
   {{node,'ns_1@db3.lan',cbas_cc_cluster_port},9112},
   {{node,'ns_1@db3.lan',cbas_cc_client_port},9113},
   {{node,'ns_1@db3.lan',cbas_console_port},9114},
   {{node,'ns_1@db3.lan',cbas_cluster_port},9115},
   {{node,'ns_1@db3.lan',cbas_data_port},9116},
   {{node,'ns_1@db3.lan',cbas_result_port},9117},
   {{node,'ns_1@db3.lan',cbas_messaging_port},9118},
   {{node,'ns_1@db3.lan',cbas_metadata_callback_port},9119},
   {{node,'ns_1@db3.lan',cbas_replication_port},9120},
   {{node,'ns_1@db3.lan',cbas_metadata_port},9121},
   {{node,'ns_1@db3.lan',cbas_parent_port},9122},
   {{node,'ns_1@db3.lan',cbas_debug_port},-1},
   {{node,'ns_1@db3.lan',prometheus_http_port},9123},
   {{node,'ns_1@db3.lan',backup_http_port},8097},
   {{node,'ns_1@db3.lan',backup_https_port},18097},
   {{node,'ns_1@db3.lan',backup_grpc_port},9124},
   {{node,'ns_1@db3.lan',capi_port},8092},
   {{node,'ns_1@db3.lan',ssl_capi_port},18092},
   {{node,'ns_1@db3.lan',{project_intact,is_vulnerable}},false},
   {retry_rebalance,
    [{enabled,false},{after_time_period,300},{max_attempts,1}]},
   {auto_failover_cfg,
    [{enabled,true},
     {timeout,120},
     {count,0},
     {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
     {failover_server_group,false},
     {max_count,1},
     {failed_over_server_groups,[]},
     {can_abort_rebalance,true}]},
   {{node,'ns_1@db3.lan',database_dir},
    "/opt/couchbase/var/lib/couchbase/data"},
   {{node,'ns_1@db3.lan',index_dir},"/opt/couchbase/var/lib/couchbase/data"},
   {resource_management,
    [{bucket,
      [{resident_ratio,
        [{enabled,false},{couchstore_minimum,1},{magma_minimum,0.2}]},
       {data_size,
        [{enabled,false},{couchstore_maximum,2},{magma_maximum,16}]}]},
     {index,[]},
     {cores_per_bucket,[{enabled,false},{minimum,0.4}]},
     {disk_usage,[{enabled,false},{maximum,96}]},
     {collections_per_quota,[{enabled,false},{maximum,1}]}]}]],
 [[{alert_limits,
    [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
   {audit,
    [{auditd_enabled,false},
     {rotate_interval,86400},
     {rotate_size,20971520},
     {prune_age,0},
     {disabled,[]},
     {enabled,[]},
     {disabled_users,[]},
     {sync,[]},
     {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
   {audit_decriptors,
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554009}}]},
     {8243,
      [{name,<<"mutate document">>},
       {description,<<"Document was mutated via the REST API">>},
       {enabled,true},
       {module,ns_server}]},
     {8255,
      [{name,<<"read document">>},
       {description,<<"Document was read via the REST API">>},
       {enabled,false},
       {module,ns_server}]},
     {8257,
      [{name,<<"alert email sent">>},
       {description,<<"An alert email was successfully sent">>},
       {enabled,true},
       {module,ns_server}]},
     {8265,
      [{name,<<"RBAC information retrieved">>},
       {description,<<"RBAC information was retrieved">>},
       {enabled,true},
       {module,ns_server}]},
     {16384,
      [{name,<<"remote cluster ref creation">>},
       {description,<<"created remote cluster ref">>},
       {enabled,true},
       {module,xdcr}]},
     {16385,
      [{name,<<"remote cluster ref update">>},
       {description,<<"updated remote cluster ref">>},
       {enabled,true},
       {module,xdcr}]},
     {16386,
      [{name,<<"remote cluster ref deletion">>},
       {description,<<"deleted remote cluster ref">>},
       {enabled,true},
       {module,xdcr}]},
     {16387,
      [{name,<<"replication creation">>},
       {description,<<"created replication">>},
       {enabled,true},
       {module,xdcr}]},
     {16388,
      [{name,<<"replication pause">>},
       {description,<<"paused replication">>},
       {enabled,true},
       {module,xdcr}]},
     {16389,
      [{name,<<"replication resume">>},
       {description,<<"resumed replication">>},
       {enabled,true},
       {module,xdcr}]},
     {16390,
      [{name,<<"replication cancellation">>},
       {description,<<"canceled replication">>},
       {enabled,true},
       {module,xdcr}]},
     {16391,
      [{name,<<"default replication settings update">>},
       {description,<<"updated default replication settings">>},
       {enabled,true},
       {module,xdcr}]},
     {16392,
      [{name,<<"individual replication settings update">>},
       {description,<<"updated individual replication settings">>},
       {enabled,true},
       {module,xdcr}]},
     {16393,
      [{name,<<"bucket settings update">>},
       {description,<<"updated bucket settings">>},
       {enabled,true},
       {module,xdcr}]},
     {16394,
      [{name,<<"authorization failure while adding remote cluster ref">>},
       {description,
        <<"failed to add remote cluster ref because of authorization failure">>},
       {enabled,true},
       {module,xdcr}]},
     {16395,
      [{name,<<"authorization failure while updating remote cluster ref">>},
       {description,
        <<"failed to update remote cluster ref because of authorization failure">>},
       {enabled,true},
       {module,xdcr}]},
     {16396,
      [{name,<<"access denied">>},
       {description,<<"access denied">>},
       {enabled,true},
       {module,xdcr}]},
     {20480,
      [{name,<<"opened DCP connection">>},
       {description,<<"opened DCP connection">>},
       {enabled,true},
       {module,memcached}]},
     {20482,
      [{name,<<"external memcached bucket flush">>},
       {description,
        <<"External user flushed the content of a memcached bucket">>},
       {enabled,true},
       {module,memcached}]},
     {20483,
      [{name,<<"invalid packet">>},
       {description,<<"Rejected an invalid packet">>},
       {enabled,true},
       {module,memcached}]},
     {20485,
      [{name,<<"authentication succeeded">>},
       {description,<<"Authentication to the cluster succeeded">>},
       {enabled,false},
       {module,memcached}]},
     {20488,
      [{name,<<"document read">>},
       {description,<<"Document was read">>},
       {enabled,false},
       {module,memcached}]},
     {20489,
      [{name,<<"document locked">>},
       {description,<<"Document was locked">>},
       {enabled,false},
       {module,memcached}]},
     {20490,
      [{name,<<"document modify">>},
       {description,<<"Document was modified">>},
       {enabled,false},
       {module,memcached}]},
     {20491,
      [{name,<<"document delete">>},
       {description,<<"Document was deleted">>},
       {enabled,false},
       {module,memcached}]},
     {20492,
      [{name,<<"select bucket">>},
       {description,<<"The specified bucket was selected">>},
       {enabled,true},
       {module,memcached}]},
     {20493,
      [{name,<<"session terminated">>},
       {description,<<"Session to the cluster has terminated">>},
       {enabled,false},
       {module,memcached}]},
     {20494,
      [{name,<<"tenant rate limited">>},
       {description,<<"The given tenant was rate limited">>},
       {enabled,true},
       {module,memcached}]},
     {24576,
      [{name,<<"Delete index">>},
       {description,<<"FTS index was deleted">>},
       {enabled,true},
       {module,fts}]},
     {24577,
      [{name,<<"Create/Update index">>},
       {description,<<"FTS index was created/Updated">>},
       {enabled,true},
       {module,fts}]},
     {24579,
      [{name,<<"Control index">>},
       {description,<<"FTS index control command was issued">>},
       {enabled,true},
       {module,fts}]},
     {24582,
      [{name,<<"GC run">>},
       {description,<<"GC run was triggered">>},
       {enabled,true},
       {module,fts}]},
     {24583,
      [{name,<<"CPU profile">>},
       {description,<<"CPU profiling was started">>},
       {enabled,true},
       {module,fts}]},
     {24584,
      [{name,<<"Memory profile">>},
       {description,<<"Memory profiling was started">>},
       {enabled,true},
       {module,fts}]},
     {28672,
      [{name,<<"SELECT statement">>},
       {description,<<"A N1QL SELECT statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28673,
      [{name,<<"EXPLAIN statement">>},
       {description,<<"A N1QL EXPLAIN statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28674,
      [{name,<<"PREPARE statement">>},
       {description,<<"A N1QL PREPARE statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28675,
      [{name,<<"INFER statement">>},
       {description,<<"A N1QL INFER statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28676,
      [{name,<<"INSERT statement">>},
       {description,<<"A N1QL INSERT statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28677,
      [{name,<<"UPSERT statement">>},
       {description,<<"A N1QL UPSERT statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28678,
      [{name,<<"DELETE statement">>},
       {description,<<"A N1QL DELETE statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28679,
      [{name,<<"UPDATE statement">>},
       {description,<<"A N1QL UPDATE statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28680,
      [{name,<<"MERGE statement">>},
       {description,<<"A N1QL MERGE statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28681,
      [{name,<<"CREATE INDEX statement">>},
       {description,<<"A N1QL CREATE INDEX statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28682,
      [{name,<<"DROP INDEX statement">>},
       {description,<<"A N1QL DROP INDEX statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28683,
      [{name,<<"ALTER INDEX statement">>},
       {description,<<"A N1QL ALTER INDEX statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28684,
      [{name,<<"BUILD INDEX statement">>},
       {description,<<"A N1QL BUILD INDEX statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28685,
      [{name,<<"GRANT ROLE statement">>},
       {description,<<"A N1QL GRANT ROLE statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28686,
      [{name,<<"REVOKE ROLE statement">>},
       {description,<<"A N1QL REVOKE ROLE statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28687,
      [{name,<<"UNRECOGNIZED statement">>},
       {description,
        <<"An unrecognized statement was received by the N1QL query engine">>},
       {enabled,false},
       {module,n1ql}]},
     {28688,
      [{name,<<"CREATE PRIMARY INDEX statement">>},
       {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28689,
      [{name,<<"/admin/stats API request">>},
       {description,
        <<"An HTTP request was made to the API at /admin/stats.">>},
       {enabled,false},
       {module,n1ql}]},
     {28690,
      [{name,<<"/admin/vitals API request">>},
       {description,
        <<"An HTTP request was made to the API at /admin/vitals.">>},
       {enabled,false},
       {module,n1ql}]},
     {28691,
      [{name,<<"/admin/prepareds API request">>},
       {description,
        <<"An HTTP request was made to the API at /admin/prepareds.">>},
       {enabled,false},
       {module,n1ql}]},
     {28692,
      [{name,<<"/admin/active_requests API request">>},
       {description,
        <<"An HTTP request was made to the API at /admin/active_requests.">>},
       {enabled,false},
       {module,n1ql}]},
     {28693,
      [{name,<<"/admin/indexes/prepareds API request">>},
       {description,
        <<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
       {enabled,false},
       {module,n1ql}]},
     {28694,
      [{name,<<"/admin/indexes/active_requests API request">>},
       {description,
        <<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
       {enabled,false},
       {module,n1ql}]},
     {28695,
      [{name,<<"/admin/indexes/completed_requests API request">>},
       {description,
        <<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
       {enabled,false},
       {module,n1ql}]},
     {28697,
      [{name,<<"/admin/ping API request">>},
       {description,<<"An HTTP request was made to the API at /admin/ping.">>},
       {enabled,false},
       {module,n1ql}]},
     {28698,
      [{name,<<"/admin/config API request">>},
       {description,
        <<"An HTTP request was made to the API at /admin/config.">>},
       {enabled,false},
       {module,n1ql}]},
     {28699,
      [{name,<<"/admin/ssl_cert API request">>},
       {description,
        <<"An HTTP request was made to the API at /admin/ssl_cert.">>},
       {enabled,false},
       {module,n1ql}]},
     {28700,
      [{name,<<"/admin/settings API request">>},
       {description,
        <<"An HTTP request was made to the API at /admin/settings.">>},
       {enabled,false},
       {module,n1ql}]},
     {28701,
      [{name,<<"/admin/clusters API request">>},
       {description,
        <<"An HTTP request was made to the API at /admin/clusters.">>},
       {enabled,false},
       {module,n1ql}]},
     {28702,
      [{name,<<"/admin/completed_requests API request">>},
       {description,
        <<"An HTTP request was made to the API at /admin/completed_requests.">>},
       {enabled,false},
       {module,n1ql}]},
     {28704,
      [{name,<<"/admin/functions API request">>},
       {description,
        <<"An HTTP request was made to the API at /admin/functions.">>},
       {enabled,false},
       {module,n1ql}]},
     {28705,
      [{name,<<"/admin/indexes/functions API request">>},
       {description,
        <<"An HTTP request was made to the API at /admin/indexes/functions.">>},
       {enabled,false},
       {module,n1ql}]},
     {28706,
      [{name,<<"CREATE FUNCTION statement">>},
       {description,<<"A N1QL CREATE FUNCTION statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28707,
      [{name,<<"DROP FUNCTION statement">>},
       {description,<<"A N1QL DROP FUNCTION statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28708,
      [{name,<<"EXECUTE FUNCTION statement">>},
       {description,<<"A N1QL EXECUTE FUNCTION statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28709,
      [{name,<<"/admin/tasks API request">>},
       {description,
        <<"An HTTP request was made to the API at /admin/tasks.">>},
       {enabled,false},
       {module,n1ql}]},
     {28710,
      [{name,<<"/admin/indexes/tasks API request">>},
       {description,
        <<"An HTTP request was made to the API at /admin/indexes/tasks.">>},
       {enabled,false},
       {module,n1ql}]},
     {28711,
      [{name,<<"/admin/dictionary_cache API request">>},
       {description,
        <<"An HTTP request was made to the API at /admin/dictionary_cache.">>},
       {enabled,false},
       {module,n1ql}]},
     {28712,
      [{name,<<"/admin/indexes/dictionary_cache API request">>},
       {description,
        <<"An HTTP request was made to the API at /admin/indexes/dictionary_cache.">>},
       {enabled,false},
       {module,n1ql}]},
     {28713,
      [{name,<<"CREATE SCOPE statement">>},
       {description,<<"A N1QL CREATE SCOPE statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28714,
      [{name,<<"DROP SCOPE statement">>},
       {description,<<"A N1QL DROP SCOPE statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28715,
      [{name,<<"CREATE COLLECTION statement">>},
       {description,<<"A N1QL CREATE COLLECTION statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28716,
      [{name,<<"DROP COLLECTION statement">>},
       {description,<<"A N1QL DROP COLLECTION statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28717,
      [{name,<<"FLUSH COLLECTION statement">>},
       {description,<<"A N1QL FLUSH COLLECTION statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28718,
      [{name,<<"UPDATE STATISTICS statement">>},
       {description,<<"A N1QL UPDATE STATISTICS statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28719,
      [{name,<<"ADVISE statement">>},
       {description,<<"A N1QL ADVISE statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28720,
      [{name,<<"START TRANSACTION statement">>},
       {description,<<"A N1QL START TRANSACTION statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28721,
      [{name,<<"COMMIT TRANSACTION statement">>},
       {description,<<"A N1QL COMMIT TRANSACTION statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28722,
      [{name,<<"ROLLBACK TRANSACTION statement">>},
       {description,<<"A N1QL ROLLBACK TRANSACTION statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28723,
      [{name,<<"ROLLBACK TRANSACTION TO SAVEPOINT statement">>},
       {description,
        <<"A N1QL ROLLBACK TRANSACTION TO SAVEPOINT statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28724,
      [{name,<<"SET TRANSACTION ISOLATION statement">>},
       {description,
        <<"A N1QL SET TRANSACTION ISOLATION statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28725,
      [{name,<<"SAVEPOINT statement">>},
       {description,<<"A N1QL SAVEPOINT statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28726,
      [{name,<<"/admin/transactions API request">>},
       {description,
        <<"An HTTP request was made to the API at /admin/transactions.">>},
       {enabled,false},
       {module,n1ql}]},
     {28727,
      [{name,<<"/admin/indexes/transactions API request">>},
       {description,
        <<"An HTTP request was made to the API at /admin/indexes/transactions.">>},
       {enabled,false},
       {module,n1ql}]},
     {28728,
      [{name,<<"N1QL backup / restore API request">>},
       {description,
        <<"An HTTP request was made to archive or restore N1QL metadata">>},
       {enabled,false},
       {module,n1ql}]},
     {28729,
      [{name,<<"/admin/shutdown API request">>},
       {description,
        <<"An HTTP request was made to initiate graceful shutdown">>},
       {enabled,false},
       {module,n1ql}]},
     {28730,
      [{name,<<"/admin/gc API request">>},
       {description,<<"An HTTP request was made to run garbage collection">>},
       {enabled,false},
       {module,n1ql}]},
     {28731,
      [{name,<<"/admin/ffdc API request">>},
       {description,<<"An HTTP request was made to run an FFDC collection">>},
       {enabled,false},
       {module,n1ql}]},
     {28732,
      [{name,<<"/admin/log/ API request">>},
       {description,<<"An HTTP request was made to access diagnostic logs">>},
       {enabled,false},
       {module,n1ql}]},
     {28733,
      [{name,<<"/admin/sequences_cache API request">>},
       {description,<<"An HTTP request was made to access sequences">>},
       {enabled,false},
       {module,n1ql}]},
     {28734,
      [{name,<<"CREATE SEQUENCE statement">>},
       {description,<<"A N1QL CREATE SEQUENCE statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28735,
      [{name,<<"ALTER SEQUENCE statement">>},
       {description,<<"A N1QL ALTER SEQUENCE statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28736,
      [{name,<<"DROP SEQUENCE statement">>},
       {description,<<"A N1QL DROP SEQUENCE statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28737,
      [{name,<<"Migration abort">>},
       {description,<<"Migration was aborted">>},
       {enabled,false},
       {module,n1ql}]},
     {32768,
      [{name,<<"Create Function">>},
       {description,
        <<"Request to create or update eventing function definition">>},
       {enabled,true},
       {module,eventing}]},
     {32769,
      [{name,<<"Delete Function">>},
       {description,<<"Request to delete eventing function definition">>},
       {enabled,true},
       {module,eventing}]},
     {32770,
      [{name,<<"Fetch Functions">>},
       {description,<<"Request to fetch eventing function definition">>},
       {enabled,false},
       {module,eventing}]},
     {32771,
      [{name,<<"List Deployed">>},
       {description,<<"Request to fetch eventing deployed functions list">>},
       {enabled,false},
       {module,eventing}]},
     {32772,
      [{name,<<"Fetch Drafts">>},
       {description,
        <<"Request to fetch eventing function draft definitions">>},
       {enabled,false},
       {module,eventing}]},
     {32773,
      [{name,<<"Delete Drafts">>},
       {description,
        <<"Request to delete eventing function draft definitions">>},
       {enabled,true},
       {module,eventing}]},
     {32774,
      [{name,<<"Save Draft">>},
       {description,<<"Request to save a draft definition">>},
       {enabled,true},
       {module,eventing}]},
     {32775,
      [{name,<<"Start Debug">>},
       {description,<<"Request to start eventing function debugger">>},
       {enabled,true},
       {module,eventing}]},
     {32776,
      [{name,<<"Stop Debug">>},
       {description,<<"Request to stop eventing function debugger">>},
       {enabled,true},
       {module,eventing}]},
     {32777,
      [{name,<<"Start Tracing">>},
       {description,<<"Request to start tracing eventing function e"...>>},
       {enabled,true},
       {module,eventing}]},
     {32778,
      [{name,<<"Stop Tracing">>},
       {description,<<"Request to stop tracing eventing functio"...>>},
       {enabled,true},
       {module,eventing}]},
     {32779,
      [{name,<<"Set Settings">>},
       {description,<<"Request to save settings for an even"...>>},
       {enabled,true},
       {module,eventing}]},
     {32780,
      [{name,<<"Fetch Config">>},
       {description,<<"Request to fetch eventing config">>},
       {enabled,false},
       {module,eventing}]},
     {32781,
      [{name,<<"Save Config">>},
       {description,<<"Request to save eventing con"...>>},
       {enabled,true},
       {module,eventing}]},
     {32783,
      [{name,<<"Get Settings">>},
       {description,<<"Request to fetch eventin"...>>},
       {enabled,false},
       {module,eventing}]},
     {32784,
      [{name,<<"Import Functions">>},
       {description,<<"Request to import on"...>>},
       {enabled,true},
       {module,eventing}]},
     {32785,
      [{name,<<"Export Functions">>},
       {description,<<"Request to expor"...>>},
       {enabled,false},
       {module,eventing}]},
     {32786,
      [{name,<<"List Running">>},
       {description,<<"Request to f"...>>},
       {enabled,false},
       {module,eventing}]},
     {32789,
      [{name,<<"Deploy Funct"...>>},
       {description,<<"Request "...>>},
       {enabled,true},
       {module,eventing}]},
     {32790,
      [{name,<<"Undeploy"...>>},
       {description,<<"Requ"...>>},
       {enabled,true},
       {module,...}]},
     {32791,[{name,<<"Paus"...>>},{description,<<...>>},{enabled,...},{...}]},
     {32792,[{name,<<...>>},{description,...},{...}|...]},
     {32793,[{name,...},{...}|...]},
     {32794,[{...}|...]},
     {32795,[...]},
     {32796,...},
     {...}|...]},
   {auto_failover_cfg,
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{3,63914554009}}]},
     {enabled,true},
     {timeout,120},
     {count,0},
     {max_count,1},
     {disable_max_count,false},
     {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
     {failover_server_group,false},
     {failed_over_server_groups,[]},
     {can_abort_rebalance,true},
     {failover_preserve_durability_majority,false}]},
   {buckets,[{configs,[]}]},
   {cbas_memory_quota,
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554041}}]}|
     1024]},
   {client_cert_auth,[{state,"disable"},{prefixes,[]}]},
   {cluster_compat_mode,undefined},
   {cluster_compat_version,
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{3,63914554009}}]},
     7,6]},
   {email_alerts,
    [{recipients,["root@localhost"]},
     {sender,"couchbase@localhost"},
     {enabled,false},
     {email_server,
      [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
     {alerts,
      [auto_failover_node,auto_failover_maximum_reached,
       auto_failover_other_nodes_down,auto_failover_cluster_too_small,
       auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
       ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
       indexer_low_resident_percentage,ep_clock_cas_drift_threshold_exceeded,
       communication_issue,time_out_of_sync,disk_usage_analyzer_stuck,
       cert_expires_soon,cert_expired,memory_threshold,history_size_warning,
       stuck_rebalance,memcached_connections]},
     {pop_up_alerts,
      [auto_failover_node,auto_failover_maximum_reached,
       auto_failover_other_nodes_down,auto_failover_cluster_too_small,
       auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
       ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
       indexer_low_resident_percentage,ep_clock_cas_drift_threshold_exceeded,
       communication_issue,time_out_of_sync,disk_usage_analyzer_stuck,
       cert_expires_soon,cert_expired,memory_threshold,history_size_warning,
       stuck_rebalance,memcached_connections]}]},
   {fts_memory_quota,512},
   {health_monitor_refresh_interval,[]},
   {index_aware_rebalance_disabled,false},
   {log_redaction_default_cfg,[{redact_level,none}]},
   {max_bucket_count,30},
   {memcached,[]},
   {memory_quota,
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554041}}]}|
     3072]},
   {nodes_wanted,[]},
   {otp,
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554009}}]},
     {cookie,{sanitized,<<"biSYTaQUFDVndTs/b+IcDmD2L0woktv9naRdv3GCK6A=">>}}]},
   {password_policy,[{min_length,6},{must_present,[]}]},
   {quorum_nodes,
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554042}}]},
     'ns_1@172.19.0.4']},
   {rbac_upgrade,
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554009}}]}|
     '_deleted']},
   {remote_clusters,[]},
   {replication,[{enabled,true}]},
   {resource_management,
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554009}}]},
     {bucket,
      [{resident_ratio,
        [{enabled,false},{couchstore_minimum,1},{magma_minimum,0.2}]},
       {data_size,
        [{enabled,false},{couchstore_maximum,2},{magma_maximum,16}]}]},
     {index,[]},
     {cores_per_bucket,[{enabled,false},{minimum,0.4}]},
     {disk_usage,[{enabled,false},{maximum,96}]},
     {collections_per_quota,[{enabled,false},{maximum,1}]}]},
   {rest,[{port,8091}]},
   {rest_creds,
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554041}}]}|
     {"<ud>jaba_admin</ud>",
      {auth,
       [{<<"hash">>,
         {[{<<"hashes">>,
            {sanitized,<<"3I14lVC6WTo2HkE1Ma2OkSoP1R5UwTP501YfMTKM7ak=">>}},
           {<<"algorithm">>,<<"argon2id">>},
           {<<"salt">>,<<"5CFoS4MqJh4VclPVzeJxBw==">>},
           {<<"parallelism">>,1},
           {<<"time">>,3},
           {<<"memory">>,524288}]}},
        {<<"scram-sha-512">>,
         {[{<<"salt">>,
            <<"pulx/Jw0I8WxLIxMcCD8CqAxiVNfHNaHWpBI6wjnArfbB+WyxXwBuk5SnOGA7W5LgTZZWGevGaY3ih1tGH4jeg==">>},
           {<<"iterations">>,15000},
           {<<"hashes">>,
            {sanitized,<<"+8aFWL6Fwfs8HKehUbj7ZkLi5LW9/xRhbkbg/BwGA9w=">>}}]}},
        {<<"scram-sha-256">>,
         {[{<<"salt">>,<<"6mOH6zwaMRxGOfTsFTCuTxfMVctxE7w9qyktmzrJtVg=">>},
           {<<"iterations">>,15000},
           {<<"hashes">>,
            {sanitized,<<"t9IO+WS5kijXIacgFn1gB5R0bN61IAKLCL/JWibrJSA=">>}}]}},
        {<<"scram-sha-1">>,
         {[{<<"salt">>,<<"pEmVjw1zOFSsjZc/8lZKrEstBkA=">>},
           {<<"iterations">>,15000},
           {<<"hashes">>,
            {sanitized,
             <<"tJt6Wglz80zHvML1mQcTdrXmQggip4HdkH+YfamhrEc=">>}}]}}]}}]},
   {retry_rebalance,
    [{enabled,false},{after_time_period,300},{max_attempts,1}]},
   {scramsha_fallback_salt,<<55,187,62,55,248,32,6,50,18,123,218,116>>},
   {secure_headers,[]},
   {service_orchestrator_weight,
    [{kv,10000},
     {index,1000},
     {fts,1000},
     {cbas,1000},
     {n1ql,100},
     {eventing,100},
     {backup,10}]},
   {set_view_update_daemon,
    [{update_interval,5000},
     {update_min_changes,5000},
     {replica_update_min_changes,5000}]},
   {settings,
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554041}}]},
     {stats,[{send_stats,true}]}]},
   {uuid,
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554041}}]}|
     <<"61933a8028482692b0278a91060e0d90">>]},
   {{couchdb,max_parallel_indexers},4},
   {{couchdb,max_parallel_replica_indexers},2},
   {{local_changes_count,<<"28569ac00b9c1d7c50e39741027d428c">>},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{24,63914554045}}]}]},
   {{local_changes_count,<<"7a33d46fd1976e65466c5efc8b45ef2e">>},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{10,63914554045}}]}]},
   {{local_changes_count,<<"80960cc3730024bccd4fc49444efdc14">>},
    [{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{6,63914554046}}]}]},
   {{metakv,<<"/analytics/settings/config">>},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554009}}]}|
     <<"{\"analytics.settings.blob_storage_bucket\":\"\",\"analytics.settings.blob_storage_prefix\":\"\",\"analytics.settings.blob_storage_region\":\"\",\"analytics.settings.blob_storage_scheme\":\"\",\"analytics.settings.num_replicas\":0}">>]},
   {{metakv,<<"/eventing/settings/config">>},<<"{\"ram_quota\":256}">>},
   {{metakv,<<"/indexing/rebalance/RebalanceToken">>},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554045}}]}|
     <<"{\"MasterId\":\"28569ac00b9c1d7c50e39741027d428c\",\"RebalId\":\"35d452100b72fa9639f18b2cc28124e2\",\"Source\":0,\"Error\":\"\",\"MasterIP\":\"172.19.0.4\",\"Version\":0,\"RebalPhase\":0,\"ActiveRebalancer\":1}">>]},
   {{metakv,<<"/indexing/settings/config">>},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{3,63914554045}}]}|
     <<"{\"indexer.plasma.backIndex.enablePageBloomFilter\":false,\"indexer.settings.compaction.abort_exceed_interval\":false,\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.s"...>>]},
   {{metakv,<<"/query/functions_cache/counter">>},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554042}}]}|
     <<"[172.19.0.4:8091]0">>]},
   {{metakv,<<"/query/sequences_cache/revision">>},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554009}}]}|
     <<"0">>]},
   {{metakv,<<"/query/settings/config">>},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554009}}]}|
     <<"{\"cleanupclientattempts\":true,\"cleanuplostattempts\":true,\"cleanupwindow\":\"60s\",\"completed-limit\":4000,\"completed-max-plan-size\":262144,\"completed-threshold\":1000,\"loglevel\":\"info\",\"max-parallelism\":1,\"memory-quota\":0,\"n1ql-feat-ctrl\":76,\"node-quota\":0,\"node-quota-val-percent\":67,\"num-cpus\":0,\"numatrs\":1024,\"pipeline-batch\""...>>]},
   {{node,'ns_1@172.19.0.4',address_family},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     inet]},
   {{node,'ns_1@172.19.0.4',audit},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}]},
   {{node,'ns_1@172.19.0.4',backup_grpc_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9124]},
   {{node,'ns_1@172.19.0.4',backup_http_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     8097]},
   {{node,'ns_1@172.19.0.4',backup_https_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     18097]},
   {{node,'ns_1@172.19.0.4',capi_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     8092]},
   {{node,'ns_1@172.19.0.4',cbas_admin_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9110]},
   {{node,'ns_1@172.19.0.4',cbas_cc_client_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9113]},
   {{node,'ns_1@172.19.0.4',cbas_cc_cluster_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9112]},
   {{node,'ns_1@172.19.0.4',cbas_cc_http_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9111]},
   {{node,'ns_1@172.19.0.4',cbas_cluster_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9115]},
   {{node,'ns_1@172.19.0.4',cbas_console_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9114]},
   {{node,'ns_1@172.19.0.4',cbas_data_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9116]},
   {{node,'ns_1@172.19.0.4',cbas_debug_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     -1]},
   {{node,'ns_1@172.19.0.4',cbas_dirs},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
     "/opt/couchbase/var/lib/couchbase/data"]},
   {{node,'ns_1@172.19.0.4',cbas_http_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     8095]},
   {{node,'ns_1@172.19.0.4',cbas_messaging_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9118]},
   {{node,'ns_1@172.19.0.4',cbas_metadata_callback_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9119]},
   {{node,'ns_1@172.19.0.4',cbas_metadata_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9121]},
   {{node,'ns_1@172.19.0.4',cbas_parent_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9122]},
   {{node,'ns_1@172.19.0.4',cbas_replication_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9120]},
   {{node,'ns_1@172.19.0.4',cbas_result_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9117]},
   {{node,'ns_1@172.19.0.4',cbas_ssl_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     18095]},
   {{node,'ns_1@172.19.0.4',client_cert},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
     {subject,<<"CN=Couchbase Internal Client (70858842)">>},
     {not_after,63985747606},
     {verified_with,
      <<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
     {load_timestamp,63914554006},
     {ca,
      <<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZS"...>>},
     {pem,
      <<"-----BEGIN CERTIFICATE-----\nMIIDWTCCAkGgAwIBAgIIGD/Hv7XHejcwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ2NDZaFw0y\nNzA4MTcxODQ2NDZaMC8xLTArBgNVBAMTJENvdWNoYm"...>>},
     {pkey_passphrase_settings,[]},
     {certs_epoch,0},
     {type,generated},
     {name,"@internal"}]},
   {{node,'ns_1@172.19.0.4',compaction_daemon},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
     {check_interval,30},
     {min_db_file_size,131072},
     {min_view_file_size,20971520}]},
   {{node,'ns_1@172.19.0.4',config_version},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     {7,6}]},
   {{node,'ns_1@172.19.0.4',database_dir},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
     47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,
     105,98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
   {{node,'ns_1@172.19.0.4',erl_external_listeners},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
     {inet,false}]},
   {{node,'ns_1@172.19.0.4',event_log},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
     {filename,"/opt/couchbase/var/lib/couchbase/event_log"}]},
   {{node,'ns_1@172.19.0.4',eventing_debug_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9140]},
   {{node,'ns_1@172.19.0.4',eventing_dir},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
     47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,
     105,98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
   {{node,'ns_1@172.19.0.4',eventing_http_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     8096]},
   {{node,'ns_1@172.19.0.4',eventing_https_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     18096]},
   {{node,'ns_1@172.19.0.4',fts_grpc_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9130]},
   {{node,'ns_1@172.19.0.4',fts_grpc_ssl_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     19130]},
   {{node,'ns_1@172.19.0.4',fts_http_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     8094]},
   {{node,'ns_1@172.19.0.4',fts_ssl_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     18094]},
   {{node,'ns_1@172.19.0.4',index_dir},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
     47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,
     105,98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
   {{node,'ns_1@172.19.0.4',indexer_admin_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9100]},
   {{node,'ns_1@172.19.0.4',indexer_http_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9102]},
   {{node,'ns_1@172.19.0.4',indexer_https_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     19102]},
   {{node,'ns_1@172.19.0.4',indexer_scan_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9101]},
   {{node,'ns_1@172.19.0.4',indexer_stcatchup_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9104]},
   {{node,'ns_1@172.19.0.4',indexer_stinit_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9103]},
   {{node,'ns_1@172.19.0.4',indexer_stmaint_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9105]},
   {{node,'ns_1@172.19.0.4',is_enterprise},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     true]},
   {{node,'ns_1@172.19.0.4',isasl},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
     {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
   {{node,'ns_1@172.19.0.4',memcached},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
     {port,11210},
     {dedicated_port,11209},
     {dedicated_ssl_port,11206},
     {ssl_port,11207},
     {admin_user,"@ns_server"},
     {other_users,
      ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
       "@cbas","@backup"]},
     {admin_pass,"*****"},
     {engines,
      [{membase,
        [{engine,"/opt/couchbase/lib/memcached/ep.so"},
         {static_config_string,"failpartialwarmup=false"}]},
       {memcached,
        [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
         {static_config_string,"vb0=true"}]}]},
     {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
     {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
     {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
     {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
     {log_prefix,"memcached.log"},
     {log_generations,20},
     {log_cyclesize,10485760},
     {log_rotation_period,39003}]},
   {{node,'ns_1@172.19.0.4',memcached_config},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     {[{interfaces,{memcached_config_mgr,get_interfaces,[]}},
       {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
       {connection_idle_time,connection_idle_time},
       {privilege_debug,privilege_debug},
       {breakpad,
        {[{enabled,breakpad_enabled},
          {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
       {deployment_model,{memcached_config_mgr,get_config_profile,[]}},
       {verbosity,verbosity},
       {audit_file,{"~s",[audit_file]}},
       {rbac_file,{"~s",[rbac_file]}},
       {dedupe_nmvb_maps,dedupe_nmvb_maps},
       {tracing_enabled,tracing_enabled},
       {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
       {xattr_enabled,true},
       {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
       {collections_enabled,true},
       {max_connections,max_connections},
       {system_connections,system_connections},
       {num_reader_threads,num_reader_threads},
       {num_writer_threads,num_writer_threads},
       {num_auxio_threads,num_auxio_threads},
       {num_nonio_threads,num_nonio_threads},
       {num_storage_threads,num_storage_threads},
       {logger,{[{filename,{"~s/~s",[...]}},{cyclesize,log_cyclesize}]}},
       {external_auth_service,
        {memcached_config_mgr,get_external_auth_service,[]}},
       {active_external_users_push_interval,
        {memcached_config_mgr,get_external_users_push_interval,[]}},
       {prometheus,{memcached_config_mgr,prometheus_cfg,[]}},
       {sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,...}},
       {ssl_sasl_mechanisms,{memcached_config_mgr,...}},
       {tcp_keepalive_idle,tcp_keepalive_idle},
       {tcp_keepalive_interval,...},
       {...}|...]}]},
   {{node,'ns_1@172.19.0.4',memcached_dedicated_ssl_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     11206]},
   {{node,'ns_1@172.19.0.4',memcached_defaults},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
     {max_connections,65000},
     {system_connections,5000},
     {connection_idle_time,0},
     {verbosity,0},
     {privilege_debug,false},
     {breakpad_enabled,true},
     {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
     {dedupe_nmvb_maps,false},
     {je_malloc_conf,undefined},
     {tracing_enabled,true},
     {datatype_snappy,true},
     {num_reader_threads,<<"default">>},
     {num_writer_threads,<<"default">>},
     {num_auxio_threads,<<"default">>},
     {num_nonio_threads,<<"default">>},
     {num_storage_threads,<<"default">>},
     {tcp_keepalive_idle,360},
     {tcp_keepalive_interval,10},
     {tcp_keepalive_probes,3},
     {tcp_user_timeout,30},
     {always_collect_trace_info,true},
     {connection_limit_mode,<<"disconnect">>},
     {free_connection_pool_size,0},
     {max_client_connection_details,0}]},
   {{node,'ns_1@172.19.0.4',memcached_prometheus},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     11280]},
   {{node,'ns_1@172.19.0.4',n2n_client_cert_auth},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     false]},
   {{node,'ns_1@172.19.0.4',node_cert},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{3,63914554042}}]},
     {subject,<<"CN=Couchbase Server Node (172.19.0.4)">>},
     {not_after,63985747642},
     {verified_with,
      <<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
     {load_timestamp,63914554042},
     {ca,
      <<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEi"...>>},
     {pem,
      <<"-----BEGIN CERTIFICATE-----\nMIIDOjCCAiKgAwIBAgIIGD/Hx/dYFPowDQYJKoZIhvcNAQELBQAw"...>>},
     {pkey_passphrase_settings,[]},
     {certs_epoch,0},
     {type,generated},
     {hostname,"172.19.0.4"}]},
   {{node,'ns_1@172.19.0.4',node_encryption},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     false]},
   {{node,'ns_1@172.19.0.4',ns_log},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
     {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
   {{node,'ns_1@172.19.0.4',port_servers},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}]},
   {{node,'ns_1@172.19.0.4',projector_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9999]},
   {{node,'ns_1@172.19.0.4',projector_ssl_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9999]},
   {{node,'ns_1@172.19.0.4',prometheus_auth_info},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{3,63914554042}}]}|
     {"@prometheus",
      {auth,
       [{<<"hash">>,
         {[{<<"hashes">>,
            {sanitized,<<"ocCjmLXobw413zJaq2v5hbk6F9JaoNKM"...>>}},
           {<<"algorithm">>,<<"argon2id">>},
           {<<"salt">>,<<"FlBs6+3flsH1DlD+dLEwjg==">>},
           {<<"parallelism">>,1},
           {<<"time">>,3},
           {<<"memory">>,524288}]}},
        {<<"scram-sha-512">>,
         {[{<<"salt">>,<<"F/Zqupd2IseMgxcuAf9fe4gpX0DBCFm1/fWg"...>>},
           {<<"iterations">>,15000},
           {<<"hashes">>,{sanitized,<<"fNgWati8g2faSgrPjZUu"...>>}}]}},
        {<<"scram-sha-256">>,
         {[{<<"salt">>,<<"OKVHe2p1Uu/nuWHnr4hDmY1YmTEi+LTh"...>>},
           {<<"iterations">>,15000},
           {<<"hashes">>,{sanitized,<<"wJZdBxOuuYIylriw"...>>}}]}},
        {<<"scram-sha-1">>,
         {[{<<"salt">>,<<"FmlE6xOIeEZ8k+MADCxX6iijEW8=">>},
           {<<"iterations">>,15000},
           {<<"hashes">>,{sanitized,<<"gfRT+N7lHhRn"...>>}}]}}]}}]},
   {{node,'ns_1@172.19.0.4',prometheus_http_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9123]},
   {{node,'ns_1@172.19.0.4',query_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     8093]},
   {{node,'ns_1@172.19.0.4',rest},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
     {port,8091},
     {port_meta,global}]},
   {{node,'ns_1@172.19.0.4',saslauthd_enabled},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     true]},
   {{node,'ns_1@172.19.0.4',ssl_capi_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     18092]},
   {{node,'ns_1@172.19.0.4',ssl_query_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     18093]},
   {{node,'ns_1@172.19.0.4',ssl_rest_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     18091]},
   {{node,'ns_1@172.19.0.4',uuid},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     <<"28569ac00b9c1d7c50e39741027d428c">>]},
   {{node,'ns_1@172.19.0.4',xdcr_rest_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9998]},
   {{node,'ns_1@172.19.0.4',{project_intact,is_vulnerable}},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     false]},
   {{node,'ns_1@db2.lan',address_family},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45"...>>,{1,63914554043}}]}|
     inet]},
   {{node,'ns_1@db2.lan',audit},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc"...>>,{1,63914554043}}]}]},
   {{node,'ns_1@db2.lan',backup_grpc_port},
    [{'_vclock',[{<<"7a33d46fd1976e65466c"...>>,{1,63914554043}}]}|9124]},
   {{node,'ns_1@db2.lan',backup_http_port},
    [{'_vclock',[{<<"7a33d46fd1976e65"...>>,{1,63914554043}}]}|8097]},
   {{node,'ns_1@db2.lan',backup_https_port},
    [{'_vclock',[{<<"7a33d46fd197"...>>,{1,63914554043}}]}|18097]},
   {{node,'ns_1@db2.lan',capi_port},
    [{'_vclock',[{<<"7a33d46f"...>>,{1,...}}]}|8092]},
   {{node,'ns_1@db2.lan',cbas_admin_port},
    [{'_vclock',[{<<"7a33"...>>,{...}}]}|9110]},
   {{node,'ns_1@db2.lan',cbas_cc_client_port},
    [{'_vclock',[{<<...>>,...}]}|9113]},
   {{node,'ns_1@db2.lan',cbas_cc_cluster_port},[{'_vclock',[{...}]}|9112]},
   {{node,'ns_1@db2.lan',cbas_cc_http_port},[{'_vclock',[...]}|9111]},
   {{node,'ns_1@db2.lan',cbas_cluster_port},[{'_vclock',...}|9115]},
   {{node,'ns_1@db2.lan',...},[{...}|...]},
   {{node,...},[...]},
   {{...},...}|...]],
 ns_config_default,
 {ns_config_default,encrypt_and_save,[]},
 <0.2441.0>,false,<<"80960cc3730024bccd4fc49444efdc14">>,
 #Fun<ns_config.18.54949477>}
[ns_server:info,2025-05-15T18:47:26.089Z,ns_1@db3.lan:ns_cluster<0.273.0>:ns_ssl_services_setup:update_cert_epoch:1334]Updated generated node_cert epoch: 0
[ns_server:debug,2025-05-15T18:47:26.089Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',node_cert} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{2,63914554046}}]},
 {certs_epoch,0},
 {subject,<<"CN=Couchbase Server Node (db3.lan)">>},
 {not_after,63985747645},
 {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
 {load_timestamp,63914554045},
 {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hX"...>>},
 {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDOjCCAiKgAwIBAgIIGD/HyL2BZrYwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjVaFw0y\nNzA4MTcxODQ3MjVaMCoxKDAmBgNVBAMTH0NvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nZGIzLmxhbikwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCoBie+jXEG\nM9mqURIO0hHIXr1th5jf7TJD7f69DGF3znRjw1VTjnaMZAQOk8Gg44nwI1o1KtJj\nUmf"...>>},
 {pkey_passphrase_settings,[]},
 {type,generated},
 {hostname,"db3.lan"}]
[ns_server:info,2025-05-15T18:47:26.090Z,ns_1@db3.lan:ns_cluster<0.273.0>:ns_ssl_services_setup:update_cert_epoch:1334]Updated generated client_cert epoch: 0
[ns_server:debug,2025-05-15T18:47:26.090Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',client_cert} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{2,63914554046}}]},
 {certs_epoch,0},
 {subject,<<"CN=Couchbase Internal Client (32660726)">>},
 {not_after,63985747645},
 {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
 {load_timestamp,63914554045},
 {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hX"...>>},
 {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDWTCCAkGgAwIBAgIIGD/HyMO1PBAwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjVaFw0y\nNzA4MTcxODQ3MjVaMC8xLTArBgNVBAMTJENvdWNoYmFzZSBJbnRlcm5hbCBDbGll\nbnQgKDMyNjYwNzI2KTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAKHe\n9VAt+X6MU19Fg7ok9H0Zxkmq5mDd0acYZWMO8WqPC7jBY2dopAKDJoF/AKgk8TLT\ncz5"...>>},
 {pkey_passphrase_settings,[]},
 {type,generated},
 {name,"@internal"}]
[cluster:debug,2025-05-15T18:47:26.091Z,ns_1@db3.lan:ns_cluster<0.273.0>:ns_cluster:perform_actual_join:1651]Join succeded, starting ns_server_cluster back
[error_logger:info,2025-05-15T18:47:26.091Z,ns_1@db3.lan:ns_server_nodes_sup<0.2446.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.2447.0>},
              {id,chronicle_compat_events},
              {mfargs,{chronicle_compat_events,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:26.091Z,ns_1@db3.lan:ns_server_nodes_sup<0.2446.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.2452.0>},
              {id,remote_monitors},
              {mfargs,{remote_monitors,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:26.091Z,ns_1@db3.lan:menelaus_barrier<0.2453.0>:one_shot_barrier:barrier_body:52]Barrier menelaus_barrier has started
[error_logger:info,2025-05-15T18:47:26.091Z,ns_1@db3.lan:ns_server_nodes_sup<0.2446.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.2453.0>},
              {id,menelaus_barrier},
              {mfargs,{menelaus_sup,barrier_start_link,[]}},
              {restart_type,temporary},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:26.091Z,ns_1@db3.lan:<0.2456.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,
                                    rest_lhttpc_pool} with delay of 1000
[error_logger:info,2025-05-15T18:47:26.091Z,ns_1@db3.lan:<0.2457.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2457.0>,suppress_max_restart_intensity}
    started: [{pid,<0.2458.0>},
              {id,rest_lhttpc_pool},
              {mfargs,{lhttpc_manager,start_link,
                                      [[{name,rest_lhttpc_pool},
                                        {connection_timeout,120000},
                                        {pool_size,20}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:26.092Z,ns_1@db3.lan:<0.2455.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2455.0>,suppress_max_restart_intensity}
    started: [{pid,<0.2456.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      rest_lhttpc_pool}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,rest_lhttpc_pool},
                       1000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 1,id => rest_lhttpc_pool,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [lhttpc_manager],
                          restart => permanent,shutdown => 1000,
                          start =>
                              {lhttpc_manager,start_link,
                                  [[{name,rest_lhttpc_pool},
                                    {connection_timeout,120000},
                                    {pool_size,20}]]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:26.092Z,ns_1@db3.lan:rest_lhttpc_pool_sup<0.2454.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,rest_lhttpc_pool_sup}
    started: [{pid,<0.2455.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,rest_lhttpc_pool}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 1,id => rest_lhttpc_pool,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [lhttpc_manager],
                         restart => permanent,shutdown => 1000,
                         start =>
                             {lhttpc_manager,start_link,
                                 [[{name,rest_lhttpc_pool},
                                   {connection_timeout,120000},
                                   {pool_size,20}]]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:26.092Z,ns_1@db3.lan:ns_server_nodes_sup<0.2446.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.2454.0>},
              {id,rest_lhttpc_pool_sup},
              {mfargs,{rest_lhttpc_pool_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:47:26.093Z,ns_1@db3.lan:memcached_refresh<0.2459.0>:memcached_refresh:init:45]Starting during memcached lifetime. Try to refresh all files.
[ns_server:debug,2025-05-15T18:47:26.093Z,ns_1@db3.lan:memcached_refresh<0.2459.0>:memcached_refresh:update_refresh_state:137]Refresh of [isasl,rbac] skipped. Retry in 1000 ms.
[error_logger:info,2025-05-15T18:47:26.093Z,ns_1@db3.lan:ns_server_nodes_sup<0.2446.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.2459.0>},
              {id,memcached_refresh},
              {mfargs,{memcached_refresh,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:26.093Z,ns_1@db3.lan:ns_server_nodes_sup<0.2446.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.2460.0>},
              {id,ns_secrets},
              {mfargs,{ns_secrets,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:26.093Z,ns_1@db3.lan:ns_ssl_services_sup<0.2461.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_ssl_services_sup}
    started: [{pid,<0.2462.0>},
              {id,ssl_service_events},
              {mfargs,{gen_event,start_link,[{local,ssl_service_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:26.096Z,ns_1@db3.lan:ns_ssl_services_setup<0.2463.0>:ns_ssl_services_setup:maybe_store_ca_certs:832]Considering to store CA certs
[ns_server:debug,2025-05-15T18:47:26.097Z,ns_1@db3.lan:ns_ssl_services_setup<0.2463.0>:ns_ssl_services_setup:maybe_store_ca_certs:847]Updating CA file with 1 certificates
[ns_server:info,2025-05-15T18:47:26.101Z,ns_1@db3.lan:ns_ssl_services_setup<0.2463.0>:ns_ssl_services_setup:maybe_store_ca_certs:850]CA file updated: 1 cert(s) written
[ns_server:debug,2025-05-15T18:47:26.106Z,ns_1@db3.lan:ns_ssl_services_setup<0.2463.0>:ns_ssl_services_setup:restart_regenerate_client_cert_timer:1447]Time left before client cert regeneration: 70588799000
[error_logger:info,2025-05-15T18:47:26.106Z,ns_1@db3.lan:ns_ssl_services_sup<0.2461.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_ssl_services_sup}
    started: [{pid,<0.2463.0>},
              {id,ns_ssl_services_setup},
              {mfargs,{ns_ssl_services_setup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:47:26.106Z,ns_1@db3.lan:ns_ssl_services_setup<0.2463.0>:ns_ssl_services_setup:validate_pkey:1032]Private key (node_cert) passphrase validation suceeded
[ns_server:info,2025-05-15T18:47:26.107Z,ns_1@db3.lan:ns_ssl_services_setup<0.2463.0>:ns_ssl_services_setup:validate_pkey:1032]Private key (client_cert) passphrase validation suceeded
[ns_server:debug,2025-05-15T18:47:26.107Z,ns_1@db3.lan:ns_ssl_services_setup<0.2463.0>:ns_ssl_services_setup:notify_services:1146]Going to notify following services: [cb_dist_tls,capi_ssl_service,memcached,
                                     server_cert_event,client_cert_event]
[ns_server:warn,2025-05-15T18:47:26.107Z,ns_1@db3.lan:<0.2480.0>:ns_ssl_services_setup:notify_service:1184]Failed to notify service memcached: {error,no_proccess}
[ns_server:info,2025-05-15T18:47:26.107Z,ns_1@db3.lan:<0.2482.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service client_cert_event
[ns_server:debug,2025-05-15T18:47:26.107Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Restarting tls distribution protocols (if any)
[ns_server:info,2025-05-15T18:47:26.107Z,ns_1@db3.lan:<0.2481.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service server_cert_event
[error_logger:info,2025-05-15T18:47:26.107Z,ns_1@db3.lan:net_kernel<0.2253.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {15946801,#Ref<0.3529285563.3650748419.3237>}}}
[ns_server:info,2025-05-15T18:47:26.107Z,ns_1@db3.lan:<0.2470.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2025-05-15T18:47:26.108Z,ns_1@db3.lan:<0.2470.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2025-05-15T18:47:26.108Z,ns_1@db3.lan:<0.2470.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2025-05-15T18:47:26.108Z,ns_1@db3.lan:<0.2470.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:debug,2025-05-15T18:47:26.108Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Ensure config is going to change listeners. Will be stopped: [], will be started: [], will be restarted: []
[ns_server:info,2025-05-15T18:47:26.108Z,ns_1@db3.lan:<0.2470.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2025-05-15T18:47:26.108Z,ns_1@db3.lan:<0.2478.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service cb_dist_tls
[ns_server:debug,2025-05-15T18:47:26.108Z,ns_1@db3.lan:net_kernel<0.2253.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:26.108Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3529285563.3650617346.4799>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:47:26.108Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3529285563.3650617346.4799>,
                                  inet_tcp_dist,<0.2484.0>,
                                  #Ref<0.3529285563.3650617346.4802>}
[ns_server:debug,2025-05-15T18:47:26.109Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3529285563.3650617346.4799>,
                               inet_tcp_dist,<0.2484.0>,
                               #Ref<0.3529285563.3650617346.4802>}
[error_logger:info,2025-05-15T18:47:26.109Z,ns_1@db3.lan:net_kernel<0.2253.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.2484.0>,shutdown}}
[error_logger:info,2025-05-15T18:47:26.109Z,ns_1@db3.lan:net_kernel<0.2253.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:47:26.109Z,ns_1@db3.lan:<0.2479.0>:ns_couchdb_api:rpc_couchdb_node:152]RPC to couchdb node failed for restart_capi_ssl_service with {badrpc,nodedown}
Stack: [{ns_couchdb_api,rpc_couchdb_node,4,
                        [{file,"src/ns_couchdb_api.erl"},{line,150}]},
        {ns_ssl_services_setup,do_notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1203}]},
        {ns_ssl_services_setup,notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1179}]},
        {async,'-async_init/4-fun-1-',3,[{file,"src/async.erl"},{line,199}]}]
[ns_server:warn,2025-05-15T18:47:26.109Z,ns_1@db3.lan:<0.2479.0>:ns_ssl_services_setup:notify_service:1184]Failed to notify service capi_ssl_service: {'EXIT',{error,{badrpc,nodedown}}}
[ns_server:info,2025-05-15T18:47:26.109Z,ns_1@db3.lan:ns_ssl_services_setup<0.2463.0>:ns_ssl_services_setup:notify_services:1162]Succesfully notified services [client_cert_event,server_cert_event,
                               cb_dist_tls]
[ns_server:info,2025-05-15T18:47:26.109Z,ns_1@db3.lan:<0.2470.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"0.0.0.0"},
 [{keyfile,"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem"},
  {certfile,"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem"},
  {versions,['tlsv1.2','tlsv1.3']},
  {cacerts,[<<48,130,3,12,48,130,1,244,160,3,2,1,2,2,8,24,63,199,191,156,197,
              82,17,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,48,
              32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,101,
              114,118,101,114,32,99,54,52,100,57,49,52,101,48,30,23,13,49,51,
              48,49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,51,
              53,57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,117,
              99,104,98,97,115,101,32,83,101,114,118,101,114,32,99,54,52,100,
              57,49,52,101,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,1,
              5,0,3,130,1,15,0,48,130,1,10,2,130,1,1,0,189,71,7,138,162,116,
              202,173,172,80,169,207,183,229,131,28,103,231,118,113,23,184,
              163,118,132,50,140,197,100,9,14,13,222,41,184,111,89,132,2,102,
              81,20,185,124,217,165,68,88,93,96,211,184,188,36,17,35,176,167,
              109,82,1,80,201,124,250,21,203,22,244,2,187,45,29,22,162,207,
              172,158,177,99,109,81,39,155,109,84,244,123,242,203,49,197,116,
              158,244,151,83,143,242,44,81,204,228,195,44,95,101,252,10,164,
              27,51,148,102,46,28,237,20,182,124,42,212,174,43,184,195,216,
              21,135,233,45,167,9,73,251,27,220,165,132,153,156,0,160,219,
              145,166,59,196,219,128,44,31,123,111,143,183,1,7,155,248,244,
              114,116,107,27,205,0,89,187,228,101,131,201,74,251,79,181,48,
              113,50,90,20,129,109,127,47,137,209,73,173,217,56,197,53,141,3,
              62,155,117,88,76,228,40,82,177,57,208,108,251,183,13,115,126,
              89,27,138,240,86,239,196,39,94,14,94,20,12,94,74,91,82,150,9,
              197,47,35,206,76,210,35,237,68,111,174,228,31,136,249,66,25,
              209,244,127,176,195,100,189,2,3,1,0,1,163,66,48,64,48,14,6,3,
              85,29,15,1,1,255,4,4,3,2,1,134,48,15,6,3,85,29,19,1,1,255,4,5,
              48,3,1,1,255,48,29,6,3,85,29,14,4,22,4,20,188,1,42,182,50,72,
              116,82,198,94,225,242,47,206,206,38,9,39,211,154,48,13,6,9,42,
              134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,7,122,181,155,55,73,
              115,117,88,215,178,67,9,164,79,41,143,213,122,120,183,161,158,
              179,24,159,107,244,185,214,40,78,200,59,208,49,4,2,152,24,193,
              11,28,199,253,134,219,80,153,22,107,94,33,144,139,251,88,92,
              185,69,22,78,200,112,130,3,234,35,142,100,178,218,128,56,66,64,
              147,226,231,103,70,132,16,120,107,97,39,6,241,133,244,226,72,
              227,53,189,39,13,56,28,212,121,112,93,92,197,98,43,69,209,137,
              1,127,228,110,207,204,246,60,244,73,57,87,206,37,77,194,32,24,
              120,98,49,146,58,191,127,2,164,235,7,154,139,200,136,193,5,110,
              232,107,27,181,32,75,241,254,5,68,41,54,188,174,154,212,192,
              249,224,20,3,106,196,82,133,216,183,147,240,201,0,75,88,247,
              105,174,187,93,204,210,55,56,242,96,140,116,69,94,95,19,60,172,
              138,198,232,182,175,206,36,240,180,239,163,144,92,150,142,84,
              114,53,180,171,234,41,126,157,228,150,39,182,108,165,58,62,101,
              129,114,231,144,128,135,156,44,194,78,139,142,141,176,149,8,31,
              25,249,73,108,167,49,3>>]},
  {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,
        10,118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,
        158,232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,
        66,211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,
        250,145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,
        104,159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,
        246,169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,
        110,167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,
        118,190,67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,
        74,8,205,174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,
        221,95,184,110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,
        76,187,66,211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,
        69,254,147,103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,
        202,133,173,72,6,69,167,89,112,174,40,229,171,2,1,2>>},
  {ciphers,[#{cipher => aes_256_gcm,key_exchange => any,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm_8,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_ecdsa,
              mac => aead,prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_rsa,
              mac => aead,prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_128_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf},
            #{cipher => aes_128_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf}]},
  {honor_cipher_order,true},
  {secure_renegotiate,true},
  {client_renegotiation,false},
  {password,"********"}],
 {ssl,true},
 {port,18091}]
[error_logger:info,2025-05-15T18:47:26.111Z,ns_1@db3.lan:<0.2470.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2470.0>,menelaus_web}
    started: [{pid,<0.2483.0>},
              {id,menelaus_web_ipv4},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet},
                        {ssl,true},
                        {name,menelaus_web_ssl},
                        {ssl_opts_fun,#Fun<ns_ssl_services_setup.11.39330155>},
                        {port,18091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:47:26.112Z,ns_1@db3.lan:<0.2470.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2025-05-15T18:47:26.112Z,ns_1@db3.lan:<0.2470.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2025-05-15T18:47:26.112Z,ns_1@db3.lan:<0.2470.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2025-05-15T18:47:26.112Z,ns_1@db3.lan:<0.2470.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2025-05-15T18:47:26.112Z,ns_1@db3.lan:<0.2470.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2025-05-15T18:47:26.113Z,ns_1@db3.lan:ns_ssl_services_setup<0.2463.0>:ns_ssl_services_setup:notify_services:1173]Failed to notify some services. Will retry in 5 sec, [{{error,no_proccess},
                                                       memcached},
                                                      {{'EXIT',
                                                        {error,
                                                         {badrpc,nodedown}}},
                                                       capi_ssl_service}]
[ns_server:info,2025-05-15T18:47:26.113Z,ns_1@db3.lan:<0.2470.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"::"},
 [{keyfile,"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem"},
  {certfile,"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem"},
  {versions,['tlsv1.2','tlsv1.3']},
  {cacerts,[<<48,130,3,12,48,130,1,244,160,3,2,1,2,2,8,24,63,199,191,156,197,
              82,17,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,48,
              32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,101,
              114,118,101,114,32,99,54,52,100,57,49,52,101,48,30,23,13,49,51,
              48,49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,51,
              53,57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,117,
              99,104,98,97,115,101,32,83,101,114,118,101,114,32,99,54,52,100,
              57,49,52,101,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,1,
              5,0,3,130,1,15,0,48,130,1,10,2,130,1,1,0,189,71,7,138,162,116,
              202,173,172,80,169,207,183,229,131,28,103,231,118,113,23,184,
              163,118,132,50,140,197,100,9,14,13,222,41,184,111,89,132,2,102,
              81,20,185,124,217,165,68,88,93,96,211,184,188,36,17,35,176,167,
              109,82,1,80,201,124,250,21,203,22,244,2,187,45,29,22,162,207,
              172,158,177,99,109,81,39,155,109,84,244,123,242,203,49,197,116,
              158,244,151,83,143,242,44,81,204,228,195,44,95,101,252,10,164,
              27,51,148,102,46,28,237,20,182,124,42,212,174,43,184,195,216,
              21,135,233,45,167,9,73,251,27,220,165,132,153,156,0,160,219,
              145,166,59,196,219,128,44,31,123,111,143,183,1,7,155,248,244,
              114,116,107,27,205,0,89,187,228,101,131,201,74,251,79,181,48,
              113,50,90,20,129,109,127,47,137,209,73,173,217,56,197,53,141,3,
              62,155,117,88,76,228,40,82,177,57,208,108,251,183,13,115,126,
              89,27,138,240,86,239,196,39,94,14,94,20,12,94,74,91,82,150,9,
              197,47,35,206,76,210,35,237,68,111,174,228,31,136,249,66,25,
              209,244,127,176,195,100,189,2,3,1,0,1,163,66,48,64,48,14,6,3,
              85,29,15,1,1,255,4,4,3,2,1,134,48,15,6,3,85,29,19,1,1,255,4,5,
              48,3,1,1,255,48,29,6,3,85,29,14,4,22,4,20,188,1,42,182,50,72,
              116,82,198,94,225,242,47,206,206,38,9,39,211,154,48,13,6,9,42,
              134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,7,122,181,155,55,73,
              115,117,88,215,178,67,9,164,79,41,143,213,122,120,183,161,158,
              179,24,159,107,244,185,214,40,78,200,59,208,49,4,2,152,24,193,
              11,28,199,253,134,219,80,153,22,107,94,33,144,139,251,88,92,
              185,69,22,78,200,112,130,3,234,35,142,100,178,218,128,56,66,64,
              147,226,231,103,70,132,16,120,107,97,39,6,241,133,244,226,72,
              227,53,189,39,13,56,28,212,121,112,93,92,197,98,43,69,209,137,
              1,127,228,110,207,204,246,60,244,73,57,87,206,37,77,194,32,24,
              120,98,49,146,58,191,127,2,164,235,7,154,139,200,136,193,5,110,
              232,107,27,181,32,75,241,254,5,68,41,54,188,174,154,212,192,
              249,224,20,3,106,196,82,133,216,183,147,240,201,0,75,88,247,
              105,174,187,93,204,210,55,56,242,96,140,116,69,94,95,19,60,172,
              138,198,232,182,175,206,36,240,180,239,163,144,92,150,142,84,
              114,53,180,171,234,41,126,157,228,150,39,182,108,165,58,62,101,
              129,114,231,144,128,135,156,44,194,78,139,142,141,176,149,8,31,
              25,249,73,108,167,49,3>>]},
  {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,
        10,118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,
        158,232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,
        66,211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,
        250,145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,
        104,159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,
        246,169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,
        110,167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,
        118,190,67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,
        74,8,205,174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,
        221,95,184,110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,
        76,187,66,211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,
        69,254,147,103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,
        202,133,173,72,6,69,167,89,112,174,40,229,171,2,1,2>>},
  {ciphers,[#{cipher => aes_256_gcm,key_exchange => any,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm_8,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_ecdsa,
              mac => aead,prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_rsa,
              mac => aead,prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_128_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf},
            #{cipher => aes_128_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf}]},
  {honor_cipher_order,true},
  {secure_renegotiate,true},
  {client_renegotiation,false},
  {password,"********"}],
 {ssl,true},
 {port,18091}]
[error_logger:info,2025-05-15T18:47:26.115Z,ns_1@db3.lan:<0.2470.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2470.0>,menelaus_web}
    started: [{pid,<0.2504.0>},
              {id,menelaus_web_ipv6},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet6},
                        {ssl,true},
                        {name,menelaus_web_ssl},
                        {ssl_opts_fun,#Fun<ns_ssl_services_setup.11.39330155>},
                        {port,18091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:26.115Z,ns_1@db3.lan:<0.2469.0>:restartable:start_child:92]Started child process <0.2470.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
[error_logger:info,2025-05-15T18:47:26.115Z,ns_1@db3.lan:ns_ssl_services_sup<0.2461.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_ssl_services_sup}
    started: [{pid,<0.2469.0>},
              {id,ns_rest_ssl_service},
              {mfargs,
                  {restartable,start_link,
                      [{ns_ssl_services_setup,start_link_rest_service,[]},
                       1000]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:26.115Z,ns_1@db3.lan:ns_server_nodes_sup<0.2446.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.2461.0>},
              {id,ns_ssl_services_sup},
              {mfargs,{ns_ssl_services_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:26.115Z,ns_1@db3.lan:ns_server_nodes_sup<0.2446.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.2523.0>},
              {id,ldap_auth_cache},
              {mfargs,{ldap_auth_cache,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:26.115Z,ns_1@db3.lan:cb_saml<0.2525.0>:cb_saml:handle_info:316]Settings have changed or this is the first start
[error_logger:info,2025-05-15T18:47:26.115Z,ns_1@db3.lan:ns_server_nodes_sup<0.2446.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.2525.0>},
              {id,cb_saml},
              {mfargs,{cb_saml,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:26.115Z,ns_1@db3.lan:cb_saml<0.2525.0>:cb_saml:restart_refresh_timer:586]Restarting refresh timer: 7040 ms
[error_logger:info,2025-05-15T18:47:26.115Z,ns_1@db3.lan:users_sup<0.2527.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_sup}
    started: [{pid,<0.2528.0>},
              {id,user_storage_events},
              {mfargs,{gen_event,start_link,[{local,user_storage_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:26.116Z,ns_1@db3.lan:users_replicator<0.2530.0>:replicated_storage:wait_for_startup:47]Start waiting for startup
[error_logger:info,2025-05-15T18:47:26.116Z,ns_1@db3.lan:users_storage_sup<0.2529.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_storage_sup}
    started: [{pid,<0.2530.0>},
              {id,users_replicator},
              {mfargs,{menelaus_users,start_replicator,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:26.116Z,ns_1@db3.lan:users_storage<0.2531.0>:replicated_storage:announce_startup:61]Announce my startup to <0.2530.0>
[ns_server:debug,2025-05-15T18:47:26.116Z,ns_1@db3.lan:users_storage<0.2531.0>:replicated_dets:open:177]Opening file "/opt/couchbase/var/lib/couchbase/config/users.dets"
[ns_server:debug,2025-05-15T18:47:26.116Z,ns_1@db3.lan:users_replicator<0.2530.0>:replicated_storage:wait_for_startup:50]Received replicated storage registration from <0.2531.0>
[error_logger:info,2025-05-15T18:47:26.116Z,ns_1@db3.lan:users_storage_sup<0.2529.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_storage_sup}
    started: [{pid,<0.2531.0>},
              {id,users_storage},
              {mfargs,{menelaus_users,start_storage,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:26.116Z,ns_1@db3.lan:users_sup<0.2527.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_sup}
    started: [{pid,<0.2529.0>},
              {id,users_storage_sup},
              {mfargs,{users_storage_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:47:26.116Z,ns_1@db3.lan:compiled_roles_cache<0.2536.0>:versioned_cache:init:41]Starting versioned cache compiled_roles_cache
[error_logger:info,2025-05-15T18:47:26.116Z,ns_1@db3.lan:users_sup<0.2527.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_sup}
    started: [{pid,<0.2536.0>},
              {id,compiled_roles_cache},
              {mfargs,{menelaus_roles,start_compiled_roles_cache,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:26.116Z,ns_1@db3.lan:users_sup<0.2527.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_sup}
    started: [{pid,<0.2539.0>},
              {id,roles_cache},
              {mfargs,{roles_cache,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:26.116Z,ns_1@db3.lan:ns_server_nodes_sup<0.2446.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.2527.0>},
              {id,users_sup},
              {mfargs,{users_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:47:26.117Z,ns_1@db3.lan:users_storage<0.2531.0>:replicated_dets:select_from_table:312][dets] Starting select with {users_storage,
                                [{{docv2,'_','_','_'},[],['$_']}],
                                100}
[ns_server:debug,2025-05-15T18:47:26.117Z,ns_1@db3.lan:users_storage<0.2531.0>:replicated_dets:init_after_ack:170]Loading 0 items, 307 words took 1ms
[ns_server:debug,2025-05-15T18:47:26.117Z,ns_1@db3.lan:users_replicator<0.2530.0>:doc_replicator:loop:79]Replicating all docs to new nodes: ['ns_1@172.19.0.4']
[ns_server:debug,2025-05-15T18:47:26.118Z,ns_1@db3.lan:users_replicator<0.2530.0>:replicated_dets:select_from_table:312][dets] Starting select with {users_storage,[{'_',[],['$_']}],500}
[ns_server:debug,2025-05-15T18:47:26.120Z,ns_1@db3.lan:<0.2544.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,
                                    start_couchdb_node} with delay of 5000
[error_logger:info,2025-05-15T18:47:26.121Z,ns_1@db3.lan:<0.2545.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2545.0>,suppress_max_restart_intensity}
    started: [{pid,<0.2546.0>},
              {id,start_couchdb_node},
              {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,86400000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:26.121Z,ns_1@db3.lan:<0.2543.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2543.0>,suppress_max_restart_intensity}
    started: [{pid,<0.2544.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      start_couchdb_node}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,start_couchdb_node},
                       5000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 5,id => start_couchdb_node,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [],restart => permanent,
                          shutdown => 86400000,
                          start => {ns_server_nodes_sup,start_couchdb_node,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:26.121Z,ns_1@db3.lan:ns_server_nodes_sup<0.2446.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.2543.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,start_couchdb_node}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 5,id => start_couchdb_node,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [],restart => permanent,
                         shutdown => 86400000,
                         start => {ns_server_nodes_sup,start_couchdb_node,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:47:26.121Z,ns_1@db3.lan:wait_link_to_couchdb_node<0.2547.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:158]Waiting for ns_couchdb node to start
[error_logger:info,2025-05-15T18:47:26.121Z,ns_1@db3.lan:net_kernel<0.2253.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {15946802,#Ref<0.3529285563.3650748419.3237>}}}
[ns_server:debug,2025-05-15T18:47:26.121Z,ns_1@db3.lan:net_kernel<0.2253.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:26.121Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3529285563.3650617352.4160>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:47:26.121Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3529285563.3650617352.4160>,
                                  inet_tcp_dist,<0.2549.0>,
                                  #Ref<0.3529285563.3650617352.4163>}
[error_logger:info,2025-05-15T18:47:26.122Z,ns_1@db3.lan:net_kernel<0.2253.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.2549.0>,shutdown}}
[ns_server:debug,2025-05-15T18:47:26.122Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3529285563.3650617352.4160>,
                               inet_tcp_dist,<0.2549.0>,
                               #Ref<0.3529285563.3650617352.4163>}
[error_logger:info,2025-05-15T18:47:26.122Z,ns_1@db3.lan:net_kernel<0.2253.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:47:26.122Z,ns_1@db3.lan:<0.2548.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2025-05-15T18:47:26.209Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Accepted new connection from <0.2254.0> DistCtrl #Port<0.99>: {con,
                                                                        #Ref<0.3529285563.3650617352.4168>,
                                                                        inet_tcp_dist,
                                                                        undefined,
                                                                        undefined}
[ns_server:debug,2025-05-15T18:47:26.210Z,ns_1@db3.lan:net_kernel<0.2253.0>:cb_dist:info_msg:1098]cb_dist: Accepting connection from <0.2254.0> using module inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:26.210Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3529285563.3650617352.4168>,
                                  inet_tcp_dist,<0.2551.0>,
                                  #Ref<0.3529285563.3650617352.4171>}
[ns_server:debug,2025-05-15T18:47:26.210Z,ns_1@db3.lan:<0.2533.0>:doc_replicator:nodeup_monitoring_loop:145]got nodeup event. Considering ddocs replication
[ns_server:debug,2025-05-15T18:47:26.211Z,ns_1@db3.lan:users_replicator<0.2530.0>:doc_replicator:loop:79]Replicating all docs to new nodes: ['ns_1@db2.lan']
[ns_server:debug,2025-05-15T18:47:26.211Z,ns_1@db3.lan:users_replicator<0.2530.0>:replicated_dets:select_from_table:312][dets] Starting select with {users_storage,[{'_',[],['$_']}],500}
[error_logger:info,2025-05-15T18:47:26.323Z,ns_1@db3.lan:net_kernel<0.2253.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {15946803,#Ref<0.3529285563.3650748419.3237>}}}
[ns_server:debug,2025-05-15T18:47:26.324Z,ns_1@db3.lan:net_kernel<0.2253.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:26.324Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3529285563.3650617352.4188>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:47:26.324Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3529285563.3650617352.4188>,
                                  inet_tcp_dist,<0.2554.0>,
                                  #Ref<0.3529285563.3650617352.4191>}
[error_logger:info,2025-05-15T18:47:26.324Z,ns_1@db3.lan:net_kernel<0.2253.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.2554.0>,shutdown}}
[error_logger:info,2025-05-15T18:47:26.324Z,ns_1@db3.lan:net_kernel<0.2253.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:47:26.324Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3529285563.3650617352.4188>,
                               inet_tcp_dist,<0.2554.0>,
                               #Ref<0.3529285563.3650617352.4191>}
[ns_server:debug,2025-05-15T18:47:26.324Z,ns_1@db3.lan:<0.2548.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-05-15T18:47:26.525Z,ns_1@db3.lan:net_kernel<0.2253.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {15946804,#Ref<0.3529285563.3650748419.3237>}}}
[ns_server:debug,2025-05-15T18:47:26.525Z,ns_1@db3.lan:net_kernel<0.2253.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:26.525Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3529285563.3650617352.4200>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:47:26.525Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3529285563.3650617352.4200>,
                                  inet_tcp_dist,<0.2557.0>,
                                  #Ref<0.3529285563.3650617352.4203>}
[error_logger:info,2025-05-15T18:47:26.550Z,ns_1@db3.lan:net_kernel<0.2253.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.2557.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2025-05-15T18:47:26.550Z,ns_1@db3.lan:net_kernel<0.2253.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:47:26.550Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3529285563.3650617352.4200>,
                               inet_tcp_dist,<0.2557.0>,
                               #Ref<0.3529285563.3650617352.4203>}
[ns_server:debug,2025-05-15T18:47:26.550Z,ns_1@db3.lan:<0.2548.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-05-15T18:47:26.751Z,ns_1@db3.lan:net_kernel<0.2253.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {15946805,#Ref<0.3529285563.3650748419.3237>}}}
[ns_server:debug,2025-05-15T18:47:26.751Z,ns_1@db3.lan:net_kernel<0.2253.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:26.751Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3529285563.3650617351.3512>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:47:26.751Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3529285563.3650617351.3512>,
                                  inet_tcp_dist,<0.2559.0>,
                                  #Ref<0.3529285563.3650617352.4207>}
[ns_server:info,2025-05-15T18:47:26.753Z,ns_1@db3.lan:ns_couchdb_port<0.2546.0>:ns_port_server:log:226]ns_couchdb<0.2546.0>: =ERROR REPORT==== 15-May-2025::18:47:26.549966 ===
ns_couchdb<0.2546.0>: ** Connection attempt to/from node 'ns_1@db3.lan' rejected. Cookie is not set. **
ns_couchdb<0.2546.0>: 

[ns_server:debug,2025-05-15T18:47:26.753Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3529285563.3650617351.3512>,
                               inet_tcp_dist,<0.2559.0>,
                               #Ref<0.3529285563.3650617352.4207>}
[error_logger:info,2025-05-15T18:47:26.753Z,ns_1@db3.lan:net_kernel<0.2253.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.2559.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2025-05-15T18:47:26.753Z,ns_1@db3.lan:net_kernel<0.2253.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:47:26.753Z,ns_1@db3.lan:<0.2548.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-05-15T18:47:26.955Z,ns_1@db3.lan:net_kernel<0.2253.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {15946806,#Ref<0.3529285563.3650748419.3237>}}}
[ns_server:debug,2025-05-15T18:47:26.955Z,ns_1@db3.lan:net_kernel<0.2253.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:26.955Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3529285563.3650617352.4219>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:47:26.955Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3529285563.3650617352.4219>,
                                  inet_tcp_dist,<0.2561.0>,
                                  #Ref<0.3529285563.3650617352.4222>}
[error_logger:info,2025-05-15T18:47:26.958Z,ns_1@db3.lan:net_kernel<0.2253.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.2561.0>,{recv_challenge_failed,{error,closed}}}}
[ns_server:debug,2025-05-15T18:47:26.958Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3529285563.3650617352.4219>,
                               inet_tcp_dist,<0.2561.0>,
                               #Ref<0.3529285563.3650617352.4222>}
[error_logger:info,2025-05-15T18:47:26.958Z,ns_1@db3.lan:net_kernel<0.2253.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:47:26.958Z,ns_1@db3.lan:<0.2548.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2025-05-15T18:47:27.094Z,ns_1@db3.lan:memcached_refresh<0.2459.0>:memcached_refresh:update_refresh_state:137]Refresh of [isasl,rbac] skipped. Retry in 1000 ms.
[error_logger:info,2025-05-15T18:47:27.161Z,ns_1@db3.lan:net_kernel<0.2253.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {15946807,#Ref<0.3529285563.3650748419.3237>}}}
[ns_server:debug,2025-05-15T18:47:27.161Z,ns_1@db3.lan:net_kernel<0.2253.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:27.161Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3529285563.3650617350.3465>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:47:27.161Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3529285563.3650617350.3465>,
                                  inet_tcp_dist,<0.2563.0>,
                                  #Ref<0.3529285563.3650617350.3468>}
[error_logger:info,2025-05-15T18:47:27.162Z,ns_1@db3.lan:net_kernel<0.2253.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.2563.0>,{recv_challenge_failed,{error,closed}}}}
[ns_server:debug,2025-05-15T18:47:27.162Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3529285563.3650617350.3465>,
                               inet_tcp_dist,<0.2563.0>,
                               #Ref<0.3529285563.3650617350.3468>}
[error_logger:info,2025-05-15T18:47:27.162Z,ns_1@db3.lan:net_kernel<0.2253.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:47:27.162Z,ns_1@db3.lan:<0.2548.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-05-15T18:47:27.364Z,ns_1@db3.lan:net_kernel<0.2253.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {15946808,#Ref<0.3529285563.3650748419.3237>}}}
[ns_server:debug,2025-05-15T18:47:27.365Z,ns_1@db3.lan:net_kernel<0.2253.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:27.365Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3529285563.3650617349.4663>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:47:27.365Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3529285563.3650617349.4663>,
                                  inet_tcp_dist,<0.2565.0>,
                                  #Ref<0.3529285563.3650617349.4666>}
[ns_server:debug,2025-05-15T18:47:27.368Z,ns_1@db3.lan:<0.2548.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: false
[error_logger:info,2025-05-15T18:47:27.602Z,ns_1@db3.lan:ns_server_nodes_sup<0.2446.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.2547.0>},
              {id,wait_for_couchdb_node},
              {mfargs,{erlang,apply,
                              [#Fun<ns_server_nodes_sup.0.120652322>,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:27.602Z,ns_1@db3.lan:<0.2571.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,ns_disksup} with delay of 4000
[error_logger:info,2025-05-15T18:47:27.602Z,ns_1@db3.lan:<0.2572.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2572.0>,suppress_max_restart_intensity}
    started: [{pid,<0.2573.0>},
              {id,ns_disksup},
              {mfargs,{ns_disksup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.602Z,ns_1@db3.lan:<0.2570.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2570.0>,suppress_max_restart_intensity}
    started: [{pid,<0.2571.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      ns_disksup}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,ns_disksup},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => ns_disksup,inherited_max_r => 20,
                          inherited_max_t => 10,modules => [],
                          restart => permanent,shutdown => 1000,
                          start => {ns_disksup,start_link,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:27.602Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2570.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,ns_disksup}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => ns_disksup,inherited_max_r => 20,
                         inherited_max_t => 10,modules => [],
                         restart => permanent,shutdown => 1000,
                         start => {ns_disksup,start_link,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:27.603Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2574.0>},
              {id,diag_handler_worker},
              {mfargs,{work_queue,start_link,[diag_handler_worker]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:47:27.603Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:dir_size:start_link:33]Starting quick version of dir_size with program name: godu
[error_logger:info,2025-05-15T18:47:27.603Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2575.0>},
              {id,dir_size},
              {mfargs,{dir_size,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.603Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2576.0>},
              {id,request_tracker},
              {mfargs,{request_tracker,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.603Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2577.0>},
              {id,chronicle_kv_log},
              {mfargs,{chronicle_kv_log,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:warn,2025-05-15T18:47:27.604Z,ns_1@db3.lan:ns_log<0.2579.0>:gossip_replicator:read_logs:244]Couldn't load logs from "/opt/couchbase/var/lib/couchbase/ns_log" (perhaps it's first
                         startup): {error,enoent}
[error_logger:info,2025-05-15T18:47:27.604Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2579.0>},
              {id,ns_log},
              {mfargs,{ns_log,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.604Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2580.0>},
              {id,event_log_events},
              {mfargs,{gen_event,start_link,[{local,event_log_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.605Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2581.0>},
              {id,event_log_server},
              {mfargs,{event_log_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.609Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2583.0>},
              {id,initargs_updater},
              {mfargs,{initargs_updater,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.609Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2585.0>},
              {id,timer_lag_recorder},
              {mfargs,{timer_lag_recorder,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:27.609Z,ns_1@db3.lan:<0.2587.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,
                                    ns_babysitter_log_consumer} with delay of 4000
[error_logger:info,2025-05-15T18:47:27.609Z,ns_1@db3.lan:<0.2588.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2588.0>,suppress_max_restart_intensity}
    started: [{pid,<0.2589.0>},
              {id,ns_babysitter_log_consumer},
              {mfargs,{ns_log,start_link_babysitter_log_consumer,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.609Z,ns_1@db3.lan:<0.2586.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2586.0>,suppress_max_restart_intensity}
    started: [{pid,<0.2587.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      ns_babysitter_log_consumer}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,
                           ns_babysitter_log_consumer},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => ns_babysitter_log_consumer,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [],restart => permanent,shutdown => 1000,
                          start =>
                              {ns_log,start_link_babysitter_log_consumer,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:27.610Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2586.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,
                      ns_babysitter_log_consumer}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => ns_babysitter_log_consumer,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [],restart => permanent,shutdown => 1000,
                         start =>
                             {ns_log,start_link_babysitter_log_consumer,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:info,2025-05-15T18:47:27.654Z,ns_1@db3.lan:ns_couchdb_port<0.2546.0>:ns_port_server:log:226]ns_couchdb<0.2546.0>: Apache CouchDB v4.5.1-330-g3e5b8f24 (LogLevel=info) is starting.
ns_couchdb<0.2546.0>: Apache CouchDB has started. Time to relax.
ns_couchdb<0.2546.0>: 404: Booted. Waiting for shutdown request
ns_couchdb<0.2546.0>: 404: Booted. Waiting for shutdown request
ns_couchdb<0.2546.0>: working as port

[ns_server:debug,2025-05-15T18:47:27.679Z,ns_1@db3.lan:<0.2594.0>:goport:handle_eof:592]Stream 'stdout' closed
[ns_server:debug,2025-05-15T18:47:27.679Z,ns_1@db3.lan:<0.2594.0>:goport:handle_eof:592]Stream 'stderr' closed
[ns_server:info,2025-05-15T18:47:27.679Z,ns_1@db3.lan:<0.2594.0>:goport:handle_process_exit:573]Port exited with status 0.
[ns_server:debug,2025-05-15T18:47:27.682Z,ns_1@db3.lan:prometheus_cfg<0.2590.0>:prometheus_cfg:ensure_prometheus_config:932]Updating prometheus config file: /opt/couchbase/var/lib/couchbase/config/prometheus.yml
[ns_server:debug,2025-05-15T18:47:27.684Z,ns_1@db3.lan:prometheus_cfg<0.2590.0>:prometheus_cfg:ensure_prometheus_config:932]Updating prometheus config file: /opt/couchbase/var/lib/couchbase/config/prometheus_rules.yml
[ns_server:debug,2025-05-15T18:47:27.697Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',prometheus_auth_info} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{2,63914554047}}]}|
 {"@prometheus",
  {auth,
   [{<<"hash">>,
     {[{<<"hashes">>,
        {sanitized,<<"K8k/NfCHEgbJA5Gjm/Uf6LIWuV5wdCnmlHHxqgxuFcg=">>}},
       {<<"algorithm">>,<<"argon2id">>},
       {<<"salt">>,<<"DJB2Ptc3EvRFRaPMQY8lxQ==">>},
       {<<"parallelism">>,1},
       {<<"time">>,3},
       {<<"memory">>,524288}]}},
    {<<"scram-sha-512">>,
     {[{<<"salt">>,
        <<"kQZBb8aKY+QE2LMIih5sI+B6I8bZhl0CgtizD2Fc7mLeSYkH0HpqLrT4l2I+g3feMZ02/xAzulfE7pW7LKeNOg==">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"3t2jsPPn+9DcO1shgR+zoz6ZswYB3twV7AiZzEoEZaw=">>}}]}},
    {<<"scram-sha-256">>,
     {[{<<"salt">>,<<"t1Mu2m4fYVtuDH4+kaa1tY0l5MgkTK5scos+czMBlSU=">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"3/WNUnYG1nDOZfZnux37H+3kYuv6M9rzdGe6kGBsDos=">>}}]}},
    {<<"scram-sha-1">>,
     {[{<<"salt">>,<<"ejCbw2caHZmnb35/ZwMKng0kuTY=">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"koVlXdNIWFlL5fpQq2C/7QKV+kuonc55IuSVFkJpchc=">>}}]}}]}}]
[ns_server:debug,2025-05-15T18:47:27.698Z,ns_1@db3.lan:prometheus_cfg<0.2590.0>:prometheus_cfg:apply_config:724]Restarting Prometheus as the start specs have changed
[error_logger:info,2025-05-15T18:47:27.701Z,ns_1@db3.lan:ale_dynamic_sup<0.78.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ale_dynamic_sup}
    started: [{pid,<0.2597.0>},
              {id,'sink-prometheus'},
              {mfargs,
                  {ale_dynamic_sup,delay_death,
                      [{ale_disk_sink,start_link,
                           ['sink-prometheus',
                            "/opt/couchbase/var/lib/couchbase/logs/prometheus.log",
                            [{rotation,
                                 [{compress,true},
                                  {size,41943040},
                                  {num_files,10},
                                  {buffer_size_max,52428800}]}]]},
                       1000]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.800Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2590.0>},
              {id,prometheus_cfg},
              {mfargs,{prometheus_cfg,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:27.800Z,ns_1@db3.lan:memcached_passwords<0.2603.0>:memcached_cfg:init:60]Init config writer for memcached_passwords, "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2025-05-15T18:47:27.801Z,ns_1@db3.lan:memcached_passwords<0.2603.0>:memcached_cfg:write_cfg:156]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2025-05-15T18:47:27.813Z,ns_1@db3.lan:memcached_passwords<0.2603.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,{auth,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2025-05-15T18:47:27.814Z,ns_1@db3.lan:memcached_refresh<0.2459.0>:memcached_refresh:handle_call:57]File rename from "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw" is requested
[ns_server:debug,2025-05-15T18:47:27.814Z,ns_1@db3.lan:memcached_passwords<0.2603.0>:memcached_cfg:rename_and_refresh:178]Successfully renamed "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2025-05-15T18:47:27.815Z,ns_1@db3.lan:memcached_refresh<0.2459.0>:memcached_refresh:handle_cast:67]Refresh of isasl requested
[error_logger:info,2025-05-15T18:47:27.815Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2603.0>},
              {id,memcached_passwords},
              {mfargs,{memcached_passwords,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:27.815Z,ns_1@db3.lan:memcached_permissions<0.2606.0>:memcached_cfg:init:60]Init config writer for memcached_permissions, "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2025-05-15T18:47:27.815Z,ns_1@db3.lan:memcached_permissions<0.2606.0>:memcached_cfg:write_cfg:156]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2025-05-15T18:47:27.815Z,ns_1@db3.lan:memcached_permissions<0.2606.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,{user,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2025-05-15T18:47:27.816Z,ns_1@db3.lan:memcached_refresh<0.2459.0>:memcached_refresh:handle_call:57]File rename from "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac" is requested
[ns_server:debug,2025-05-15T18:47:27.816Z,ns_1@db3.lan:memcached_permissions<0.2606.0>:memcached_cfg:rename_and_refresh:178]Successfully renamed "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2025-05-15T18:47:27.816Z,ns_1@db3.lan:memcached_refresh<0.2459.0>:memcached_refresh:handle_cast:67]Refresh of rbac requested
[error_logger:info,2025-05-15T18:47:27.816Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2606.0>},
              {id,memcached_permissions},
              {mfargs,{memcached_permissions,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.816Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2609.0>},
              {id,ns_email_alert},
              {mfargs,{ns_email_alert,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.816Z,ns_1@db3.lan:ns_node_disco_sup<0.2610.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_node_disco_sup}
    started: [{pid,<0.2611.0>},
              {id,ns_node_disco_events},
              {mfargs,{gen_event,start_link,[{local,ns_node_disco_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:27.816Z,ns_1@db3.lan:ns_node_disco<0.2612.0>:ns_node_disco:init:111]Initting ns_node_disco with ['ns_1@172.19.0.4','ns_1@db2.lan']
[ns_server:debug,2025-05-15T18:47:27.817Z,ns_1@db3.lan:ns_cookie_manager<0.238.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[error_logger:info,2025-05-15T18:47:27.817Z,ns_1@db3.lan:ns_node_disco_sup<0.2610.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_node_disco_sup}
    started: [{pid,<0.2612.0>},
              {id,ns_node_disco},
              {mfargs,{ns_node_disco,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.817Z,ns_1@db3.lan:ns_node_disco_sup<0.2610.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_node_disco_sup}
    started: [{pid,<0.2615.0>},
              {id,ns_node_disco_log},
              {mfargs,{ns_node_disco_log,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:27.817Z,ns_1@db3.lan:<0.2614.0>:ns_node_disco:do_nodes_wanted_updated_fun:210]ns_node_disco: nodes_wanted updated: ['ns_1@172.19.0.4','ns_1@db2.lan',
                                      'ns_1@db3.lan'], with cookie: {sanitized,
                                                                     <<"biSYTaQUFDVndTs/b+IcDmD2L0woktv9naRdv3GCK6A=">>}
[error_logger:info,2025-05-15T18:47:27.817Z,ns_1@db3.lan:ns_config_rep_sup<0.2616.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_rep_sup}
    started: [{pid,<0.2617.0>},
              {id,ns_config_rep_merger},
              {mfargs,{ns_config_rep,start_link_merger,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:27.817Z,ns_1@db3.lan:ns_config_rep<0.2618.0>:ns_config_rep:init:80]init pulling
[ns_server:info,2025-05-15T18:47:27.817Z,ns_1@db3.lan:ns_config_rep<0.2618.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@db2.lan'
[ns_server:debug,2025-05-15T18:47:27.817Z,ns_1@db3.lan:<0.2614.0>:ns_node_disco:do_nodes_wanted_updated_fun:213]ns_node_disco: nodes_wanted pong: ['ns_1@172.19.0.4','ns_1@db2.lan',
                                   'ns_1@db3.lan'], with cookie: {sanitized,
                                                                  <<"biSYTaQUFDVndTs/b+IcDmD2L0woktv9naRdv3GCK6A=">>}
[ns_server:debug,2025-05-15T18:47:27.820Z,ns_1@db3.lan:ns_config_rep<0.2618.0>:ns_config_rep:init:82]init pushing
[error_logger:info,2025-05-15T18:47:27.822Z,ns_1@db3.lan:ns_config_rep_sup<0.2616.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_rep_sup}
    started: [{pid,<0.2618.0>},
              {id,ns_config_rep},
              {mfargs,{ns_config_rep,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.822Z,ns_1@db3.lan:ns_node_disco_sup<0.2610.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_node_disco_sup}
    started: [{pid,<0.2616.0>},
              {id,ns_config_rep_sup},
              {mfargs,{ns_config_rep_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:27.823Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2610.0>},
              {id,ns_node_disco_sup},
              {mfargs,{ns_node_disco_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:47:27.823Z,ns_1@db3.lan:tombstone_keeper<0.277.0>:tombstone_keeper:handle_call:49]Refreshed with timestamps []
[error_logger:info,2025-05-15T18:47:27.823Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2640.0>},
              {id,tombstone_agent},
              {mfargs,{tombstone_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.823Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2642.0>},
              {id,vbucket_map_mirror},
              {mfargs,{vbucket_map_mirror,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.823Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2644.0>},
              {id,capi_url_cache},
              {mfargs,{capi_url_cache,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.823Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2647.0>},
              {id,bucket_info_cache},
              {mfargs,{bucket_info_cache,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.823Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2650.0>},
              {id,ns_tick_event},
              {mfargs,{gen_event,start_link,[{local,ns_tick_event}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.823Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2651.0>},
              {id,buckets_events},
              {mfargs,{gen_event,start_link,[{local,buckets_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.823Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2652.0>},
              {id,ns_stats_event},
              {mfargs,{gen_event,start_link,[{local,ns_stats_event}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.823Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2653.0>},
              {id,samples_loader_tasks},
              {mfargs,{samples_loader_tasks,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.824Z,ns_1@db3.lan:ns_heart_sup<0.2654.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_heart_sup}
    started: [{pid,<0.2655.0>},
              {id,ns_heart},
              {mfargs,{ns_heart,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.824Z,ns_1@db3.lan:ns_heart_sup<0.2654.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_heart_sup}
    started: [{pid,<0.2657.0>},
              {id,ns_heart_slow_updater},
              {mfargs,{ns_heart,start_link_slow_updater,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.824Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2654.0>},
              {id,ns_heart_sup},
              {mfargs,{ns_heart_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:27.824Z,ns_1@db3.lan:ns_doctor_sup<0.2659.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_doctor_sup}
    started: [{pid,<0.2660.0>},
              {id,ns_doctor_events},
              {mfargs,{gen_event,start_link,[{local,ns_doctor_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.824Z,ns_1@db3.lan:ns_doctor_sup<0.2659.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_doctor_sup}
    started: [{pid,<0.2661.0>},
              {id,ns_doctor},
              {mfargs,{ns_doctor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:27.824Z,ns_1@db3.lan:<0.2658.0>:restartable:start_child:92]Started child process <0.2659.0>
  MFA: {ns_doctor_sup,start_link,[]}
[error_logger:info,2025-05-15T18:47:27.824Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2658.0>},
              {id,ns_doctor_sup},
              {mfargs,{restartable,start_link,
                                   [{ns_doctor_sup,start_link,[]},infinity]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:27.824Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2668.0>},
              {id,master_activity_events},
              {mfargs,{gen_event,start_link,[{local,master_activity_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.828Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2669.0>},
              {id,xdcr_ckpt_store},
              {mfargs,{simple_store,start_link,[xdcr_ckpt_data]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.828Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2672.0>},
              {id,metakv_worker},
              {mfargs,{work_queue,start_link,[metakv_worker]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.828Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2673.0>},
              {id,index_events},
              {mfargs,{gen_event,start_link,[{local,index_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.828Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2674.0>},
              {id,index_settings_manager},
              {mfargs,{index_settings_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.831Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2676.0>},
              {id,query_settings_manager},
              {mfargs,{query_settings_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.832Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2678.0>},
              {id,eventing_settings_manager},
              {mfargs,{eventing_settings_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.832Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2680.0>},
              {id,analytics_settings_manager},
              {mfargs,{analytics_settings_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.832Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2682.0>},
              {id,audit_events},
              {mfargs,{gen_event,start_link,[{local,audit_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:27.832Z,ns_1@db3.lan:<0.2684.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,
                                    encryption_service} with delay of 1000
[ns_server:debug,2025-05-15T18:47:27.833Z,ns_1@db3.lan:encryption_service<0.2686.0>:encryption_service:recover:199]Config marker /opt/couchbase/var/lib/couchbase/config/sm_load_config_marker doesn't exist
[error_logger:info,2025-05-15T18:47:27.833Z,ns_1@db3.lan:<0.2685.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2685.0>,suppress_max_restart_intensity}
    started: [{pid,<0.2686.0>},
              {id,encryption_service},
              {mfargs,{encryption_service,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.833Z,ns_1@db3.lan:<0.2683.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2683.0>,suppress_max_restart_intensity}
    started: [{pid,<0.2684.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      encryption_service}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,encryption_service},
                       1000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 1,id => encryption_service,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [encryption_service],
                          restart => permanent,shutdown => 5000,
                          start => {encryption_service,start_link,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:27.833Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2683.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,encryption_service}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 1,id => encryption_service,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [encryption_service],
                         restart => permanent,shutdown => 5000,
                         start => {encryption_service,start_link,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:27.833Z,ns_1@db3.lan:menelaus_sup<0.2690.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.2691.0>},
              {id,menelaus_ui_auth},
              {mfargs,{menelaus_ui_auth,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.833Z,ns_1@db3.lan:menelaus_sup<0.2690.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.2693.0>},
              {id,scram_sha},
              {mfargs,{scram_sha,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.833Z,ns_1@db3.lan:menelaus_sup<0.2690.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.2694.0>},
              {id,menelaus_local_auth},
              {mfargs,{menelaus_local_auth,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.834Z,ns_1@db3.lan:menelaus_sup<0.2690.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.2695.0>},
              {id,menelaus_web_cache},
              {mfargs,{menelaus_web_cache,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.834Z,ns_1@db3.lan:menelaus_sup<0.2690.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.2696.0>},
              {id,menelaus_stats_gatherer},
              {mfargs,{menelaus_stats_gatherer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.834Z,ns_1@db3.lan:menelaus_sup<0.2690.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.2697.0>},
              {id,json_rpc_events},
              {mfargs,{gen_event,start_link,[{local,json_rpc_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.834Z,ns_1@db3.lan:menelaus_web_sup<0.2698.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_web_sup}
    started: [{pid,<0.2699.0>},
              {id,menelaus_event},
              {mfargs,{menelaus_event,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:47:27.835Z,ns_1@db3.lan:<0.2701.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2025-05-15T18:47:27.835Z,ns_1@db3.lan:<0.2701.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2025-05-15T18:47:27.835Z,ns_1@db3.lan:<0.2701.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2025-05-15T18:47:27.836Z,ns_1@db3.lan:<0.2701.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2025-05-15T18:47:27.836Z,ns_1@db3.lan:<0.2701.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2025-05-15T18:47:27.836Z,ns_1@db3.lan:<0.2701.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"0.0.0.0"},{port,8091}]
[error_logger:info,2025-05-15T18:47:27.836Z,ns_1@db3.lan:<0.2701.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2701.0>,menelaus_web}
    started: [{pid,<0.2702.0>},
              {id,menelaus_web_ipv4},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet},
                        {name,menelaus_web},
                        {port,8091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:47:27.837Z,ns_1@db3.lan:<0.2701.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2025-05-15T18:47:27.837Z,ns_1@db3.lan:<0.2701.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2025-05-15T18:47:27.837Z,ns_1@db3.lan:<0.2701.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2025-05-15T18:47:27.838Z,ns_1@db3.lan:<0.2701.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2025-05-15T18:47:27.838Z,ns_1@db3.lan:<0.2701.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2025-05-15T18:47:27.838Z,ns_1@db3.lan:<0.2701.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"::"},{port,8091}]
[error_logger:info,2025-05-15T18:47:27.838Z,ns_1@db3.lan:<0.2701.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2701.0>,menelaus_web}
    started: [{pid,<0.2719.0>},
              {id,menelaus_web_ipv6},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet6},
                        {name,menelaus_web},
                        {port,8091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:27.838Z,ns_1@db3.lan:<0.2700.0>:restartable:start_child:92]Started child process <0.2701.0>
  MFA: {menelaus_web,start_link,[]}
[error_logger:info,2025-05-15T18:47:27.838Z,ns_1@db3.lan:menelaus_web_sup<0.2698.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_web_sup}
    started: [{pid,<0.2700.0>},
              {id,menelaus_web},
              {mfargs,{restartable,start_link,
                                   [{menelaus_web,start_link,[]},infinity]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[user:info,2025-05-15T18:47:27.838Z,ns_1@db3.lan:menelaus_sup<0.2690.0>:menelaus_web_sup:start_link:38]Couchbase Server has started on web port 8091 on node 'ns_1@db3.lan'. Version: "7.6.2-3721-enterprise".
[error_logger:info,2025-05-15T18:47:27.838Z,ns_1@db3.lan:menelaus_sup<0.2690.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.2698.0>},
              {id,menelaus_web_sup},
              {mfargs,{menelaus_web_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:27.838Z,ns_1@db3.lan:menelaus_sup<0.2690.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.2736.0>},
              {id,menelaus_web_alerts_srv},
              {mfargs,{menelaus_web_alerts_srv,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.839Z,ns_1@db3.lan:menelaus_sup<0.2690.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.2738.0>},
              {id,guardrail_monitor},
              {mfargs,{guardrail_monitor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.839Z,ns_1@db3.lan:menelaus_sup<0.2690.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.2739.0>},
              {id,menelaus_cbauth},
              {mfargs,{menelaus_cbauth,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.839Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2690.0>},
              {id,menelaus},
              {mfargs,{menelaus_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:47:27.839Z,ns_1@db3.lan:<0.2746.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,ns_ports_setup} with delay of 4000
[error_logger:info,2025-05-15T18:47:27.839Z,ns_1@db3.lan:<0.2747.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2747.0>,suppress_max_restart_intensity}
    started: [{pid,<0.2748.0>},
              {id,ns_ports_setup},
              {mfargs,{ns_ports_setup,start,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.839Z,ns_1@db3.lan:<0.2745.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2745.0>,suppress_max_restart_intensity}
    started: [{pid,<0.2746.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      ns_ports_setup}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,ns_ports_setup},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => ns_ports_setup,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [],restart => permanent,
                          shutdown => brutal_kill,
                          start => {ns_ports_setup,start,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:27.839Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2745.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,ns_ports_setup}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => ns_ports_setup,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [],restart => permanent,
                         shutdown => brutal_kill,
                         start => {ns_ports_setup,start,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:27.839Z,ns_1@db3.lan:service_agent_sup<0.2751.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_agent_sup}
    started: [{pid,<0.2752.0>},
              {id,service_agent_children_sup},
              {mfargs,{supervisor,start_link,
                                  [{local,service_agent_children_sup},
                                   service_agent_sup,child]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:27.839Z,ns_1@db3.lan:service_agent_sup<0.2751.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_agent_sup}
    started: [{pid,<0.2753.0>},
              {id,service_agent_worker},
              {mfargs,{erlang,apply,[#Fun<service_agent_sup.0.125430937>,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.839Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2751.0>},
              {id,service_agent_sup},
              {mfargs,{service_agent_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:27.840Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2755.0>},
              {id,ns_memcached_sockets_pool},
              {mfargs,{ns_memcached_sockets_pool,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:27.840Z,ns_1@db3.lan:memcached_auth_server<0.2756.0>:memcached_auth_server:reconnect:239]Skipping creation of 'Auth provider' connection because external users are disabled
[error_logger:info,2025-05-15T18:47:27.840Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2756.0>},
              {id,memcached_auth_server},
              {mfargs,{memcached_auth_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:27.840Z,ns_1@db3.lan:<0.2759.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,ns_audit_cfg} with delay of 4000
[ns_server:debug,2025-05-15T18:47:27.840Z,ns_1@db3.lan:ns_audit_cfg<0.2761.0>:ns_audit_cfg:write_audit_json:238]Writing new content to "/opt/couchbase/var/lib/couchbase/config/audit.json", Params [{descriptors_path,
                                                                                      "/opt/couchbase/etc/security"},
                                                                                     {version,
                                                                                      2},
                                                                                     {uuid,
                                                                                      "33985209"},
                                                                                     {event_states,
                                                                                      {[]}},
                                                                                     {filtering_enabled,
                                                                                      true},
                                                                                     {disabled_userids,
                                                                                      []},
                                                                                     {auditd_enabled,
                                                                                      false},
                                                                                     {log_path,
                                                                                      "/opt/couchbase/var/lib/couchbase/logs"},
                                                                                     {prune_age,
                                                                                      0},
                                                                                     {rotate_interval,
                                                                                      86400},
                                                                                     {rotate_size,
                                                                                      20971520},
                                                                                     {sync,
                                                                                      []}]
[ns_server:debug,2025-05-15T18:47:27.841Z,ns_1@db3.lan:ns_ports_setup<0.2748.0>:ns_ports_manager:set_dynamic_children:48]Setting children [memcached,saslauthd_port,goxdcr]
[ns_server:debug,2025-05-15T18:47:27.844Z,ns_1@db3.lan:ns_audit_cfg<0.2761.0>:ns_audit_cfg:notify_memcached:152]Instruct memcached to reload audit config
[error_logger:info,2025-05-15T18:47:27.844Z,ns_1@db3.lan:<0.2760.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2760.0>,suppress_max_restart_intensity}
    started: [{pid,<0.2761.0>},
              {id,ns_audit_cfg},
              {mfargs,{ns_audit_cfg,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.844Z,ns_1@db3.lan:<0.2758.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2758.0>,suppress_max_restart_intensity}
    started: [{pid,<0.2759.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      ns_audit_cfg}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,ns_audit_cfg},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => ns_audit_cfg,inherited_max_r => 20,
                          inherited_max_t => 10,modules => [],
                          restart => permanent,shutdown => 1000,
                          start => {ns_audit_cfg,start_link,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:27.845Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2758.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,ns_audit_cfg}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => ns_audit_cfg,inherited_max_r => 20,
                         inherited_max_t => 10,modules => [],
                         restart => permanent,shutdown => 1000,
                         start => {ns_audit_cfg,start_link,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:47:27.845Z,ns_1@db3.lan:<0.2767.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,ns_audit} with delay of 4000
[error_logger:info,2025-05-15T18:47:27.845Z,ns_1@db3.lan:<0.2768.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2768.0>,suppress_max_restart_intensity}
    started: [{pid,<0.2769.0>},
              {id,ns_audit},
              {mfargs,{ns_audit,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.845Z,ns_1@db3.lan:<0.2766.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2766.0>,suppress_max_restart_intensity}
    started: [{pid,<0.2767.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      ns_audit}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,ns_audit},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => ns_audit,inherited_max_r => 20,
                          inherited_max_t => 10,modules => [],
                          restart => permanent,shutdown => 1000,
                          start => {ns_audit,start_link,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:27.845Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2766.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,ns_audit}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => ns_audit,inherited_max_r => 20,
                         inherited_max_t => 10,modules => [],
                         restart => permanent,shutdown => 1000,
                         start => {ns_audit,start_link,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:47:27.846Z,ns_1@db3.lan:<0.2772.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,
                                    memcached_config_mgr} with delay of 4000
[ns_server:debug,2025-05-15T18:47:27.846Z,ns_1@db3.lan:memcached_config_mgr<0.2774.0>:memcached_config_mgr:memcached_port_pid:154]waiting for completion of initial ns_ports_setup round
[error_logger:info,2025-05-15T18:47:27.846Z,ns_1@db3.lan:<0.2773.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2773.0>,suppress_max_restart_intensity}
    started: [{pid,<0.2774.0>},
              {id,memcached_config_mgr},
              {mfargs,{memcached_config_mgr,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.846Z,ns_1@db3.lan:<0.2771.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2771.0>,suppress_max_restart_intensity}
    started: [{pid,<0.2772.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      memcached_config_mgr}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,memcached_config_mgr},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => memcached_config_mgr,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [],restart => permanent,shutdown => 1000,
                          start => {memcached_config_mgr,start_link,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:27.846Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2771.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,memcached_config_mgr}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => memcached_config_mgr,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [],restart => permanent,shutdown => 1000,
                         start => {memcached_config_mgr,start_link,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:info,2025-05-15T18:47:27.846Z,ns_1@db3.lan:<0.2775.0>:ns_memcached_log_rotator:init:36]Starting log rotator on "/opt/couchbase/var/lib/couchbase/logs"/"memcached.log"* with an initial period of 39003ms
[error_logger:info,2025-05-15T18:47:27.846Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2775.0>},
              {id,ns_memcached_log_rotator},
              {mfargs,{ns_memcached_log_rotator,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.848Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2776.0>},
              {id,testconditions_store},
              {mfargs,{simple_store,start_link,[testconditions]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:27.848Z,ns_1@db3.lan:<0.2777.0>:memcached_config_mgr:memcached_port_pid:154]waiting for completion of initial ns_ports_setup round
[error_logger:info,2025-05-15T18:47:27.848Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2777.0>},
              {id,terse_cluster_info_uploader},
              {mfargs,{terse_cluster_info_uploader,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.848Z,ns_1@db3.lan:ns_bucket_worker_sup<0.2779.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_bucket_worker_sup}
    started: [{pid,<0.2780.0>},
              {id,ns_bucket_sup},
              {mfargs,{ns_bucket_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:27.848Z,ns_1@db3.lan:ns_bucket_worker_sup<0.2779.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_bucket_worker_sup}
    started: [{pid,<0.2781.0>},
              {id,ns_bucket_worker},
              {mfargs,{ns_bucket_worker,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.849Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2779.0>},
              {id,ns_bucket_worker_sup},
              {mfargs,{ns_bucket_worker_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:27.853Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2785.0>},
              {id,ns_server_stats},
              {mfargs,{ns_server_stats,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.853Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2797.0>},
              {id,{stats_reader,"@system"}},
              {mfargs,{stats_reader,start_link,["@system"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.853Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2800.0>},
              {id,{stats_reader,"@system-processes"}},
              {mfargs,{stats_reader,start_link,["@system-processes"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.853Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2802.0>},
              {id,{stats_reader,"@query"}},
              {mfargs,{stats_reader,start_link,["@query"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.853Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2804.0>},
              {id,{stats_reader,"@global"}},
              {mfargs,{stats_reader,start_link,["@global"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:27.853Z,ns_1@db3.lan:<0.2787.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ale_stats_events,<0.2785.0>} exited with reason {noproc,
                                                                                 {gen_statem,
                                                                                  call,
                                                                                  [mb_master,
                                                                                   master_node,
                                                                                   infinity]}}
[error_logger:info,2025-05-15T18:47:27.854Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2807.0>},
              {id,goxdcr_status_keeper},
              {mfargs,{goxdcr_status_keeper,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.854Z,ns_1@db3.lan:services_stats_sup<0.2808.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,services_stats_sup}
    started: [{pid,<0.2809.0>},
              {id,service_stats_children_sup},
              {mfargs,{supervisor,start_link,
                                  [{local,service_stats_children_sup},
                                   services_stats_sup,child]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:27.854Z,ns_1@db3.lan:service_status_keeper_sup<0.2810.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_status_keeper_sup}
    started: [{pid,<0.2811.0>},
              {id,service_status_keeper_worker},
              {mfargs,{work_queue,start_link,[service_status_keeper_worker]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.854Z,ns_1@db3.lan:service_status_keeper_sup<0.2810.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_status_keeper_sup}
    started: [{pid,<0.2812.0>},
              {id,service_status_keeper_index},
              {mfargs,{service_index,start_keeper,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.854Z,ns_1@db3.lan:service_status_keeper_sup<0.2810.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_status_keeper_sup}
    started: [{pid,<0.2815.0>},
              {id,service_status_keeper_fts},
              {mfargs,{service_fts,start_keeper,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.854Z,ns_1@db3.lan:service_status_keeper_sup<0.2810.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_status_keeper_sup}
    started: [{pid,<0.2818.0>},
              {id,service_status_keeper_eventing},
              {mfargs,{service_eventing,start_keeper,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.854Z,ns_1@db3.lan:services_stats_sup<0.2808.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,services_stats_sup}
    started: [{pid,<0.2810.0>},
              {id,service_status_keeper_sup},
              {mfargs,{service_status_keeper_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:27.855Z,ns_1@db3.lan:services_stats_sup<0.2808.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,services_stats_sup}
    started: [{pid,<0.2821.0>},
              {id,service_stats_worker},
              {mfargs,{erlang,apply,[#Fun<services_stats_sup.0.35188242>,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.855Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2808.0>},
              {id,services_stats_sup},
              {mfargs,{services_stats_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:47:27.855Z,ns_1@db3.lan:<0.2824.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,
                                    compaction_daemon} with delay of 4000
[ns_server:debug,2025-05-15T18:47:27.855Z,ns_1@db3.lan:<0.2828.0>:new_concurrency_throttle:init:109]init concurrent throttle process, pid: <0.2828.0>, type: kv_throttle# of available token: 1
[ns_server:debug,2025-05-15T18:47:27.855Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2025-05-15T18:47:27.855Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[error_logger:info,2025-05-15T18:47:27.855Z,ns_1@db3.lan:<0.2825.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2825.0>,suppress_max_restart_intensity}
    started: [{pid,<0.2826.0>},
              {id,compaction_daemon},
              {mfargs,{compaction_daemon,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,86400000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:27.856Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2025-05-15T18:47:27.856Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:47:27.856Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2025-05-15T18:47:27.856Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_master too soon. Next run will be in 3600s
[error_logger:info,2025-05-15T18:47:27.856Z,ns_1@db3.lan:<0.2823.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2823.0>,suppress_max_restart_intensity}
    started: [{pid,<0.2824.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      compaction_daemon}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,compaction_daemon},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => compaction_daemon,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [compaction_daemon],
                          restart => permanent,shutdown => 86400000,
                          start => {compaction_daemon,start_link,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:27.856Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2823.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,compaction_daemon}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => compaction_daemon,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [compaction_daemon],
                         restart => permanent,shutdown => 86400000,
                         start => {compaction_daemon,start_link,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:27.856Z,ns_1@db3.lan:cluster_logs_sup<0.2829.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,cluster_logs_sup}
    started: [{pid,<0.2830.0>},
              {id,ets_holder},
              {mfargs,{cluster_logs_collection_task,start_link_ets_holder,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.856Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2829.0>},
              {id,cluster_logs_sup},
              {mfargs,{cluster_logs_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:27.856Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2831.0>},
              {id,collections},
              {mfargs,{collections,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.856Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2833.0>},
              {id,leader_events},
              {mfargs,{gen_event,start_link,[{local,leader_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.856Z,ns_1@db3.lan:leader_leases_sup<0.2836.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_leases_sup}
    started: [{pid,<0.2837.0>},
              {id,leader_activities},
              {mfargs,{leader_activities,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,10000},
              {child_type,worker}]

[ns_server:warn,2025-05-15T18:47:27.857Z,ns_1@db3.lan:leader_lease_agent<0.2838.0>:leader_lease_agent:maybe_recover_persisted_lease:393]Found persisted lease [{node,'ns_1@cb.local'},
                       {uuid,<<"9a39908e5882941cd97af5604964e304">>},
                       {time_left,13615},
                       {status,active}]
[error_logger:info,2025-05-15T18:47:27.857Z,ns_1@db3.lan:leader_leases_sup<0.2836.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_leases_sup}
    started: [{pid,<0.2838.0>},
              {id,leader_lease_agent},
              {mfargs,{leader_lease_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.857Z,ns_1@db3.lan:leader_services_sup<0.2835.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_services_sup}
    started: [{pid,<0.2836.0>},
              {id,leader_leases_sup},
              {mfargs,{leader_leases_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:27.857Z,ns_1@db3.lan:leader_registry_sup<0.2839.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_registry_sup}
    started: [{pid,<0.2840.0>},
              {id,leader_registry},
              {mfargs,{leader_registry,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:27.857Z,ns_1@db3.lan:leader_registry_sup<0.2839.0>:mb_master:check_master_takeover_needed:183]Sending master node question to the following nodes: ['ns_1@db2.lan',
                                                      'ns_1@172.19.0.4']
[ns_server:debug,2025-05-15T18:47:27.858Z,ns_1@db3.lan:leader_registry_sup<0.2839.0>:mb_master:check_master_takeover_needed:187]Got replies: ['ns_1@172.19.0.4','ns_1@172.19.0.4']
[ns_server:debug,2025-05-15T18:47:27.858Z,ns_1@db3.lan:leader_registry_sup<0.2839.0>:mb_master:check_master_takeover_needed:203]Checking version of current master: 'ns_1@172.19.0.4'
[ns_server:debug,2025-05-15T18:47:27.858Z,ns_1@db3.lan:leader_registry_sup<0.2839.0>:mb_master:check_master_takeover_needed:219]Current master's compat version: [7,6,0] services [index,kv,n1ql]. This node's compat version: {[7,
                                                                                                 6,
                                                                                                 0],
                                                                                                release,
                                                                                                0} services [index,
                                                                                                             kv,
                                                                                                             n1ql]
[ns_server:debug,2025-05-15T18:47:27.858Z,ns_1@db3.lan:leader_registry_sup<0.2839.0>:mb_master:check_master_takeover_needed:233]Current master is strongly higher priority, not taking over
[ns_server:debug,2025-05-15T18:47:27.858Z,ns_1@db3.lan:mb_master<0.2842.0>:mb_master:init:86]Heartbeat interval is 2000
[ns_server:debug,2025-05-15T18:47:27.858Z,ns_1@db3.lan:mb_master<0.2842.0>:mb_master:init:102]Starting as candidate. Peers: ['ns_1@172.19.0.4','ns_1@db2.lan',
                               'ns_1@db3.lan']
[error_logger:info,2025-05-15T18:47:27.858Z,ns_1@db3.lan:leader_registry_sup<0.2839.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_registry_sup}
    started: [{pid,<0.2842.0>},
              {id,mb_master},
              {mfargs,{mb_master,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:27.858Z,ns_1@db3.lan:leader_services_sup<0.2835.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_services_sup}
    started: [{pid,<0.2839.0>},
              {id,leader_registry_sup},
              {mfargs,{leader_registry_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:47:27.858Z,ns_1@db3.lan:<0.2834.0>:restartable:start_child:92]Started child process <0.2835.0>
  MFA: {leader_services_sup,start_link,[]}
[error_logger:info,2025-05-15T18:47:27.858Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2834.0>},
              {id,leader_services_sup},
              {mfargs,{restartable,start_link,
                                   [{leader_services_sup,start_link,[]},
                                    infinity]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:27.859Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2844.0>},
              {id,ns_tick_agent},
              {mfargs,{ns_tick_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.859Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2846.0>},
              {id,master_activity_events_ingress},
              {mfargs,{gen_event,start_link,
                                 [{local,master_activity_events_ingress}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.859Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2847.0>},
              {id,master_activity_events_timestamper},
              {mfargs,{master_activity_events,start_link_timestamper,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.859Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2848.0>},
              {id,master_activity_events_pids_watcher},
              {mfargs,{master_activity_events_pids_watcher,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.859Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2849.0>},
              {id,master_activity_events_keeper},
              {mfargs,{master_activity_events_keeper,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.859Z,ns_1@db3.lan:health_monitor_sup<0.2851.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.2852.0>},
              {id,ns_server_monitor},
              {mfargs,{ns_server_monitor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.859Z,ns_1@db3.lan:health_monitor_sup<0.2851.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.2862.0>},
              {id,service_monitor_children_sup},
              {mfargs,{supervisor,start_link,
                                  [{local,service_monitor_children_sup},
                                   health_monitor_sup,child]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:27.859Z,ns_1@db3.lan:health_monitor_sup<0.2851.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.2863.0>},
              {id,service_monitor_worker},
              {mfargs,{erlang,apply,[#Fun<health_monitor_sup.0.30770475>,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.859Z,ns_1@db3.lan:health_monitor_sup<0.2851.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.2865.0>},
              {id,node_monitor},
              {mfargs,{node_monitor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.859Z,ns_1@db3.lan:health_monitor_sup<0.2851.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.2869.0>},
              {id,node_status_analyzer},
              {mfargs,{node_status_analyzer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.859Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2851.0>},
              {id,health_monitor_sup},
              {mfargs,{health_monitor_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:27.860Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2875.0>},
              {id,rebalance_agent},
              {mfargs,{rebalance_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.860Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2876.0>},
              {id,kv_hibernation_agent},
              {mfargs,{kv_hibernation_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.860Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2877.0>},
              {id,ns_rebalance_report_manager},
              {mfargs,{ns_rebalance_report_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:27.861Z,ns_1@db3.lan:cb_creds_rotation<0.2879.0>:cb_creds_rotation:start_rotate_timer:214]Starting creds rotation timer (1800000ms)
[error_logger:info,2025-05-15T18:47:27.861Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2879.0>},
              {id,creds_rotation},
              {mfargs,{cb_creds_rotation,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.861Z,ns_1@db3.lan:ns_server_nodes_sup<0.2446.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.2569.0>},
              {id,ns_server_sup},
              {mfargs,{ns_server_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:47:27.861Z,ns_1@db3.lan:ns_server_nodes_sup<0.2446.0>:one_shot_barrier:notify:21]Notifying on barrier menelaus_barrier
[error_logger:error,2025-05-15T18:47:27.861Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {local,ns_server_sup}
    errorContext: child_terminated
    reason: {noproc,{gen_statem,call,[mb_master,master_node,infinity]}}
    offender: [{pid,<0.2785.0>},
               {id,ns_server_stats},
               {mfargs,{ns_server_stats,start_link,[]}},
               {restart_type,permanent},
               {significant,false},
               {shutdown,1000},
               {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:27.861Z,ns_1@db3.lan:menelaus_barrier<0.2453.0>:one_shot_barrier:barrier_body:56]Barrier menelaus_barrier got notification from <0.2446.0>
[ns_server:debug,2025-05-15T18:47:27.861Z,ns_1@db3.lan:ns_server_nodes_sup<0.2446.0>:one_shot_barrier:notify:26]Successfuly notified on barrier menelaus_barrier
[ns_server:debug,2025-05-15T18:47:27.861Z,ns_1@db3.lan:<0.2445.0>:restartable:start_child:92]Started child process <0.2446.0>
  MFA: {ns_server_nodes_sup,start_link,[]}
[error_logger:info,2025-05-15T18:47:27.861Z,ns_1@db3.lan:ns_server_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2881.0>},
              {id,ns_server_stats},
              {mfargs,{ns_server_stats,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:27.861Z,ns_1@db3.lan:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.2445.0>},
              {id,ns_server_nodes_sup},
              {mfargs,{restartable,start_link,
                                   [{ns_server_nodes_sup,start_link,[]},
                                    infinity]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[user:info,2025-05-15T18:47:27.861Z,ns_1@db3.lan:ns_cluster<0.273.0>:ns_cluster:perform_actual_join:1660]Node ns_1@db3.lan joined cluster
[cluster:debug,2025-05-15T18:47:27.862Z,ns_1@db3.lan:ns_cluster<0.273.0>:ns_cluster:handle_call:433]complete_join([{<<"targetNode">>,<<"ns_1@db3.lan">>},
               {<<"requestedServices">>,[<<"index">>,<<"kv">>,<<"n1ql">>]},
               {<<"chronicleInfo">>,
                <<"g3QAAAAEZAAPY29tbWl0dGVkX3NlcW5vYRhkAA5jb21wYXRfdmVyc2lvbmEAZAAGY29uZmlnaAVkAAlsb2dfZW50cnltAAAAIDVhYWIwM2RiZWNhZDA2YjUwZmZiNDc0ZGE1OWEwMGM5aAJhBGQAD25zXzFAMTcyLjE5LjAuNGEYaApkAAZjb25maWdoA20AAAAgMWRkNTk5MTQ2M2Q1MTllMjM3MjlkZGEyYzgwYzM3NmJhAGEDYQBtAAAAIDQ1NzI0OTE1ZGFhMmE0ODljMGE2ODE2ZDIwMTk4NTg1dAAAAANkAA9uc18xQDE3Mi4xOS4wLjR0AAAAAmQAAmlkbQAAACAxZGQ1OTkxNDYzZDUxOWUyMzcyOWRkYTJjODBjMzc2YmQABHJvbGVkAAV2b3RlcmQADG5zXzFAZGIyLmxhbnQAAAACZAACaWRtAAAAIDlmNjg5NGI1MWZmYTQ5N2NkZmM4Yjc5MmQ5ZTY3MjZiZAAEcm9sZWQAB3JlcGxpY2FkAAxuc18xQGRiMy5sYW50AAAAAmQAAmlkbQAAACA2NTllYzAzYzgxMmY1Y2M5NDU2YmQ3MWZlM2MxYTA0N2QABHJvbGVkAAdyZXBsaWNhZAAJdW5kZWZpbmVkdAAAAAJkABRjaHJvbmljbGVfY29uZmlnX3JzbWgDZAAKcnNtX2NvbmZpZ2QAFGNocm9uaWNsZV9jb25maWdfcnNtamQAAmt2aANkAApyc21fY29uZmlnZAAMY2hyb25pY2xlX2t2anQAAAAAZAAJdW5kZWZpbmVkbAAAAAFoAm0AAAAgNWFhYjAzZGJlY2FkMDZiNTBmZmI0NzRkYTU5YTAwYzlhAGpkAApoaXN0b3J5X2lkbQAAACA1YWFiMDNkYmVjYWQwNmI1MGZmYjQ3NGRhNTlhMDBjOQ==">>},
               {<<"availableStorage">>,
                {[{<<"hdd">>,
                   [{[{<<"path">>,<<"/">>},
                      {<<"sizeKBytes">>,1056557396},
                      {<<"usagePercent">>,2}]},
                    {[{<<"path">>,<<"/dev">>},
                      {<<"sizeKBytes">>,65536},
                      {<<"usagePercent">>,0}]},
                    {[{<<"path">>,<<"/dev/shm">>},
                      {<<"sizeKBytes">>,65536},
                      {<<"usagePercent">>,0}]},
                    {[{<<"path">>,<<"/etc/resolv.conf">>},
                      {<<"sizeKBytes">>,1056557396},
                      {<<"usagePercent">>,2}]},
                    {[{<<"path">>,<<"/etc/hostname">>},
                      {<<"sizeKBytes">>,1056557396},
                      {<<"usagePercent">>,2}]},
                    {[{<<"path">>,<<"/etc/hosts">>},
                      {<<"sizeKBytes">>,1056557396},
                      {<<"usagePercent">>,2}]},
                    {[{<<"path">>,<<"/opt/couchbase/var">>},
                      {<<"sizeKBytes">>,482797652},
                      {<<"usagePercent">>,92}]},
                    {[{<<"path">>,<<"/proc/kcore">>},
                      {<<"sizeKBytes">>,65536},
                      {<<"usagePercent">>,0}]},
                    {[{<<"path">>,<<"/proc/keys">>},
                      {<<"sizeKBytes">>,65536},
                      {<<"usagePercent">>,0}]},
                    {[{<<"path">>,<<"/proc/timer_list">>},
                      {<<"sizeKBytes">>,65536},
                      {<<"usagePercent">>,0}]},
                    {[{<<"path">>,<<"/proc/scsi">>},
                      {<<"sizeKBytes">>,4012680},
                      {<<"usagePercent">>,0}]},
                    {[{<<"path">>,<<"/sys/firmware">>},
                      {<<"sizeKBytes">>,4012680},
                      {<<"usagePercent">>,0}]}]}]}},
               {<<"storageTotals">>,
                {[{<<"ram">>,
                   {[{<<"total">>,8217968640},
                     {<<"quotaTotal">>,3221225472},
                     {<<"quotaUsed">>,0},
                     {<<"used">>,4251119616},
                     {<<"usedByData">>,0},
                     {<<"quotaUsedPerNode">>,0},
                     {<<"quotaTotalPerNode">>,3221225472}]}},
                  {<<"hdd">>,
                   {[{<<"total">>,494384795648},
                     {<<"quotaTotal">>,494384795648},
                     {<<"used">>,454834011996},
                     {<<"usedByData">>,0},
                     {<<"free">>,39550783652}]}}]}},
               {<<"storage">>,
                {[{<<"ssd">>,[]},
                  {<<"hdd">>,
                   [{[{<<"path">>,<<"/opt/couchbase/var/lib/couchbase/data">>},
                      {<<"index_path">>,
                       <<"/opt/couchbase/var/lib/couchbase/data">>},
                      {<<"cbas_dirs">>,
                       [<<"/opt/couchbase/var/lib/couchbase/data">>]},
                      {<<"eventing_path">>,
                       <<"/opt/couchbase/var/lib/couchbase/data">>},
                      {<<"java_home">>,<<>>},
                      {<<"quotaMb">>,<<"none">>},
                      {<<"state">>,<<"ok">>}]}]}]}},
               {<<"clusterMembership">>,<<"active">>},
               {<<"recoveryType">>,<<"none">>},
               {<<"status">>,<<"healthy">>},
               {<<"otpNode">>,<<"ns_1@172.19.0.4">>},
               {<<"thisNode">>,true},
               {<<"hostname">>,<<"172.19.0.4:8091">>},
               {<<"nodeUUID">>,<<"28569ac00b9c1d7c50e39741027d428c">>},
               {<<"clusterCompatibility">>,458758},
               {<<"version">>,<<"7.6.2-3721-enterprise">>},
               {<<"os">>,<<"aarch64-unknown-linux-gnu">>},
               {<<"cpuCount">>,10},
               {<<"ports">>,
                {[{<<"direct">>,11210},
                  {<<"httpsMgmt">>,18091},
                  {<<"httpsCAPI">>,18092},
                  {<<"distTCP">>,21100},
                  {<<"distTLS">>,21150}]}},
               {<<"services">>,[<<"index">>,<<"kv">>,<<"n1ql">>]},
               {<<"nodeEncryption">>,false},
               {<<"nodeEncryptionClientCertVerification">>,false},
               {<<"addressFamilyOnly">>,false},
               {<<"configuredHostname">>,<<"172.19.0.4:8091">>},
               {<<"addressFamily">>,<<"inet">>},
               {<<"externalListeners">>,
                [{[{<<"afamily">>,<<"inet">>},{<<"nodeEncryption">>,false}]}]},
               {<<"serverGroup">>,<<"Group 1">>},
               {<<"otpCookie">>,
                {sanitized,<<"biSYTaQUFDVndTs/b+IcDmD2L0woktv9naRdv3GCK6A=">>}},
               {<<"couchApiBase">>,<<"http://172.19.0.4:8092/">>},
               {<<"couchApiBaseHTTPS">>,<<"https://172.19.0.4:18092/">>},
               {<<"nodeHash">>,38045879},
               {<<"systemStats">>,
                {[{<<"cpu_utilization_rate">>,3.89883035089008},
                  {<<"cpu_stolen_rate">>,0},
                  {<<"swap_total">>,1073737728},
                  {<<"swap_used">>,3956736},
                  {<<"mem_total">>,8217968640},
                  {<<"mem_free">>,4805804032},
                  {<<"mem_limit">>,8217968640},
                  {<<"cpu_cores_available">>,10},
                  {<<"allocstall">>,17}]}},
               {<<"interestingStats">>,{[]}},
               {<<"uptime">>,<<"44">>},
               {<<"memoryTotal">>,8217968640},
               {<<"memoryFree">>,4805804032},
               {<<"mcdMemoryReserved">>,6269},
               {<<"mcdMemoryAllocated">>,6269},
               {<<"memoryQuota">>,3072},
               {<<"queryMemoryQuota">>,0},
               {<<"indexMemoryQuota">>,512},
               {<<"ftsMemoryQuota">>,512},
               {<<"cbasMemoryQuota">>,1024},
               {<<"eventingMemoryQuota">>,256}]) -> {ok,ok}
[ns_server:info,2025-05-15T18:47:27.863Z,ns_1@db3.lan:ns_cluster<0.273.0>:ns_cluster:handle_info:514]Chronicle state is: provisioned
[ns_server:info,2025-05-15T18:47:27.863Z,ns_1@db3.lan:ns_cluster<0.273.0>:ns_cluster:handle_info:514]Chronicle state is: provisioned
[ns_server:debug,2025-05-15T18:47:27.980Z,ns_1@db3.lan:ns_ports_setup<0.2748.0>:ns_ports_setup:set_children:65]Monitor ns_child_ports_sup <16971.133.0>
[ns_server:debug,2025-05-15T18:47:27.981Z,ns_1@db3.lan:memcached_config_mgr<0.2774.0>:memcached_config_mgr:memcached_port_pid:156]ns_ports_setup seems to be ready
[ns_server:debug,2025-05-15T18:47:27.981Z,ns_1@db3.lan:<0.2777.0>:memcached_config_mgr:memcached_port_pid:156]ns_ports_setup seems to be ready
[ns_server:debug,2025-05-15T18:47:27.981Z,ns_1@db3.lan:memcached_config_mgr<0.2774.0>:memcached_config_mgr:find_port_pid_loop:164]Found memcached port <16971.139.0>
[ns_server:debug,2025-05-15T18:47:27.981Z,ns_1@db3.lan:<0.2777.0>:memcached_config_mgr:find_port_pid_loop:164]Found memcached port <16971.139.0>
[ns_server:debug,2025-05-15T18:47:27.982Z,ns_1@db3.lan:memcached_config_mgr<0.2774.0>:memcached_config_mgr:do_read_current_memcached_config:371]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2025-05-15T18:47:27.983Z,ns_1@db3.lan:memcached_config_mgr<0.2774.0>:memcached_config_mgr:init:100]found memcached port to be already active
[ns_server:debug,2025-05-15T18:47:27.983Z,ns_1@db3.lan:<0.2777.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":68,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"indexAdmin\":9100,\"indexHttp\":9102,\"indexHttps\":19102,\"indexScan\":9101,\"indexStreamCatchup\":9104,\"indexStreamInit\":9103,\"indexStreamMaint\":9105,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"hostname\":\"172.19.0.4\",\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[ns_server:debug,2025-05-15T18:47:27.986Z,ns_1@db3.lan:memcached_config_mgr<0.2774.0>:memcached_config_mgr:apply_changed_memcached_config:262]New memcached config is hot-reloadable.
[ns_server:debug,2025-05-15T18:47:27.987Z,ns_1@db3.lan:memcached_config_mgr<0.2774.0>:memcached_config_mgr:do_read_current_memcached_config:371]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[user:info,2025-05-15T18:47:27.996Z,ns_1@db3.lan:memcached_config_mgr<0.2774.0>:memcached_config_mgr:hot_reload_config:331]Hot-reloaded memcached.json for config change of the following keys: [<<"scramsha_fallback_salt">>]
[ns_server:info,2025-05-15T18:47:27.996Z,ns_1@db3.lan:memcached_config_mgr<0.2774.0>:memcached_config_mgr:push_tls_config:233]Pushing TLS config to memcached:
{[{<<"private key">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem">>},
  {<<"certificate chain">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem">>},
  {<<"CA file">>,<<"/opt/couchbase/var/lib/couchbase/config/certs/ca.pem">>},
  {<<"minimum version">>,<<"TLS 1.2">>},
  {<<"cipher list">>,
   {[{<<"TLS 1.2">>,<<"HIGH">>},
     {<<"TLS 1.3">>,
      <<"TLS_AES_256_GCM_SHA384:TLS_AES_128_GCM_SHA256:TLS_CHACHA20_POLY1305_SHA256">>}]}},
  {<<"cipher order">>,true},
  {<<"client cert auth">>,<<"disabled">>}]}
[ns_server:info,2025-05-15T18:47:27.998Z,ns_1@db3.lan:memcached_config_mgr<0.2774.0>:memcached_config_mgr:push_tls_config:237]Successfully pushed TLS config to memcached
[ns_server:debug,2025-05-15T18:47:28.001Z,ns_1@db3.lan:compiled_roles_cache<0.2536.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@",admin}
[ns_server:debug,2025-05-15T18:47:28.002Z,ns_1@db3.lan:json_rpc_connection-saslauthd-saslauthd-port<0.2929.0>:json_rpc_connection:init:71]Observed revrpc connection: label "saslauthd-saslauthd-port", handling process <0.2929.0>
[ns_server:debug,2025-05-15T18:47:28.006Z,ns_1@db3.lan:json_rpc_connection-goxdcr-cbauth<0.2934.0>:json_rpc_connection:init:71]Observed revrpc connection: label "goxdcr-cbauth", handling process <0.2934.0>
[ns_server:debug,2025-05-15T18:47:28.006Z,ns_1@db3.lan:menelaus_cbauth<0.2739.0>:menelaus_cbauth:handle_cast:203]Observed json rpc process {"goxdcr-cbauth",[{internal,true}],<0.2934.0>} started
[ns_server:debug,2025-05-15T18:47:28.011Z,ns_1@db3.lan:compiled_roles_cache<0.2536.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@goxdcr",admin}
[ns_server:debug,2025-05-15T18:47:28.098Z,ns_1@db3.lan:<0.2945.0>:memcached_refresh:do_refresh:153]Successfully connected to memcached, Trying to refresh [isasl,rbac]
[ns_server:debug,2025-05-15T18:47:28.100Z,ns_1@db3.lan:memcached_refresh<0.2459.0>:memcached_refresh:update_refresh_state:116]Refresh of [isasl,rbac] succeeded
[ns_server:debug,2025-05-15T18:47:28.106Z,ns_1@db3.lan:chronicle_kv_log<0.2577.0>:chronicle_kv_log:log:59]update (key: counters, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,26})
[{rebalance_start,{1747334848,1}}]
[ns_server:debug,2025-05-15T18:47:28.107Z,ns_1@db3.lan:<0.2777.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":69,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"indexAdmin\":9100,\"indexHttp\":9102,\"indexHttps\":19102,\"indexScan\":9101,\"indexStreamCatchup\":9104,\"indexStreamInit\":9103,\"indexStreamMaint\":9105,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"hostname\":\"172.19.0.4\",\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[ns_server:debug,2025-05-15T18:47:28.133Z,ns_1@db3.lan:chronicle_kv_log<0.2577.0>:chronicle_kv_log:log:59]update (key: rebalance_status, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                     27})
running
[ns_server:debug,2025-05-15T18:47:28.133Z,ns_1@db3.lan:chronicle_kv_log<0.2577.0>:chronicle_kv_log:log:59]update (key: rebalance_status_uuid, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                          27})
<<"1827da82069c38267f07251824989d63">>
[ns_server:debug,2025-05-15T18:47:28.133Z,ns_1@db3.lan:chronicle_kv_log<0.2577.0>:chronicle_kv_log:log:59]update (key: rebalancer_pid, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,27})
<34787.2826.0>
[ns_server:debug,2025-05-15T18:47:28.133Z,ns_1@db3.lan:chronicle_kv_log<0.2577.0>:chronicle_kv_log:log:59]update (key: rebalance_type, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,27})
rebalance
[ns_server:info,2025-05-15T18:47:28.286Z,ns_1@db3.lan:mb_master<0.2842.0>:mb_master:candidate:370]Changing master from undefined to 'ns_1@172.19.0.4'
[ns_server:debug,2025-05-15T18:47:28.286Z,ns_1@db3.lan:leader_registry<0.2840.0>:leader_registry:handle_new_leader:275]New leader is 'ns_1@172.19.0.4'. Invalidating name cache.
[ns_server:debug,2025-05-15T18:47:28.582Z,ns_1@db3.lan:compiled_roles_cache<0.2536.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@ns_server",admin}
[ns_server:debug,2025-05-15T18:47:30.115Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/indexing/rebalance/RebalanceToken">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554050}}]}|
 '_deleted']
[ns_server:debug,2025-05-15T18:47:30.134Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/indexing/info/versionToken">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554050}}]}|
 <<"{\"Version\":9}">>]
[ns_server:debug,2025-05-15T18:47:31.115Z,ns_1@db3.lan:ns_ssl_services_setup<0.2463.0>:ns_ssl_services_setup:notify_services:1146]Going to notify following services: [memcached,capi_ssl_service]
[ns_server:info,2025-05-15T18:47:31.115Z,ns_1@db3.lan:<0.3051.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service memcached
[ns_server:info,2025-05-15T18:47:31.117Z,ns_1@db3.lan:memcached_config_mgr<0.2774.0>:memcached_config_mgr:push_tls_config:233]Pushing TLS config to memcached:
{[{<<"private key">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem">>},
  {<<"certificate chain">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem">>},
  {<<"CA file">>,<<"/opt/couchbase/var/lib/couchbase/config/certs/ca.pem">>},
  {<<"minimum version">>,<<"TLS 1.2">>},
  {<<"cipher list">>,
   {[{<<"TLS 1.2">>,<<"HIGH">>},
     {<<"TLS 1.3">>,
      <<"TLS_AES_256_GCM_SHA384:TLS_AES_128_GCM_SHA256:TLS_CHACHA20_POLY1305_SHA256">>}]}},
  {<<"cipher order">>,true},
  {<<"client cert auth">>,<<"disabled">>}]}
[ns_server:info,2025-05-15T18:47:31.123Z,ns_1@db3.lan:memcached_config_mgr<0.2774.0>:memcached_config_mgr:push_tls_config:237]Successfully pushed TLS config to memcached
[ns_server:info,2025-05-15T18:47:31.128Z,ns_1@db3.lan:<0.3052.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service capi_ssl_service
[ns_server:info,2025-05-15T18:47:31.128Z,ns_1@db3.lan:ns_ssl_services_setup<0.2463.0>:ns_ssl_services_setup:notify_services:1162]Succesfully notified services [capi_ssl_service,memcached]
[error_logger:info,2025-05-15T18:47:32.719Z,ns_1@db3.lan:net_kernel<0.2253.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.2258.0>,setup_timer_timeout}}
[ns_server:debug,2025-05-15T18:47:32.719Z,ns_1@db3.lan:cb_dist<0.2251.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3529285563.3650617353.3611>,
                               inet_tcp_dist,<0.2258.0>,
                               #Ref<0.3529285563.3650617353.3614>}
[error_logger:info,2025-05-15T18:47:32.720Z,ns_1@db3.lan:net_kernel<0.2253.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:47:33.157Z,ns_1@db3.lan:cb_saml<0.2525.0>:cb_saml:refresh_metadata:526]Refreshing metadata
[ns_server:debug,2025-05-15T18:47:33.157Z,ns_1@db3.lan:cb_saml<0.2525.0>:cb_saml:restart_refresh_timer:583]Disabling refresh timer
[ns_server:debug,2025-05-15T18:47:36.817Z,ns_1@db3.lan:chronicle_kv_log<0.2577.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@172.19.0.4',recovery_type}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                           30})
none
[ns_server:debug,2025-05-15T18:47:36.818Z,ns_1@db3.lan:chronicle_kv_log<0.2577.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@db2.lan',recovery_type}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                        30})
none
[ns_server:debug,2025-05-15T18:47:36.818Z,ns_1@db3.lan:chronicle_kv_log<0.2577.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@db3.lan',recovery_type}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                        30})
none
[ns_server:debug,2025-05-15T18:47:36.818Z,ns_1@db3.lan:chronicle_kv_log<0.2577.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@172.19.0.4',failover_vbuckets}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                               30})
[]
[ns_server:debug,2025-05-15T18:47:36.818Z,ns_1@db3.lan:chronicle_kv_log<0.2577.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@db2.lan',failover_vbuckets}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                            30})
[]
[ns_server:debug,2025-05-15T18:47:36.818Z,ns_1@db3.lan:chronicle_kv_log<0.2577.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@db3.lan',failover_vbuckets}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                            30})
[]
[ns_server:debug,2025-05-15T18:47:36.847Z,ns_1@db3.lan:chronicle_kv_log<0.2577.0>:chronicle_kv_log:log:59]update (key: {service_map,n1ql}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                       31})
['ns_1@172.19.0.4']
[ns_server:debug,2025-05-15T18:47:36.860Z,ns_1@db3.lan:<0.2777.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":76,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"indexAdmin\":9100,\"indexHttp\":9102,\"indexHttps\":19102,\"indexScan\":9101,\"indexStreamCatchup\":9104,\"indexStreamInit\":9103,\"indexStreamMaint\":9105,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"n1ql\":8093,\"n1qlSSL\":18093,\"projector\":9999},\"hostname\":\"172.19.0.4\",\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[ns_server:debug,2025-05-15T18:47:36.884Z,ns_1@db3.lan:chronicle_kv_log<0.2577.0>:chronicle_kv_log:log:59]update (key: {service_map,n1ql}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                       32})
['ns_1@172.19.0.4']
[ns_server:debug,2025-05-15T18:47:36.886Z,ns_1@db3.lan:<0.2777.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":77,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"indexAdmin\":9100,\"indexHttp\":9102,\"indexHttps\":19102,\"indexScan\":9101,\"indexStreamCatchup\":9104,\"indexStreamInit\":9103,\"indexStreamMaint\":9105,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"n1ql\":8093,\"n1qlSSL\":18093,\"projector\":9999},\"hostname\":\"172.19.0.4\",\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[ns_server:debug,2025-05-15T18:47:36.953Z,ns_1@db3.lan:chronicle_kv_log<0.2577.0>:chronicle_kv_log:log:59]update (key: unfinished_topology_operation, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                  35})
{regular,{activate_nodes,['ns_1@172.19.0.4','ns_1@db2.lan','ns_1@db3.lan']},
         <<"2c2b97b761cc8e298503d744e5749bcb">>,
         #Ref<34787.2327127311.3650355203.217418>}
[ns_server:debug,2025-05-15T18:47:36.953Z,ns_1@db3.lan:chronicle_kv_log<0.2577.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@172.19.0.4',membership}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                        35})
active
[ns_server:debug,2025-05-15T18:47:36.953Z,ns_1@db3.lan:chronicle_kv_log<0.2577.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@db2.lan',membership}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                     35})
active
[ns_server:debug,2025-05-15T18:47:36.953Z,ns_1@db3.lan:chronicle_kv_log<0.2577.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@db3.lan',membership}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                     35})
active
[ns_server:debug,2025-05-15T18:47:36.957Z,ns_1@db3.lan:<0.2777.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":80,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"indexAdmin\":9100,\"indexHttp\":9102,\"indexHttps\":19102,\"indexScan\":9101,\"indexStreamCatchup\":9104,\"indexStreamInit\":9103,\"indexStreamMaint\":9105,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"n1ql\":8093,\"n1qlSSL\":18093,\"projector\":9999},\"hostname\":\"172.19.0.4\",\"serverGroup\":\"Group 1\"},{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"hostname\":\"db2.lan\",\"serverGroup\":\"Group 1\"},{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true,\"hostname\":\"db3.lan\",\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[error_logger:info,2025-05-15T18:47:36.966Z,ns_1@db3.lan:service_stats_children_sup<0.2809.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_stats_children_sup}
    started: [{pid,<0.3319.0>},
              {id,{service_index,stats_reader,"@index"}},
              {mfargs,{stats_reader,start_link,["@index"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:36.969Z,ns_1@db3.lan:service_monitor_children_sup<0.2862.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_monitor_children_sup}
    started: [{pid,<0.3321.0>},
              {id,{kv,dcp_traffic_monitor}},
              {mfargs,{dcp_traffic_monitor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:36.976Z,ns_1@db3.lan:menelaus_cbauth<0.2739.0>:menelaus_cbauth:handle_cast:203]Observed json rpc process {"goxdcr-cbauth",[{internal,true}],<0.2934.0>} needs_update
[error_logger:info,2025-05-15T18:47:36.976Z,ns_1@db3.lan:service_agent_children_sup<0.2752.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_agent_children_sup}
    started: [{pid,<0.3323.0>},
              {id,{service_agent,index}},
              {mfargs,{service_agent,start_link,[index]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:36.977Z,ns_1@db3.lan:menelaus_cbauth<0.2739.0>:menelaus_cbauth:handle_cast:203]Observed json rpc process {"goxdcr-cbauth",[{internal,true}],<0.2934.0>} needs_update
[error_logger:info,2025-05-15T18:47:36.977Z,ns_1@db3.lan:service_agent_children_sup<0.2752.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_agent_children_sup}
    started: [{pid,<0.3327.0>},
              {id,{service_agent,n1ql}},
              {mfargs,{service_agent,start_link,[n1ql]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:36.978Z,ns_1@db3.lan:service_monitor_children_sup<0.2862.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_monitor_children_sup}
    started: [{pid,<0.3331.0>},
              {id,{kv,kv_stats_monitor}},
              {mfargs,{kv_stats_monitor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:36.979Z,ns_1@db3.lan:service_monitor_children_sup<0.2862.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_monitor_children_sup}
    started: [{pid,<0.3335.0>},
              {id,{kv,kv_monitor}},
              {mfargs,{kv_monitor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[chronicle:info,2025-05-15T18:47:36.980Z,ns_1@db3.lan:chronicle_leader<0.2420.0>:chronicle_leader:handle_new_config:525]Our electability (the new value is true) changed. Becoming an observer.
[ns_server:debug,2025-05-15T18:47:37.011Z,ns_1@db3.lan:ns_ports_setup<0.2748.0>:ns_ports_manager:set_dynamic_children:48]Setting children [memcached,saslauthd_port,goxdcr,index,n1ql,projector]
[ns_server:debug,2025-05-15T18:47:37.024Z,ns_1@db3.lan:chronicle_kv_log<0.2577.0>:chronicle_kv_log:handle_info:42]delete (key: unfinished_topology_operation, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                  39})
[ns_server:debug,2025-05-15T18:47:37.036Z,ns_1@db3.lan:compiled_roles_cache<0.2536.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@prometheus",stats_reader}
[ns_server:debug,2025-05-15T18:47:37.202Z,ns_1@db3.lan:json_rpc_connection-index-cbauth<0.3344.0>:json_rpc_connection:init:71]Observed revrpc connection: label "index-cbauth", handling process <0.3344.0>
[ns_server:debug,2025-05-15T18:47:37.202Z,ns_1@db3.lan:menelaus_cbauth<0.2739.0>:menelaus_cbauth:handle_cast:203]Observed json rpc process {"index-cbauth",[{internal,true}],<0.3344.0>} started
[ns_server:debug,2025-05-15T18:47:37.204Z,ns_1@db3.lan:compiled_roles_cache<0.2536.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@index",admin}
[ns_server:debug,2025-05-15T18:47:37.313Z,ns_1@db3.lan:json_rpc_connection-index-service_api<0.3366.0>:json_rpc_connection:init:71]Observed revrpc connection: label "index-service_api", handling process <0.3366.0>
[ns_server:debug,2025-05-15T18:47:37.313Z,ns_1@db3.lan:service_agent-index<0.3323.0>:service_agent:do_handle_connection:410]Observed new json rpc connection for index: <0.3366.0>
[ns_server:debug,2025-05-15T18:47:37.313Z,ns_1@db3.lan:<0.3326.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {json_rpc_events,<0.3324.0>} exited with reason normal
[ns_server:debug,2025-05-15T18:47:37.456Z,ns_1@db3.lan:json_rpc_connection-cbq-engine-cbauth<0.3377.0>:json_rpc_connection:init:71]Observed revrpc connection: label "cbq-engine-cbauth", handling process <0.3377.0>
[ns_server:debug,2025-05-15T18:47:37.456Z,ns_1@db3.lan:menelaus_cbauth<0.2739.0>:menelaus_cbauth:handle_cast:203]Observed json rpc process {"cbq-engine-cbauth",[{internal,true}],<0.3377.0>} started
[ns_server:debug,2025-05-15T18:47:37.463Z,ns_1@db3.lan:compiled_roles_cache<0.2536.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@cbq-engine",admin}
[ns_server:debug,2025-05-15T18:47:37.474Z,ns_1@db3.lan:<0.3388.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.3355.0>} exited with reason normal
[ns_server:debug,2025-05-15T18:47:37.556Z,ns_1@db3.lan:json_rpc_connection-projector-cbauth<0.3405.0>:json_rpc_connection:init:71]Observed revrpc connection: label "projector-cbauth", handling process <0.3405.0>
[ns_server:debug,2025-05-15T18:47:37.556Z,ns_1@db3.lan:menelaus_cbauth<0.2739.0>:menelaus_cbauth:handle_cast:203]Observed json rpc process {"projector-cbauth",[{internal,true}],<0.3405.0>} started
[ns_server:debug,2025-05-15T18:47:37.557Z,ns_1@db3.lan:compiled_roles_cache<0.2536.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@projector",admin}
[ns_server:debug,2025-05-15T18:47:39.503Z,ns_1@db3.lan:ns_config_rep<0.2618.0>:ns_config_rep:handle_call:149]Got full synchronization request from 'ns_1@172.19.0.4'
[ns_server:debug,2025-05-15T18:47:39.510Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
quorum_nodes ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554059}}]},
 'ns_1@172.19.0.4','ns_1@db2.lan','ns_1@db3.lan']
[ns_server:debug,2025-05-15T18:47:39.511Z,ns_1@db3.lan:ns_config_rep<0.2618.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 8078 us
[ns_server:debug,2025-05-15T18:47:39.636Z,ns_1@db3.lan:json_rpc_connection-cbq-engine-service_api<0.3520.0>:json_rpc_connection:init:71]Observed revrpc connection: label "cbq-engine-service_api", handling process <0.3520.0>
[ns_server:debug,2025-05-15T18:47:39.636Z,ns_1@db3.lan:<0.3330.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {json_rpc_events,<0.3328.0>} exited with reason normal
[ns_server:debug,2025-05-15T18:47:39.636Z,ns_1@db3.lan:service_agent-n1ql<0.3327.0>:service_agent:do_handle_connection:410]Observed new json rpc connection for n1ql: <0.3520.0>
[ns_server:debug,2025-05-15T18:47:39.691Z,ns_1@db3.lan:chronicle_kv_log<0.2577.0>:chronicle_kv_log:log:59]update (key: {service_map,n1ql}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                       40})
['ns_1@172.19.0.4','ns_1@db2.lan','ns_1@db3.lan']
[ns_server:debug,2025-05-15T18:47:39.694Z,ns_1@db3.lan:<0.2777.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":86,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"indexAdmin\":9100,\"indexHttp\":9102,\"indexHttps\":19102,\"indexScan\":9101,\"indexStreamCatchup\":9104,\"indexStreamInit\":9103,\"indexStreamMaint\":9105,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"n1ql\":8093,\"n1qlSSL\":18093,\"projector\":9999},\"hostname\":\"172.19.0.4\",\"serverGroup\":\"Group 1\"},{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"n1ql\":8093,\"n1qlSSL\":18093,\"projector\":9999},\"hostname\":\"db2.lan\",\"serverGroup\":\"Group 1\"},{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"n1ql\":8093,\"n1qlSSL\":18093,\"projector\":9999},\"thisNode\":true,\"hostname\":\"db3.lan\",\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[ns_server:debug,2025-05-15T18:47:39.728Z,ns_1@db3.lan:chronicle_kv_log<0.2577.0>:chronicle_kv_log:log:59]update (key: {service_map,index}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                        41})
['ns_1@172.19.0.4','ns_1@db2.lan','ns_1@db3.lan']
[ns_server:debug,2025-05-15T18:47:39.729Z,ns_1@db3.lan:<0.2777.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":87,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"indexAdmin\":9100,\"indexHttp\":9102,\"indexHttps\":19102,\"indexScan\":9101,\"indexStreamCatchup\":9104,\"indexStreamInit\":9103,\"indexStreamMaint\":9105,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"n1ql\":8093,\"n1qlSSL\":18093,\"projector\":9999},\"hostname\":\"172.19.0.4\",\"serverGroup\":\"Group 1\"},{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"indexAdmin\":9100,\"indexHttp\":9102,\"indexHttps\":19102,\"indexScan\":9101,\"indexStreamCatchup\":9104,\"indexStreamInit\":9103,\"indexStreamMaint\":9105,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"n1ql\":8093,\"n1qlSSL\":18093,\"projector\":9999},\"hostname\":\"db2.lan\",\"serverGroup\":\"Group 1\"},{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"indexAdmin\":9100,\"indexHttp\":9102,\"indexHttps\":19102,\"indexScan\":9101,\"indexStreamCatchup\":9104,\"indexStreamInit\":9103,\"indexStreamMaint\":9105,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"n1ql\":8093,\"n1qlSSL\":18093,\"projector\":9999},\"thisNode\":true,\"hostname\":\"db3.lan\",\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[ns_server:debug,2025-05-15T18:47:39.760Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/indexing/rebalance/RebalanceToken">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{3,63914554059}}]}|
 <<"{\"MasterId\":\"28569ac00b9c1d7c50e39741027d428c\",\"RebalId\":\"62c17460b6c2fb91a6a08c24ea499705\",\"Source\":0,\"Error\":\"\",\"MasterIP\":\"172.19.0.4\",\"Version\":0,\"RebalPhase\":0,\"ActiveRebalancer\":1}">>]
[ns_server:debug,2025-05-15T18:47:41.097Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{4,63914554061}}]}|
 <<"{\"indexer.plasma.backIndex.enablePageBloomFilter\":true,\"indexer.settings.allow_large_keys\":true,\"indexer.settings.bufferPoolBlockSize\":16384,\"indexer.settings.build.batch_size\":5,\"indexer.settings.compaction.abort_exceed_interval\":false,\"indexer.settings.compaction.check_period\":30,\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.days_of_week\":\"Sund"...>>]
[ns_server:debug,2025-05-15T18:47:41.099Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/indexing/settings/config/features/PlasmaEnablePageBloomFilterBackIndex">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554061}}]}|
 <<"{}">>]
[ns_server:debug,2025-05-15T18:47:41.475Z,ns_1@db3.lan:leader_lease_agent<0.2838.0>:leader_lease_agent:handle_lease_expired:277]Lease held by {lease_holder,<<"9a39908e5882941cd97af5604964e304">>,
                            'ns_1@cb.local'} expired. Starting expirer.
[ns_server:debug,2025-05-15T18:47:41.479Z,ns_1@db3.lan:leader_lease_agent<0.2838.0>:leader_lease_agent:do_handle_acquire_lease:140]Granting lease to {lease_holder,<<"60878074b48f15926e51cf002c4824e4">>,
                                'ns_1@172.19.0.4'} for 15000ms
[ns_server:error,2025-05-15T18:47:42.873Z,ns_1@db3.lan:service_status_keeper_worker<0.2811.0>:rest_utils:get_json:62]Request to (indexer) getIndexStatus with headers [{"If-None-Match",
                                                   "968a6f4a2618448e"}] failed: {ok,
                                                                                 {{500,
                                                                                   "Internal Server Error"},
                                                                                  [{"Content-Length",
                                                                                    "130"},
                                                                                   {"Date",
                                                                                    "Thu, 15 May 2025 18:47:42 GMT"},
                                                                                   {"Content-Type",
                                                                                    "application/json"}],
                                                                                  <<"{\"code\":\"error\",\"error\":\"Fail to retrieve cluster-wide metadata from index service\",\"failedNodes\":[\"db2.lan:8091\",\"db3.lan:8091\"]}">>}}
[ns_server:error,2025-05-15T18:47:42.873Z,ns_1@db3.lan:service_status_keeper-index<0.2812.0>:service_status_keeper:handle_cast:103]Service service_index returned incorrect status
[ns_server:debug,2025-05-15T18:47:49.777Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/indexing/rebalance/RebalanceToken">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{4,63914554069}}]}|
 '_deleted']
[ns_server:debug,2025-05-15T18:47:49.791Z,ns_1@db3.lan:service_agent-index<0.3323.0>:service_agent:cleanup_service:641]Cleaning up stale tasks:
[[{<<"rev">>,<<"AAAAAAAAAAA=">>},
  {<<"id">>,<<"prepare/62c17460b6c2fb91a6a08c24ea499705">>},
  {<<"type">>,<<"task-prepared">>},
  {<<"status">>,<<"task-running">>},
  {<<"isCancelable">>,true},
  {<<"progress">>,0},
  {<<"extra">>,
   {[{<<"rebalanceId">>,<<"62c17460b6c2fb91a6a08c24ea499705">>}]}}]]
[ns_server:debug,2025-05-15T18:47:49.795Z,ns_1@db3.lan:<0.3545.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.3338.0>} exited with reason normal
[ns_server:debug,2025-05-15T18:47:49.828Z,ns_1@db3.lan:chronicle_kv_log<0.2577.0>:chronicle_kv_log:log:59]update (key: counters, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,44})
[{rebalance_success,{1747334869,1}},{rebalance_start,{1747334848,1}}]
[ns_server:debug,2025-05-15T18:47:49.830Z,ns_1@db3.lan:<0.2777.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":94,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"indexAdmin\":9100,\"indexHttp\":9102,\"indexHttps\":19102,\"indexScan\":9101,\"indexStreamCatchup\":9104,\"indexStreamInit\":9103,\"indexStreamMaint\":9105,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"n1ql\":8093,\"n1qlSSL\":18093,\"projector\":9999},\"hostname\":\"172.19.0.4\",\"serverGroup\":\"Group 1\"},{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"indexAdmin\":9100,\"indexHttp\":9102,\"indexHttps\":19102,\"indexScan\":9101,\"indexStreamCatchup\":9104,\"indexStreamInit\":9103,\"indexStreamMaint\":9105,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"n1ql\":8093,\"n1qlSSL\":18093,\"projector\":9999},\"hostname\":\"db2.lan\",\"serverGroup\":\"Group 1\"},{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"indexAdmin\":9100,\"indexHttp\":9102,\"indexHttps\":19102,\"indexScan\":9101,\"indexStreamCatchup\":9104,\"indexStreamInit\":9103,\"indexStreamMaint\":9105,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"n1ql\":8093,\"n1qlSSL\":18093,\"projector\":9999},\"thisNode\":true,\"hostname\":\"db3.lan\",\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[ns_server:debug,2025-05-15T18:47:49.837Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
rebalance_reports ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554069}}]},
 {<<"c1b1e82bc3018faaa44cc124f5be1eeb">>,
  [{node,'ns_1@172.19.0.4'},
   {filename,"rebalance_report_20250515T184749.json"}]}]
[ns_server:debug,2025-05-15T18:47:49.857Z,ns_1@db3.lan:chronicle_kv_log<0.2577.0>:chronicle_kv_log:log:59]update (key: rebalance_status, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                     45})
none
[ns_server:debug,2025-05-15T18:47:49.857Z,ns_1@db3.lan:chronicle_kv_log<0.2577.0>:chronicle_kv_log:log:59]update (key: rebalance_status_uuid, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                          45})
<<"e30f0e0de45bc0da5422537090074df6">>
[ns_server:debug,2025-05-15T18:47:49.858Z,ns_1@db3.lan:chronicle_kv_log<0.2577.0>:chronicle_kv_log:log:59]update (key: rebalancer_pid, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,45})
undefined
[ns_server:debug,2025-05-15T18:47:49.858Z,ns_1@db3.lan:chronicle_kv_log<0.2577.0>:chronicle_kv_log:log:59]update (key: rebalance_type, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,45})
rebalance
[ns_server:debug,2025-05-15T18:47:50.300Z,ns_1@db3.lan:chronicle_kv_log<0.2577.0>:chronicle_kv_log:log:59]update (key: bucket_names, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,46})
["doom-scrolling"]
[ns_server:debug,2025-05-15T18:47:50.300Z,ns_1@db3.lan:roles_cache<0.2539.0>:active_cache:clean:181]Clearing the roles_cache cache
[ns_server:debug,2025-05-15T18:47:50.301Z,ns_1@db3.lan:ns_bucket_worker<0.2781.0>:ns_bucket_worker:start_one_uploader:153]Starting uploader for bucket: "doom-scrolling"
[ns_server:debug,2025-05-15T18:47:50.301Z,ns_1@db3.lan:roles_cache<0.2539.0>:active_cache:clean:181]Clearing the roles_cache cache
[ns_server:debug,2025-05-15T18:47:50.301Z,ns_1@db3.lan:compiled_roles_cache<0.2536.0>:versioned_cache:handle_info:89]Flushing cache compiled_roles_cache due to version change from undefined to {[7,
                                                                              6],
                                                                             {0,
                                                                              4132762956},
                                                                             {0,
                                                                              4132762956},
                                                                             true,
                                                                             [{"doom-scrolling",
                                                                               <<"f70f8d64d14cf7513107bff35eb6561d">>,
                                                                               1}]}
[ns_server:debug,2025-05-15T18:47:50.302Z,ns_1@db3.lan:chronicle_kv_log<0.2577.0>:chronicle_kv_log:log:59]update (key: {bucket,"doom-scrolling",props}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                    46})
[{replica_index,true},
 {ram_quota,536870912},
 {durability_min_level,none},
 {num_vbuckets,1024},
 {cross_cluster_versioning_enabled,false},
 {version_pruning_window_hrs,720},
 {pitr_enabled,false},
 {pitr_granularity,600},
 {pitr_max_history_age,86400},
 {flush_enabled,false},
 {num_threads,3},
 {eviction_policy,value_only},
 {conflict_resolution_type,seqno},
 {storage_mode,couchstore},
 {max_ttl,0},
 {compression_mode,passive},
 {rank,0},
 {type,membase},
 {num_replicas,1},
 {replication_topology,star},
 {repl_type,dcp},
 {servers,[]}]
[ns_server:debug,2025-05-15T18:47:50.302Z,ns_1@db3.lan:chronicle_kv_log<0.2577.0>:chronicle_kv_log:log:59]update (key: {bucket,"doom-scrolling",uuid}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                   46})
<<"f70f8d64d14cf7513107bff35eb6561d">>
[ns_server:debug,2025-05-15T18:47:50.302Z,ns_1@db3.lan:chronicle_kv_log<0.2577.0>:chronicle_kv_log:log:59]update (key: {bucket,"doom-scrolling",collections}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                          46})
[{uid,1},
 {next_uid,2},
 {next_scope_uid,9},
 {next_coll_uid,10},
 {num_scopes,0},
 {num_collections,0},
 {scopes,[{"_default",
           [{uid,0},{collections,[{"_default",[{uid,0},{maxTTL,0}]}]}]},
          {"_system",
           [{uid,8},
            {collections,[{"_query",[{uid,9},{maxTTL,-1},{history,false}]},
                          {"_mobile",
                           [{uid,8},{maxTTL,-1},{history,false}]}]}]}]}]
[ns_server:debug,2025-05-15T18:47:50.302Z,ns_1@db3.lan:chronicle_kv_log<0.2577.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@172.19.0.4',
                   {"doom-scrolling",last_seen_collection_ids}}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                                       46})
[2,9,10]
[ns_server:debug,2025-05-15T18:47:50.302Z,ns_1@db3.lan:chronicle_kv_log<0.2577.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@db2.lan',{"doom-scrolling",last_seen_collection_ids}}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                                                      46})
[2,9,10]
[ns_server:debug,2025-05-15T18:47:50.302Z,ns_1@db3.lan:chronicle_kv_log<0.2577.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@db3.lan',{"doom-scrolling",last_seen_collection_ids}}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                                                      46})
[2,9,10]
[ns_server:debug,2025-05-15T18:47:50.304Z,ns_1@db3.lan:memcached_permissions<0.2606.0>:memcached_cfg:write_cfg:156]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2025-05-15T18:47:50.307Z,ns_1@db3.lan:memcached_permissions<0.2606.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,{user,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2025-05-15T18:47:50.313Z,ns_1@db3.lan:memcached_refresh<0.2459.0>:memcached_refresh:handle_call:57]File rename from "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac" is requested
[ns_server:debug,2025-05-15T18:47:50.314Z,ns_1@db3.lan:terse_bucket_info_uploader-doom-scrolling<0.4053.0>:memcached_config_mgr:memcached_port_pid:154]waiting for completion of initial ns_ports_setup round
[error_logger:info,2025-05-15T18:47:50.314Z,ns_1@db3.lan:ns_bucket_sup<0.2780.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_bucket_sup}
    started: [{pid,<0.4053.0>},
              {id,{terse_bucket_info_uploader,"doom-scrolling"}},
              {mfargs,
                  {terse_bucket_info_uploader,start_link,["doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:50.314Z,ns_1@db3.lan:memcached_refresh<0.2459.0>:memcached_refresh:handle_cast:67]Refresh of rbac requested
[ns_server:debug,2025-05-15T18:47:50.314Z,ns_1@db3.lan:memcached_permissions<0.2606.0>:memcached_cfg:rename_and_refresh:178]Successfully renamed "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2025-05-15T18:47:50.315Z,ns_1@db3.lan:memcached_permissions<0.2606.0>:memcached_cfg:write_cfg:156]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2025-05-15T18:47:50.317Z,ns_1@db3.lan:memcached_permissions<0.2606.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,{user,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2025-05-15T18:47:50.318Z,ns_1@db3.lan:memcached_refresh<0.2459.0>:memcached_refresh:handle_call:57]File rename from "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac" is requested
[ns_server:debug,2025-05-15T18:47:50.319Z,ns_1@db3.lan:memcached_permissions<0.2606.0>:memcached_cfg:rename_and_refresh:178]Successfully renamed "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2025-05-15T18:47:50.319Z,ns_1@db3.lan:memcached_refresh<0.2459.0>:memcached_refresh:handle_cast:67]Refresh of rbac requested
[ns_server:debug,2025-05-15T18:47:50.320Z,ns_1@db3.lan:<0.4057.0>:memcached_refresh:do_refresh:153]Successfully connected to memcached, Trying to refresh [rbac]
[ns_server:debug,2025-05-15T18:47:50.322Z,ns_1@db3.lan:memcached_refresh<0.2459.0>:memcached_refresh:update_refresh_state:116]Refresh of [rbac] succeeded
[ns_server:debug,2025-05-15T18:47:50.324Z,ns_1@db3.lan:<0.4058.0>:memcached_refresh:do_refresh:153]Successfully connected to memcached, Trying to refresh [rbac]
[ns_server:debug,2025-05-15T18:47:50.326Z,ns_1@db3.lan:memcached_refresh<0.2459.0>:memcached_refresh:update_refresh_state:116]Refresh of [rbac] succeeded
[ns_server:debug,2025-05-15T18:47:50.341Z,ns_1@db3.lan:ns_bucket_worker<0.2781.0>:ns_bucket_worker:start_one_bucket:125]Starting new bucket: "doom-scrolling"
[ns_server:debug,2025-05-15T18:47:50.341Z,ns_1@db3.lan:chronicle_kv_log<0.2577.0>:chronicle_kv_log:log:59]update (key: {bucket,"doom-scrolling",props}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                    47})
[{replica_index,true},
 {ram_quota,536870912},
 {durability_min_level,none},
 {num_vbuckets,1024},
 {cross_cluster_versioning_enabled,false},
 {version_pruning_window_hrs,720},
 {pitr_enabled,false},
 {pitr_granularity,600},
 {pitr_max_history_age,86400},
 {flush_enabled,false},
 {num_threads,3},
 {eviction_policy,value_only},
 {conflict_resolution_type,seqno},
 {storage_mode,couchstore},
 {max_ttl,0},
 {compression_mode,passive},
 {rank,0},
 {type,membase},
 {num_replicas,1},
 {replication_topology,star},
 {repl_type,dcp},
 {servers,['ns_1@172.19.0.4','ns_1@db2.lan','ns_1@db3.lan']},
 {map_diff,[]},
 {fastForwardMap_diff,[]}]
[ns_server:debug,2025-05-15T18:47:50.341Z,ns_1@db3.lan:roles_cache<0.2539.0>:active_cache:clean:181]Clearing the roles_cache cache
[ns_server:debug,2025-05-15T18:47:50.343Z,ns_1@db3.lan:single_bucket_kv_sup-doom-scrolling<0.4061.0>:single_bucket_kv_sup:sync_config_to_couchdb_node:64]Syncing config to couchdb node
[ns_server:debug,2025-05-15T18:47:50.344Z,ns_1@db3.lan:ns_config_rep<0.2618.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 4 us
[ns_server:debug,2025-05-15T18:47:50.344Z,ns_1@db3.lan:single_bucket_kv_sup-doom-scrolling<0.4061.0>:single_bucket_kv_sup:sync_config_to_couchdb_node:70]Synced config to couchdb node successfully
[ns_server:debug,2025-05-15T18:47:50.351Z,ns_1@db3.lan:compiled_roles_cache<0.2536.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@projector",admin}
[ns_server:debug,2025-05-15T18:47:50.351Z,ns_1@db3.lan:compiled_roles_cache<0.2536.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@index",admin}
[ns_server:debug,2025-05-15T18:47:50.352Z,ns_1@db3.lan:compiled_roles_cache<0.2536.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@cbq-engine",admin}
[ns_server:debug,2025-05-15T18:47:50.352Z,ns_1@db3.lan:compiled_roles_cache<0.2536.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@goxdcr",admin}
[ns_server:debug,2025-05-15T18:47:50.352Z,ns_1@db3.lan:capi_doc_replicator-doom-scrolling<0.4068.0>:replicated_storage:wait_for_startup:47]Start waiting for startup
[error_logger:info,2025-05-15T18:47:50.352Z,ns_1@db3.lan:<0.4067.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.4067.0>,docs_kv_sup}
    started: [{pid,<0.4068.0>},
              {id,doc_replicator},
              {mfargs,{capi_ddoc_manager,start_replicator,["doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:50.352Z,ns_1@db3.lan:capi_ddoc_replication_srv-doom-scrolling<0.4069.0>:replicated_storage:wait_for_startup:47]Start waiting for startup
[error_logger:info,2025-05-15T18:47:50.353Z,ns_1@db3.lan:<0.4067.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.4067.0>,docs_kv_sup}
    started: [{pid,<0.4069.0>},
              {id,doc_replication_srv},
              {mfargs,{doc_replication_srv,start_link,["doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:50.354Z,ns_1@db3.lan:terse_bucket_info_uploader-doom-scrolling<0.4053.0>:memcached_config_mgr:memcached_port_pid:156]ns_ports_setup seems to be ready
[ns_server:debug,2025-05-15T18:47:50.355Z,ns_1@db3.lan:terse_bucket_info_uploader-doom-scrolling<0.4053.0>:memcached_config_mgr:find_port_pid_loop:164]Found memcached port <16971.139.0>
[ns_server:debug,2025-05-15T18:47:50.355Z,ns_1@db3.lan:terse_bucket_info_uploader-doom-scrolling<0.4053.0>:terse_bucket_info_uploader:flush_refresh_msgs:94]Flushed 1 refresh messages
[ns_server:debug,2025-05-15T18:47:50.356Z,ns_1@db3.lan:terse_bucket_info_uploader-doom-scrolling<0.4053.0>:terse_bucket_info_uploader:maybe_set_cluster_config:144]Bucket 'doom-scrolling' needs vbmap before setting cluster config
[error_logger:info,2025-05-15T18:47:50.373Z,ns_1@db3.lan:logger_proxy<0.70.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'capi_ddoc_manager_sup-doom-scrolling'}
    started: [{pid,<16972.610.0>},
              {id,capi_ddoc_manager_events},
              {mfargs,
                  {capi_ddoc_manager,start_link_event_manager,
                      ["doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:50.377Z,ns_1@db3.lan:capi_doc_replicator-doom-scrolling<0.4068.0>:replicated_storage:wait_for_startup:50]Received replicated storage registration from <16972.611.0>
[error_logger:info,2025-05-15T18:47:50.377Z,ns_1@db3.lan:logger_proxy<0.70.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'capi_ddoc_manager_sup-doom-scrolling'}
    started: [{pid,<16972.611.0>},
              {id,capi_ddoc_manager},
              {mfargs,
                  {capi_ddoc_manager,start_link,
                      ["doom-scrolling",<0.4068.0>,<0.4069.0>]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:50.377Z,ns_1@db3.lan:capi_ddoc_replication_srv-doom-scrolling<0.4069.0>:replicated_storage:wait_for_startup:50]Received replicated storage registration from <16972.611.0>
[error_logger:info,2025-05-15T18:47:50.377Z,ns_1@db3.lan:<0.4067.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.4067.0>,docs_kv_sup}
    started: [{pid,<16972.609.0>},
              {id,capi_ddoc_manager_sup},
              {mfargs,
                  {capi_ddoc_manager_sup,start_link_remote,
                      ['couchdb_ns_1@cb.local',"doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:50.389Z,ns_1@db3.lan:<0.4067.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.4067.0>,docs_kv_sup}
    started: [{pid,<16972.613.0>},
              {id,capi_set_view_manager},
              {mfargs,
                  {capi_set_view_manager,start_link_remote,
                      ['couchdb_ns_1@cb.local',"doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:50.397Z,ns_1@db3.lan:<0.4067.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.4067.0>,docs_kv_sup}
    started: [{pid,<16972.618.0>},
              {id,couch_stats_reader},
              {mfargs,
                  {couch_stats_reader,start_link_remote,
                      ['couchdb_ns_1@cb.local',"doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:50.398Z,ns_1@db3.lan:single_bucket_kv_sup-doom-scrolling<0.4061.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'single_bucket_kv_sup-doom-scrolling'}
    started: [{pid,<0.4067.0>},
              {id,{docs_kv_sup,"doom-scrolling"}},
              {mfargs,{docs_kv_sup,start_link,["doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:47:50.400Z,ns_1@db3.lan:ns_memcached-doom-scrolling<0.4097.0>:ns_memcached:init:161]Starting ns_memcached
[ns_server:debug,2025-05-15T18:47:50.400Z,ns_1@db3.lan:<0.4098.0>:ns_memcached:run_connect_phase:188]Started 'connecting' phase of ns_memcached-doom-scrolling. Parent is <0.4097.0>
[error_logger:info,2025-05-15T18:47:50.400Z,ns_1@db3.lan:<0.4096.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.4096.0>,ns_memcached_sup}
    started: [{pid,<0.4097.0>},
              {id,{ns_memcached,"doom-scrolling"}},
              {mfargs,{ns_memcached,start_link,["doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,86400000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:50.400Z,ns_1@db3.lan:single_bucket_kv_sup-doom-scrolling<0.4061.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'single_bucket_kv_sup-doom-scrolling'}
    started: [{pid,<0.4096.0>},
              {id,{ns_memcached_sup,"doom-scrolling"}},
              {mfargs,{ns_memcached_sup,start_link,["doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:50.404Z,ns_1@db3.lan:single_bucket_kv_sup-doom-scrolling<0.4061.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'single_bucket_kv_sup-doom-scrolling'}
    started: [{pid,<0.4101.0>},
              {id,{dcp_sup,"doom-scrolling"}},
              {mfargs,{dcp_sup,start_link,["doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:50.410Z,ns_1@db3.lan:single_bucket_kv_sup-doom-scrolling<0.4061.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'single_bucket_kv_sup-doom-scrolling'}
    started: [{pid,<0.4102.0>},
              {id,{dcp_replication_manager,"doom-scrolling"}},
              {mfargs,{dcp_replication_manager,start_link,["doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:50.416Z,ns_1@db3.lan:single_bucket_kv_sup-doom-scrolling<0.4061.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'single_bucket_kv_sup-doom-scrolling'}
    started: [{pid,<0.4103.0>},
              {id,{replication_manager,"doom-scrolling"}},
              {mfargs,{replication_manager,start_link,["doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:50.424Z,ns_1@db3.lan:janitor_agent_sup-doom-scrolling<0.4104.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'janitor_agent_sup-doom-scrolling'}
    started: [{pid,<0.4105.0>},
              {id,rebalance_subprocesses_registry},
              {mfargs,
                  {ns_process_registry,start_link,
                      ['rebalance_subprocesses_registry-doom-scrolling',
                       [{terminate_command,kill}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,86400000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:50.438Z,ns_1@db3.lan:janitor_agent_sup-doom-scrolling<0.4104.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'janitor_agent_sup-doom-scrolling'}
    started: [{pid,<0.4106.0>},
              {id,janitor_agent},
              {mfargs,{janitor_agent,start_link,["doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:50.439Z,ns_1@db3.lan:single_bucket_kv_sup-doom-scrolling<0.4061.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'single_bucket_kv_sup-doom-scrolling'}
    started: [{pid,<0.4104.0>},
              {id,{janitor_agent_sup,"doom-scrolling"}},
              {mfargs,{janitor_agent_sup,start_link,["doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:50.439Z,ns_1@db3.lan:single_bucket_kv_sup-doom-scrolling<0.4061.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'single_bucket_kv_sup-doom-scrolling'}
    started: [{pid,<0.4107.0>},
              {id,{stats_reader,"doom-scrolling"}},
              {mfargs,{stats_reader,start_link,["doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:50.439Z,ns_1@db3.lan:single_bucket_kv_sup-doom-scrolling<0.4061.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'single_bucket_kv_sup-doom-scrolling'}
    started: [{pid,<0.4109.0>},
              {id,{goxdcr_stats_reader,"doom-scrolling"}},
              {mfargs,{stats_reader,start_link,["@xdcr-doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:50.439Z,ns_1@db3.lan:ns_bucket_sup<0.2780.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_bucket_sup}
    started: [{pid,<0.4061.0>},
              {id,{single_bucket_kv_sup,"doom-scrolling"}},
              {mfargs,{single_bucket_kv_sup,start_link,["doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:47:50.439Z,ns_1@db3.lan:janitor_agent-doom-scrolling<0.4106.0>:dcp_sup:nuke:110]Nuking DCP replicators for bucket "doom-scrolling":
[]
[ns_server:debug,2025-05-15T18:47:50.443Z,ns_1@db3.lan:compiled_roles_cache<0.2536.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@prometheus",stats_reader}
[ns_server:debug,2025-05-15T18:47:50.447Z,ns_1@db3.lan:capi_doc_replicator-doom-scrolling<0.4068.0>:doc_replicator:loop:79]Replicating all docs to new nodes: ['ns_1@172.19.0.4','ns_1@db2.lan']
[ns_server:info,2025-05-15T18:47:50.457Z,ns_1@db3.lan:ns_memcached-doom-scrolling<0.4097.0>:ns_memcached:do_ensure_bucket:1592]Created bucket "doom-scrolling" with config string "max_size=536870912;dbname=/opt/couchbase/var/lib/couchbase/data/doom-scrolling;backend=couchdb;couch_bucket=doom-scrolling;max_vbuckets=1024;alog_path=/opt/couchbase/var/lib/couchbase/data/doom-scrolling/access.log;data_traffic_enabled=false;max_num_workers=3;uuid=f70f8d64d14cf7513107bff35eb6561d;conflict_resolution_type=seqno;bucket_type=persistent;durability_min_level=none;pitr_enabled=false;pitr_granularity=600;pitr_max_history_age=86400;item_eviction_policy=value_only;persistent_metadata_purge_age=259200;max_ttl=0;ht_locks=47;compression_mode=passive;max_num_shards=0;failpartialwarmup=false"
[ns_server:debug,2025-05-15T18:47:50.459Z,ns_1@db3.lan:ns_memcached-doom-scrolling<0.4097.0>:memcached_bucket_config:ensure_collections:291]Applying collection manifest to bucket "doom-scrolling" due to id change from <<"0">> to <<"1">>.
[ns_server:info,2025-05-15T18:47:50.469Z,ns_1@db3.lan:ns_memcached-doom-scrolling<0.4097.0>:ns_memcached:handle_info:837]Main ns_memcached connection established: {ok,#Port<0.203>}
[ns_server:debug,2025-05-15T18:47:50.470Z,ns_1@db3.lan:terse_bucket_info_uploader-doom-scrolling<0.4053.0>:terse_bucket_info_uploader:maybe_set_cluster_config:144]Bucket 'doom-scrolling' needs vbmap before setting cluster config
[user:info,2025-05-15T18:47:50.470Z,ns_1@db3.lan:ns_memcached-doom-scrolling<0.4097.0>:ns_memcached:handle_cast:806]Bucket "doom-scrolling" loaded on node 'ns_1@db3.lan' in 0 seconds.
[ns_server:info,2025-05-15T18:47:50.477Z,ns_1@db3.lan:janitor_agent-doom-scrolling<0.4106.0>:janitor_agent:read_flush_counter:969]Loading flushseq failed: {error,enoent}. Assuming it's equal to global config.
[ns_server:info,2025-05-15T18:47:50.477Z,ns_1@db3.lan:janitor_agent-doom-scrolling<0.4106.0>:janitor_agent:read_flush_counter_from_config:977]Initialized flushseq 0 from bucket config
[ns_server:debug,2025-05-15T18:47:50.611Z,ns_1@db3.lan:compiled_roles_cache<0.2536.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@",admin}
[ns_server:debug,2025-05-15T18:47:51.358Z,ns_1@db3.lan:terse_bucket_info_uploader-doom-scrolling<0.4053.0>:terse_bucket_info_uploader:maybe_set_cluster_config:144]Bucket 'doom-scrolling' needs vbmap before setting cluster config
[ns_server:debug,2025-05-15T18:47:51.472Z,ns_1@db3.lan:terse_bucket_info_uploader-doom-scrolling<0.4053.0>:terse_bucket_info_uploader:maybe_set_cluster_config:144]Bucket 'doom-scrolling' needs vbmap before setting cluster config
[ns_server:debug,2025-05-15T18:47:51.990Z,ns_1@db3.lan:chronicle_kv_log<0.2577.0>:chronicle_kv_log:log:59]update (key: {bucket,"doom-scrolling",last_balanced_vbmap}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                                  48})
{[['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4'|...],
  [...]|...],
 [{replication_topology,star},
  {tags,undefined},
  {use_vbmap_greedy_optimization,true},
  {max_slaves,10}]}
[ns_server:debug,2025-05-15T18:47:51.992Z,ns_1@db3.lan:terse_bucket_info_uploader-doom-scrolling<0.4053.0>:terse_bucket_info_uploader:maybe_set_cluster_config:144]Bucket 'doom-scrolling' needs vbmap before setting cluster config
[ns_server:debug,2025-05-15T18:47:52.015Z,ns_1@db3.lan:chronicle_kv_log<0.2577.0>:chronicle_kv_log:log:59]update (key: {bucket,"doom-scrolling",props}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                    49})
[{map_diff,[{0,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {1,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {2,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {3,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {4,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {5,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {6,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {7,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {8,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {9,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {10,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {11,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {12,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {13,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {14,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {15,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {16,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {17,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {18,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {19,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {20,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {21,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {22,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {23,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {24,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {25,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {26,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {27,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {28,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {29,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {30,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {31,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {32,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {33,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {34,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {35,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {36,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {37,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {38,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {39,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {40,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {41,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {42,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {43,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {44,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {45,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {46,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {47,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {48,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {49,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {50,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {51,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {52,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {53,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {54,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {55,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {56,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {57,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {58,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {59,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {60,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {61,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {62,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {63,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {64,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {65,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {66,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {67,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {68,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {69,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {70,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {71,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {72,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {73,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {74,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {75,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {76,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {77,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {78,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {79,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {80,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {81,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {82,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {83,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {84,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {85,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {86,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {87,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {88,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {89,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {90,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {91,[],['ns_1@172.19.0.4'|...]},
            {92,[],[...]},
            {93,[],...},
            {94,...},
            {...}|...]},
 {map_opts_hash,42591107},
 {replica_index,true},
 {ram_quota,536870912},
 {durability_min_level,none},
 {num_vbuckets,1024},
 {cross_cluster_versioning_enabled,false},
 {version_pruning_window_hrs,720},
 {pitr_enabled,false},
 {pitr_granularity,600},
 {pitr_max_history_age,86400},
 {flush_enabled,false},
 {num_threads,3},
 {eviction_policy,value_only},
 {conflict_resolution_type,seqno},
 {storage_mode,couchstore},
 {max_ttl,0},
 {compression_mode,passive},
 {rank,0},
 {type,membase},
 {num_replicas,1},
 {replication_topology,star},
 {repl_type,dcp},
 {servers,['ns_1@172.19.0.4','ns_1@db2.lan','ns_1@db3.lan']},
 {fastForwardMap_diff,[]}]
[ns_server:debug,2025-05-15T18:47:52.015Z,ns_1@db3.lan:roles_cache<0.2539.0>:active_cache:clean:181]Clearing the roles_cache cache
[ns_server:info,2025-05-15T18:47:52.210Z,ns_1@db3.lan:<0.4125.0>:ns_memcached:do_handle_call:710]Changed vbucket state 
[{1023,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {1022,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {1021,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {1020,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {1019,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {1018,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {1017,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {1016,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {1015,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {1014,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {1013,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {1012,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {1011,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {1010,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {1009,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {1008,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {1007,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {1006,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {1005,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {1004,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {1003,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {1002,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {1001,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {1000,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {999,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {998,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {997,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {996,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {995,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {994,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {993,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {992,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {991,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {990,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {989,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {988,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {987,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {986,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {985,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {984,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {983,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {982,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {981,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {980,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {979,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {978,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {977,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {976,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {975,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {974,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {973,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {972,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {971,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {970,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {969,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {968,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {967,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {966,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {965,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {964,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {963,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {962,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {961,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {960,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {959,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {958,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {957,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {956,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {955,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {954,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {953,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {952,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {951,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {950,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {949,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {948,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {947,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {946,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {945,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {944,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {943,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {942,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {941,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {940,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {939,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {938,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {937,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {936,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {935,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {934,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {933,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {932,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {931,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {930,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {929,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {928,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {927,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {926,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {925,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {924,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {923,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {922,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {921,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {920,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {919,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {918,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {917,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {916,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {915,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {914,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {913,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {912,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {911,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {910,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {909,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {908,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {907,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {906,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {905,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {904,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {903,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {902,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {901,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {900,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {899,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {898,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {897,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {896,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {895,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {894,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {893,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {892,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {891,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {890,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {889,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {888,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {887,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {886,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {885,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {884,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {883,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {882,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {881,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {880,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {879,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {878,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {877,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {876,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {875,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {874,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {873,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {872,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {871,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {870,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {869,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {868,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {867,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {866,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {865,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {864,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {863,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {862,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {861,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {860,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {859,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {858,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {857,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {856,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {855,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {854,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {853,active,{[{topology,[['ns_1@db3.lan','ns_1@db2.lan']]}]}},
 {852,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {851,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {850,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {849,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {848,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {847,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {846,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {845,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {844,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {843,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {842,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {841,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {840,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {839,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {838,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {837,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {836,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {835,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {834,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {833,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {832,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {831,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {830,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {829,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {828,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {827,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {826,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {825,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {824,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {823,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {822,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {821,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {820,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {819,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {818,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {817,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {816,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {815,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {814,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {813,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {812,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {811,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {810,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {809,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {808,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {807,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {806,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {805,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {804,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {803,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {802,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {801,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {800,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {799,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {798,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {797,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {796,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {795,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {794,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {793,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {792,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {791,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {790,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {789,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {788,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {787,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {786,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {785,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {784,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {783,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {782,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {781,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {780,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {779,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {778,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {777,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {776,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {775,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {774,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {773,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {772,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {771,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {770,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {769,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {768,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {767,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {766,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {765,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {764,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {763,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {762,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {761,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {760,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {759,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {758,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {757,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {756,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {755,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {754,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {753,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {752,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {751,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {750,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {749,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {748,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {747,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {746,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {745,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {744,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {743,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {742,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {741,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {740,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {739,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {738,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {737,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {736,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {735,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {734,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {733,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {732,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {731,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {730,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {729,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {728,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {727,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {726,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {725,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {724,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {723,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {722,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {721,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {720,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {719,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {718,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {717,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {716,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {715,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {714,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {713,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {712,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {711,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {710,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {709,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {708,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {707,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {706,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {705,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {704,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {703,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {702,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {701,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {700,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {699,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {698,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {697,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {696,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {695,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {694,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {693,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {692,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {691,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {690,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {689,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {688,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {687,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {686,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {685,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {684,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {683,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {682,active,{[{topology,[['ns_1@db3.lan','ns_1@172.19.0.4']]}]}},
 {681,replica,undefined},
 {680,replica,undefined},
 {679,replica,undefined},
 {678,replica,undefined},
 {677,replica,undefined},
 {676,replica,undefined},
 {675,replica,undefined},
 {674,replica,undefined},
 {673,replica,undefined},
 {672,replica,undefined},
 {671,replica,undefined},
 {670,replica,undefined},
 {669,replica,undefined},
 {668,replica,undefined},
 {667,replica,undefined},
 {666,replica,undefined},
 {665,replica,undefined},
 {664,replica,undefined},
 {663,replica,undefined},
 {662,replica,undefined},
 {661,replica,undefined},
 {660,replica,undefined},
 {659,replica,undefined},
 {658,replica,undefined},
 {657,replica,undefined},
 {656,replica,undefined},
 {655,replica,undefined},
 {654,replica,undefined},
 {653,replica,undefined},
 {652,replica,undefined},
 {651,replica,undefined},
 {650,replica,undefined},
 {649,replica,undefined},
 {648,replica,undefined},
 {647,replica,undefined},
 {646,replica,undefined},
 {645,replica,undefined},
 {644,replica,undefined},
 {643,replica,undefined},
 {642,replica,undefined},
 {641,replica,undefined},
 {640,replica,undefined},
 {639,replica,undefined},
 {638,replica,undefined},
 {637,replica,undefined},
 {636,replica,undefined},
 {635,replica,undefined},
 {634,replica,undefined},
 {633,replica,undefined},
 {632,replica,undefined},
 {631,replica,undefined},
 {630,replica,undefined},
 {629,replica,undefined},
 {628,replica,undefined},
 {627,replica,undefined},
 {626,replica,undefined},
 {625,replica,undefined},
 {624,replica,undefined},
 {623,replica,undefined},
 {622,replica,undefined},
 {621,replica,undefined},
 {620,replica,undefined},
 {619,replica,undefined},
 {618,replica,undefined},
 {617,replica,undefined},
 {616,replica,undefined},
 {615,replica,undefined},
 {614,replica,undefined},
 {613,replica,undefined},
 {612,replica,undefined},
 {611,replica,undefined},
 {610,replica,undefined},
 {609,replica,undefined},
 {608,replica,undefined},
 {607,replica,undefined},
 {606,replica,undefined},
 {605,replica,undefined},
 {604,replica,undefined},
 {603,replica,undefined},
 {602,replica,undefined},
 {601,replica,undefined},
 {600,replica,undefined},
 {599,replica,undefined},
 {598,replica,undefined},
 {597,replica,undefined},
 {596,replica,undefined},
 {595,replica,undefined},
 {594,replica,undefined},
 {593,replica,undefined},
 {592,replica,undefined},
 {591,replica,undefined},
 {590,replica,undefined},
 {589,replica,undefined},
 {588,replica,undefined},
 {587,replica,undefined},
 {586,replica,undefined},
 {585,replica,undefined},
 {584,replica,undefined},
 {583,replica,undefined},
 {582,replica,undefined},
 {581,replica,undefined},
 {580,replica,undefined},
 {579,replica,undefined},
 {578,replica,undefined},
 {577,replica,undefined},
 {576,replica,undefined},
 {575,replica,undefined},
 {574,replica,undefined},
 {573,replica,undefined},
 {572,replica,undefined},
 {571,replica,undefined},
 {570,replica,undefined},
 {569,replica,undefined},
 {568,replica,undefined},
 {567,replica,undefined},
 {566,replica,undefined},
 {565,replica,undefined},
 {564,replica,undefined},
 {563,replica,undefined},
 {562,replica,undefined},
 {561,replica,undefined},
 {560,replica,undefined},
 {559,replica,undefined},
 {558,replica,undefined},
 {557,replica,undefined},
 {556,replica,undefined},
 {555,replica,undefined},
 {554,replica,undefined},
 {553,replica,undefined},
 {552,replica,undefined},
 {551,replica,undefined},
 {550,replica,undefined},
 {549,replica,undefined},
 {548,replica,undefined},
 {547,replica,undefined},
 {546,replica,undefined},
 {545,replica,undefined},
 {544,replica,undefined},
 {543,replica,undefined},
 {542,replica,undefined},
 {541,replica,undefined},
 {540,replica,undefined},
 {539,replica,undefined},
 {538,replica,undefined},
 {537,replica,undefined},
 {536,replica,undefined},
 {535,replica,undefined},
 {534,replica,undefined},
 {533,replica,undefined},
 {532,replica,undefined},
 {531,replica,undefined},
 {530,replica,undefined},
 {529,replica,undefined},
 {528,replica,undefined},
 {527,replica,undefined},
 {526,replica,undefined},
 {525,replica,undefined},
 {524,replica,undefined},
 {523,replica,undefined},
 {522,replica,undefined},
 {521,replica,undefined},
 {520,replica,undefined},
 {519,replica,undefined},
 {518,replica,undefined},
 {517,replica,undefined},
 {516,replica,undefined},
 {515,replica,undefined},
 {514,replica,undefined},
 {513,replica,undefined},
 {512,replica,undefined},
 {340,replica,undefined},
 {339,replica,undefined},
 {338,replica,undefined},
 {337,replica,undefined},
 {336,replica,undefined},
 {335,replica,undefined},
 {334,replica,undefined},
 {333,replica,undefined},
 {332,replica,undefined},
 {331,replica,undefined},
 {330,replica,undefined},
 {329,replica,undefined},
 {328,replica,undefined},
 {327,replica,undefined},
 {326,replica,undefined},
 {325,replica,undefined},
 {324,replica,undefined},
 {323,replica,undefined},
 {322,replica,undefined},
 {321,replica,undefined},
 {320,replica,undefined},
 {319,replica,undefined},
 {318,replica,undefined},
 {317,replica,undefined},
 {316,replica,undefined},
 {315,replica,undefined},
 {314,replica,undefined},
 {313,replica,undefined},
 {312,replica,undefined},
 {311,replica,undefined},
 {310,replica,undefined},
 {309,replica,undefined},
 {308,replica,undefined},
 {307,replica,undefined},
 {306,replica,undefined},
 {305,replica,undefined},
 {304,replica,undefined},
 {303,replica,undefined},
 {302,replica,undefined},
 {301,replica,undefined},
 {300,replica,undefined},
 {299,replica,undefined},
 {298,replica,undefined},
 {297,replica,undefined},
 {296,replica,undefined},
 {295,replica,undefined},
 {294,replica,undefined},
 {293,replica,undefined},
 {292,replica,undefined},
 {291,replica,undefined},
 {290,replica,undefined},
 {289,replica,undefined},
 {288,replica,undefined},
 {287,replica,undefined},
 {286,replica,undefined},
 {285,replica,undefined},
 {284,replica,undefined},
 {283,replica,undefined},
 {282,replica,undefined},
 {281,replica,undefined},
 {280,replica,undefined},
 {279,replica,undefined},
 {278,replica,undefined},
 {277,replica,undefined},
 {276,replica,undefined},
 {275,replica,undefined},
 {274,replica,undefined},
 {273,replica,undefined},
 {272,replica,undefined},
 {271,replica,undefined},
 {270,replica,undefined},
 {269,replica,undefined},
 {268,replica,undefined},
 {267,replica,undefined},
 {266,replica,undefined},
 {265,replica,undefined},
 {264,replica,undefined},
 {263,replica,undefined},
 {262,replica,undefined},
 {261,replica,undefined},
 {260,replica,undefined},
 {259,replica,undefined},
 {258,replica,undefined},
 {257,replica,undefined},
 {256,replica,undefined},
 {255,replica,undefined},
 {254,replica,undefined},
 {253,replica,undefined},
 {252,replica,undefined},
 {251,replica,undefined},
 {250,replica,undefined},
 {249,replica,undefined},
 {248,replica,undefined},
 {247,replica,undefined},
 {246,replica,undefined},
 {245,replica,undefined},
 {244,replica,undefined},
 {243,replica,undefined},
 {242,replica,undefined},
 {241,replica,undefined},
 {240,replica,undefined},
 {239,replica,undefined},
 {238,replica,undefined},
 {237,replica,undefined},
 {236,replica,undefined},
 {235,replica,undefined},
 {234,replica,undefined},
 {233,replica,undefined},
 {232,replica,undefined},
 {231,replica,undefined},
 {230,replica,undefined},
 {229,replica,undefined},
 {228,replica,undefined},
 {227,replica,undefined},
 {226,replica,undefined},
 {225,replica,undefined},
 {224,replica,undefined},
 {223,replica,undefined},
 {222,replica,undefined},
 {221,replica,undefined},
 {220,replica,undefined},
 {219,replica,undefined},
 {218,replica,undefined},
 {217,replica,undefined},
 {216,replica,undefined},
 {215,replica,undefined},
 {214,replica,undefined},
 {213,replica,undefined},
 {212,replica,undefined},
 {211,replica,undefined},
 {210,replica,undefined},
 {209,replica,undefined},
 {208,replica,undefined},
 {207,replica,undefined},
 {206,replica,undefined},
 {205,replica,undefined},
 {204,replica,undefined},
 {203,replica,undefined},
 {202,replica,undefined},
 {201,replica,undefined},
 {200,replica,undefined},
 {199,replica,undefined},
 {198,replica,undefined},
 {197,replica,undefined},
 {196,replica,undefined},
 {195,replica,undefined},
 {194,replica,undefined},
 {193,replica,undefined},
 {192,replica,undefined},
 {191,replica,undefined},
 {190,replica,undefined},
 {189,replica,undefined},
 {188,replica,undefined},
 {187,replica,undefined},
 {186,replica,undefined},
 {185,replica,undefined},
 {184,replica,undefined},
 {183,replica,undefined},
 {182,replica,undefined},
 {181,replica,undefined},
 {180,replica,undefined},
 {179,replica,undefined},
 {178,replica,undefined},
 {177,replica,undefined},
 {176,replica,undefined},
 {175,replica,undefined},
 {174,replica,undefined},
 {173,replica,undefined},
 {172,replica,undefined},
 {171,replica,undefined},
 {170,replica,undefined}]
[ns_server:debug,2025-05-15T18:47:52.261Z,ns_1@db3.lan:dcp_replication_manager-doom-scrolling<0.4102.0>:dcp_sup:start_replicator:48]Starting DCP replication from 'ns_1@172.19.0.4' for bucket "doom-scrolling" (Features = [collections,
                                                                                         del_times,
                                                                                         del_user_xattr,
                                                                                         json,
                                                                                         set_consumer_name,
                                                                                         snappy,
                                                                                         xattr])
[ns_server:debug,2025-05-15T18:47:52.276Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:open_connection:72]Open consumer connection "replication:ns_1@172.19.0.4->ns_1@db3.lan:doom-scrolling" on socket #Port<0.208>: Body <<"{\"consumer_name\":\"ns_1@db3.lan\"}">>
[ns_server:debug,2025-05-15T18:47:52.277Z,ns_1@db3.lan:dcp_replicator-doom-scrolling-ns_1@172.19.0.4<0.4230.0>:dcp_replicator:init:53]Opened connection to local memcached <0.4231.0>
[error_logger:info,2025-05-15T18:47:52.280Z,ns_1@db3.lan:dcp_sup-doom-scrolling<0.4101.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'dcp_sup-doom-scrolling'}
    started: [{pid,<0.4230.0>},
              {id,{'ns_1@172.19.0.4',[collections,del_times,del_user_xattr,
                                      json,set_consumer_name,snappy,xattr]}},
              {mfargs,{dcp_replicator,start_link,
                                      ['ns_1@172.19.0.4',"doom-scrolling",
                                       [collections,del_times,del_user_xattr,
                                        json,set_consumer_name,snappy,
                                        xattr]]}},
              {restart_type,temporary},
              {significant,false},
              {shutdown,60000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:52.280Z,ns_1@db3.lan:dcp_replication_manager-doom-scrolling<0.4102.0>:dcp_sup:start_replicator:48]Starting DCP replication from 'ns_1@db2.lan' for bucket "doom-scrolling" (Features = [collections,
                                                                                      del_times,
                                                                                      del_user_xattr,
                                                                                      json,
                                                                                      set_consumer_name,
                                                                                      snappy,
                                                                                      xattr])
[ns_server:debug,2025-05-15T18:47:52.283Z,ns_1@db3.lan:<0.4232.0>:dcp_commands:open_connection:72]Open producer connection "replication:ns_1@172.19.0.4->ns_1@db3.lan:doom-scrolling" on socket #Port<0.210>: Body undefined
[ns_server:debug,2025-05-15T18:47:52.283Z,ns_1@db3.lan:<0.4233.0>:dcp_replicator:connect_to_producer:81]initiated new dcp replication with consumer side: <0.4231.0> and producer side: <0.4232.0>
[ns_server:debug,2025-05-15T18:47:52.283Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:open_connection:72]Open consumer connection "replication:ns_1@db2.lan->ns_1@db3.lan:doom-scrolling" on socket #Port<0.209>: Body <<"{\"consumer_name\":\"ns_1@db3.lan\"}">>
[ns_server:debug,2025-05-15T18:47:52.284Z,ns_1@db3.lan:dcp_replicator-doom-scrolling-ns_1@db2.lan<0.4234.0>:dcp_replicator:init:53]Opened connection to local memcached <0.4235.0>
[error_logger:info,2025-05-15T18:47:52.284Z,ns_1@db3.lan:dcp_sup-doom-scrolling<0.4101.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'dcp_sup-doom-scrolling'}
    started: [{pid,<0.4234.0>},
              {id,{'ns_1@db2.lan',[collections,del_times,del_user_xattr,json,
                                   set_consumer_name,snappy,xattr]}},
              {mfargs,{dcp_replicator,start_link,
                                      ['ns_1@db2.lan',"doom-scrolling",
                                       [collections,del_times,del_user_xattr,
                                        json,set_consumer_name,snappy,
                                        xattr]]}},
              {restart_type,temporary},
              {significant,false},
              {shutdown,60000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:52.284Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 170, opaque = 0xAA, type = add
[ns_server:debug,2025-05-15T18:47:52.284Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 171, opaque = 0xAB, type = add
[ns_server:debug,2025-05-15T18:47:52.284Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 172, opaque = 0xAC, type = add
[ns_server:debug,2025-05-15T18:47:52.284Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 173, opaque = 0xAD, type = add
[ns_server:debug,2025-05-15T18:47:52.284Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 174, opaque = 0xAE, type = add
[ns_server:debug,2025-05-15T18:47:52.284Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 175, opaque = 0xAF, type = add
[ns_server:debug,2025-05-15T18:47:52.284Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 176, opaque = 0xB0, type = add
[ns_server:debug,2025-05-15T18:47:52.284Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 177, opaque = 0xB1, type = add
[ns_server:debug,2025-05-15T18:47:52.284Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 178, opaque = 0xB2, type = add
[ns_server:debug,2025-05-15T18:47:52.284Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 179, opaque = 0xB3, type = add
[ns_server:debug,2025-05-15T18:47:52.284Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 180, opaque = 0xB4, type = add
[ns_server:debug,2025-05-15T18:47:52.284Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 181, opaque = 0xB5, type = add
[ns_server:debug,2025-05-15T18:47:52.285Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 182, opaque = 0xB6, type = add
[ns_server:debug,2025-05-15T18:47:52.285Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 183, opaque = 0xB7, type = add
[ns_server:debug,2025-05-15T18:47:52.285Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 184, opaque = 0xB8, type = add
[ns_server:debug,2025-05-15T18:47:52.285Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 185, opaque = 0xB9, type = add
[ns_server:debug,2025-05-15T18:47:52.285Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 186, opaque = 0xBA, type = add
[ns_server:debug,2025-05-15T18:47:52.285Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 187, opaque = 0xBB, type = add
[ns_server:debug,2025-05-15T18:47:52.285Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 188, opaque = 0xBC, type = add
[ns_server:debug,2025-05-15T18:47:52.285Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 189, opaque = 0xBD, type = add
[ns_server:debug,2025-05-15T18:47:52.285Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 190, opaque = 0xBE, type = add
[ns_server:debug,2025-05-15T18:47:52.285Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 191, opaque = 0xBF, type = add
[ns_server:debug,2025-05-15T18:47:52.285Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 192, opaque = 0xC0, type = add
[ns_server:debug,2025-05-15T18:47:52.285Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 193, opaque = 0xC1, type = add
[ns_server:debug,2025-05-15T18:47:52.285Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 194, opaque = 0xC2, type = add
[ns_server:debug,2025-05-15T18:47:52.285Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 195, opaque = 0xC3, type = add
[ns_server:debug,2025-05-15T18:47:52.285Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 196, opaque = 0xC4, type = add
[ns_server:debug,2025-05-15T18:47:52.285Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 197, opaque = 0xC5, type = add
[ns_server:debug,2025-05-15T18:47:52.285Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 198, opaque = 0xC6, type = add
[ns_server:debug,2025-05-15T18:47:52.285Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 199, opaque = 0xC7, type = add
[ns_server:debug,2025-05-15T18:47:52.285Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 200, opaque = 0xC8, type = add
[ns_server:debug,2025-05-15T18:47:52.285Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 201, opaque = 0xC9, type = add
[ns_server:debug,2025-05-15T18:47:52.285Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 202, opaque = 0xCA, type = add
[ns_server:debug,2025-05-15T18:47:52.285Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 203, opaque = 0xCB, type = add
[ns_server:debug,2025-05-15T18:47:52.285Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 204, opaque = 0xCC, type = add
[ns_server:debug,2025-05-15T18:47:52.285Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 205, opaque = 0xCD, type = add
[ns_server:debug,2025-05-15T18:47:52.285Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 206, opaque = 0xCE, type = add
[ns_server:debug,2025-05-15T18:47:52.285Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 207, opaque = 0xCF, type = add
[ns_server:debug,2025-05-15T18:47:52.285Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 208, opaque = 0xD0, type = add
[ns_server:debug,2025-05-15T18:47:52.285Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 209, opaque = 0xD1, type = add
[ns_server:debug,2025-05-15T18:47:52.285Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 210, opaque = 0xD2, type = add
[ns_server:debug,2025-05-15T18:47:52.285Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 211, opaque = 0xD3, type = add
[ns_server:debug,2025-05-15T18:47:52.285Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 212, opaque = 0xD4, type = add
[ns_server:debug,2025-05-15T18:47:52.286Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 213, opaque = 0xD5, type = add
[ns_server:debug,2025-05-15T18:47:52.286Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 214, opaque = 0xD6, type = add
[ns_server:debug,2025-05-15T18:47:52.286Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 215, opaque = 0xD7, type = add
[ns_server:debug,2025-05-15T18:47:52.286Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 216, opaque = 0xD8, type = add
[ns_server:debug,2025-05-15T18:47:52.286Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 217, opaque = 0xD9, type = add
[ns_server:debug,2025-05-15T18:47:52.286Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 218, opaque = 0xDA, type = add
[ns_server:debug,2025-05-15T18:47:52.286Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 219, opaque = 0xDB, type = add
[ns_server:debug,2025-05-15T18:47:52.286Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 220, opaque = 0xDC, type = add
[ns_server:debug,2025-05-15T18:47:52.286Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 221, opaque = 0xDD, type = add
[ns_server:debug,2025-05-15T18:47:52.286Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 222, opaque = 0xDE, type = add
[ns_server:debug,2025-05-15T18:47:52.286Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 223, opaque = 0xDF, type = add
[ns_server:debug,2025-05-15T18:47:52.286Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 224, opaque = 0xE0, type = add
[ns_server:debug,2025-05-15T18:47:52.286Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 225, opaque = 0xE1, type = add
[ns_server:debug,2025-05-15T18:47:52.286Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 226, opaque = 0xE2, type = add
[ns_server:debug,2025-05-15T18:47:52.287Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 227, opaque = 0xE3, type = add
[ns_server:debug,2025-05-15T18:47:52.287Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 228, opaque = 0xE4, type = add
[ns_server:debug,2025-05-15T18:47:52.287Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 229, opaque = 0xE5, type = add
[ns_server:debug,2025-05-15T18:47:52.287Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 230, opaque = 0xE6, type = add
[ns_server:debug,2025-05-15T18:47:52.287Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 231, opaque = 0xE7, type = add
[ns_server:debug,2025-05-15T18:47:52.287Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 232, opaque = 0xE8, type = add
[ns_server:debug,2025-05-15T18:47:52.287Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 233, opaque = 0xE9, type = add
[ns_server:debug,2025-05-15T18:47:52.287Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 234, opaque = 0xEA, type = add
[ns_server:debug,2025-05-15T18:47:52.287Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 235, opaque = 0xEB, type = add
[ns_server:debug,2025-05-15T18:47:52.287Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 236, opaque = 0xEC, type = add
[ns_server:debug,2025-05-15T18:47:52.287Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 237, opaque = 0xED, type = add
[ns_server:debug,2025-05-15T18:47:52.287Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 238, opaque = 0xEE, type = add
[ns_server:debug,2025-05-15T18:47:52.287Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 239, opaque = 0xEF, type = add
[ns_server:debug,2025-05-15T18:47:52.287Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 240, opaque = 0xF0, type = add
[ns_server:debug,2025-05-15T18:47:52.287Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 241, opaque = 0xF1, type = add
[ns_server:debug,2025-05-15T18:47:52.287Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 242, opaque = 0xF2, type = add
[ns_server:debug,2025-05-15T18:47:52.287Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 243, opaque = 0xF3, type = add
[ns_server:debug,2025-05-15T18:47:52.287Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 244, opaque = 0xF4, type = add
[ns_server:debug,2025-05-15T18:47:52.287Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 245, opaque = 0xF5, type = add
[ns_server:debug,2025-05-15T18:47:52.287Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 246, opaque = 0xF6, type = add
[ns_server:debug,2025-05-15T18:47:52.288Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 247, opaque = 0xF7, type = add
[ns_server:debug,2025-05-15T18:47:52.288Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 248, opaque = 0xF8, type = add
[ns_server:debug,2025-05-15T18:47:52.288Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 249, opaque = 0xF9, type = add
[ns_server:debug,2025-05-15T18:47:52.288Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 250, opaque = 0xFA, type = add
[ns_server:debug,2025-05-15T18:47:52.288Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 251, opaque = 0xFB, type = add
[ns_server:debug,2025-05-15T18:47:52.288Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 252, opaque = 0xFC, type = add
[ns_server:debug,2025-05-15T18:47:52.288Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 253, opaque = 0xFD, type = add
[ns_server:debug,2025-05-15T18:47:52.288Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 254, opaque = 0xFE, type = add
[ns_server:debug,2025-05-15T18:47:52.288Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 255, opaque = 0xFF, type = add
[ns_server:debug,2025-05-15T18:47:52.289Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 256, opaque = 0x100, type = add
[ns_server:debug,2025-05-15T18:47:52.289Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 257, opaque = 0x101, type = add
[ns_server:debug,2025-05-15T18:47:52.289Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 258, opaque = 0x102, type = add
[ns_server:debug,2025-05-15T18:47:52.289Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 259, opaque = 0x103, type = add
[ns_server:debug,2025-05-15T18:47:52.289Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 260, opaque = 0x104, type = add
[ns_server:debug,2025-05-15T18:47:52.289Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 261, opaque = 0x105, type = add
[ns_server:debug,2025-05-15T18:47:52.289Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 262, opaque = 0x106, type = add
[ns_server:debug,2025-05-15T18:47:52.289Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 263, opaque = 0x107, type = add
[ns_server:debug,2025-05-15T18:47:52.289Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 264, opaque = 0x108, type = add
[ns_server:debug,2025-05-15T18:47:52.289Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 265, opaque = 0x109, type = add
[ns_server:debug,2025-05-15T18:47:52.289Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 266, opaque = 0x10A, type = add
[ns_server:debug,2025-05-15T18:47:52.289Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 267, opaque = 0x10B, type = add
[ns_server:debug,2025-05-15T18:47:52.289Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 268, opaque = 0x10C, type = add
[ns_server:debug,2025-05-15T18:47:52.289Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 269, opaque = 0x10D, type = add
[ns_server:debug,2025-05-15T18:47:52.289Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 270, opaque = 0x10E, type = add
[ns_server:debug,2025-05-15T18:47:52.289Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 271, opaque = 0x10F, type = add
[ns_server:debug,2025-05-15T18:47:52.289Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 272, opaque = 0x110, type = add
[ns_server:debug,2025-05-15T18:47:52.289Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 273, opaque = 0x111, type = add
[ns_server:debug,2025-05-15T18:47:52.289Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 274, opaque = 0x112, type = add
[ns_server:debug,2025-05-15T18:47:52.289Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 275, opaque = 0x113, type = add
[ns_server:debug,2025-05-15T18:47:52.289Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 276, opaque = 0x114, type = add
[ns_server:debug,2025-05-15T18:47:52.289Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 277, opaque = 0x115, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 278, opaque = 0x116, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 279, opaque = 0x117, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 280, opaque = 0x118, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 281, opaque = 0x119, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 282, opaque = 0x11A, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 283, opaque = 0x11B, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 284, opaque = 0x11C, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 285, opaque = 0x11D, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 286, opaque = 0x11E, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 287, opaque = 0x11F, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 288, opaque = 0x120, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 289, opaque = 0x121, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 290, opaque = 0x122, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 291, opaque = 0x123, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 292, opaque = 0x124, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 293, opaque = 0x125, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 294, opaque = 0x126, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 295, opaque = 0x127, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 296, opaque = 0x128, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 297, opaque = 0x129, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 298, opaque = 0x12A, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 299, opaque = 0x12B, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 300, opaque = 0x12C, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 301, opaque = 0x12D, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 302, opaque = 0x12E, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 303, opaque = 0x12F, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 304, opaque = 0x130, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 305, opaque = 0x131, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 306, opaque = 0x132, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 307, opaque = 0x133, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 308, opaque = 0x134, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 309, opaque = 0x135, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 310, opaque = 0x136, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 311, opaque = 0x137, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 312, opaque = 0x138, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 313, opaque = 0x139, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 314, opaque = 0x13A, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 315, opaque = 0x13B, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 316, opaque = 0x13C, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 317, opaque = 0x13D, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 318, opaque = 0x13E, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 319, opaque = 0x13F, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 320, opaque = 0x140, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 321, opaque = 0x141, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 322, opaque = 0x142, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 323, opaque = 0x143, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 324, opaque = 0x144, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 325, opaque = 0x145, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 326, opaque = 0x146, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 327, opaque = 0x147, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 328, opaque = 0x148, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 329, opaque = 0x149, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 330, opaque = 0x14A, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 331, opaque = 0x14B, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 332, opaque = 0x14C, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 333, opaque = 0x14D, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 334, opaque = 0x14E, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 335, opaque = 0x14F, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 336, opaque = 0x150, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 337, opaque = 0x151, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 338, opaque = 0x152, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 339, opaque = 0x153, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@db3.lan:<0.4231.0>:dcp_commands:add_stream:83]Add stream for partition 340, opaque = 0x154, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_call:206]Setup DCP streams:
Current []
Streams to open [170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340]
Streams to close []

[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0x5000000
80 5E 00 16
00 00 00 00
00 00 00 1E
05 00 00 00
00 00 00 00
00 00 00 00
63 6F 6E 6E
65 63 74 69
6F 6E 5F 62
75 66 66 65
72 5F 73 69
7A 65 31 33
34 32 31 37
37 33 
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0xFE (cmd_get_error_map) vbucket = 0 opaque = 0x6000000
80 FE 00 00
00 00 00 00
00 00 00 02
06 00 00 00
00 00 00 00
00 00 00 00
00 00 
[ns_server:debug,2025-05-15T18:47:52.298Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0x5000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
05 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.298Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0xFE (cmd_get_error_map) vbucket = 0 opaque = 0x6000000 status = 0x1 (key_enoent)
81 FE 00 00
00 00 00 01
00 00 00 00
06 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.299Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xAE000000
80 5E 00 0B
00 00 00 00
00 00 00 0F
AE 00 00 00
00 00 00 00
00 00 00 00
65 6E 61 62
6C 65 5F 6E
6F 6F 70 74
72 75 65 
[ns_server:debug,2025-05-15T18:47:52.299Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xAF000000
80 5E 00 11
00 00 00 00
00 00 00 19
AF 00 00 00
00 00 00 00
00 00 00 00
73 65 74 5F
6E 6F 6F 70
5F 69 6E 74
65 72 76 61
6C 30 2E 31
30 30 30 30
30 
[ns_server:debug,2025-05-15T18:47:52.300Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xAE000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
AE 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.300Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xAF000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
AF 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.301Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB0000000
80 5E 00 0C
00 00 00 00
00 00 00 10
B0 00 00 00
00 00 00 00
00 00 00 00
73 65 74 5F
70 72 69 6F
72 69 74 79
68 69 67 68

[ns_server:debug,2025-05-15T18:47:52.301Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB1000000
80 5E 00 1F
00 00 00 00
00 00 00 23
B1 00 00 00
00 00 00 00
00 00 00 00
73 75 70 70
6F 72 74 73
5F 63 75 72
73 6F 72 5F
64 72 6F 70
70 69 6E 67
5F 76 75 6C
63 61 6E 74
72 75 65 
[ns_server:debug,2025-05-15T18:47:52.301Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB2000000
80 5E 00 11
00 00 00 00
00 00 00 15
B2 00 00 00
00 00 00 00
00 00 00 00
73 75 70 70
6F 72 74 73
5F 68 69 66
69 5F 4D 46
55 74 72 75
65 
[ns_server:debug,2025-05-15T18:47:52.301Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB3000000
80 5E 00 26
00 00 00 00
00 00 00 2A
B3 00 00 00
00 00 00 00
00 00 00 00
73 65 6E 64
5F 73 74 72
65 61 6D 5F
65 6E 64 5F
6F 6E 5F 63
6C 69 65 6E
74 5F 63 6C
6F 73 65 5F
73 74 72 65
61 6D 74 72
75 65 
[ns_server:debug,2025-05-15T18:47:52.301Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB4000000
80 5E 00 14
00 00 00 00
00 00 00 18
B4 00 00 00
00 00 00 00
00 00 00 00
65 6E 61 62
6C 65 5F 65
78 70 69 72
79 5F 6F 70
63 6F 64 65
74 72 75 65

[ns_server:debug,2025-05-15T18:47:52.301Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB5000000
80 5E 00 12
00 00 00 00
00 00 00 16
B5 00 00 00
00 00 00 00
00 00 00 00
65 6E 61 62
6C 65 5F 73
79 6E 63 5F
77 72 69 74
65 73 74 72
75 65 
[ns_server:debug,2025-05-15T18:47:52.302Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB0000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B0 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.303Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB1000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B1 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.303Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB2000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B2 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.303Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB3000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B3 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.303Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB4000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B4 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.303Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB5000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B5 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.304Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB6000000
80 5E 00 0D
00 00 00 00
00 00 00 19
B6 00 00 00
00 00 00 00
00 00 00 00
63 6F 6E 73
75 6D 65 72
5F 6E 61 6D
65 6E 73 5F
31 40 64 62
33 2E 6C 61
6E 
[ns_server:debug,2025-05-15T18:47:52.304Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB7000000
80 5E 00 1B
00 00 00 00
00 00 00 1F
B7 00 00 00
00 00 00 00
00 00 00 00
69 6E 63 6C
75 64 65 5F
64 65 6C 65
74 65 64 5F
75 73 65 72
5F 78 61 74
74 72 73 74
72 75 65 
[ns_server:debug,2025-05-15T18:47:52.306Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB6000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B6 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.307Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB7000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B7 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.307Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB8000000
80 5E 00 13
00 00 00 00
00 00 00 17
B8 00 00 00
00 00 00 00
00 00 00 00
76 37 5F 64
63 70 5F 73
74 61 74 75
73 5F 63 6F
64 65 73 74
72 75 65 
[ns_server:debug,2025-05-15T18:47:52.307Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB8000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B8 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.308Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB9000000
80 5E 00 19
00 00 00 00
00 00 00 1D
B9 00 00 00
00 00 00 00
00 00 00 00
66 6C 61 74
62 75 66 66
65 72 73 5F
73 79 73 74
65 6D 5F 65
76 65 6E 74
73 74 72 75
65 
[ns_server:debug,2025-05-15T18:47:52.309Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB9000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B9 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.309Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xBA000000
80 5E 00 0E
00 00 00 00
00 00 00 12
BA 00 00 00
00 00 00 00
00 00 00 00
63 68 61 6E
67 65 5F 73
74 72 65 61
6D 73 74 72
75 65 
[ns_server:debug,2025-05-15T18:47:52.311Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xBA000000 status = 0x83 (not_supported)
81 5E 00 00
00 00 00 83
00 00 00 00
BA 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.312Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 170 opaque = 0x1000000
80 53 00 00
30 00 00 AA
00 00 00 30
01 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.312Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 171 opaque = 0x2000000
80 53 00 00
30 00 00 AB
00 00 00 30
02 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.312Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 172 opaque = 0x3000000
80 53 00 00
30 00 00 AC
00 00 00 30
03 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.312Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 173 opaque = 0x4000000
80 53 00 00
30 00 00 AD
00 00 00 30
04 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.312Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 174 opaque = 0x7000000
80 53 00 00
30 00 00 AE
00 00 00 30
07 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.312Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 175 opaque = 0x8000000
80 53 00 00
30 00 00 AF
00 00 00 30
08 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.313Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 176 opaque = 0x9000000
80 53 00 00
30 00 00 B0
00 00 00 30
09 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.313Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 177 opaque = 0xA000000
80 53 00 00
30 00 00 B1
00 00 00 30
0A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.313Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 178 opaque = 0xB000000
80 53 00 00
30 00 00 B2
00 00 00 30
0B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.313Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 179 opaque = 0xC000000
80 53 00 00
30 00 00 B3
00 00 00 30
0C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.313Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 180 opaque = 0xD000000
80 53 00 00
30 00 00 B4
00 00 00 30
0D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.313Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 181 opaque = 0xE000000
80 53 00 00
30 00 00 B5
00 00 00 30
0E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.313Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 182 opaque = 0xF000000
80 53 00 00
30 00 00 B6
00 00 00 30
0F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.314Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 183 opaque = 0x10000000
80 53 00 00
30 00 00 B7
00 00 00 30
10 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.314Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 184 opaque = 0x11000000
80 53 00 00
30 00 00 B8
00 00 00 30
11 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.314Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 185 opaque = 0x12000000
80 53 00 00
30 00 00 B9
00 00 00 30
12 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.314Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 186 opaque = 0x13000000
80 53 00 00
30 00 00 BA
00 00 00 30
13 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.314Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 187 opaque = 0x14000000
80 53 00 00
30 00 00 BB
00 00 00 30
14 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.314Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 188 opaque = 0x15000000
80 53 00 00
30 00 00 BC
00 00 00 30
15 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.314Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 189 opaque = 0x16000000
80 53 00 00
30 00 00 BD
00 00 00 30
16 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.314Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 190 opaque = 0x17000000
80 53 00 00
30 00 00 BE
00 00 00 30
17 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.315Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 191 opaque = 0x18000000
80 53 00 00
30 00 00 BF
00 00 00 30
18 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.315Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 192 opaque = 0x19000000
80 53 00 00
30 00 00 C0
00 00 00 30
19 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.315Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 193 opaque = 0x1A000000
80 53 00 00
30 00 00 C1
00 00 00 30
1A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.315Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 194 opaque = 0x1B000000
80 53 00 00
30 00 00 C2
00 00 00 30
1B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.315Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 195 opaque = 0x1C000000
80 53 00 00
30 00 00 C3
00 00 00 30
1C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.315Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 196 opaque = 0x1D000000
80 53 00 00
30 00 00 C4
00 00 00 30
1D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.315Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 197 opaque = 0x1E000000
80 53 00 00
30 00 00 C5
00 00 00 30
1E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.315Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 198 opaque = 0x1F000000
80 53 00 00
30 00 00 C6
00 00 00 30
1F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.315Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 199 opaque = 0x20000000
80 53 00 00
30 00 00 C7
00 00 00 30
20 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.316Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 200 opaque = 0x21000000
80 53 00 00
30 00 00 C8
00 00 00 30
21 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.316Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 201 opaque = 0x22000000
80 53 00 00
30 00 00 C9
00 00 00 30
22 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.316Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 202 opaque = 0x23000000
80 53 00 00
30 00 00 CA
00 00 00 30
23 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.316Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 203 opaque = 0x24000000
80 53 00 00
30 00 00 CB
00 00 00 30
24 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.316Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 204 opaque = 0x25000000
80 53 00 00
30 00 00 CC
00 00 00 30
25 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.316Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 205 opaque = 0x26000000
80 53 00 00
30 00 00 CD
00 00 00 30
26 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.316Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 206 opaque = 0x27000000
80 53 00 00
30 00 00 CE
00 00 00 30
27 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.316Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 207 opaque = 0x28000000
80 53 00 00
30 00 00 CF
00 00 00 30
28 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.317Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 208 opaque = 0x29000000
80 53 00 00
30 00 00 D0
00 00 00 30
29 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.317Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 209 opaque = 0x2A000000
80 53 00 00
30 00 00 D1
00 00 00 30
2A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.317Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 210 opaque = 0x2B000000
80 53 00 00
30 00 00 D2
00 00 00 30
2B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.317Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 211 opaque = 0x2C000000
80 53 00 00
30 00 00 D3
00 00 00 30
2C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.317Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 212 opaque = 0x2D000000
80 53 00 00
30 00 00 D4
00 00 00 30
2D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.317Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 213 opaque = 0x2E000000
80 53 00 00
30 00 00 D5
00 00 00 30
2E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.317Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 214 opaque = 0x2F000000
80 53 00 00
30 00 00 D6
00 00 00 30
2F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.317Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 215 opaque = 0x30000000
80 53 00 00
30 00 00 D7
00 00 00 30
30 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.317Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 216 opaque = 0x31000000
80 53 00 00
30 00 00 D8
00 00 00 30
31 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.317Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 217 opaque = 0x32000000
80 53 00 00
30 00 00 D9
00 00 00 30
32 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.318Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 218 opaque = 0x33000000
80 53 00 00
30 00 00 DA
00 00 00 30
33 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.318Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 219 opaque = 0x34000000
80 53 00 00
30 00 00 DB
00 00 00 30
34 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.318Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 220 opaque = 0x35000000
80 53 00 00
30 00 00 DC
00 00 00 30
35 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.318Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 221 opaque = 0x36000000
80 53 00 00
30 00 00 DD
00 00 00 30
36 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.318Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 222 opaque = 0x37000000
80 53 00 00
30 00 00 DE
00 00 00 30
37 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.318Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 223 opaque = 0x38000000
80 53 00 00
30 00 00 DF
00 00 00 30
38 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.318Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 224 opaque = 0x39000000
80 53 00 00
30 00 00 E0
00 00 00 30
39 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.319Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 225 opaque = 0x3A000000
80 53 00 00
30 00 00 E1
00 00 00 30
3A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.319Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 226 opaque = 0x3B000000
80 53 00 00
30 00 00 E2
00 00 00 30
3B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.319Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 227 opaque = 0x3C000000
80 53 00 00
30 00 00 E3
00 00 00 30
3C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.319Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 228 opaque = 0x3D000000
80 53 00 00
30 00 00 E4
00 00 00 30
3D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.319Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 229 opaque = 0x3E000000
80 53 00 00
30 00 00 E5
00 00 00 30
3E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.319Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 230 opaque = 0x3F000000
80 53 00 00
30 00 00 E6
00 00 00 30
3F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.320Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 231 opaque = 0x40000000
80 53 00 00
30 00 00 E7
00 00 00 30
40 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.320Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 232 opaque = 0x41000000
80 53 00 00
30 00 00 E8
00 00 00 30
41 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.320Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 233 opaque = 0x42000000
80 53 00 00
30 00 00 E9
00 00 00 30
42 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.320Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 234 opaque = 0x43000000
80 53 00 00
30 00 00 EA
00 00 00 30
43 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.320Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 235 opaque = 0x44000000
80 53 00 00
30 00 00 EB
00 00 00 30
44 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.320Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 236 opaque = 0x45000000
80 53 00 00
30 00 00 EC
00 00 00 30
45 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.320Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 237 opaque = 0x46000000
80 53 00 00
30 00 00 ED
00 00 00 30
46 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.320Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 238 opaque = 0x47000000
80 53 00 00
30 00 00 EE
00 00 00 30
47 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.321Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 239 opaque = 0x48000000
80 53 00 00
30 00 00 EF
00 00 00 30
48 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.321Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 240 opaque = 0x49000000
80 53 00 00
30 00 00 F0
00 00 00 30
49 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.321Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 241 opaque = 0x4A000000
80 53 00 00
30 00 00 F1
00 00 00 30
4A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.321Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 242 opaque = 0x4B000000
80 53 00 00
30 00 00 F2
00 00 00 30
4B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.321Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 243 opaque = 0x4C000000
80 53 00 00
30 00 00 F3
00 00 00 30
4C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.321Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 244 opaque = 0x4D000000
80 53 00 00
30 00 00 F4
00 00 00 30
4D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.321Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 245 opaque = 0x4E000000
80 53 00 00
30 00 00 F5
00 00 00 30
4E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.321Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 246 opaque = 0x4F000000
80 53 00 00
30 00 00 F6
00 00 00 30
4F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.321Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 247 opaque = 0x50000000
80 53 00 00
30 00 00 F7
00 00 00 30
50 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.321Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 248 opaque = 0x51000000
80 53 00 00
30 00 00 F8
00 00 00 30
51 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.322Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 249 opaque = 0x52000000
80 53 00 00
30 00 00 F9
00 00 00 30
52 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.322Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 250 opaque = 0x53000000
80 53 00 00
30 00 00 FA
00 00 00 30
53 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.322Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 251 opaque = 0x54000000
80 53 00 00
30 00 00 FB
00 00 00 30
54 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.322Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 252 opaque = 0x55000000
80 53 00 00
30 00 00 FC
00 00 00 30
55 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.322Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 253 opaque = 0x56000000
80 53 00 00
30 00 00 FD
00 00 00 30
56 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.322Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 254 opaque = 0x57000000
80 53 00 00
30 00 00 FE
00 00 00 30
57 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.322Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 255 opaque = 0x58000000
80 53 00 00
30 00 00 FF
00 00 00 30
58 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.322Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 256 opaque = 0x59000000
80 53 00 00
30 00 01 00
00 00 00 30
59 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.322Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 257 opaque = 0x5A000000
80 53 00 00
30 00 01 01
00 00 00 30
5A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.322Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 258 opaque = 0x5B000000
80 53 00 00
30 00 01 02
00 00 00 30
5B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.323Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 259 opaque = 0x5C000000
80 53 00 00
30 00 01 03
00 00 00 30
5C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.323Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 260 opaque = 0x5D000000
80 53 00 00
30 00 01 04
00 00 00 30
5D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.323Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 261 opaque = 0x5E000000
80 53 00 00
30 00 01 05
00 00 00 30
5E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.323Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 262 opaque = 0x5F000000
80 53 00 00
30 00 01 06
00 00 00 30
5F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.323Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 263 opaque = 0x60000000
80 53 00 00
30 00 01 07
00 00 00 30
60 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.323Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 264 opaque = 0x61000000
80 53 00 00
30 00 01 08
00 00 00 30
61 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.323Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 265 opaque = 0x62000000
80 53 00 00
30 00 01 09
00 00 00 30
62 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.324Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 266 opaque = 0x63000000
80 53 00 00
30 00 01 0A
00 00 00 30
63 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.324Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 267 opaque = 0x64000000
80 53 00 00
30 00 01 0B
00 00 00 30
64 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.324Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 268 opaque = 0x65000000
80 53 00 00
30 00 01 0C
00 00 00 30
65 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.324Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 269 opaque = 0x66000000
80 53 00 00
30 00 01 0D
00 00 00 30
66 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.324Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 270 opaque = 0x67000000
80 53 00 00
30 00 01 0E
00 00 00 30
67 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.324Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 271 opaque = 0x68000000
80 53 00 00
30 00 01 0F
00 00 00 30
68 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.325Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 272 opaque = 0x69000000
80 53 00 00
30 00 01 10
00 00 00 30
69 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.325Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 273 opaque = 0x6A000000
80 53 00 00
30 00 01 11
00 00 00 30
6A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.325Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 274 opaque = 0x6B000000
80 53 00 00
30 00 01 12
00 00 00 30
6B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.325Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 275 opaque = 0x6C000000
80 53 00 00
30 00 01 13
00 00 00 30
6C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.325Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 276 opaque = 0x6D000000
80 53 00 00
30 00 01 14
00 00 00 30
6D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.325Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 277 opaque = 0x6E000000
80 53 00 00
30 00 01 15
00 00 00 30
6E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.325Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 278 opaque = 0x6F000000
80 53 00 00
30 00 01 16
00 00 00 30
6F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.326Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 279 opaque = 0x70000000
80 53 00 00
30 00 01 17
00 00 00 30
70 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.326Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 280 opaque = 0x71000000
80 53 00 00
30 00 01 18
00 00 00 30
71 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.326Z,ns_1@db3.lan:<0.4236.0>:dcp_commands:open_connection:72]Open producer connection "replication:ns_1@db2.lan->ns_1@db3.lan:doom-scrolling" on socket #Port<0.211>: Body undefined
[ns_server:debug,2025-05-15T18:47:52.326Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 281 opaque = 0x72000000
80 53 00 00
30 00 01 19
00 00 00 30
72 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.326Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 282 opaque = 0x73000000
80 53 00 00
30 00 01 1A
00 00 00 30
73 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.326Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 283 opaque = 0x74000000
80 53 00 00
30 00 01 1B
00 00 00 30
74 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.326Z,ns_1@db3.lan:<0.4237.0>:dcp_replicator:connect_to_producer:81]initiated new dcp replication with consumer side: <0.4235.0> and producer side: <0.4236.0>
[ns_server:debug,2025-05-15T18:47:52.326Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 284 opaque = 0x75000000
80 53 00 00
30 00 01 1C
00 00 00 30
75 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.326Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 285 opaque = 0x76000000
80 53 00 00
30 00 01 1D
00 00 00 30
76 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.326Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 286 opaque = 0x77000000
80 53 00 00
30 00 01 1E
00 00 00 30
77 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.327Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 287 opaque = 0x78000000
80 53 00 00
30 00 01 1F
00 00 00 30
78 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.327Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 288 opaque = 0x79000000
80 53 00 00
30 00 01 20
00 00 00 30
79 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.327Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 289 opaque = 0x7A000000
80 53 00 00
30 00 01 21
00 00 00 30
7A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.327Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 290 opaque = 0x7B000000
80 53 00 00
30 00 01 22
00 00 00 30
7B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.327Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 291 opaque = 0x7C000000
80 53 00 00
30 00 01 23
00 00 00 30
7C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.328Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 292 opaque = 0x7D000000
80 53 00 00
30 00 01 24
00 00 00 30
7D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.328Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 293 opaque = 0x7E000000
80 53 00 00
30 00 01 25
00 00 00 30
7E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.328Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 294 opaque = 0x7F000000
80 53 00 00
30 00 01 26
00 00 00 30
7F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.328Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 295 opaque = 0x80000000
80 53 00 00
30 00 01 27
00 00 00 30
80 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.328Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 296 opaque = 0x81000000
80 53 00 00
30 00 01 28
00 00 00 30
81 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.329Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 297 opaque = 0x82000000
80 53 00 00
30 00 01 29
00 00 00 30
82 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.329Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 298 opaque = 0x83000000
80 53 00 00
30 00 01 2A
00 00 00 30
83 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.329Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 299 opaque = 0x84000000
80 53 00 00
30 00 01 2B
00 00 00 30
84 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.329Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 300 opaque = 0x85000000
80 53 00 00
30 00 01 2C
00 00 00 30
85 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.329Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 301 opaque = 0x86000000
80 53 00 00
30 00 01 2D
00 00 00 30
86 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.329Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 302 opaque = 0x87000000
80 53 00 00
30 00 01 2E
00 00 00 30
87 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.330Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 303 opaque = 0x88000000
80 53 00 00
30 00 01 2F
00 00 00 30
88 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.330Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 304 opaque = 0x89000000
80 53 00 00
30 00 01 30
00 00 00 30
89 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.330Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 305 opaque = 0x8A000000
80 53 00 00
30 00 01 31
00 00 00 30
8A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.330Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 306 opaque = 0x8B000000
80 53 00 00
30 00 01 32
00 00 00 30
8B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.330Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 307 opaque = 0x8C000000
80 53 00 00
30 00 01 33
00 00 00 30
8C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.330Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 308 opaque = 0x8D000000
80 53 00 00
30 00 01 34
00 00 00 30
8D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.330Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 309 opaque = 0x8E000000
80 53 00 00
30 00 01 35
00 00 00 30
8E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.330Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 310 opaque = 0x8F000000
80 53 00 00
30 00 01 36
00 00 00 30
8F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.330Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 311 opaque = 0x90000000
80 53 00 00
30 00 01 37
00 00 00 30
90 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.331Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 312 opaque = 0x91000000
80 53 00 00
30 00 01 38
00 00 00 30
91 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.331Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 313 opaque = 0x92000000
80 53 00 00
30 00 01 39
00 00 00 30
92 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.331Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 314 opaque = 0x93000000
80 53 00 00
30 00 01 3A
00 00 00 30
93 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.331Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 315 opaque = 0x94000000
80 53 00 00
30 00 01 3B
00 00 00 30
94 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.331Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 316 opaque = 0x95000000
80 53 00 00
30 00 01 3C
00 00 00 30
95 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.331Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 317 opaque = 0x96000000
80 53 00 00
30 00 01 3D
00 00 00 30
96 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.331Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 318 opaque = 0x97000000
80 53 00 00
30 00 01 3E
00 00 00 30
97 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.331Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 319 opaque = 0x98000000
80 53 00 00
30 00 01 3F
00 00 00 30
98 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.331Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 320 opaque = 0x99000000
80 53 00 00
30 00 01 40
00 00 00 30
99 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.332Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 321 opaque = 0x9A000000
80 53 00 00
30 00 01 41
00 00 00 30
9A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.332Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 322 opaque = 0x9B000000
80 53 00 00
30 00 01 42
00 00 00 30
9B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.332Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 323 opaque = 0x9C000000
80 53 00 00
30 00 01 43
00 00 00 30
9C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.332Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 324 opaque = 0x9D000000
80 53 00 00
30 00 01 44
00 00 00 30
9D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.332Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 325 opaque = 0x9E000000
80 53 00 00
30 00 01 45
00 00 00 30
9E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.332Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 326 opaque = 0x9F000000
80 53 00 00
30 00 01 46
00 00 00 30
9F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.332Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 327 opaque = 0xA0000000
80 53 00 00
30 00 01 47
00 00 00 30
A0 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.332Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 328 opaque = 0xA1000000
80 53 00 00
30 00 01 48
00 00 00 30
A1 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.332Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 329 opaque = 0xA2000000
80 53 00 00
30 00 01 49
00 00 00 30
A2 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.333Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 330 opaque = 0xA3000000
80 53 00 00
30 00 01 4A
00 00 00 30
A3 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.333Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 331 opaque = 0xA4000000
80 53 00 00
30 00 01 4B
00 00 00 30
A4 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.333Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 332 opaque = 0xA5000000
80 53 00 00
30 00 01 4C
00 00 00 30
A5 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.333Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 333 opaque = 0xA6000000
80 53 00 00
30 00 01 4D
00 00 00 30
A6 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.333Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 334 opaque = 0xA7000000
80 53 00 00
30 00 01 4E
00 00 00 30
A7 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.333Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 335 opaque = 0xA8000000
80 53 00 00
30 00 01 4F
00 00 00 30
A8 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.333Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 336 opaque = 0xA9000000
80 53 00 00
30 00 01 50
00 00 00 30
A9 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.333Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 337 opaque = 0xAA000000
80 53 00 00
30 00 01 51
00 00 00 30
AA 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.334Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 338 opaque = 0xAB000000
80 53 00 00
30 00 01 52
00 00 00 30
AB 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.334Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 339 opaque = 0xAC000000
80 53 00 00
30 00 01 53
00 00 00 30
AC 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.334Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 340 opaque = 0xAD000000
80 53 00 00
30 00 01 54
00 00 00 30
AD 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.337Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x1000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
01 00 00 00
00 00 00 00
00 00 00 00
00 00 A5 47
61 88 52 8D
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.337Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x2000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
02 00 00 00
00 00 00 00
00 00 00 00
00 00 21 E9
B6 24 F5 BB
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.337Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x3000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
03 00 00 00
00 00 00 00
00 00 00 00
00 00 42 52
7D FB 13 F9
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.337Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x4000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
04 00 00 00
00 00 00 00
00 00 00 00
00 00 FC F3
C7 66 2E 15
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.337Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x7000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
07 00 00 00
00 00 00 00
00 00 00 00
00 00 2C 0F
B1 26 31 7F
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.337Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x8000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
08 00 00 00
00 00 00 00
00 00 00 00
00 00 C1 EE
3B CC B4 AD
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.337Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x9000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
09 00 00 00
00 00 00 00
00 00 00 00
00 00 8B 97
E5 34 59 CD
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.337Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
0A 00 00 00
00 00 00 00
00 00 00 00
00 00 68 18
4D 8F 40 D3
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.337Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xB000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
0B 00 00 00
00 00 00 00
00 00 00 00
00 00 CF F9
6D 95 4F 8B
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.337Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xC000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
0C 00 00 00
00 00 00 00
00 00 00 00
00 00 CD F1
07 A7 18 F3
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.338Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xD000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
0D 00 00 00
00 00 00 00
00 00 00 00
00 00 68 AD
E6 5C 97 07
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.338Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xE000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
0E 00 00 00
00 00 00 00
00 00 00 00
00 00 CB 60
70 D0 F0 4D
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.338Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xF000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
0F 00 00 00
00 00 00 00
00 00 00 00
00 00 81 71
76 38 DC 0F
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.338Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x10000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
10 00 00 00
00 00 00 00
00 00 00 00
00 00 46 FD
B6 53 00 4F
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.338Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x11000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
11 00 00 00
00 00 00 00
00 00 00 00
00 00 74 D2
7D 72 92 3A
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.338Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x12000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
12 00 00 00
00 00 00 00
00 00 00 00
00 00 A2 3A
ED 18 A0 5A
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.338Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x13000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
13 00 00 00
00 00 00 00
00 00 00 00
00 00 75 34
46 9C 42 E0
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.338Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x14000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
14 00 00 00
00 00 00 00
00 00 00 00
00 00 22 5F
31 30 5B 1E
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.338Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x15000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
15 00 00 00
00 00 00 00
00 00 00 00
00 00 ED 3E
2F 53 BA FA
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.338Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x16000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
16 00 00 00
00 00 00 00
00 00 00 00
00 00 BE D3
DF 32 51 B8
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.338Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x17000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
17 00 00 00
00 00 00 00
00 00 00 00
00 00 FD FC
74 2F A5 3E
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.338Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x18000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
18 00 00 00
00 00 00 00
00 00 00 00
00 00 29 D9
19 2D 81 EE
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.338Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x19000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
19 00 00 00
00 00 00 00
00 00 00 00
00 00 4B 48
49 94 20 13
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.339Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x1A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
1A 00 00 00
00 00 00 00
00 00 00 00
00 00 91 15
3B 9A 58 0C
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.339Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x1B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
1B 00 00 00
00 00 00 00
00 00 00 00
00 00 4F 9C
85 7A 9F 22
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.339Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x1C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
1C 00 00 00
00 00 00 00
00 00 00 00
00 00 2C 23
3C B9 A3 ED
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.339Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x1D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
1D 00 00 00
00 00 00 00
00 00 00 00
00 00 7F DA
61 6C A5 83
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.339Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x1E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
1E 00 00 00
00 00 00 00
00 00 00 00
00 00 C7 B4
B1 9A BA E3
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.339Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x1F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
1F 00 00 00
00 00 00 00
00 00 00 00
00 00 22 36
3C ED 9B C8
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.339Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x20000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
20 00 00 00
00 00 00 00
00 00 00 00
00 00 62 30
F6 7B 3B CB
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.339Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x21000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
21 00 00 00
00 00 00 00
00 00 00 00
00 00 72 49
60 81 70 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.339Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x22000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
22 00 00 00
00 00 00 00
00 00 00 00
00 00 6A C8
E4 92 69 2E
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.339Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x23000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
23 00 00 00
00 00 00 00
00 00 00 00
00 00 F8 6B
0C 20 A3 59
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.339Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x24000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
24 00 00 00
00 00 00 00
00 00 00 00
00 00 BD 1C
39 19 5B 55
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.339Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x25000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
25 00 00 00
00 00 00 00
00 00 00 00
00 00 82 AB
CC E3 3F BB
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.339Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x26000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
26 00 00 00
00 00 00 00
00 00 00 00
00 00 B5 21
40 2B 50 23
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.339Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x27000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
27 00 00 00
00 00 00 00
00 00 00 00
00 00 CB AB
18 E8 DA AF
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.339Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x28000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
28 00 00 00
00 00 00 00
00 00 00 00
00 00 39 95
DC 04 9B DC
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.339Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x29000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
29 00 00 00
00 00 00 00
00 00 00 00
00 00 A4 87
1F B7 62 3D
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.339Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x2A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
2A 00 00 00
00 00 00 00
00 00 00 00
00 00 34 79
6E 2D 97 76
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.339Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x2B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
2B 00 00 00
00 00 00 00
00 00 00 00
00 00 D5 4A
B3 9B 72 3E
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.340Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x2C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
2C 00 00 00
00 00 00 00
00 00 00 00
00 00 EA 6F
07 EC 9C 27
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.340Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x2D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
2D 00 00 00
00 00 00 00
00 00 00 00
00 00 DC A2
76 11 80 39
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.340Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x2E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
2E 00 00 00
00 00 00 00
00 00 00 00
00 00 EC D0
87 9B 98 0F
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.340Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x2F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
2F 00 00 00
00 00 00 00
00 00 00 00
00 00 EA DB
1A F3 FB E9
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.340Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x30000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
30 00 00 00
00 00 00 00
00 00 00 00
00 00 CF 33
9C 1A 79 BF
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.340Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x31000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
31 00 00 00
00 00 00 00
00 00 00 00
00 00 08 09
2E 5C 39 7F
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.340Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x32000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
32 00 00 00
00 00 00 00
00 00 00 00
00 00 12 4A
2C CF 29 3D
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.340Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x33000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
33 00 00 00
00 00 00 00
00 00 00 00
00 00 27 72
D1 42 B6 2E
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.340Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x34000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
34 00 00 00
00 00 00 00
00 00 00 00
00 00 4B B0
40 3D 83 20
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.340Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x35000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
35 00 00 00
00 00 00 00
00 00 00 00
00 00 90 C6
0B B4 94 24
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.340Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x36000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
36 00 00 00
00 00 00 00
00 00 00 00
00 00 AA 6C
2D 73 F9 D6
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.340Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x37000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
37 00 00 00
00 00 00 00
00 00 00 00
00 00 9E 0A
1A 64 7F 86
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.340Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x38000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
38 00 00 00
00 00 00 00
00 00 00 00
00 00 C6 6A
78 36 EC AF
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.340Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x39000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
39 00 00 00
00 00 00 00
00 00 00 00
00 00 CA CD
40 97 D5 4B
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.340Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x3A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
3A 00 00 00
00 00 00 00
00 00 00 00
00 00 2D CE
04 9E 0F 2D
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.341Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x3B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
3B 00 00 00
00 00 00 00
00 00 00 00
00 00 3B C0
9A 25 2F 30
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.341Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x3C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
3C 00 00 00
00 00 00 00
00 00 00 00
00 00 43 8D
57 D6 DC 56
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.341Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x3D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
3D 00 00 00
00 00 00 00
00 00 00 00
00 00 C7 13
58 E8 50 BB
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.341Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x3E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
3E 00 00 00
00 00 00 00
00 00 00 00
00 00 BA 6B
D9 82 29 1A
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.341Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x3F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
3F 00 00 00
00 00 00 00
00 00 00 00
00 00 5F 56
C8 BB E6 F9
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.341Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x40000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
40 00 00 00
00 00 00 00
00 00 00 00
00 00 CF E5
80 7A 7D AE
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.341Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x41000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
41 00 00 00
00 00 00 00
00 00 00 00
00 00 10 68
27 94 10 A0
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.341Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x42000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
42 00 00 00
00 00 00 00
00 00 00 00
00 00 93 6D
8E 95 F9 DA
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.341Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x43000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
43 00 00 00
00 00 00 00
00 00 00 00
00 00 7C 23
82 5F 4B 3F
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.341Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x44000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
44 00 00 00
00 00 00 00
00 00 00 00
00 00 76 1C
9C 0D 81 0E
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.341Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x45000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
45 00 00 00
00 00 00 00
00 00 00 00
00 00 EB 37
7A 7F 20 A8
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.341Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x46000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
46 00 00 00
00 00 00 00
00 00 00 00
00 00 4C CD
2D E2 77 D8
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.342Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x47000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
47 00 00 00
00 00 00 00
00 00 00 00
00 00 15 8D
ED 3C 91 A0
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.342Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x48000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
48 00 00 00
00 00 00 00
00 00 00 00
00 00 37 62
63 F5 8F 68
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.342Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xAA status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 AA
00 00 00 00
00 00 00 00
00 00 00 01

[rebalance:debug,2025-05-15T18:47:52.342Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 170, stream opaque = 0x1000000
[ns_server:debug,2025-05-15T18:47:52.342Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x49000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
49 00 00 00
00 00 00 00
00 00 00 00
00 00 68 82
9E 0C 33 E5
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.342Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xAB status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 AB
00 00 00 00
00 00 00 00
00 00 00 02

[ns_server:debug,2025-05-15T18:47:52.342Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x4A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
4A 00 00 00
00 00 00 00
00 00 00 00
00 00 70 28
E1 BC 9B E4
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.342Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x4B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
4B 00 00 00
00 00 00 00
00 00 00 00
00 00 87 41
51 14 9F CD
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.342Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 171, stream opaque = 0x2000000
[ns_server:debug,2025-05-15T18:47:52.342Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xAC status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 AC
00 00 00 00
00 00 00 00
00 00 00 03

[rebalance:debug,2025-05-15T18:47:52.342Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 172, stream opaque = 0x3000000
[ns_server:debug,2025-05-15T18:47:52.342Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xAD status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 AD
00 00 00 00
00 00 00 00
00 00 00 04

[rebalance:debug,2025-05-15T18:47:52.342Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 173, stream opaque = 0x4000000
[ns_server:debug,2025-05-15T18:47:52.342Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xAE status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 AE
00 00 00 00
00 00 00 00
00 00 00 07

[rebalance:debug,2025-05-15T18:47:52.342Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 174, stream opaque = 0x7000000
[ns_server:debug,2025-05-15T18:47:52.343Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xAF status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 AF
00 00 00 00
00 00 00 00
00 00 00 08

[rebalance:debug,2025-05-15T18:47:52.343Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 175, stream opaque = 0x8000000
[ns_server:debug,2025-05-15T18:47:52.343Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xB0 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 B0
00 00 00 00
00 00 00 00
00 00 00 09

[rebalance:debug,2025-05-15T18:47:52.343Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 176, stream opaque = 0x9000000
[ns_server:debug,2025-05-15T18:47:52.343Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xB1 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 B1
00 00 00 00
00 00 00 00
00 00 00 0A

[rebalance:debug,2025-05-15T18:47:52.343Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 177, stream opaque = 0xA000000
[ns_server:debug,2025-05-15T18:47:52.343Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xB2 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 B2
00 00 00 00
00 00 00 00
00 00 00 0B

[rebalance:debug,2025-05-15T18:47:52.343Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 178, stream opaque = 0xB000000
[ns_server:debug,2025-05-15T18:47:52.343Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xB3 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 B3
00 00 00 00
00 00 00 00
00 00 00 0C

[rebalance:debug,2025-05-15T18:47:52.343Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 179, stream opaque = 0xC000000
[ns_server:debug,2025-05-15T18:47:52.343Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xB4 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 B4
00 00 00 00
00 00 00 00
00 00 00 0D

[rebalance:debug,2025-05-15T18:47:52.343Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 180, stream opaque = 0xD000000
[ns_server:debug,2025-05-15T18:47:52.342Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x4C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
4C 00 00 00
00 00 00 00
00 00 00 00
00 00 92 56
59 D1 3C C7
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.343Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xB5 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 B5
00 00 00 00
00 00 00 00
00 00 00 0E

[rebalance:debug,2025-05-15T18:47:52.344Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 181, stream opaque = 0xE000000
[ns_server:debug,2025-05-15T18:47:52.344Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xB6 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 B6
00 00 00 00
00 00 00 00
00 00 00 0F

[rebalance:debug,2025-05-15T18:47:52.344Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 182, stream opaque = 0xF000000
[ns_server:debug,2025-05-15T18:47:52.344Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xB7 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 B7
00 00 00 00
00 00 00 00
00 00 00 10

[rebalance:debug,2025-05-15T18:47:52.344Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 183, stream opaque = 0x10000000
[ns_server:debug,2025-05-15T18:47:52.344Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xB8 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 B8
00 00 00 00
00 00 00 00
00 00 00 11

[rebalance:debug,2025-05-15T18:47:52.344Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 184, stream opaque = 0x11000000
[ns_server:debug,2025-05-15T18:47:52.344Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xB9 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 B9
00 00 00 00
00 00 00 00
00 00 00 12

[rebalance:debug,2025-05-15T18:47:52.344Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 185, stream opaque = 0x12000000
[ns_server:debug,2025-05-15T18:47:52.344Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xBA status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 BA
00 00 00 00
00 00 00 00
00 00 00 13

[rebalance:debug,2025-05-15T18:47:52.344Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 186, stream opaque = 0x13000000
[ns_server:debug,2025-05-15T18:47:52.344Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xBB status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 BB
00 00 00 00
00 00 00 00
00 00 00 14

[rebalance:debug,2025-05-15T18:47:52.344Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 187, stream opaque = 0x14000000
[ns_server:debug,2025-05-15T18:47:52.344Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xBC status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 BC
00 00 00 00
00 00 00 00
00 00 00 15

[rebalance:debug,2025-05-15T18:47:52.344Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 188, stream opaque = 0x15000000
[ns_server:debug,2025-05-15T18:47:52.344Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xBD status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 BD
00 00 00 00
00 00 00 00
00 00 00 16

[rebalance:debug,2025-05-15T18:47:52.344Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 189, stream opaque = 0x16000000
[ns_server:debug,2025-05-15T18:47:52.344Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xBE status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 BE
00 00 00 00
00 00 00 00
00 00 00 17

[rebalance:debug,2025-05-15T18:47:52.345Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 190, stream opaque = 0x17000000
[ns_server:debug,2025-05-15T18:47:52.345Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xBF status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 BF
00 00 00 00
00 00 00 00
00 00 00 18

[rebalance:debug,2025-05-15T18:47:52.345Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 191, stream opaque = 0x18000000
[ns_server:debug,2025-05-15T18:47:52.345Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x4D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
4D 00 00 00
00 00 00 00
00 00 00 00
00 00 8F 16
89 6C E7 B8
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.345Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xC0 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 C0
00 00 00 00
00 00 00 00
00 00 00 19

[rebalance:debug,2025-05-15T18:47:52.345Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 192, stream opaque = 0x19000000
[ns_server:debug,2025-05-15T18:47:52.345Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xC1 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 C1
00 00 00 00
00 00 00 00
00 00 00 1A

[ns_server:debug,2025-05-15T18:47:52.345Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x4E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
4E 00 00 00
00 00 00 00
00 00 00 00
00 00 A6 60
F8 BE 84 43
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.345Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 193, stream opaque = 0x1A000000
[ns_server:debug,2025-05-15T18:47:52.345Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xC2 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 C2
00 00 00 00
00 00 00 00
00 00 00 1B

[ns_server:debug,2025-05-15T18:47:52.345Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x4F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
4F 00 00 00
00 00 00 00
00 00 00 00
00 00 DB 76
6D 49 E0 55
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.345Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 194, stream opaque = 0x1B000000
[ns_server:debug,2025-05-15T18:47:52.345Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xC3 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 C3
00 00 00 00
00 00 00 00
00 00 00 1C

[ns_server:debug,2025-05-15T18:47:52.345Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x50000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
50 00 00 00
00 00 00 00
00 00 00 00
00 00 D7 2A
1F CF D7 58
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.345Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 195, stream opaque = 0x1C000000
[ns_server:debug,2025-05-15T18:47:52.345Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xC4 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 C4
00 00 00 00
00 00 00 00
00 00 00 1D

[ns_server:debug,2025-05-15T18:47:52.345Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x51000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
51 00 00 00
00 00 00 00
00 00 00 00
00 00 3F 23
C8 C4 16 88
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.345Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 196, stream opaque = 0x1D000000
[ns_server:debug,2025-05-15T18:47:52.346Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xC5 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 C5
00 00 00 00
00 00 00 00
00 00 00 1E

[ns_server:debug,2025-05-15T18:47:52.346Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x52000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
52 00 00 00
00 00 00 00
00 00 00 00
00 00 F2 0D
A4 A3 1A 1E
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.346Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 197, stream opaque = 0x1E000000
[ns_server:debug,2025-05-15T18:47:52.346Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xC6 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 C6
00 00 00 00
00 00 00 00
00 00 00 1F

[rebalance:debug,2025-05-15T18:47:52.346Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 198, stream opaque = 0x1F000000
[ns_server:debug,2025-05-15T18:47:52.346Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xC7 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 C7
00 00 00 00
00 00 00 00
00 00 00 20

[rebalance:debug,2025-05-15T18:47:52.346Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 199, stream opaque = 0x20000000
[ns_server:debug,2025-05-15T18:47:52.346Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x53000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
53 00 00 00
00 00 00 00
00 00 00 00
00 00 54 72
A1 5C 24 15
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.346Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x54000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
54 00 00 00
00 00 00 00
00 00 00 00
00 00 0B 31
5F C4 2B BA
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.346Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xC8 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 C8
00 00 00 00
00 00 00 00
00 00 00 21

[rebalance:debug,2025-05-15T18:47:52.346Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 200, stream opaque = 0x21000000
[ns_server:debug,2025-05-15T18:47:52.346Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xC9 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 C9
00 00 00 00
00 00 00 00
00 00 00 22

[ns_server:debug,2025-05-15T18:47:52.346Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x55000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
55 00 00 00
00 00 00 00
00 00 00 00
00 00 D9 C1
03 E7 35 C3
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.346Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 201, stream opaque = 0x22000000
[ns_server:debug,2025-05-15T18:47:52.346Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xCA status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 CA
00 00 00 00
00 00 00 00
00 00 00 23

[rebalance:debug,2025-05-15T18:47:52.347Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 202, stream opaque = 0x23000000
[ns_server:debug,2025-05-15T18:47:52.347Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x56000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
56 00 00 00
00 00 00 00
00 00 00 00
00 00 BF 45
33 50 7A 51
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.347Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xCB status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 CB
00 00 00 00
00 00 00 00
00 00 00 24

[ns_server:debug,2025-05-15T18:47:52.347Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x57000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
57 00 00 00
00 00 00 00
00 00 00 00
00 00 B6 93
8F 75 5F 7B
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.347Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 203, stream opaque = 0x24000000
[ns_server:debug,2025-05-15T18:47:52.347Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x58000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
58 00 00 00
00 00 00 00
00 00 00 00
00 00 83 3A
F0 C4 07 A3
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.347Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xCC status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 CC
00 00 00 00
00 00 00 00
00 00 00 25

[rebalance:debug,2025-05-15T18:47:52.347Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 204, stream opaque = 0x25000000
[ns_server:debug,2025-05-15T18:47:52.347Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xCD status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 CD
00 00 00 00
00 00 00 00
00 00 00 26

[rebalance:debug,2025-05-15T18:47:52.347Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 205, stream opaque = 0x26000000
[ns_server:debug,2025-05-15T18:47:52.347Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x59000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
59 00 00 00
00 00 00 00
00 00 00 00
00 00 D5 4F
92 DF 55 4E
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.347Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xCE status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 CE
00 00 00 00
00 00 00 00
00 00 00 27

[rebalance:debug,2025-05-15T18:47:52.347Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 206, stream opaque = 0x27000000
[ns_server:debug,2025-05-15T18:47:52.347Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xCF status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 CF
00 00 00 00
00 00 00 00
00 00 00 28

[ns_server:debug,2025-05-15T18:47:52.347Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x5A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
5A 00 00 00
00 00 00 00
00 00 00 00
00 00 DE F8
F3 90 F3 B6
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.347Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 207, stream opaque = 0x28000000
[ns_server:debug,2025-05-15T18:47:52.347Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x5B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
5B 00 00 00
00 00 00 00
00 00 00 00
00 00 8B EB
4F 88 99 B7
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.347Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xD0 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 D0
00 00 00 00
00 00 00 00
00 00 00 29

[rebalance:debug,2025-05-15T18:47:52.347Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 208, stream opaque = 0x29000000
[ns_server:debug,2025-05-15T18:47:52.347Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x5C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
5C 00 00 00
00 00 00 00
00 00 00 00
00 00 ED 46
BF FB 12 48
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.347Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xD1 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 D1
00 00 00 00
00 00 00 00
00 00 00 2A

[rebalance:debug,2025-05-15T18:47:52.348Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 209, stream opaque = 0x2A000000
[ns_server:debug,2025-05-15T18:47:52.348Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x5D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
5D 00 00 00
00 00 00 00
00 00 00 00
00 00 2D B0
B9 44 A3 A3
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.348Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x5E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
5E 00 00 00
00 00 00 00
00 00 00 00
00 00 17 98
28 8F BE 95
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.348Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x5F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
5F 00 00 00
00 00 00 00
00 00 00 00
00 00 28 82
5C 04 EB 2C
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.348Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x60000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
60 00 00 00
00 00 00 00
00 00 00 00
00 00 33 49
85 EC A8 80
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.348Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x61000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
61 00 00 00
00 00 00 00
00 00 00 00
00 00 85 14
06 47 D0 8F
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.348Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x62000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
62 00 00 00
00 00 00 00
00 00 00 00
00 00 27 40
A2 61 81 33
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.348Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x63000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
63 00 00 00
00 00 00 00
00 00 00 00
00 00 F2 BA
5D CA EF 06
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.348Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x64000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
64 00 00 00
00 00 00 00
00 00 00 00
00 00 7E C0
48 AD D3 5F
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.348Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x65000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
65 00 00 00
00 00 00 00
00 00 00 00
00 00 B0 FD
4E 6D 04 E5
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.348Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x66000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
66 00 00 00
00 00 00 00
00 00 00 00
00 00 D4 A3
10 17 56 B9
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.348Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x67000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
67 00 00 00
00 00 00 00
00 00 00 00
00 00 0A 79
3B 2C 1B A3
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.348Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xD2 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 D2
00 00 00 00
00 00 00 00
00 00 00 2B

[ns_server:debug,2025-05-15T18:47:52.348Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x68000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
68 00 00 00
00 00 00 00
00 00 00 00
00 00 F5 AF
33 0B 99 FC
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.348Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 210, stream opaque = 0x2B000000
[ns_server:debug,2025-05-15T18:47:52.349Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xD3 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 D3
00 00 00 00
00 00 00 00
00 00 00 2C

[rebalance:debug,2025-05-15T18:47:52.349Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 211, stream opaque = 0x2C000000
[ns_server:debug,2025-05-15T18:47:52.349Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x69000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
69 00 00 00
00 00 00 00
00 00 00 00
00 00 33 DA
0A 5D 22 1F
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.349Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xD4 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 D4
00 00 00 00
00 00 00 00
00 00 00 2D

[ns_server:debug,2025-05-15T18:47:52.349Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x6A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
6A 00 00 00
00 00 00 00
00 00 00 00
00 00 D2 4A
91 30 82 B7
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.349Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 212, stream opaque = 0x2D000000
[ns_server:debug,2025-05-15T18:47:52.349Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xD5 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 D5
00 00 00 00
00 00 00 00
00 00 00 2E

[ns_server:debug,2025-05-15T18:47:52.349Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x6B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
6B 00 00 00
00 00 00 00
00 00 00 00
00 00 87 28
B2 13 00 86
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.349Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 213, stream opaque = 0x2E000000
[ns_server:debug,2025-05-15T18:47:52.349Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x6C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
6C 00 00 00
00 00 00 00
00 00 00 00
00 00 41 23
C1 73 DF 93
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.349Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xD6 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 D6
00 00 00 00
00 00 00 00
00 00 00 2F

[rebalance:debug,2025-05-15T18:47:52.349Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 214, stream opaque = 0x2F000000
[ns_server:debug,2025-05-15T18:47:52.349Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xD7 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 D7
00 00 00 00
00 00 00 00
00 00 00 30

[ns_server:debug,2025-05-15T18:47:52.349Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x6D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
6D 00 00 00
00 00 00 00
00 00 00 00
00 00 0B 95
86 4B 8A 71
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.349Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 215, stream opaque = 0x30000000
[ns_server:debug,2025-05-15T18:47:52.349Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xD8 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 D8
00 00 00 00
00 00 00 00
00 00 00 31

[rebalance:debug,2025-05-15T18:47:52.349Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 216, stream opaque = 0x31000000
[ns_server:debug,2025-05-15T18:47:52.349Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x6E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
6E 00 00 00
00 00 00 00
00 00 00 00
00 00 F8 AA
96 92 19 02
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.349Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xD9 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 D9
00 00 00 00
00 00 00 00
00 00 00 32

[ns_server:debug,2025-05-15T18:47:52.349Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x6F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
6F 00 00 00
00 00 00 00
00 00 00 00
00 00 F5 12
9E 33 17 B2
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.349Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 217, stream opaque = 0x32000000
[ns_server:debug,2025-05-15T18:47:52.350Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xDA status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 DA
00 00 00 00
00 00 00 00
00 00 00 33

[ns_server:debug,2025-05-15T18:47:52.350Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x70000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
70 00 00 00
00 00 00 00
00 00 00 00
00 00 F5 77
D5 F8 CA 9B
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.350Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x71000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
71 00 00 00
00 00 00 00
00 00 00 00
00 00 02 E4
35 2D 3D 42
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.350Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 218, stream opaque = 0x33000000
[ns_server:debug,2025-05-15T18:47:52.350Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xDB status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 DB
00 00 00 00
00 00 00 00
00 00 00 34

[rebalance:debug,2025-05-15T18:47:52.350Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 219, stream opaque = 0x34000000
[ns_server:debug,2025-05-15T18:47:52.350Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x72000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
72 00 00 00
00 00 00 00
00 00 00 00
00 00 09 ED
48 79 BC D3
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.350Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xDC status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 DC
00 00 00 00
00 00 00 00
00 00 00 35

[rebalance:debug,2025-05-15T18:47:52.350Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 220, stream opaque = 0x35000000
[ns_server:debug,2025-05-15T18:47:52.350Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xDD status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 DD
00 00 00 00
00 00 00 00
00 00 00 36

[ns_server:debug,2025-05-15T18:47:52.350Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x73000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
73 00 00 00
00 00 00 00
00 00 00 00
00 00 46 6A
68 5D 54 4F
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.350Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 221, stream opaque = 0x36000000
[ns_server:debug,2025-05-15T18:47:52.350Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xDE status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 DE
00 00 00 00
00 00 00 00
00 00 00 37

[rebalance:debug,2025-05-15T18:47:52.350Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 222, stream opaque = 0x37000000
[ns_server:debug,2025-05-15T18:47:52.350Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x74000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
74 00 00 00
00 00 00 00
00 00 00 00
00 00 34 2A
32 AC 5D 1B
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.350Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xDF status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 DF
00 00 00 00
00 00 00 00
00 00 00 38

[ns_server:debug,2025-05-15T18:47:52.351Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x75000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
75 00 00 00
00 00 00 00
00 00 00 00
00 00 B9 AE
2B F4 0C 93
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.351Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 223, stream opaque = 0x38000000
[ns_server:debug,2025-05-15T18:47:52.351Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xE0 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 E0
00 00 00 00
00 00 00 00
00 00 00 39

[rebalance:debug,2025-05-15T18:47:52.351Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 224, stream opaque = 0x39000000
[ns_server:debug,2025-05-15T18:47:52.351Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x76000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
76 00 00 00
00 00 00 00
00 00 00 00
00 00 8D 18
DF 1B E3 5F
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.351Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xE1 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 E1
00 00 00 00
00 00 00 00
00 00 00 3A

[ns_server:debug,2025-05-15T18:47:52.351Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x77000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
77 00 00 00
00 00 00 00
00 00 00 00
00 00 D1 0F
7B 95 10 8B
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.351Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 225, stream opaque = 0x3A000000
[ns_server:debug,2025-05-15T18:47:52.352Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x78000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
78 00 00 00
00 00 00 00
00 00 00 00
00 00 B4 D5
96 A5 D1 46
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.352Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x79000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
79 00 00 00
00 00 00 00
00 00 00 00
00 00 AE 6C
87 4C CD 7A
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.352Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x7A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
7A 00 00 00
00 00 00 00
00 00 00 00
00 00 F6 CD
74 98 C3 E3
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.352Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x7B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
7B 00 00 00
00 00 00 00
00 00 00 00
00 00 63 45
18 6A B2 3B
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.352Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x7C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
7C 00 00 00
00 00 00 00
00 00 00 00
00 00 8E 79
00 61 A4 E4
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.352Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x7D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
7D 00 00 00
00 00 00 00
00 00 00 00
00 00 7C 3C
6C 5A 5A A4
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.352Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x7E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
7E 00 00 00
00 00 00 00
00 00 00 00
00 00 74 3E
42 4E 3C C1
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.352Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x7F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
7F 00 00 00
00 00 00 00
00 00 00 00
00 00 BF 3F
F9 5E 77 D3
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.352Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x80000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
80 00 00 00
00 00 00 00
00 00 00 00
00 00 A4 0D
47 E0 58 93
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.352Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x81000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
81 00 00 00
00 00 00 00
00 00 00 00
00 00 50 17
6C AF 08 C3
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.353Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x82000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
82 00 00 00
00 00 00 00
00 00 00 00
00 00 AC 1D
FF 42 B5 A8
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.353Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x83000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
83 00 00 00
00 00 00 00
00 00 00 00
00 00 73 09
58 90 D4 1E
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.353Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x84000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
84 00 00 00
00 00 00 00
00 00 00 00
00 00 97 19
21 D8 D6 0F
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.353Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x85000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
85 00 00 00
00 00 00 00
00 00 00 00
00 00 DD 4A
8B F2 02 A3
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.353Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x86000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
86 00 00 00
00 00 00 00
00 00 00 00
00 00 31 AD
54 E7 AD 3B
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.353Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x87000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
87 00 00 00
00 00 00 00
00 00 00 00
00 00 7E 43
FC 69 37 92
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.353Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x88000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
88 00 00 00
00 00 00 00
00 00 00 00
00 00 C0 21
95 D7 B9 B8
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.353Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x89000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
89 00 00 00
00 00 00 00
00 00 00 00
00 00 2B 26
A0 34 E6 D0
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.353Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x8A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
8A 00 00 00
00 00 00 00
00 00 00 00
00 00 97 8B
66 83 13 59
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.353Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x8B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
8B 00 00 00
00 00 00 00
00 00 00 00
00 00 A1 2A
D0 9C D1 9B
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.353Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x8C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
8C 00 00 00
00 00 00 00
00 00 00 00
00 00 58 18
BC 1E 53 1B
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.353Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x8D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
8D 00 00 00
00 00 00 00
00 00 00 00
00 00 05 84
74 A0 8F 01
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.354Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x8E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
8E 00 00 00
00 00 00 00
00 00 00 00
00 00 9D 6B
93 2F FA 93
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.354Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x8F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
8F 00 00 00
00 00 00 00
00 00 00 00
00 00 B5 01
CD 6C 8F 98
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.354Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x90000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
90 00 00 00
00 00 00 00
00 00 00 00
00 00 7D C1
9A 03 18 62
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.354Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x91000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
91 00 00 00
00 00 00 00
00 00 00 00
00 00 88 BD
32 94 83 AA
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.354Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x92000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
92 00 00 00
00 00 00 00
00 00 00 00
00 00 47 C4
78 7A 37 58
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.354Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x93000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
93 00 00 00
00 00 00 00
00 00 00 00
00 00 83 23
7B 4F AC 5D
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.354Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x94000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
94 00 00 00
00 00 00 00
00 00 00 00
00 00 7A 0E
86 AC 6F 56
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.354Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x95000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
95 00 00 00
00 00 00 00
00 00 00 00
00 00 B4 E1
8C 9B EE F0
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.355Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x96000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
96 00 00 00
00 00 00 00
00 00 00 00
00 00 E1 58
E1 4A F9 05
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.355Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x97000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
97 00 00 00
00 00 00 00
00 00 00 00
00 00 89 C9
D0 46 BD BD
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.355Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x98000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
98 00 00 00
00 00 00 00
00 00 00 00
00 00 13 68
DF 2D DA 9F
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.355Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x99000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
99 00 00 00
00 00 00 00
00 00 00 00
00 00 99 5A
04 FA 02 93
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.355Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x9A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
9A 00 00 00
00 00 00 00
00 00 00 00
00 00 14 66
7D E6 12 C5
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.355Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x9B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
9B 00 00 00
00 00 00 00
00 00 00 00
00 00 B5 7F
DB 88 39 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.355Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x9C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
9C 00 00 00
00 00 00 00
00 00 00 00
00 00 21 97
4C 42 17 C4
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.355Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x9D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
9D 00 00 00
00 00 00 00
00 00 00 00
00 00 78 F7
77 28 F0 F6
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.356Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x9E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
9E 00 00 00
00 00 00 00
00 00 00 00
00 00 78 8E
03 F0 E4 5F
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.356Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x9F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
9F 00 00 00
00 00 00 00
00 00 00 00
00 00 80 E3
61 6C 41 16
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.356Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA0000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A0 00 00 00
00 00 00 00
00 00 00 00
00 00 07 DE
38 E7 48 66
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.356Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA1000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A1 00 00 00
00 00 00 00
00 00 00 00
00 00 97 35
6F 7E C1 BA
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.356Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA2000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A2 00 00 00
00 00 00 00
00 00 00 00
00 00 53 B8
43 F9 87 03
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.356Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA3000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A3 00 00 00
00 00 00 00
00 00 00 00
00 00 C8 1E
53 95 6B CE
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.356Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA4000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A4 00 00 00
00 00 00 00
00 00 00 00
00 00 F7 79
90 B2 2E CE
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.356Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA5000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A5 00 00 00
00 00 00 00
00 00 00 00
00 00 03 98
80 8D 97 89
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.356Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA6000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A6 00 00 00
00 00 00 00
00 00 00 00
00 00 DA 98
6A C8 E7 5C
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.356Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA7000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A7 00 00 00
00 00 00 00
00 00 00 00
00 00 18 8A
E3 75 87 B7
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.356Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA8000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A8 00 00 00
00 00 00 00
00 00 00 00
00 00 B2 5C
DD 38 0C B2
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.356Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA9000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A9 00 00 00
00 00 00 00
00 00 00 00
00 00 BC 32
80 72 CD A0
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.356Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xAA000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
AA 00 00 00
00 00 00 00
00 00 00 00
00 00 81 38
6D 8A 3F 4F
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.356Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xAB000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
AB 00 00 00
00 00 00 00
00 00 00 00
00 00 55 2D
2E 1B 10 5E
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.356Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xAC000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
AC 00 00 00
00 00 00 00
00 00 00 00
00 00 4F 08
16 50 5C AA
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.356Z,ns_1@db3.lan:<0.4232.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xAD000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
AD 00 00 00
00 00 00 00
00 00 00 00
00 00 35 D4
2E CC 53 EB
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.358Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xE2 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 E2
00 00 00 00
00 00 00 00
00 00 00 3B

[rebalance:debug,2025-05-15T18:47:52.358Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 226, stream opaque = 0x3B000000
[ns_server:debug,2025-05-15T18:47:52.359Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xE3 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 E3
00 00 00 00
00 00 00 00
00 00 00 3C

[rebalance:debug,2025-05-15T18:47:52.359Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 227, stream opaque = 0x3C000000
[ns_server:debug,2025-05-15T18:47:52.359Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xE4 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 E4
00 00 00 00
00 00 00 00
00 00 00 3D

[rebalance:debug,2025-05-15T18:47:52.359Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 228, stream opaque = 0x3D000000
[ns_server:debug,2025-05-15T18:47:52.359Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xE5 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 E5
00 00 00 00
00 00 00 00
00 00 00 3E

[rebalance:debug,2025-05-15T18:47:52.359Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 229, stream opaque = 0x3E000000
[ns_server:debug,2025-05-15T18:47:52.359Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xE6 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 E6
00 00 00 00
00 00 00 00
00 00 00 3F

[rebalance:debug,2025-05-15T18:47:52.359Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 230, stream opaque = 0x3F000000
[ns_server:debug,2025-05-15T18:47:52.359Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xE7 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 E7
00 00 00 00
00 00 00 00
00 00 00 40

[rebalance:debug,2025-05-15T18:47:52.359Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 231, stream opaque = 0x40000000
[ns_server:debug,2025-05-15T18:47:52.359Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xE8 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 E8
00 00 00 00
00 00 00 00
00 00 00 41

[rebalance:debug,2025-05-15T18:47:52.359Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 232, stream opaque = 0x41000000
[ns_server:debug,2025-05-15T18:47:52.359Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xE9 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 E9
00 00 00 00
00 00 00 00
00 00 00 42

[rebalance:debug,2025-05-15T18:47:52.360Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 233, stream opaque = 0x42000000
[ns_server:debug,2025-05-15T18:47:52.360Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xEA status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 EA
00 00 00 00
00 00 00 00
00 00 00 43

[rebalance:debug,2025-05-15T18:47:52.360Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 234, stream opaque = 0x43000000
[ns_server:debug,2025-05-15T18:47:52.360Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xEB status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 EB
00 00 00 00
00 00 00 00
00 00 00 44

[rebalance:debug,2025-05-15T18:47:52.360Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 235, stream opaque = 0x44000000
[ns_server:debug,2025-05-15T18:47:52.360Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xEC status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 EC
00 00 00 00
00 00 00 00
00 00 00 45

[rebalance:debug,2025-05-15T18:47:52.360Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 236, stream opaque = 0x45000000
[ns_server:debug,2025-05-15T18:47:52.360Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xED status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 ED
00 00 00 00
00 00 00 00
00 00 00 46

[rebalance:debug,2025-05-15T18:47:52.360Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 237, stream opaque = 0x46000000
[ns_server:debug,2025-05-15T18:47:52.360Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xEE status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 EE
00 00 00 00
00 00 00 00
00 00 00 47

[rebalance:debug,2025-05-15T18:47:52.360Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 238, stream opaque = 0x47000000
[ns_server:debug,2025-05-15T18:47:52.360Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xEF status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 EF
00 00 00 00
00 00 00 00
00 00 00 48

[rebalance:debug,2025-05-15T18:47:52.360Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 239, stream opaque = 0x48000000
[ns_server:debug,2025-05-15T18:47:52.360Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xF0 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 F0
00 00 00 00
00 00 00 00
00 00 00 49

[rebalance:debug,2025-05-15T18:47:52.360Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 240, stream opaque = 0x49000000
[ns_server:debug,2025-05-15T18:47:52.360Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xF1 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 F1
00 00 00 00
00 00 00 00
00 00 00 4A

[rebalance:debug,2025-05-15T18:47:52.361Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 241, stream opaque = 0x4A000000
[ns_server:debug,2025-05-15T18:47:52.361Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xF2 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 F2
00 00 00 00
00 00 00 00
00 00 00 4B

[rebalance:debug,2025-05-15T18:47:52.361Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 242, stream opaque = 0x4B000000
[ns_server:debug,2025-05-15T18:47:52.361Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xF3 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 F3
00 00 00 00
00 00 00 00
00 00 00 4C

[rebalance:debug,2025-05-15T18:47:52.361Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 243, stream opaque = 0x4C000000
[ns_server:debug,2025-05-15T18:47:52.361Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xF4 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 F4
00 00 00 00
00 00 00 00
00 00 00 4D

[rebalance:debug,2025-05-15T18:47:52.361Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 244, stream opaque = 0x4D000000
[ns_server:debug,2025-05-15T18:47:52.361Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xF5 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 F5
00 00 00 00
00 00 00 00
00 00 00 4E

[rebalance:debug,2025-05-15T18:47:52.361Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 245, stream opaque = 0x4E000000
[ns_server:debug,2025-05-15T18:47:52.361Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xF6 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 F6
00 00 00 00
00 00 00 00
00 00 00 4F

[rebalance:debug,2025-05-15T18:47:52.362Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 246, stream opaque = 0x4F000000
[ns_server:debug,2025-05-15T18:47:52.362Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xF7 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 F7
00 00 00 00
00 00 00 00
00 00 00 50

[rebalance:debug,2025-05-15T18:47:52.362Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 247, stream opaque = 0x50000000
[ns_server:debug,2025-05-15T18:47:52.362Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xF8 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 F8
00 00 00 00
00 00 00 00
00 00 00 51

[rebalance:debug,2025-05-15T18:47:52.362Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 248, stream opaque = 0x51000000
[ns_server:debug,2025-05-15T18:47:52.362Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xF9 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 F9
00 00 00 00
00 00 00 00
00 00 00 52

[rebalance:debug,2025-05-15T18:47:52.362Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 249, stream opaque = 0x52000000
[ns_server:debug,2025-05-15T18:47:52.362Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xFA status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 FA
00 00 00 00
00 00 00 00
00 00 00 53

[rebalance:debug,2025-05-15T18:47:52.362Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 250, stream opaque = 0x53000000
[ns_server:debug,2025-05-15T18:47:52.362Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xFB status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 FB
00 00 00 00
00 00 00 00
00 00 00 54

[rebalance:debug,2025-05-15T18:47:52.362Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 251, stream opaque = 0x54000000
[ns_server:debug,2025-05-15T18:47:52.362Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xFC status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 FC
00 00 00 00
00 00 00 00
00 00 00 55

[rebalance:debug,2025-05-15T18:47:52.362Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 252, stream opaque = 0x55000000
[ns_server:debug,2025-05-15T18:47:52.362Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xFD status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 FD
00 00 00 00
00 00 00 00
00 00 00 56

[rebalance:debug,2025-05-15T18:47:52.373Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 253, stream opaque = 0x56000000
[ns_server:debug,2025-05-15T18:47:52.373Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xFE status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 FE
00 00 00 00
00 00 00 00
00 00 00 57

[rebalance:debug,2025-05-15T18:47:52.373Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 254, stream opaque = 0x57000000
[ns_server:debug,2025-05-15T18:47:52.373Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xFF status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 FF
00 00 00 00
00 00 00 00
00 00 00 58

[rebalance:debug,2025-05-15T18:47:52.373Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 255, stream opaque = 0x58000000
[ns_server:debug,2025-05-15T18:47:52.374Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x100 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 00
00 00 00 00
00 00 00 00
00 00 00 59

[rebalance:debug,2025-05-15T18:47:52.374Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 256, stream opaque = 0x59000000
[ns_server:debug,2025-05-15T18:47:52.374Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x101 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 01
00 00 00 00
00 00 00 00
00 00 00 5A

[rebalance:debug,2025-05-15T18:47:52.374Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 257, stream opaque = 0x5A000000
[ns_server:debug,2025-05-15T18:47:52.374Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x102 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 02
00 00 00 00
00 00 00 00
00 00 00 5B

[rebalance:debug,2025-05-15T18:47:52.374Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 258, stream opaque = 0x5B000000
[ns_server:debug,2025-05-15T18:47:52.374Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x103 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 03
00 00 00 00
00 00 00 00
00 00 00 5C

[rebalance:debug,2025-05-15T18:47:52.374Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 259, stream opaque = 0x5C000000
[ns_server:debug,2025-05-15T18:47:52.374Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x104 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 04
00 00 00 00
00 00 00 00
00 00 00 5D

[rebalance:debug,2025-05-15T18:47:52.374Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 260, stream opaque = 0x5D000000
[ns_server:debug,2025-05-15T18:47:52.374Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x105 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 05
00 00 00 00
00 00 00 00
00 00 00 5E

[rebalance:debug,2025-05-15T18:47:52.374Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 261, stream opaque = 0x5E000000
[ns_server:debug,2025-05-15T18:47:52.374Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x106 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 06
00 00 00 00
00 00 00 00
00 00 00 5F

[rebalance:debug,2025-05-15T18:47:52.374Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 262, stream opaque = 0x5F000000
[ns_server:debug,2025-05-15T18:47:52.374Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x107 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 07
00 00 00 00
00 00 00 00
00 00 00 60

[rebalance:debug,2025-05-15T18:47:52.374Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 263, stream opaque = 0x60000000
[ns_server:debug,2025-05-15T18:47:52.374Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x108 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 08
00 00 00 00
00 00 00 00
00 00 00 61

[rebalance:debug,2025-05-15T18:47:52.374Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 264, stream opaque = 0x61000000
[ns_server:debug,2025-05-15T18:47:52.374Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x109 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 09
00 00 00 00
00 00 00 00
00 00 00 62

[rebalance:debug,2025-05-15T18:47:52.374Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 265, stream opaque = 0x62000000
[ns_server:debug,2025-05-15T18:47:52.374Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x10A status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 0A
00 00 00 00
00 00 00 00
00 00 00 63

[rebalance:debug,2025-05-15T18:47:52.374Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 266, stream opaque = 0x63000000
[ns_server:debug,2025-05-15T18:47:52.374Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x10B status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 0B
00 00 00 00
00 00 00 00
00 00 00 64

[rebalance:debug,2025-05-15T18:47:52.375Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 267, stream opaque = 0x64000000
[ns_server:debug,2025-05-15T18:47:52.375Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x10C status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 0C
00 00 00 00
00 00 00 00
00 00 00 65

[rebalance:debug,2025-05-15T18:47:52.375Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 268, stream opaque = 0x65000000
[ns_server:debug,2025-05-15T18:47:52.375Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x10D status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 0D
00 00 00 00
00 00 00 00
00 00 00 66

[rebalance:debug,2025-05-15T18:47:52.375Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 269, stream opaque = 0x66000000
[ns_server:debug,2025-05-15T18:47:52.375Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x10E status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 0E
00 00 00 00
00 00 00 00
00 00 00 67

[rebalance:debug,2025-05-15T18:47:52.375Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 270, stream opaque = 0x67000000
[ns_server:debug,2025-05-15T18:47:52.375Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x10F status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 0F
00 00 00 00
00 00 00 00
00 00 00 68

[rebalance:debug,2025-05-15T18:47:52.375Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 271, stream opaque = 0x68000000
[ns_server:debug,2025-05-15T18:47:52.375Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x110 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 10
00 00 00 00
00 00 00 00
00 00 00 69

[rebalance:debug,2025-05-15T18:47:52.375Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 272, stream opaque = 0x69000000
[ns_server:debug,2025-05-15T18:47:52.375Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x111 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 11
00 00 00 00
00 00 00 00
00 00 00 6A

[rebalance:debug,2025-05-15T18:47:52.375Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 273, stream opaque = 0x6A000000
[ns_server:debug,2025-05-15T18:47:52.375Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x112 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 12
00 00 00 00
00 00 00 00
00 00 00 6B

[rebalance:debug,2025-05-15T18:47:52.375Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 274, stream opaque = 0x6B000000
[ns_server:debug,2025-05-15T18:47:52.375Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x113 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 13
00 00 00 00
00 00 00 00
00 00 00 6C

[rebalance:debug,2025-05-15T18:47:52.375Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 275, stream opaque = 0x6C000000
[ns_server:debug,2025-05-15T18:47:52.375Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x114 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 14
00 00 00 00
00 00 00 00
00 00 00 6D

[rebalance:debug,2025-05-15T18:47:52.375Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 276, stream opaque = 0x6D000000
[ns_server:debug,2025-05-15T18:47:52.375Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x115 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 15
00 00 00 00
00 00 00 00
00 00 00 6E

[rebalance:debug,2025-05-15T18:47:52.375Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 277, stream opaque = 0x6E000000
[ns_server:debug,2025-05-15T18:47:52.375Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x116 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 16
00 00 00 00
00 00 00 00
00 00 00 6F

[rebalance:debug,2025-05-15T18:47:52.375Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 278, stream opaque = 0x6F000000
[ns_server:debug,2025-05-15T18:47:52.375Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x117 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 17
00 00 00 00
00 00 00 00
00 00 00 70

[rebalance:debug,2025-05-15T18:47:52.375Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 279, stream opaque = 0x70000000
[ns_server:debug,2025-05-15T18:47:52.376Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x118 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 18
00 00 00 00
00 00 00 00
00 00 00 71

[rebalance:debug,2025-05-15T18:47:52.376Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 280, stream opaque = 0x71000000
[ns_server:debug,2025-05-15T18:47:52.376Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x119 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 19
00 00 00 00
00 00 00 00
00 00 00 72

[rebalance:debug,2025-05-15T18:47:52.376Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 281, stream opaque = 0x72000000
[ns_server:debug,2025-05-15T18:47:52.376Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x11A status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 1A
00 00 00 00
00 00 00 00
00 00 00 73

[rebalance:debug,2025-05-15T18:47:52.376Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 282, stream opaque = 0x73000000
[ns_server:debug,2025-05-15T18:47:52.376Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x11B status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 1B
00 00 00 00
00 00 00 00
00 00 00 74

[rebalance:debug,2025-05-15T18:47:52.376Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 283, stream opaque = 0x74000000
[ns_server:debug,2025-05-15T18:47:52.376Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x11C status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 1C
00 00 00 00
00 00 00 00
00 00 00 75

[rebalance:debug,2025-05-15T18:47:52.376Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 284, stream opaque = 0x75000000
[ns_server:debug,2025-05-15T18:47:52.376Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x11D status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 1D
00 00 00 00
00 00 00 00
00 00 00 76

[rebalance:debug,2025-05-15T18:47:52.376Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 285, stream opaque = 0x76000000
[ns_server:debug,2025-05-15T18:47:52.376Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x11E status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 1E
00 00 00 00
00 00 00 00
00 00 00 77

[rebalance:debug,2025-05-15T18:47:52.376Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 286, stream opaque = 0x77000000
[ns_server:debug,2025-05-15T18:47:52.376Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x11F status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 1F
00 00 00 00
00 00 00 00
00 00 00 78

[rebalance:debug,2025-05-15T18:47:52.376Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 287, stream opaque = 0x78000000
[ns_server:debug,2025-05-15T18:47:52.376Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x120 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 20
00 00 00 00
00 00 00 00
00 00 00 79

[rebalance:debug,2025-05-15T18:47:52.376Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 288, stream opaque = 0x79000000
[ns_server:debug,2025-05-15T18:47:52.376Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x121 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 21
00 00 00 00
00 00 00 00
00 00 00 7A

[rebalance:debug,2025-05-15T18:47:52.377Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 289, stream opaque = 0x7A000000
[ns_server:debug,2025-05-15T18:47:52.377Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x122 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 22
00 00 00 00
00 00 00 00
00 00 00 7B

[rebalance:debug,2025-05-15T18:47:52.377Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 290, stream opaque = 0x7B000000
[ns_server:debug,2025-05-15T18:47:52.377Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x123 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 23
00 00 00 00
00 00 00 00
00 00 00 7C

[rebalance:debug,2025-05-15T18:47:52.377Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 291, stream opaque = 0x7C000000
[ns_server:debug,2025-05-15T18:47:52.377Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x124 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 24
00 00 00 00
00 00 00 00
00 00 00 7D

[rebalance:debug,2025-05-15T18:47:52.377Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 292, stream opaque = 0x7D000000
[ns_server:debug,2025-05-15T18:47:52.377Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x125 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 25
00 00 00 00
00 00 00 00
00 00 00 7E

[rebalance:debug,2025-05-15T18:47:52.377Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 293, stream opaque = 0x7E000000
[ns_server:debug,2025-05-15T18:47:52.377Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x126 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 26
00 00 00 00
00 00 00 00
00 00 00 7F

[rebalance:debug,2025-05-15T18:47:52.377Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 294, stream opaque = 0x7F000000
[ns_server:debug,2025-05-15T18:47:52.377Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x127 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 27
00 00 00 00
00 00 00 00
00 00 00 80

[rebalance:debug,2025-05-15T18:47:52.377Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 295, stream opaque = 0x80000000
[ns_server:debug,2025-05-15T18:47:52.378Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x128 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 28
00 00 00 00
00 00 00 00
00 00 00 81

[rebalance:debug,2025-05-15T18:47:52.378Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 296, stream opaque = 0x81000000
[ns_server:debug,2025-05-15T18:47:52.378Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x129 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 29
00 00 00 00
00 00 00 00
00 00 00 82

[rebalance:debug,2025-05-15T18:47:52.378Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 297, stream opaque = 0x82000000
[ns_server:debug,2025-05-15T18:47:52.378Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x12A status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 2A
00 00 00 00
00 00 00 00
00 00 00 83

[rebalance:debug,2025-05-15T18:47:52.378Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 298, stream opaque = 0x83000000
[ns_server:debug,2025-05-15T18:47:52.378Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x12B status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 2B
00 00 00 00
00 00 00 00
00 00 00 84

[rebalance:debug,2025-05-15T18:47:52.378Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 299, stream opaque = 0x84000000
[ns_server:debug,2025-05-15T18:47:52.378Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x12C status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 2C
00 00 00 00
00 00 00 00
00 00 00 85

[rebalance:debug,2025-05-15T18:47:52.378Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 300, stream opaque = 0x85000000
[ns_server:debug,2025-05-15T18:47:52.378Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x12D status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 2D
00 00 00 00
00 00 00 00
00 00 00 86

[rebalance:debug,2025-05-15T18:47:52.378Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 301, stream opaque = 0x86000000
[ns_server:debug,2025-05-15T18:47:52.378Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x12E status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 2E
00 00 00 00
00 00 00 00
00 00 00 87

[rebalance:debug,2025-05-15T18:47:52.378Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 302, stream opaque = 0x87000000
[ns_server:debug,2025-05-15T18:47:52.379Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x12F status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 2F
00 00 00 00
00 00 00 00
00 00 00 88

[rebalance:debug,2025-05-15T18:47:52.379Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 303, stream opaque = 0x88000000
[ns_server:debug,2025-05-15T18:47:52.379Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x130 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 30
00 00 00 00
00 00 00 00
00 00 00 89

[rebalance:debug,2025-05-15T18:47:52.379Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 304, stream opaque = 0x89000000
[ns_server:debug,2025-05-15T18:47:52.379Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x131 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 31
00 00 00 00
00 00 00 00
00 00 00 8A

[rebalance:debug,2025-05-15T18:47:52.379Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 305, stream opaque = 0x8A000000
[ns_server:debug,2025-05-15T18:47:52.379Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x132 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 32
00 00 00 00
00 00 00 00
00 00 00 8B

[rebalance:debug,2025-05-15T18:47:52.379Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 306, stream opaque = 0x8B000000
[ns_server:debug,2025-05-15T18:47:52.379Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x133 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 33
00 00 00 00
00 00 00 00
00 00 00 8C

[rebalance:debug,2025-05-15T18:47:52.379Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 307, stream opaque = 0x8C000000
[ns_server:debug,2025-05-15T18:47:52.379Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x134 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 34
00 00 00 00
00 00 00 00
00 00 00 8D

[rebalance:debug,2025-05-15T18:47:52.379Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 308, stream opaque = 0x8D000000
[ns_server:debug,2025-05-15T18:47:52.379Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x135 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 35
00 00 00 00
00 00 00 00
00 00 00 8E

[rebalance:debug,2025-05-15T18:47:52.379Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 309, stream opaque = 0x8E000000
[ns_server:debug,2025-05-15T18:47:52.379Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x136 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 36
00 00 00 00
00 00 00 00
00 00 00 8F

[rebalance:debug,2025-05-15T18:47:52.379Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 310, stream opaque = 0x8F000000
[ns_server:debug,2025-05-15T18:47:52.379Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x137 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 37
00 00 00 00
00 00 00 00
00 00 00 90

[rebalance:debug,2025-05-15T18:47:52.379Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 311, stream opaque = 0x90000000
[ns_server:debug,2025-05-15T18:47:52.379Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x138 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 38
00 00 00 00
00 00 00 00
00 00 00 91

[rebalance:debug,2025-05-15T18:47:52.379Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 312, stream opaque = 0x91000000
[ns_server:debug,2025-05-15T18:47:52.379Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x139 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 39
00 00 00 00
00 00 00 00
00 00 00 92

[rebalance:debug,2025-05-15T18:47:52.379Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 313, stream opaque = 0x92000000
[ns_server:debug,2025-05-15T18:47:52.379Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x13A status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 3A
00 00 00 00
00 00 00 00
00 00 00 93

[rebalance:debug,2025-05-15T18:47:52.379Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 314, stream opaque = 0x93000000
[ns_server:debug,2025-05-15T18:47:52.379Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x13B status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 3B
00 00 00 00
00 00 00 00
00 00 00 94

[rebalance:debug,2025-05-15T18:47:52.379Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 315, stream opaque = 0x94000000
[ns_server:debug,2025-05-15T18:47:52.379Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x13C status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 3C
00 00 00 00
00 00 00 00
00 00 00 95

[rebalance:debug,2025-05-15T18:47:52.379Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 316, stream opaque = 0x95000000
[ns_server:debug,2025-05-15T18:47:52.379Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x13D status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 3D
00 00 00 00
00 00 00 00
00 00 00 96

[rebalance:debug,2025-05-15T18:47:52.379Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 317, stream opaque = 0x96000000
[ns_server:debug,2025-05-15T18:47:52.379Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x13E status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 3E
00 00 00 00
00 00 00 00
00 00 00 97

[rebalance:debug,2025-05-15T18:47:52.379Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 318, stream opaque = 0x97000000
[ns_server:debug,2025-05-15T18:47:52.379Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x13F status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 3F
00 00 00 00
00 00 00 00
00 00 00 98

[rebalance:debug,2025-05-15T18:47:52.379Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 319, stream opaque = 0x98000000
[ns_server:debug,2025-05-15T18:47:52.380Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x140 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 40
00 00 00 00
00 00 00 00
00 00 00 99

[rebalance:debug,2025-05-15T18:47:52.380Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 320, stream opaque = 0x99000000
[ns_server:debug,2025-05-15T18:47:52.380Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x141 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 41
00 00 00 00
00 00 00 00
00 00 00 9A

[rebalance:debug,2025-05-15T18:47:52.380Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 321, stream opaque = 0x9A000000
[ns_server:debug,2025-05-15T18:47:52.380Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x142 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 42
00 00 00 00
00 00 00 00
00 00 00 9B

[rebalance:debug,2025-05-15T18:47:52.380Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 322, stream opaque = 0x9B000000
[ns_server:debug,2025-05-15T18:47:52.380Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x143 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 43
00 00 00 00
00 00 00 00
00 00 00 9C

[rebalance:debug,2025-05-15T18:47:52.380Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 323, stream opaque = 0x9C000000
[ns_server:debug,2025-05-15T18:47:52.380Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x144 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 44
00 00 00 00
00 00 00 00
00 00 00 9D

[rebalance:debug,2025-05-15T18:47:52.380Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 324, stream opaque = 0x9D000000
[ns_server:debug,2025-05-15T18:47:52.380Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x145 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 45
00 00 00 00
00 00 00 00
00 00 00 9E

[rebalance:debug,2025-05-15T18:47:52.380Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 325, stream opaque = 0x9E000000
[ns_server:debug,2025-05-15T18:47:52.380Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x146 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 46
00 00 00 00
00 00 00 00
00 00 00 9F

[rebalance:debug,2025-05-15T18:47:52.380Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 326, stream opaque = 0x9F000000
[ns_server:debug,2025-05-15T18:47:52.380Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x147 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 47
00 00 00 00
00 00 00 00
00 00 00 A0

[rebalance:debug,2025-05-15T18:47:52.380Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 327, stream opaque = 0xA0000000
[ns_server:debug,2025-05-15T18:47:52.380Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x148 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 48
00 00 00 00
00 00 00 00
00 00 00 A1

[rebalance:debug,2025-05-15T18:47:52.380Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 328, stream opaque = 0xA1000000
[ns_server:debug,2025-05-15T18:47:52.380Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x149 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 49
00 00 00 00
00 00 00 00
00 00 00 A2

[rebalance:debug,2025-05-15T18:47:52.380Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 329, stream opaque = 0xA2000000
[ns_server:debug,2025-05-15T18:47:52.380Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x14A status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 4A
00 00 00 00
00 00 00 00
00 00 00 A3

[rebalance:debug,2025-05-15T18:47:52.380Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 330, stream opaque = 0xA3000000
[ns_server:debug,2025-05-15T18:47:52.380Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x14B status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 4B
00 00 00 00
00 00 00 00
00 00 00 A4

[rebalance:debug,2025-05-15T18:47:52.380Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 331, stream opaque = 0xA4000000
[ns_server:debug,2025-05-15T18:47:52.380Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x14C status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 4C
00 00 00 00
00 00 00 00
00 00 00 A5

[rebalance:debug,2025-05-15T18:47:52.380Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 332, stream opaque = 0xA5000000
[ns_server:debug,2025-05-15T18:47:52.380Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x14D status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 4D
00 00 00 00
00 00 00 00
00 00 00 A6

[rebalance:debug,2025-05-15T18:47:52.380Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 333, stream opaque = 0xA6000000
[ns_server:debug,2025-05-15T18:47:52.380Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x14E status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 4E
00 00 00 00
00 00 00 00
00 00 00 A7

[rebalance:debug,2025-05-15T18:47:52.380Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 334, stream opaque = 0xA7000000
[ns_server:debug,2025-05-15T18:47:52.380Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x14F status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 4F
00 00 00 00
00 00 00 00
00 00 00 A8

[rebalance:debug,2025-05-15T18:47:52.381Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 335, stream opaque = 0xA8000000
[ns_server:debug,2025-05-15T18:47:52.381Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x150 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 50
00 00 00 00
00 00 00 00
00 00 00 A9

[rebalance:debug,2025-05-15T18:47:52.381Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 336, stream opaque = 0xA9000000
[ns_server:debug,2025-05-15T18:47:52.381Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x151 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 51
00 00 00 00
00 00 00 00
00 00 00 AA

[rebalance:debug,2025-05-15T18:47:52.381Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 337, stream opaque = 0xAA000000
[ns_server:debug,2025-05-15T18:47:52.381Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x152 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 52
00 00 00 00
00 00 00 00
00 00 00 AB

[rebalance:debug,2025-05-15T18:47:52.381Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 338, stream opaque = 0xAB000000
[ns_server:debug,2025-05-15T18:47:52.381Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x153 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 53
00 00 00 00
00 00 00 00
00 00 00 AC

[rebalance:debug,2025-05-15T18:47:52.381Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 339, stream opaque = 0xAC000000
[ns_server:debug,2025-05-15T18:47:52.381Z,ns_1@db3.lan:<0.4231.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x154 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 54
00 00 00 00
00 00 00 00
00 00 00 AD

[rebalance:debug,2025-05-15T18:47:52.381Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 340, stream opaque = 0xAD000000
[ns_server:debug,2025-05-15T18:47:52.381Z,ns_1@db3.lan:dcp_traffic_monitor<0.3321.0>:dcp_traffic_monitor:update_bucket:148]Saw that bucket "doom-scrolling" became alive on node 'ns_1@172.19.0.4'
[ns_server:debug,2025-05-15T18:47:52.381Z,ns_1@db3.lan:<0.4231.0>:dcp_consumer_conn:maybe_reply_setup_streams:499]Setup stream request completed with ok. Moving to idle state
[ns_server:debug,2025-05-15T18:47:52.381Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 512, opaque = 0x200, type = add
[ns_server:debug,2025-05-15T18:47:52.381Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 513, opaque = 0x201, type = add
[ns_server:debug,2025-05-15T18:47:52.381Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 514, opaque = 0x202, type = add
[ns_server:debug,2025-05-15T18:47:52.381Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 515, opaque = 0x203, type = add
[ns_server:debug,2025-05-15T18:47:52.382Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 516, opaque = 0x204, type = add
[ns_server:debug,2025-05-15T18:47:52.382Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 517, opaque = 0x205, type = add
[ns_server:debug,2025-05-15T18:47:52.382Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 518, opaque = 0x206, type = add
[ns_server:debug,2025-05-15T18:47:52.382Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 519, opaque = 0x207, type = add
[ns_server:debug,2025-05-15T18:47:52.382Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 520, opaque = 0x208, type = add
[ns_server:debug,2025-05-15T18:47:52.382Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 521, opaque = 0x209, type = add
[ns_server:debug,2025-05-15T18:47:52.382Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 522, opaque = 0x20A, type = add
[ns_server:debug,2025-05-15T18:47:52.382Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 523, opaque = 0x20B, type = add
[ns_server:debug,2025-05-15T18:47:52.382Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 524, opaque = 0x20C, type = add
[ns_server:debug,2025-05-15T18:47:52.382Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 525, opaque = 0x20D, type = add
[ns_server:debug,2025-05-15T18:47:52.382Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 526, opaque = 0x20E, type = add
[ns_server:debug,2025-05-15T18:47:52.382Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 527, opaque = 0x20F, type = add
[ns_server:debug,2025-05-15T18:47:52.382Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 528, opaque = 0x210, type = add
[ns_server:debug,2025-05-15T18:47:52.382Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 529, opaque = 0x211, type = add
[ns_server:debug,2025-05-15T18:47:52.382Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 530, opaque = 0x212, type = add
[ns_server:debug,2025-05-15T18:47:52.382Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 531, opaque = 0x213, type = add
[ns_server:debug,2025-05-15T18:47:52.382Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 532, opaque = 0x214, type = add
[ns_server:debug,2025-05-15T18:47:52.382Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 533, opaque = 0x215, type = add
[ns_server:debug,2025-05-15T18:47:52.382Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 534, opaque = 0x216, type = add
[ns_server:debug,2025-05-15T18:47:52.382Z,ns_1@db3.lan:dcp_traffic_monitor<0.3321.0>:dcp_traffic_monitor:update_bucket:148]Saw that bucket "doom-scrolling" became alive on node 'ns_1@db3.lan'
[ns_server:debug,2025-05-15T18:47:52.382Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 535, opaque = 0x217, type = add
[ns_server:debug,2025-05-15T18:47:52.383Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 536, opaque = 0x218, type = add
[ns_server:debug,2025-05-15T18:47:52.383Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 537, opaque = 0x219, type = add
[ns_server:debug,2025-05-15T18:47:52.383Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 538, opaque = 0x21A, type = add
[ns_server:debug,2025-05-15T18:47:52.383Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 539, opaque = 0x21B, type = add
[ns_server:debug,2025-05-15T18:47:52.383Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 540, opaque = 0x21C, type = add
[ns_server:debug,2025-05-15T18:47:52.383Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 541, opaque = 0x21D, type = add
[ns_server:debug,2025-05-15T18:47:52.384Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 542, opaque = 0x21E, type = add
[ns_server:debug,2025-05-15T18:47:52.384Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 543, opaque = 0x21F, type = add
[ns_server:debug,2025-05-15T18:47:52.384Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 544, opaque = 0x220, type = add
[ns_server:debug,2025-05-15T18:47:52.384Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 545, opaque = 0x221, type = add
[ns_server:debug,2025-05-15T18:47:52.384Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 546, opaque = 0x222, type = add
[ns_server:debug,2025-05-15T18:47:52.384Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 547, opaque = 0x223, type = add
[ns_server:debug,2025-05-15T18:47:52.384Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 548, opaque = 0x224, type = add
[ns_server:debug,2025-05-15T18:47:52.384Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 549, opaque = 0x225, type = add
[ns_server:debug,2025-05-15T18:47:52.384Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 550, opaque = 0x226, type = add
[ns_server:debug,2025-05-15T18:47:52.384Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 551, opaque = 0x227, type = add
[ns_server:debug,2025-05-15T18:47:52.384Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 552, opaque = 0x228, type = add
[ns_server:debug,2025-05-15T18:47:52.384Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 553, opaque = 0x229, type = add
[ns_server:debug,2025-05-15T18:47:52.384Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 554, opaque = 0x22A, type = add
[ns_server:debug,2025-05-15T18:47:52.384Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 555, opaque = 0x22B, type = add
[ns_server:debug,2025-05-15T18:47:52.384Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 556, opaque = 0x22C, type = add
[ns_server:debug,2025-05-15T18:47:52.384Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 557, opaque = 0x22D, type = add
[ns_server:debug,2025-05-15T18:47:52.384Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 558, opaque = 0x22E, type = add
[ns_server:debug,2025-05-15T18:47:52.384Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 559, opaque = 0x22F, type = add
[ns_server:debug,2025-05-15T18:47:52.384Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 560, opaque = 0x230, type = add
[ns_server:debug,2025-05-15T18:47:52.384Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 561, opaque = 0x231, type = add
[ns_server:debug,2025-05-15T18:47:52.384Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 562, opaque = 0x232, type = add
[ns_server:debug,2025-05-15T18:47:52.385Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 563, opaque = 0x233, type = add
[ns_server:debug,2025-05-15T18:47:52.385Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 564, opaque = 0x234, type = add
[ns_server:debug,2025-05-15T18:47:52.385Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 565, opaque = 0x235, type = add
[ns_server:debug,2025-05-15T18:47:52.385Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 566, opaque = 0x236, type = add
[ns_server:debug,2025-05-15T18:47:52.385Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 567, opaque = 0x237, type = add
[ns_server:debug,2025-05-15T18:47:52.385Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 568, opaque = 0x238, type = add
[ns_server:debug,2025-05-15T18:47:52.385Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 569, opaque = 0x239, type = add
[ns_server:debug,2025-05-15T18:47:52.385Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 570, opaque = 0x23A, type = add
[ns_server:debug,2025-05-15T18:47:52.385Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 571, opaque = 0x23B, type = add
[ns_server:debug,2025-05-15T18:47:52.385Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 572, opaque = 0x23C, type = add
[ns_server:debug,2025-05-15T18:47:52.385Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 573, opaque = 0x23D, type = add
[ns_server:debug,2025-05-15T18:47:52.385Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 574, opaque = 0x23E, type = add
[ns_server:debug,2025-05-15T18:47:52.385Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 575, opaque = 0x23F, type = add
[ns_server:debug,2025-05-15T18:47:52.385Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 576, opaque = 0x240, type = add
[ns_server:debug,2025-05-15T18:47:52.385Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 577, opaque = 0x241, type = add
[ns_server:debug,2025-05-15T18:47:52.385Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 578, opaque = 0x242, type = add
[ns_server:debug,2025-05-15T18:47:52.385Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 579, opaque = 0x243, type = add
[ns_server:debug,2025-05-15T18:47:52.385Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 580, opaque = 0x244, type = add
[ns_server:debug,2025-05-15T18:47:52.385Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 581, opaque = 0x245, type = add
[ns_server:debug,2025-05-15T18:47:52.385Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 582, opaque = 0x246, type = add
[ns_server:debug,2025-05-15T18:47:52.385Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 583, opaque = 0x247, type = add
[ns_server:debug,2025-05-15T18:47:52.385Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 584, opaque = 0x248, type = add
[ns_server:debug,2025-05-15T18:47:52.385Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 585, opaque = 0x249, type = add
[ns_server:debug,2025-05-15T18:47:52.385Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 586, opaque = 0x24A, type = add
[ns_server:debug,2025-05-15T18:47:52.385Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 587, opaque = 0x24B, type = add
[ns_server:debug,2025-05-15T18:47:52.385Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 588, opaque = 0x24C, type = add
[ns_server:debug,2025-05-15T18:47:52.385Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 589, opaque = 0x24D, type = add
[ns_server:debug,2025-05-15T18:47:52.385Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 590, opaque = 0x24E, type = add
[ns_server:debug,2025-05-15T18:47:52.386Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 591, opaque = 0x24F, type = add
[ns_server:debug,2025-05-15T18:47:52.386Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 592, opaque = 0x250, type = add
[ns_server:debug,2025-05-15T18:47:52.386Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 593, opaque = 0x251, type = add
[ns_server:debug,2025-05-15T18:47:52.386Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 594, opaque = 0x252, type = add
[ns_server:debug,2025-05-15T18:47:52.386Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 595, opaque = 0x253, type = add
[ns_server:debug,2025-05-15T18:47:52.386Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 596, opaque = 0x254, type = add
[ns_server:debug,2025-05-15T18:47:52.386Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 597, opaque = 0x255, type = add
[ns_server:debug,2025-05-15T18:47:52.386Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 598, opaque = 0x256, type = add
[ns_server:debug,2025-05-15T18:47:52.386Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 599, opaque = 0x257, type = add
[ns_server:debug,2025-05-15T18:47:52.386Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 600, opaque = 0x258, type = add
[ns_server:debug,2025-05-15T18:47:52.386Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 601, opaque = 0x259, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 602, opaque = 0x25A, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 603, opaque = 0x25B, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 604, opaque = 0x25C, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 605, opaque = 0x25D, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 606, opaque = 0x25E, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 607, opaque = 0x25F, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 608, opaque = 0x260, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 609, opaque = 0x261, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 610, opaque = 0x262, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 611, opaque = 0x263, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 612, opaque = 0x264, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 613, opaque = 0x265, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 614, opaque = 0x266, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 615, opaque = 0x267, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 616, opaque = 0x268, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 617, opaque = 0x269, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 618, opaque = 0x26A, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 619, opaque = 0x26B, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 620, opaque = 0x26C, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 621, opaque = 0x26D, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 622, opaque = 0x26E, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 623, opaque = 0x26F, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 624, opaque = 0x270, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 625, opaque = 0x271, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 626, opaque = 0x272, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 627, opaque = 0x273, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 628, opaque = 0x274, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 629, opaque = 0x275, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 630, opaque = 0x276, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 631, opaque = 0x277, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 632, opaque = 0x278, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 633, opaque = 0x279, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 634, opaque = 0x27A, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 635, opaque = 0x27B, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 636, opaque = 0x27C, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 637, opaque = 0x27D, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 638, opaque = 0x27E, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 639, opaque = 0x27F, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 640, opaque = 0x280, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 641, opaque = 0x281, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 642, opaque = 0x282, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 643, opaque = 0x283, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 644, opaque = 0x284, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 645, opaque = 0x285, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 646, opaque = 0x286, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 647, opaque = 0x287, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 648, opaque = 0x288, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 649, opaque = 0x289, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 650, opaque = 0x28A, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 651, opaque = 0x28B, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 652, opaque = 0x28C, type = add
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 653, opaque = 0x28D, type = add
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 654, opaque = 0x28E, type = add
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 655, opaque = 0x28F, type = add
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 656, opaque = 0x290, type = add
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 657, opaque = 0x291, type = add
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 658, opaque = 0x292, type = add
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 659, opaque = 0x293, type = add
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 660, opaque = 0x294, type = add
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 661, opaque = 0x295, type = add
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 662, opaque = 0x296, type = add
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 663, opaque = 0x297, type = add
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 664, opaque = 0x298, type = add
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 665, opaque = 0x299, type = add
[ns_server:debug,2025-05-15T18:47:52.391Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 666, opaque = 0x29A, type = add
[ns_server:debug,2025-05-15T18:47:52.391Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 667, opaque = 0x29B, type = add
[ns_server:debug,2025-05-15T18:47:52.391Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 668, opaque = 0x29C, type = add
[ns_server:debug,2025-05-15T18:47:52.391Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 669, opaque = 0x29D, type = add
[ns_server:debug,2025-05-15T18:47:52.391Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 670, opaque = 0x29E, type = add
[ns_server:debug,2025-05-15T18:47:52.391Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 671, opaque = 0x29F, type = add
[ns_server:debug,2025-05-15T18:47:52.391Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 672, opaque = 0x2A0, type = add
[ns_server:debug,2025-05-15T18:47:52.392Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 673, opaque = 0x2A1, type = add
[ns_server:debug,2025-05-15T18:47:52.392Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 674, opaque = 0x2A2, type = add
[ns_server:debug,2025-05-15T18:47:52.392Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 675, opaque = 0x2A3, type = add
[ns_server:debug,2025-05-15T18:47:52.392Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 676, opaque = 0x2A4, type = add
[ns_server:debug,2025-05-15T18:47:52.392Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 677, opaque = 0x2A5, type = add
[ns_server:debug,2025-05-15T18:47:52.392Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 678, opaque = 0x2A6, type = add
[ns_server:debug,2025-05-15T18:47:52.392Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 679, opaque = 0x2A7, type = add
[ns_server:debug,2025-05-15T18:47:52.392Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 680, opaque = 0x2A8, type = add
[ns_server:debug,2025-05-15T18:47:52.392Z,ns_1@db3.lan:<0.4235.0>:dcp_commands:add_stream:83]Add stream for partition 681, opaque = 0x2A9, type = add
[ns_server:debug,2025-05-15T18:47:52.392Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_call:206]Setup DCP streams:
Current []
Streams to open [512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681]
Streams to close []

[ns_server:debug,2025-05-15T18:47:52.392Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB000000
80 5E 00 16
00 00 00 00
00 00 00 1E
0B 00 00 00
00 00 00 00
00 00 00 00
63 6F 6E 6E
65 63 74 69
6F 6E 5F 62
75 66 66 65
72 5F 73 69
7A 65 31 33
34 32 31 37
37 33 
[ns_server:debug,2025-05-15T18:47:52.392Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0xFE (cmd_get_error_map) vbucket = 0 opaque = 0xC000000
80 FE 00 00
00 00 00 00
00 00 00 02
0C 00 00 00
00 00 00 00
00 00 00 00
00 00 
[ns_server:debug,2025-05-15T18:47:52.393Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
0B 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.393Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0xFE (cmd_get_error_map) vbucket = 0 opaque = 0xC000000 status = 0x1 (key_enoent)
81 FE 00 00
00 00 00 01
00 00 00 00
0C 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.394Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xAD000000
80 5E 00 0B
00 00 00 00
00 00 00 0F
AD 00 00 00
00 00 00 00
00 00 00 00
65 6E 61 62
6C 65 5F 6E
6F 6F 70 74
72 75 65 
[ns_server:debug,2025-05-15T18:47:52.394Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xAE000000
80 5E 00 11
00 00 00 00
00 00 00 19
AE 00 00 00
00 00 00 00
00 00 00 00
73 65 74 5F
6E 6F 6F 70
5F 69 6E 74
65 72 76 61
6C 30 2E 31
30 30 30 30
30 
[ns_server:debug,2025-05-15T18:47:52.394Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xAD000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
AD 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.394Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xAE000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
AE 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.394Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xAF000000
80 5E 00 0C
00 00 00 00
00 00 00 10
AF 00 00 00
00 00 00 00
00 00 00 00
73 65 74 5F
70 72 69 6F
72 69 74 79
68 69 67 68

[ns_server:debug,2025-05-15T18:47:52.395Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB0000000
80 5E 00 1F
00 00 00 00
00 00 00 23
B0 00 00 00
00 00 00 00
00 00 00 00
73 75 70 70
6F 72 74 73
5F 63 75 72
73 6F 72 5F
64 72 6F 70
70 69 6E 67
5F 76 75 6C
63 61 6E 74
72 75 65 
[ns_server:debug,2025-05-15T18:47:52.395Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB1000000
80 5E 00 11
00 00 00 00
00 00 00 15
B1 00 00 00
00 00 00 00
00 00 00 00
73 75 70 70
6F 72 74 73
5F 68 69 66
69 5F 4D 46
55 74 72 75
65 
[ns_server:debug,2025-05-15T18:47:52.395Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB2000000
80 5E 00 26
00 00 00 00
00 00 00 2A
B2 00 00 00
00 00 00 00
00 00 00 00
73 65 6E 64
5F 73 74 72
65 61 6D 5F
65 6E 64 5F
6F 6E 5F 63
6C 69 65 6E
74 5F 63 6C
6F 73 65 5F
73 74 72 65
61 6D 74 72
75 65 
[ns_server:debug,2025-05-15T18:47:52.395Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB3000000
80 5E 00 14
00 00 00 00
00 00 00 18
B3 00 00 00
00 00 00 00
00 00 00 00
65 6E 61 62
6C 65 5F 65
78 70 69 72
79 5F 6F 70
63 6F 64 65
74 72 75 65

[ns_server:debug,2025-05-15T18:47:52.395Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB4000000
80 5E 00 12
00 00 00 00
00 00 00 16
B4 00 00 00
00 00 00 00
00 00 00 00
65 6E 61 62
6C 65 5F 73
79 6E 63 5F
77 72 69 74
65 73 74 72
75 65 
[ns_server:debug,2025-05-15T18:47:52.395Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xAF000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
AF 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.395Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB0000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B0 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.395Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB1000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B1 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.395Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB2000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B2 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.395Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB3000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B3 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.396Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB4000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B4 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.396Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB5000000
80 5E 00 0D
00 00 00 00
00 00 00 19
B5 00 00 00
00 00 00 00
00 00 00 00
63 6F 6E 73
75 6D 65 72
5F 6E 61 6D
65 6E 73 5F
31 40 64 62
33 2E 6C 61
6E 
[ns_server:debug,2025-05-15T18:47:52.396Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB6000000
80 5E 00 1B
00 00 00 00
00 00 00 1F
B6 00 00 00
00 00 00 00
00 00 00 00
69 6E 63 6C
75 64 65 5F
64 65 6C 65
74 65 64 5F
75 73 65 72
5F 78 61 74
74 72 73 74
72 75 65 
[ns_server:debug,2025-05-15T18:47:52.396Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB5000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B5 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.396Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB6000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B6 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.396Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB7000000
80 5E 00 13
00 00 00 00
00 00 00 17
B7 00 00 00
00 00 00 00
00 00 00 00
76 37 5F 64
63 70 5F 73
74 61 74 75
73 5F 63 6F
64 65 73 74
72 75 65 
[ns_server:debug,2025-05-15T18:47:52.397Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB7000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B7 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.397Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB8000000
80 5E 00 19
00 00 00 00
00 00 00 1D
B8 00 00 00
00 00 00 00
00 00 00 00
66 6C 61 74
62 75 66 66
65 72 73 5F
73 79 73 74
65 6D 5F 65
76 65 6E 74
73 74 72 75
65 
[ns_server:debug,2025-05-15T18:47:52.397Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB8000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B8 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.398Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB9000000
80 5E 00 0E
00 00 00 00
00 00 00 12
B9 00 00 00
00 00 00 00
00 00 00 00
63 68 61 6E
67 65 5F 73
74 72 65 61
6D 73 74 72
75 65 
[ns_server:debug,2025-05-15T18:47:52.398Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB9000000 status = 0x83 (not_supported)
81 5E 00 00
00 00 00 83
00 00 00 00
B9 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.399Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 512 opaque = 0x1000000
80 53 00 00
30 00 02 00
00 00 00 30
01 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.399Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 513 opaque = 0x2000000
80 53 00 00
30 00 02 01
00 00 00 30
02 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.399Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 514 opaque = 0x3000000
80 53 00 00
30 00 02 02
00 00 00 30
03 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.399Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 515 opaque = 0x4000000
80 53 00 00
30 00 02 03
00 00 00 30
04 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.399Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 516 opaque = 0x5000000
80 53 00 00
30 00 02 04
00 00 00 30
05 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.400Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 517 opaque = 0x6000000
80 53 00 00
30 00 02 05
00 00 00 30
06 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.400Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 518 opaque = 0x7000000
80 53 00 00
30 00 02 06
00 00 00 30
07 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.400Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 519 opaque = 0x8000000
80 53 00 00
30 00 02 07
00 00 00 30
08 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.400Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 520 opaque = 0x9000000
80 53 00 00
30 00 02 08
00 00 00 30
09 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.400Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 521 opaque = 0xA000000
80 53 00 00
30 00 02 09
00 00 00 30
0A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.400Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 522 opaque = 0xD000000
80 53 00 00
30 00 02 0A
00 00 00 30
0D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.400Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 523 opaque = 0xE000000
80 53 00 00
30 00 02 0B
00 00 00 30
0E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.400Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 524 opaque = 0xF000000
80 53 00 00
30 00 02 0C
00 00 00 30
0F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.401Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 525 opaque = 0x10000000
80 53 00 00
30 00 02 0D
00 00 00 30
10 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.401Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 526 opaque = 0x11000000
80 53 00 00
30 00 02 0E
00 00 00 30
11 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.401Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 527 opaque = 0x12000000
80 53 00 00
30 00 02 0F
00 00 00 30
12 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.401Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 528 opaque = 0x13000000
80 53 00 00
30 00 02 10
00 00 00 30
13 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.401Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 529 opaque = 0x14000000
80 53 00 00
30 00 02 11
00 00 00 30
14 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.401Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 530 opaque = 0x15000000
80 53 00 00
30 00 02 12
00 00 00 30
15 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.402Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 531 opaque = 0x16000000
80 53 00 00
30 00 02 13
00 00 00 30
16 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.402Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 532 opaque = 0x17000000
80 53 00 00
30 00 02 14
00 00 00 30
17 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.402Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 533 opaque = 0x18000000
80 53 00 00
30 00 02 15
00 00 00 30
18 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.402Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 534 opaque = 0x19000000
80 53 00 00
30 00 02 16
00 00 00 30
19 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.402Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 535 opaque = 0x1A000000
80 53 00 00
30 00 02 17
00 00 00 30
1A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.402Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 536 opaque = 0x1B000000
80 53 00 00
30 00 02 18
00 00 00 30
1B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.403Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 537 opaque = 0x1C000000
80 53 00 00
30 00 02 19
00 00 00 30
1C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.403Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 538 opaque = 0x1D000000
80 53 00 00
30 00 02 1A
00 00 00 30
1D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.403Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 539 opaque = 0x1E000000
80 53 00 00
30 00 02 1B
00 00 00 30
1E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.403Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 540 opaque = 0x1F000000
80 53 00 00
30 00 02 1C
00 00 00 30
1F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.403Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 541 opaque = 0x20000000
80 53 00 00
30 00 02 1D
00 00 00 30
20 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.403Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 542 opaque = 0x21000000
80 53 00 00
30 00 02 1E
00 00 00 30
21 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.403Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 543 opaque = 0x22000000
80 53 00 00
30 00 02 1F
00 00 00 30
22 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.403Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 544 opaque = 0x23000000
80 53 00 00
30 00 02 20
00 00 00 30
23 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.404Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 545 opaque = 0x24000000
80 53 00 00
30 00 02 21
00 00 00 30
24 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.404Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 546 opaque = 0x25000000
80 53 00 00
30 00 02 22
00 00 00 30
25 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.404Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 547 opaque = 0x26000000
80 53 00 00
30 00 02 23
00 00 00 30
26 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.404Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 548 opaque = 0x27000000
80 53 00 00
30 00 02 24
00 00 00 30
27 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.404Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 549 opaque = 0x28000000
80 53 00 00
30 00 02 25
00 00 00 30
28 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.404Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 550 opaque = 0x29000000
80 53 00 00
30 00 02 26
00 00 00 30
29 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.404Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 551 opaque = 0x2A000000
80 53 00 00
30 00 02 27
00 00 00 30
2A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.404Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 552 opaque = 0x2B000000
80 53 00 00
30 00 02 28
00 00 00 30
2B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.404Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 553 opaque = 0x2C000000
80 53 00 00
30 00 02 29
00 00 00 30
2C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.405Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 554 opaque = 0x2D000000
80 53 00 00
30 00 02 2A
00 00 00 30
2D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.405Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 555 opaque = 0x2E000000
80 53 00 00
30 00 02 2B
00 00 00 30
2E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.405Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 556 opaque = 0x2F000000
80 53 00 00
30 00 02 2C
00 00 00 30
2F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.405Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 557 opaque = 0x30000000
80 53 00 00
30 00 02 2D
00 00 00 30
30 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.405Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 558 opaque = 0x31000000
80 53 00 00
30 00 02 2E
00 00 00 30
31 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.405Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 559 opaque = 0x32000000
80 53 00 00
30 00 02 2F
00 00 00 30
32 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.405Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 560 opaque = 0x33000000
80 53 00 00
30 00 02 30
00 00 00 30
33 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.405Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 561 opaque = 0x34000000
80 53 00 00
30 00 02 31
00 00 00 30
34 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.405Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 562 opaque = 0x35000000
80 53 00 00
30 00 02 32
00 00 00 30
35 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.406Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 563 opaque = 0x36000000
80 53 00 00
30 00 02 33
00 00 00 30
36 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.406Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 564 opaque = 0x37000000
80 53 00 00
30 00 02 34
00 00 00 30
37 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.406Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 565 opaque = 0x38000000
80 53 00 00
30 00 02 35
00 00 00 30
38 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.406Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 566 opaque = 0x39000000
80 53 00 00
30 00 02 36
00 00 00 30
39 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.406Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 567 opaque = 0x3A000000
80 53 00 00
30 00 02 37
00 00 00 30
3A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.406Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 568 opaque = 0x3B000000
80 53 00 00
30 00 02 38
00 00 00 30
3B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.406Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 569 opaque = 0x3C000000
80 53 00 00
30 00 02 39
00 00 00 30
3C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.406Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 570 opaque = 0x3D000000
80 53 00 00
30 00 02 3A
00 00 00 30
3D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.406Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 571 opaque = 0x3E000000
80 53 00 00
30 00 02 3B
00 00 00 30
3E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.407Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 572 opaque = 0x3F000000
80 53 00 00
30 00 02 3C
00 00 00 30
3F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.407Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 573 opaque = 0x40000000
80 53 00 00
30 00 02 3D
00 00 00 30
40 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.407Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 574 opaque = 0x41000000
80 53 00 00
30 00 02 3E
00 00 00 30
41 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.407Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 575 opaque = 0x42000000
80 53 00 00
30 00 02 3F
00 00 00 30
42 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.407Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 576 opaque = 0x43000000
80 53 00 00
30 00 02 40
00 00 00 30
43 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.408Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 577 opaque = 0x44000000
80 53 00 00
30 00 02 41
00 00 00 30
44 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.408Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 578 opaque = 0x45000000
80 53 00 00
30 00 02 42
00 00 00 30
45 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.408Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 579 opaque = 0x46000000
80 53 00 00
30 00 02 43
00 00 00 30
46 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.408Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 580 opaque = 0x47000000
80 53 00 00
30 00 02 44
00 00 00 30
47 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.408Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 581 opaque = 0x48000000
80 53 00 00
30 00 02 45
00 00 00 30
48 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.408Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 582 opaque = 0x49000000
80 53 00 00
30 00 02 46
00 00 00 30
49 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.409Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 583 opaque = 0x4A000000
80 53 00 00
30 00 02 47
00 00 00 30
4A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.409Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 584 opaque = 0x4B000000
80 53 00 00
30 00 02 48
00 00 00 30
4B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.409Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 585 opaque = 0x4C000000
80 53 00 00
30 00 02 49
00 00 00 30
4C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.409Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 586 opaque = 0x4D000000
80 53 00 00
30 00 02 4A
00 00 00 30
4D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.409Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 587 opaque = 0x4E000000
80 53 00 00
30 00 02 4B
00 00 00 30
4E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.410Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 588 opaque = 0x4F000000
80 53 00 00
30 00 02 4C
00 00 00 30
4F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.410Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 589 opaque = 0x50000000
80 53 00 00
30 00 02 4D
00 00 00 30
50 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.411Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 590 opaque = 0x51000000
80 53 00 00
30 00 02 4E
00 00 00 30
51 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.411Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 591 opaque = 0x52000000
80 53 00 00
30 00 02 4F
00 00 00 30
52 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.411Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 592 opaque = 0x53000000
80 53 00 00
30 00 02 50
00 00 00 30
53 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.411Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 593 opaque = 0x54000000
80 53 00 00
30 00 02 51
00 00 00 30
54 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.411Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 594 opaque = 0x55000000
80 53 00 00
30 00 02 52
00 00 00 30
55 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.412Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 595 opaque = 0x56000000
80 53 00 00
30 00 02 53
00 00 00 30
56 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.412Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 596 opaque = 0x57000000
80 53 00 00
30 00 02 54
00 00 00 30
57 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.412Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 597 opaque = 0x58000000
80 53 00 00
30 00 02 55
00 00 00 30
58 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.412Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 598 opaque = 0x59000000
80 53 00 00
30 00 02 56
00 00 00 30
59 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.413Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 599 opaque = 0x5A000000
80 53 00 00
30 00 02 57
00 00 00 30
5A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.413Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 600 opaque = 0x5B000000
80 53 00 00
30 00 02 58
00 00 00 30
5B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.413Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 601 opaque = 0x5C000000
80 53 00 00
30 00 02 59
00 00 00 30
5C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.413Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 602 opaque = 0x5D000000
80 53 00 00
30 00 02 5A
00 00 00 30
5D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.413Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 603 opaque = 0x5E000000
80 53 00 00
30 00 02 5B
00 00 00 30
5E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.413Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 604 opaque = 0x5F000000
80 53 00 00
30 00 02 5C
00 00 00 30
5F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.413Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 605 opaque = 0x60000000
80 53 00 00
30 00 02 5D
00 00 00 30
60 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.413Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 606 opaque = 0x61000000
80 53 00 00
30 00 02 5E
00 00 00 30
61 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.414Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 607 opaque = 0x62000000
80 53 00 00
30 00 02 5F
00 00 00 30
62 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.414Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 608 opaque = 0x63000000
80 53 00 00
30 00 02 60
00 00 00 30
63 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.414Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 609 opaque = 0x64000000
80 53 00 00
30 00 02 61
00 00 00 30
64 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.414Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 610 opaque = 0x65000000
80 53 00 00
30 00 02 62
00 00 00 30
65 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.414Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 611 opaque = 0x66000000
80 53 00 00
30 00 02 63
00 00 00 30
66 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.414Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 612 opaque = 0x67000000
80 53 00 00
30 00 02 64
00 00 00 30
67 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.414Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 613 opaque = 0x68000000
80 53 00 00
30 00 02 65
00 00 00 30
68 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.414Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 614 opaque = 0x69000000
80 53 00 00
30 00 02 66
00 00 00 30
69 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.414Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 615 opaque = 0x6A000000
80 53 00 00
30 00 02 67
00 00 00 30
6A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.415Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 616 opaque = 0x6B000000
80 53 00 00
30 00 02 68
00 00 00 30
6B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.415Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 617 opaque = 0x6C000000
80 53 00 00
30 00 02 69
00 00 00 30
6C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.415Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 618 opaque = 0x6D000000
80 53 00 00
30 00 02 6A
00 00 00 30
6D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.415Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 619 opaque = 0x6E000000
80 53 00 00
30 00 02 6B
00 00 00 30
6E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.415Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 620 opaque = 0x6F000000
80 53 00 00
30 00 02 6C
00 00 00 30
6F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.415Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 621 opaque = 0x70000000
80 53 00 00
30 00 02 6D
00 00 00 30
70 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.415Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 622 opaque = 0x71000000
80 53 00 00
30 00 02 6E
00 00 00 30
71 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.415Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 623 opaque = 0x72000000
80 53 00 00
30 00 02 6F
00 00 00 30
72 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.416Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 624 opaque = 0x73000000
80 53 00 00
30 00 02 70
00 00 00 30
73 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.416Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 625 opaque = 0x74000000
80 53 00 00
30 00 02 71
00 00 00 30
74 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.416Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 626 opaque = 0x75000000
80 53 00 00
30 00 02 72
00 00 00 30
75 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.417Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 627 opaque = 0x76000000
80 53 00 00
30 00 02 73
00 00 00 30
76 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.417Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 628 opaque = 0x77000000
80 53 00 00
30 00 02 74
00 00 00 30
77 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.417Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 629 opaque = 0x78000000
80 53 00 00
30 00 02 75
00 00 00 30
78 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.417Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 630 opaque = 0x79000000
80 53 00 00
30 00 02 76
00 00 00 30
79 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.417Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 631 opaque = 0x7A000000
80 53 00 00
30 00 02 77
00 00 00 30
7A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.418Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 632 opaque = 0x7B000000
80 53 00 00
30 00 02 78
00 00 00 30
7B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.418Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 633 opaque = 0x7C000000
80 53 00 00
30 00 02 79
00 00 00 30
7C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.418Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 634 opaque = 0x7D000000
80 53 00 00
30 00 02 7A
00 00 00 30
7D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.418Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 635 opaque = 0x7E000000
80 53 00 00
30 00 02 7B
00 00 00 30
7E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.418Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 636 opaque = 0x7F000000
80 53 00 00
30 00 02 7C
00 00 00 30
7F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.418Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 637 opaque = 0x80000000
80 53 00 00
30 00 02 7D
00 00 00 30
80 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.418Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 638 opaque = 0x81000000
80 53 00 00
30 00 02 7E
00 00 00 30
81 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.418Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 639 opaque = 0x82000000
80 53 00 00
30 00 02 7F
00 00 00 30
82 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.418Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 640 opaque = 0x83000000
80 53 00 00
30 00 02 80
00 00 00 30
83 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.419Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 641 opaque = 0x84000000
80 53 00 00
30 00 02 81
00 00 00 30
84 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.419Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 642 opaque = 0x85000000
80 53 00 00
30 00 02 82
00 00 00 30
85 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.419Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 643 opaque = 0x86000000
80 53 00 00
30 00 02 83
00 00 00 30
86 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.419Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 644 opaque = 0x87000000
80 53 00 00
30 00 02 84
00 00 00 30
87 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.419Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 645 opaque = 0x88000000
80 53 00 00
30 00 02 85
00 00 00 30
88 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.419Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 646 opaque = 0x89000000
80 53 00 00
30 00 02 86
00 00 00 30
89 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.419Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 647 opaque = 0x8A000000
80 53 00 00
30 00 02 87
00 00 00 30
8A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.420Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 648 opaque = 0x8B000000
80 53 00 00
30 00 02 88
00 00 00 30
8B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.420Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 649 opaque = 0x8C000000
80 53 00 00
30 00 02 89
00 00 00 30
8C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.420Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 650 opaque = 0x8D000000
80 53 00 00
30 00 02 8A
00 00 00 30
8D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.420Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 651 opaque = 0x8E000000
80 53 00 00
30 00 02 8B
00 00 00 30
8E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.420Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 652 opaque = 0x8F000000
80 53 00 00
30 00 02 8C
00 00 00 30
8F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.420Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 653 opaque = 0x90000000
80 53 00 00
30 00 02 8D
00 00 00 30
90 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.420Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 654 opaque = 0x91000000
80 53 00 00
30 00 02 8E
00 00 00 30
91 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.420Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 655 opaque = 0x92000000
80 53 00 00
30 00 02 8F
00 00 00 30
92 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.421Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 656 opaque = 0x93000000
80 53 00 00
30 00 02 90
00 00 00 30
93 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.421Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 657 opaque = 0x94000000
80 53 00 00
30 00 02 91
00 00 00 30
94 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.421Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 658 opaque = 0x95000000
80 53 00 00
30 00 02 92
00 00 00 30
95 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.421Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 659 opaque = 0x96000000
80 53 00 00
30 00 02 93
00 00 00 30
96 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.421Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 660 opaque = 0x97000000
80 53 00 00
30 00 02 94
00 00 00 30
97 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.421Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 661 opaque = 0x98000000
80 53 00 00
30 00 02 95
00 00 00 30
98 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.421Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 662 opaque = 0x99000000
80 53 00 00
30 00 02 96
00 00 00 30
99 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.421Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 663 opaque = 0x9A000000
80 53 00 00
30 00 02 97
00 00 00 30
9A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.421Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 664 opaque = 0x9B000000
80 53 00 00
30 00 02 98
00 00 00 30
9B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.422Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 665 opaque = 0x9C000000
80 53 00 00
30 00 02 99
00 00 00 30
9C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.422Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 666 opaque = 0x9D000000
80 53 00 00
30 00 02 9A
00 00 00 30
9D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.422Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 667 opaque = 0x9E000000
80 53 00 00
30 00 02 9B
00 00 00 30
9E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.422Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 668 opaque = 0x9F000000
80 53 00 00
30 00 02 9C
00 00 00 30
9F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.422Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 669 opaque = 0xA0000000
80 53 00 00
30 00 02 9D
00 00 00 30
A0 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.422Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 670 opaque = 0xA1000000
80 53 00 00
30 00 02 9E
00 00 00 30
A1 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.422Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 671 opaque = 0xA2000000
80 53 00 00
30 00 02 9F
00 00 00 30
A2 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.422Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 672 opaque = 0xA3000000
80 53 00 00
30 00 02 A0
00 00 00 30
A3 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.422Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 673 opaque = 0xA4000000
80 53 00 00
30 00 02 A1
00 00 00 30
A4 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.422Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 674 opaque = 0xA5000000
80 53 00 00
30 00 02 A2
00 00 00 30
A5 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.423Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 675 opaque = 0xA6000000
80 53 00 00
30 00 02 A3
00 00 00 30
A6 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.423Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 676 opaque = 0xA7000000
80 53 00 00
30 00 02 A4
00 00 00 30
A7 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.423Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 677 opaque = 0xA8000000
80 53 00 00
30 00 02 A5
00 00 00 30
A8 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.423Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 678 opaque = 0xA9000000
80 53 00 00
30 00 02 A6
00 00 00 30
A9 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.423Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 679 opaque = 0xAA000000
80 53 00 00
30 00 02 A7
00 00 00 30
AA 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.423Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 680 opaque = 0xAB000000
80 53 00 00
30 00 02 A8
00 00 00 30
AB 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.423Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 681 opaque = 0xAC000000
80 53 00 00
30 00 02 A9
00 00 00 30
AC 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.425Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x1000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
01 00 00 00
00 00 00 00
00 00 00 00
00 00 0C 6F
23 B7 77 E2
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.425Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x2000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
02 00 00 00
00 00 00 00
00 00 00 00
00 00 D5 A7
A6 53 16 0D
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.425Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x3000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
03 00 00 00
00 00 00 00
00 00 00 00
00 00 30 E4
DA 42 50 B6
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.425Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x4000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
04 00 00 00
00 00 00 00
00 00 00 00
00 00 7E 55
45 F8 6C 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.425Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x5000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
05 00 00 00
00 00 00 00
00 00 00 00
00 00 A5 0D
25 DC 55 75
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.425Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x6000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
06 00 00 00
00 00 00 00
00 00 00 00
00 00 2C 79
02 B8 6D 22
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.425Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x7000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
07 00 00 00
00 00 00 00
00 00 00 00
00 00 0B F5
71 F3 11 56
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.425Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x8000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
08 00 00 00
00 00 00 00
00 00 00 00
00 00 A1 FC
81 9A 36 D4
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.425Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x9000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
09 00 00 00
00 00 00 00
00 00 00 00
00 00 8F 7F
16 60 5D CE
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.425Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
0A 00 00 00
00 00 00 00
00 00 00 00
00 00 5F C6
49 D0 E2 01
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.426Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xD000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
0D 00 00 00
00 00 00 00
00 00 00 00
00 00 3E 7C
67 43 5C 3B
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.426Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xE000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
0E 00 00 00
00 00 00 00
00 00 00 00
00 00 2E 4E
73 CE A6 CA
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.426Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xF000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
0F 00 00 00
00 00 00 00
00 00 00 00
00 00 ED 2F
7A 59 A0 3E
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.426Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x10000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
10 00 00 00
00 00 00 00
00 00 00 00
00 00 81 05
2D 55 21 C8
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.426Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x11000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
11 00 00 00
00 00 00 00
00 00 00 00
00 00 6A D4
23 C2 4B 5B
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.426Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x12000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
12 00 00 00
00 00 00 00
00 00 00 00
00 00 BF 4D
DD 6D D7 71
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.427Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x13000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
13 00 00 00
00 00 00 00
00 00 00 00
00 00 3A 67
B8 45 B1 A1
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.427Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x14000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
14 00 00 00
00 00 00 00
00 00 00 00
00 00 70 66
8E 86 C4 D1
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.427Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x15000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
15 00 00 00
00 00 00 00
00 00 00 00
00 00 F6 F3
AE 69 5E 89
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.427Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x16000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
16 00 00 00
00 00 00 00
00 00 00 00
00 00 BF 94
CD 09 AB 5F
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.427Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x17000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
17 00 00 00
00 00 00 00
00 00 00 00
00 00 64 8E
A9 F1 B0 70
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.427Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x18000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
18 00 00 00
00 00 00 00
00 00 00 00
00 00 22 4B
8E 08 95 26
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.427Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x19000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
19 00 00 00
00 00 00 00
00 00 00 00
00 00 C5 2D
3A 1C 1B 68
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.427Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x1A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
1A 00 00 00
00 00 00 00
00 00 00 00
00 00 7D E5
4D 49 EF 8D
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.427Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x1B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
1B 00 00 00
00 00 00 00
00 00 00 00
00 00 3A D8
5C EA 4B 6E
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.428Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x1C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
1C 00 00 00
00 00 00 00
00 00 00 00
00 00 E7 AC
A2 72 A1 69
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.429Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x1D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
1D 00 00 00
00 00 00 00
00 00 00 00
00 00 E3 D3
C4 1F 44 CB
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.429Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x1E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
1E 00 00 00
00 00 00 00
00 00 00 00
00 00 99 1C
E4 DE 02 FE
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.429Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x1F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
1F 00 00 00
00 00 00 00
00 00 00 00
00 00 68 C5
A1 70 6C 3A
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.429Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x20000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
20 00 00 00
00 00 00 00
00 00 00 00
00 00 C4 39
44 5A B3 3D
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.429Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x21000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
21 00 00 00
00 00 00 00
00 00 00 00
00 00 45 07
86 3F C4 98
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.430Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x22000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
22 00 00 00
00 00 00 00
00 00 00 00
00 00 DF AE
FD 46 DA 6B
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.430Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x23000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
23 00 00 00
00 00 00 00
00 00 00 00
00 00 92 27
F4 D8 7F 5A
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.430Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x24000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
24 00 00 00
00 00 00 00
00 00 00 00
00 00 1D 09
C7 9D B2 07
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.430Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x25000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
25 00 00 00
00 00 00 00
00 00 00 00
00 00 75 C7
7B 42 D6 8C
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.430Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x26000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
26 00 00 00
00 00 00 00
00 00 00 00
00 00 73 19
FB C3 DE 5E
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.430Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x27000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
27 00 00 00
00 00 00 00
00 00 00 00
00 00 98 FA
2A 84 4D 4F
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.430Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x28000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
28 00 00 00
00 00 00 00
00 00 00 00
00 00 71 A1
F8 43 2D 78
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.430Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x29000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
29 00 00 00
00 00 00 00
00 00 00 00
00 00 6D 28
4B 61 EB 1E
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.430Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x2A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
2A 00 00 00
00 00 00 00
00 00 00 00
00 00 81 E1
3D 4B 04 6E
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.430Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x2B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
2B 00 00 00
00 00 00 00
00 00 00 00
00 00 AA B0
8B 8C F9 5F
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.430Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x2C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
2C 00 00 00
00 00 00 00
00 00 00 00
00 00 16 D3
88 74 6D 40
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.431Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x2D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
2D 00 00 00
00 00 00 00
00 00 00 00
00 00 7A 40
99 8E BA 11
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.431Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x2E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
2E 00 00 00
00 00 00 00
00 00 00 00
00 00 46 D6
B3 BA 06 8E
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.431Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x2F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
2F 00 00 00
00 00 00 00
00 00 00 00
00 00 C7 43
53 91 3C D3
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.431Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x30000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
30 00 00 00
00 00 00 00
00 00 00 00
00 00 49 D6
57 DF 6B 95
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.431Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x31000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
31 00 00 00
00 00 00 00
00 00 00 00
00 00 2C C3
24 A1 4B 8C
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.431Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x32000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
32 00 00 00
00 00 00 00
00 00 00 00
00 00 D4 DB
D7 B5 E2 AD
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.431Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x33000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
33 00 00 00
00 00 00 00
00 00 00 00
00 00 A1 DC
DC 86 C8 55
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.431Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x34000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
34 00 00 00
00 00 00 00
00 00 00 00
00 00 0B 8C
B7 D0 0E 5E
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.431Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x35000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
35 00 00 00
00 00 00 00
00 00 00 00
00 00 18 C7
E3 56 ED 77
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.432Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x36000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
36 00 00 00
00 00 00 00
00 00 00 00
00 00 D2 69
ED CA F2 18
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.432Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x37000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
37 00 00 00
00 00 00 00
00 00 00 00
00 00 4C A7
D7 5D 64 C2
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.432Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x38000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
38 00 00 00
00 00 00 00
00 00 00 00
00 00 FE E5
37 42 CA 52
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.432Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x39000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
39 00 00 00
00 00 00 00
00 00 00 00
00 00 6E FE
65 94 2D 18
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.432Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x3A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
3A 00 00 00
00 00 00 00
00 00 00 00
00 00 A6 B9
DE 19 1E 46
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.433Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x3B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
3B 00 00 00
00 00 00 00
00 00 00 00
00 00 62 84
A4 97 39 3D
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.433Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x3C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
3C 00 00 00
00 00 00 00
00 00 00 00
00 00 06 E8
BD E6 6F B5
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.433Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x3D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
3D 00 00 00
00 00 00 00
00 00 00 00
00 00 17 02
3E D8 28 1F
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.433Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x3E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
3E 00 00 00
00 00 00 00
00 00 00 00
00 00 C1 77
7F DD 2A E8
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.433Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x3F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
3F 00 00 00
00 00 00 00
00 00 00 00
00 00 94 CB
E3 A0 03 6C
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.433Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x40000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
40 00 00 00
00 00 00 00
00 00 00 00
00 00 C6 EB
BA 06 23 D5
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.433Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x41000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
41 00 00 00
00 00 00 00
00 00 00 00
00 00 13 EE
B6 8B A3 0F
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.434Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x42000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
42 00 00 00
00 00 00 00
00 00 00 00
00 00 89 30
47 EF D8 6B
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.434Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x43000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
43 00 00 00
00 00 00 00
00 00 00 00
00 00 77 7E
75 A5 B2 64
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.434Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x44000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
44 00 00 00
00 00 00 00
00 00 00 00
00 00 26 BC
09 64 C1 E8
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.434Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x45000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
45 00 00 00
00 00 00 00
00 00 00 00
00 00 74 29
F8 A6 BF EE
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.434Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x46000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
46 00 00 00
00 00 00 00
00 00 00 00
00 00 15 DA
7A 20 52 C9
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.434Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x47000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
47 00 00 00
00 00 00 00
00 00 00 00
00 00 D5 F1
A6 BB 4A 71
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.434Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x48000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
48 00 00 00
00 00 00 00
00 00 00 00
00 00 6F 13
44 B9 C6 C6
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.434Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x49000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
49 00 00 00
00 00 00 00
00 00 00 00
00 00 9D 34
3C 55 57 8A
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.434Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x4A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
4A 00 00 00
00 00 00 00
00 00 00 00
00 00 A2 2E
9C 14 01 C8
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.434Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x4B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
4B 00 00 00
00 00 00 00
00 00 00 00
00 00 95 51
03 68 1B CC
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.434Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x4C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
4C 00 00 00
00 00 00 00
00 00 00 00
00 00 2E BE
60 E9 B3 45
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.434Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x4D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
4D 00 00 00
00 00 00 00
00 00 00 00
00 00 A2 25
A2 5E D1 98
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.434Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x4E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
4E 00 00 00
00 00 00 00
00 00 00 00
00 00 15 37
CB 61 09 1F
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.434Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x4F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
4F 00 00 00
00 00 00 00
00 00 00 00
00 00 47 A0
B3 7D E5 EA
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.434Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x50000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
50 00 00 00
00 00 00 00
00 00 00 00
00 00 13 71
70 49 0C 8B
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.435Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x51000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
51 00 00 00
00 00 00 00
00 00 00 00
00 00 01 31
71 F9 93 FE
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.435Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x52000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
52 00 00 00
00 00 00 00
00 00 00 00
00 00 10 3E
2B 49 9F 23
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.435Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x200 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 00
00 00 00 00
00 00 00 00
00 00 00 01

[ns_server:debug,2025-05-15T18:47:52.435Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x53000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
53 00 00 00
00 00 00 00
00 00 00 00
00 00 9E 23
C8 2B 00 43
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.435Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 512, stream opaque = 0x1000000
[ns_server:debug,2025-05-15T18:47:52.435Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x54000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
54 00 00 00
00 00 00 00
00 00 00 00
00 00 9D A8
B3 E8 B9 3A
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.435Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x201 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 01
00 00 00 00
00 00 00 00
00 00 00 02

[ns_server:debug,2025-05-15T18:47:52.435Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x55000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
55 00 00 00
00 00 00 00
00 00 00 00
00 00 0A 10
1A EA 13 D7
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.435Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 513, stream opaque = 0x2000000
[ns_server:debug,2025-05-15T18:47:52.435Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x56000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
56 00 00 00
00 00 00 00
00 00 00 00
00 00 E9 BC
76 E8 FE FE
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.435Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x202 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 02
00 00 00 00
00 00 00 00
00 00 00 03

[rebalance:debug,2025-05-15T18:47:52.435Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 514, stream opaque = 0x3000000
[ns_server:debug,2025-05-15T18:47:52.435Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x203 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 03
00 00 00 00
00 00 00 00
00 00 00 04

[rebalance:debug,2025-05-15T18:47:52.435Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 515, stream opaque = 0x4000000
[ns_server:debug,2025-05-15T18:47:52.435Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x204 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 04
00 00 00 00
00 00 00 00
00 00 00 05

[rebalance:debug,2025-05-15T18:47:52.435Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 516, stream opaque = 0x5000000
[ns_server:debug,2025-05-15T18:47:52.435Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x205 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 05
00 00 00 00
00 00 00 00
00 00 00 06

[rebalance:debug,2025-05-15T18:47:52.435Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 517, stream opaque = 0x6000000
[ns_server:debug,2025-05-15T18:47:52.435Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x57000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
57 00 00 00
00 00 00 00
00 00 00 00
00 00 97 78
EC 0D E0 DC
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.435Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x206 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 06
00 00 00 00
00 00 00 00
00 00 00 07

[rebalance:debug,2025-05-15T18:47:52.435Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 518, stream opaque = 0x7000000
[ns_server:debug,2025-05-15T18:47:52.435Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x58000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
58 00 00 00
00 00 00 00
00 00 00 00
00 00 0B 0E
7F D7 06 73
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.436Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x207 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 07
00 00 00 00
00 00 00 00
00 00 00 08

[ns_server:debug,2025-05-15T18:47:52.436Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x59000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
59 00 00 00
00 00 00 00
00 00 00 00
00 00 B7 8A
F5 A6 69 95
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.436Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 519, stream opaque = 0x8000000
[ns_server:debug,2025-05-15T18:47:52.436Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x5A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
5A 00 00 00
00 00 00 00
00 00 00 00
00 00 E2 BB
69 28 7D F9
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.436Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x208 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 08
00 00 00 00
00 00 00 00
00 00 00 09

[ns_server:debug,2025-05-15T18:47:52.436Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x5B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
5B 00 00 00
00 00 00 00
00 00 00 00
00 00 CE 15
35 98 9E DD
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.436Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 520, stream opaque = 0x9000000
[ns_server:debug,2025-05-15T18:47:52.436Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x5C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
5C 00 00 00
00 00 00 00
00 00 00 00
00 00 13 AB
93 E5 EB 24
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.436Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x209 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 09
00 00 00 00
00 00 00 00
00 00 00 0A

[rebalance:debug,2025-05-15T18:47:52.436Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 521, stream opaque = 0xA000000
[ns_server:debug,2025-05-15T18:47:52.436Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x5D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
5D 00 00 00
00 00 00 00
00 00 00 00
00 00 D6 BD
C0 0F 67 5C
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.436Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x20A status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 0A
00 00 00 00
00 00 00 00
00 00 00 0D

[rebalance:debug,2025-05-15T18:47:52.436Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 522, stream opaque = 0xD000000
[ns_server:debug,2025-05-15T18:47:52.436Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x20B status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 0B
00 00 00 00
00 00 00 00
00 00 00 0E

[ns_server:debug,2025-05-15T18:47:52.436Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x5E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
5E 00 00 00
00 00 00 00
00 00 00 00
00 00 40 82
03 A3 1C E4
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.436Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 523, stream opaque = 0xE000000
[ns_server:debug,2025-05-15T18:47:52.437Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x5F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
5F 00 00 00
00 00 00 00
00 00 00 00
00 00 52 EB
06 89 31 69
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.437Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x20C status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 0C
00 00 00 00
00 00 00 00
00 00 00 0F

[rebalance:debug,2025-05-15T18:47:52.437Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 524, stream opaque = 0xF000000
[ns_server:debug,2025-05-15T18:47:52.437Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x60000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
60 00 00 00
00 00 00 00
00 00 00 00
00 00 4F 00
BF 2E 35 E8
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.437Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x20D status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 0D
00 00 00 00
00 00 00 00
00 00 00 10

[ns_server:debug,2025-05-15T18:47:52.437Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x61000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
61 00 00 00
00 00 00 00
00 00 00 00
00 00 E8 98
27 18 4C 91
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.437Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x62000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
62 00 00 00
00 00 00 00
00 00 00 00
00 00 DF 19
90 E8 19 67
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.437Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x63000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
63 00 00 00
00 00 00 00
00 00 00 00
00 00 E1 C3
5C 47 0E BD
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.437Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 525, stream opaque = 0x10000000
[ns_server:debug,2025-05-15T18:47:52.437Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x64000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
64 00 00 00
00 00 00 00
00 00 00 00
00 00 34 52
89 FE EB 39
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.438Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x65000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
65 00 00 00
00 00 00 00
00 00 00 00
00 00 58 AB
65 0A CD D3
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.438Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x20E status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 0E
00 00 00 00
00 00 00 00
00 00 00 11

[ns_server:debug,2025-05-15T18:47:52.438Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x66000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
66 00 00 00
00 00 00 00
00 00 00 00
00 00 25 A6
64 29 41 45
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.438Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x67000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
67 00 00 00
00 00 00 00
00 00 00 00
00 00 3C AE
B3 C0 9D C2
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.438Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x68000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
68 00 00 00
00 00 00 00
00 00 00 00
00 00 44 C5
61 40 39 8F
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.438Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 526, stream opaque = 0x11000000
[ns_server:debug,2025-05-15T18:47:52.438Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x20F status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 0F
00 00 00 00
00 00 00 00
00 00 00 12

[rebalance:debug,2025-05-15T18:47:52.438Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 527, stream opaque = 0x12000000
[ns_server:debug,2025-05-15T18:47:52.438Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x210 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 10
00 00 00 00
00 00 00 00
00 00 00 13

[ns_server:debug,2025-05-15T18:47:52.438Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x69000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
69 00 00 00
00 00 00 00
00 00 00 00
00 00 7B 93
E9 16 B8 BD
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.438Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 528, stream opaque = 0x13000000
[ns_server:debug,2025-05-15T18:47:52.438Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x6A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
6A 00 00 00
00 00 00 00
00 00 00 00
00 00 5B 9D
5E 91 CB 41
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.438Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x6B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
6B 00 00 00
00 00 00 00
00 00 00 00
00 00 0F 5E
DD 5C 53 16
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.439Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x6C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
6C 00 00 00
00 00 00 00
00 00 00 00
00 00 6C 41
70 EA A7 9C
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.439Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x6D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
6D 00 00 00
00 00 00 00
00 00 00 00
00 00 9C 2B
9E 34 0C 91
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.439Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x6E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
6E 00 00 00
00 00 00 00
00 00 00 00
00 00 C3 AD
54 07 18 E7
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.438Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x211 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 11
00 00 00 00
00 00 00 00
00 00 00 14

[rebalance:debug,2025-05-15T18:47:52.439Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 529, stream opaque = 0x14000000
[ns_server:debug,2025-05-15T18:47:52.439Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x212 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 12
00 00 00 00
00 00 00 00
00 00 00 15

[rebalance:debug,2025-05-15T18:47:52.439Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 530, stream opaque = 0x15000000
[ns_server:debug,2025-05-15T18:47:52.439Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x213 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 13
00 00 00 00
00 00 00 00
00 00 00 16

[rebalance:debug,2025-05-15T18:47:52.439Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 531, stream opaque = 0x16000000
[ns_server:debug,2025-05-15T18:47:52.439Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x214 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 14
00 00 00 00
00 00 00 00
00 00 00 17

[rebalance:debug,2025-05-15T18:47:52.439Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 532, stream opaque = 0x17000000
[ns_server:debug,2025-05-15T18:47:52.439Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x215 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 15
00 00 00 00
00 00 00 00
00 00 00 18

[rebalance:debug,2025-05-15T18:47:52.439Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 533, stream opaque = 0x18000000
[ns_server:debug,2025-05-15T18:47:52.439Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x216 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 16
00 00 00 00
00 00 00 00
00 00 00 19

[rebalance:debug,2025-05-15T18:47:52.439Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 534, stream opaque = 0x19000000
[ns_server:debug,2025-05-15T18:47:52.439Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x217 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 17
00 00 00 00
00 00 00 00
00 00 00 1A

[rebalance:debug,2025-05-15T18:47:52.439Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 535, stream opaque = 0x1A000000
[ns_server:debug,2025-05-15T18:47:52.439Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x218 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 18
00 00 00 00
00 00 00 00
00 00 00 1B

[rebalance:debug,2025-05-15T18:47:52.439Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 536, stream opaque = 0x1B000000
[ns_server:debug,2025-05-15T18:47:52.439Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x219 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 19
00 00 00 00
00 00 00 00
00 00 00 1C

[rebalance:debug,2025-05-15T18:47:52.439Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 537, stream opaque = 0x1C000000
[ns_server:debug,2025-05-15T18:47:52.439Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x21A status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 1A
00 00 00 00
00 00 00 00
00 00 00 1D

[rebalance:debug,2025-05-15T18:47:52.439Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 538, stream opaque = 0x1D000000
[ns_server:debug,2025-05-15T18:47:52.439Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x21B status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 1B
00 00 00 00
00 00 00 00
00 00 00 1E

[rebalance:debug,2025-05-15T18:47:52.439Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 539, stream opaque = 0x1E000000
[ns_server:debug,2025-05-15T18:47:52.440Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x21C status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 1C
00 00 00 00
00 00 00 00
00 00 00 1F

[rebalance:debug,2025-05-15T18:47:52.440Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 540, stream opaque = 0x1F000000
[ns_server:debug,2025-05-15T18:47:52.440Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x21D status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 1D
00 00 00 00
00 00 00 00
00 00 00 20

[rebalance:debug,2025-05-15T18:47:52.440Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 541, stream opaque = 0x20000000
[ns_server:debug,2025-05-15T18:47:52.440Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x21E status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 1E
00 00 00 00
00 00 00 00
00 00 00 21

[rebalance:debug,2025-05-15T18:47:52.440Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 542, stream opaque = 0x21000000
[ns_server:debug,2025-05-15T18:47:52.440Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x21F status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 1F
00 00 00 00
00 00 00 00
00 00 00 22

[rebalance:debug,2025-05-15T18:47:52.440Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 543, stream opaque = 0x22000000
[ns_server:debug,2025-05-15T18:47:52.440Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x220 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 20
00 00 00 00
00 00 00 00
00 00 00 23

[rebalance:debug,2025-05-15T18:47:52.440Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 544, stream opaque = 0x23000000
[ns_server:debug,2025-05-15T18:47:52.440Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x221 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 21
00 00 00 00
00 00 00 00
00 00 00 24

[rebalance:debug,2025-05-15T18:47:52.440Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 545, stream opaque = 0x24000000
[ns_server:debug,2025-05-15T18:47:52.440Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x222 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 22
00 00 00 00
00 00 00 00
00 00 00 25

[rebalance:debug,2025-05-15T18:47:52.440Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 546, stream opaque = 0x25000000
[ns_server:debug,2025-05-15T18:47:52.440Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x223 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 23
00 00 00 00
00 00 00 00
00 00 00 26

[rebalance:debug,2025-05-15T18:47:52.440Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 547, stream opaque = 0x26000000
[ns_server:debug,2025-05-15T18:47:52.440Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x224 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 24
00 00 00 00
00 00 00 00
00 00 00 27

[rebalance:debug,2025-05-15T18:47:52.440Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 548, stream opaque = 0x27000000
[ns_server:debug,2025-05-15T18:47:52.440Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x225 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 25
00 00 00 00
00 00 00 00
00 00 00 28

[rebalance:debug,2025-05-15T18:47:52.440Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 549, stream opaque = 0x28000000
[ns_server:debug,2025-05-15T18:47:52.440Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x226 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 26
00 00 00 00
00 00 00 00
00 00 00 29

[rebalance:debug,2025-05-15T18:47:52.440Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 550, stream opaque = 0x29000000
[ns_server:debug,2025-05-15T18:47:52.440Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x227 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 27
00 00 00 00
00 00 00 00
00 00 00 2A

[rebalance:debug,2025-05-15T18:47:52.440Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 551, stream opaque = 0x2A000000
[ns_server:debug,2025-05-15T18:47:52.440Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x228 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 28
00 00 00 00
00 00 00 00
00 00 00 2B

[rebalance:debug,2025-05-15T18:47:52.440Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 552, stream opaque = 0x2B000000
[ns_server:debug,2025-05-15T18:47:52.440Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x229 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 29
00 00 00 00
00 00 00 00
00 00 00 2C

[rebalance:debug,2025-05-15T18:47:52.440Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 553, stream opaque = 0x2C000000
[ns_server:debug,2025-05-15T18:47:52.440Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x22A status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 2A
00 00 00 00
00 00 00 00
00 00 00 2D

[rebalance:debug,2025-05-15T18:47:52.440Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 554, stream opaque = 0x2D000000
[ns_server:debug,2025-05-15T18:47:52.441Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x22B status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 2B
00 00 00 00
00 00 00 00
00 00 00 2E

[rebalance:debug,2025-05-15T18:47:52.441Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 555, stream opaque = 0x2E000000
[ns_server:debug,2025-05-15T18:47:52.441Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x22C status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 2C
00 00 00 00
00 00 00 00
00 00 00 2F

[rebalance:debug,2025-05-15T18:47:52.441Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 556, stream opaque = 0x2F000000
[ns_server:debug,2025-05-15T18:47:52.441Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x22D status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 2D
00 00 00 00
00 00 00 00
00 00 00 30

[rebalance:debug,2025-05-15T18:47:52.441Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 557, stream opaque = 0x30000000
[ns_server:debug,2025-05-15T18:47:52.441Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x22E status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 2E
00 00 00 00
00 00 00 00
00 00 00 31

[rebalance:debug,2025-05-15T18:47:52.441Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 558, stream opaque = 0x31000000
[ns_server:debug,2025-05-15T18:47:52.441Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x22F status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 2F
00 00 00 00
00 00 00 00
00 00 00 32

[rebalance:debug,2025-05-15T18:47:52.441Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 559, stream opaque = 0x32000000
[ns_server:debug,2025-05-15T18:47:52.441Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x230 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 30
00 00 00 00
00 00 00 00
00 00 00 33

[rebalance:debug,2025-05-15T18:47:52.441Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 560, stream opaque = 0x33000000
[ns_server:debug,2025-05-15T18:47:52.441Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x231 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 31
00 00 00 00
00 00 00 00
00 00 00 34

[rebalance:debug,2025-05-15T18:47:52.441Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 561, stream opaque = 0x34000000
[ns_server:debug,2025-05-15T18:47:52.441Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x232 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 32
00 00 00 00
00 00 00 00
00 00 00 35

[rebalance:debug,2025-05-15T18:47:52.441Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 562, stream opaque = 0x35000000
[ns_server:debug,2025-05-15T18:47:52.441Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x233 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 33
00 00 00 00
00 00 00 00
00 00 00 36

[rebalance:debug,2025-05-15T18:47:52.441Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 563, stream opaque = 0x36000000
[ns_server:debug,2025-05-15T18:47:52.441Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x234 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 34
00 00 00 00
00 00 00 00
00 00 00 37

[rebalance:debug,2025-05-15T18:47:52.441Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 564, stream opaque = 0x37000000
[ns_server:debug,2025-05-15T18:47:52.441Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x235 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 35
00 00 00 00
00 00 00 00
00 00 00 38

[rebalance:debug,2025-05-15T18:47:52.441Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 565, stream opaque = 0x38000000
[ns_server:debug,2025-05-15T18:47:52.441Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x236 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 36
00 00 00 00
00 00 00 00
00 00 00 39

[rebalance:debug,2025-05-15T18:47:52.441Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 566, stream opaque = 0x39000000
[ns_server:debug,2025-05-15T18:47:52.441Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x237 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 37
00 00 00 00
00 00 00 00
00 00 00 3A

[rebalance:debug,2025-05-15T18:47:52.441Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 567, stream opaque = 0x3A000000
[ns_server:debug,2025-05-15T18:47:52.442Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x6F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
6F 00 00 00
00 00 00 00
00 00 00 00
00 00 80 64
03 2A EB 3B
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.442Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x70000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
70 00 00 00
00 00 00 00
00 00 00 00
00 00 60 F7
E9 77 0D 43
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.442Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x71000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
71 00 00 00
00 00 00 00
00 00 00 00
00 00 1D 64
75 C4 9C E8
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.442Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x72000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
72 00 00 00
00 00 00 00
00 00 00 00
00 00 0E 04
3B 06 A8 F8
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.442Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x73000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
73 00 00 00
00 00 00 00
00 00 00 00
00 00 42 CB
9F 3D 7E 77
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.442Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x74000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
74 00 00 00
00 00 00 00
00 00 00 00
00 00 44 A7
EA 65 E4 CD
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.442Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x75000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
75 00 00 00
00 00 00 00
00 00 00 00
00 00 A9 93
BA 91 B2 41
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.442Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x76000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
76 00 00 00
00 00 00 00
00 00 00 00
00 00 57 88
DB DC 48 9E
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.443Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x77000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
77 00 00 00
00 00 00 00
00 00 00 00
00 00 56 C6
7C 94 60 16
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.443Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x78000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
78 00 00 00
00 00 00 00
00 00 00 00
00 00 F1 6C
E4 7C 6D 65
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.443Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x79000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
79 00 00 00
00 00 00 00
00 00 00 00
00 00 A9 B9
3F D2 9E E9
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.443Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x7A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
7A 00 00 00
00 00 00 00
00 00 00 00
00 00 A8 DF
FB FB 77 5A
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.443Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x7B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
7B 00 00 00
00 00 00 00
00 00 00 00
00 00 5C 75
40 6B CE A2
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.443Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x7C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
7C 00 00 00
00 00 00 00
00 00 00 00
00 00 18 D3
6D 37 C8 91
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.443Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x7D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
7D 00 00 00
00 00 00 00
00 00 00 00
00 00 5B EA
7C F1 41 12
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.443Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x7E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
7E 00 00 00
00 00 00 00
00 00 00 00
00 00 4A 5E
AC 22 8C F0
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.443Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x7F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
7F 00 00 00
00 00 00 00
00 00 00 00
00 00 3B 63
71 DD F0 28
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.443Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x80000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
80 00 00 00
00 00 00 00
00 00 00 00
00 00 C7 E8
DF CF B9 1E
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.444Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x81000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
81 00 00 00
00 00 00 00
00 00 00 00
00 00 FC 53
F5 77 B1 A6
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.444Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x82000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
82 00 00 00
00 00 00 00
00 00 00 00
00 00 6D 13
06 44 43 76
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.444Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x83000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
83 00 00 00
00 00 00 00
00 00 00 00
00 00 E4 94
0B 42 61 6E
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.444Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x84000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
84 00 00 00
00 00 00 00
00 00 00 00
00 00 08 19
2D CB 62 57
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.444Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x85000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
85 00 00 00
00 00 00 00
00 00 00 00
00 00 9F 3F
5D CA 05 E0
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.444Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x86000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
86 00 00 00
00 00 00 00
00 00 00 00
00 00 3B 47
A7 81 1E 83
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.444Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x87000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
87 00 00 00
00 00 00 00
00 00 00 00
00 00 12 22
99 E2 86 E3
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.444Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x88000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
88 00 00 00
00 00 00 00
00 00 00 00
00 00 00 92
0A 0B B1 9B
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.444Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x89000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
89 00 00 00
00 00 00 00
00 00 00 00
00 00 F3 59
C1 57 A0 85
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.444Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x8A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
8A 00 00 00
00 00 00 00
00 00 00 00
00 00 3F 4B
F2 DB 32 21
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.444Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x8B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
8B 00 00 00
00 00 00 00
00 00 00 00
00 00 CD 02
70 E5 06 DC
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.444Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x8C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
8C 00 00 00
00 00 00 00
00 00 00 00
00 00 6E B8
1B 36 47 9C
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.444Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x8D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
8D 00 00 00
00 00 00 00
00 00 00 00
00 00 E6 1A
A9 28 44 76
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.444Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x8E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
8E 00 00 00
00 00 00 00
00 00 00 00
00 00 39 9B
F3 C4 2C 25
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.444Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x8F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
8F 00 00 00
00 00 00 00
00 00 00 00
00 00 1E 70
A7 6E C7 E1
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.444Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x90000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
90 00 00 00
00 00 00 00
00 00 00 00
00 00 73 D6
14 6F AB 27
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.444Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x91000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
91 00 00 00
00 00 00 00
00 00 00 00
00 00 E2 80
29 0E 7F 0F
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.444Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x92000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
92 00 00 00
00 00 00 00
00 00 00 00
00 00 B4 25
0D 28 6D 99
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.444Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x93000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
93 00 00 00
00 00 00 00
00 00 00 00
00 00 03 4A
00 7A 08 82
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.445Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x94000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
94 00 00 00
00 00 00 00
00 00 00 00
00 00 99 92
93 5E FB 88
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.445Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x95000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
95 00 00 00
00 00 00 00
00 00 00 00
00 00 FC 5F
1D 51 0E 6E
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.445Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x96000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
96 00 00 00
00 00 00 00
00 00 00 00
00 00 59 28
1D 97 FE 43
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.445Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x97000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
97 00 00 00
00 00 00 00
00 00 00 00
00 00 D3 6B
E9 9C 6B FC
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.445Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x98000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
98 00 00 00
00 00 00 00
00 00 00 00
00 00 38 9E
0D 6A 0E B7
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.445Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x99000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
99 00 00 00
00 00 00 00
00 00 00 00
00 00 4E EC
A9 E8 1A CA
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.445Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x9A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
9A 00 00 00
00 00 00 00
00 00 00 00
00 00 DA 12
D2 71 31 D5
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.445Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x9B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
9B 00 00 00
00 00 00 00
00 00 00 00
00 00 17 64
18 9E 5A AE
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.445Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x9C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
9C 00 00 00
00 00 00 00
00 00 00 00
00 00 C7 A5
B7 56 00 2B
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.445Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x9D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
9D 00 00 00
00 00 00 00
00 00 00 00
00 00 33 87
25 E9 3D D9
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.445Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x9E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
9E 00 00 00
00 00 00 00
00 00 00 00
00 00 5C EF
A7 40 A5 72
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.445Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x9F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
9F 00 00 00
00 00 00 00
00 00 00 00
00 00 53 94
DE EC 15 06
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.445Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA0000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A0 00 00 00
00 00 00 00
00 00 00 00
00 00 30 6D
2B 7F D3 09
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.445Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA1000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A1 00 00 00
00 00 00 00
00 00 00 00
00 00 EC F2
17 9E 79 D6
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.446Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA2000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A2 00 00 00
00 00 00 00
00 00 00 00
00 00 E6 D9
B1 7D D5 32
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.446Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA3000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A3 00 00 00
00 00 00 00
00 00 00 00
00 00 FC 0A
F3 00 19 64
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.446Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA4000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A4 00 00 00
00 00 00 00
00 00 00 00
00 00 22 36
F7 87 57 39
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.446Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA5000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A5 00 00 00
00 00 00 00
00 00 00 00
00 00 86 B9
FF 15 E6 79
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.446Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA6000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A6 00 00 00
00 00 00 00
00 00 00 00
00 00 5A 5E
67 6A 80 AF
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.446Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA7000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A7 00 00 00
00 00 00 00
00 00 00 00
00 00 D8 D3
94 65 1E B9
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.446Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA8000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A8 00 00 00
00 00 00 00
00 00 00 00
00 00 9B 21
F4 7D 3E E1
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.446Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA9000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A9 00 00 00
00 00 00 00
00 00 00 00
00 00 10 1C
AD E3 84 B7
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.446Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xAA000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
AA 00 00 00
00 00 00 00
00 00 00 00
00 00 5F CA
A2 6C 2B F2
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.446Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xAB000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
AB 00 00 00
00 00 00 00
00 00 00 00
00 00 32 77
5D 8B 21 E9
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.446Z,ns_1@db3.lan:<0.4236.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xAC000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
AC 00 00 00
00 00 00 00
00 00 00 00
00 00 74 6F
F4 04 5C 64
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.449Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x238 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 38
00 00 00 00
00 00 00 00
00 00 00 3B

[rebalance:debug,2025-05-15T18:47:52.449Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 568, stream opaque = 0x3B000000
[ns_server:debug,2025-05-15T18:47:52.450Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x239 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 39
00 00 00 00
00 00 00 00
00 00 00 3C

[rebalance:debug,2025-05-15T18:47:52.450Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 569, stream opaque = 0x3C000000
[ns_server:debug,2025-05-15T18:47:52.450Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x23A status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 3A
00 00 00 00
00 00 00 00
00 00 00 3D

[rebalance:debug,2025-05-15T18:47:52.450Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 570, stream opaque = 0x3D000000
[ns_server:debug,2025-05-15T18:47:52.450Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x23B status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 3B
00 00 00 00
00 00 00 00
00 00 00 3E

[rebalance:debug,2025-05-15T18:47:52.450Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 571, stream opaque = 0x3E000000
[ns_server:debug,2025-05-15T18:47:52.450Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x23C status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 3C
00 00 00 00
00 00 00 00
00 00 00 3F

[rebalance:debug,2025-05-15T18:47:52.451Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 572, stream opaque = 0x3F000000
[ns_server:debug,2025-05-15T18:47:52.451Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x23D status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 3D
00 00 00 00
00 00 00 00
00 00 00 40

[rebalance:debug,2025-05-15T18:47:52.451Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 573, stream opaque = 0x40000000
[ns_server:debug,2025-05-15T18:47:52.451Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x23E status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 3E
00 00 00 00
00 00 00 00
00 00 00 41

[rebalance:debug,2025-05-15T18:47:52.451Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 574, stream opaque = 0x41000000
[ns_server:debug,2025-05-15T18:47:52.451Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x23F status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 3F
00 00 00 00
00 00 00 00
00 00 00 42

[rebalance:debug,2025-05-15T18:47:52.451Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 575, stream opaque = 0x42000000
[ns_server:debug,2025-05-15T18:47:52.451Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x240 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 40
00 00 00 00
00 00 00 00
00 00 00 43

[rebalance:debug,2025-05-15T18:47:52.451Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 576, stream opaque = 0x43000000
[ns_server:debug,2025-05-15T18:47:52.451Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x241 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 41
00 00 00 00
00 00 00 00
00 00 00 44

[rebalance:debug,2025-05-15T18:47:52.451Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 577, stream opaque = 0x44000000
[ns_server:debug,2025-05-15T18:47:52.451Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x242 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 42
00 00 00 00
00 00 00 00
00 00 00 45

[rebalance:debug,2025-05-15T18:47:52.451Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 578, stream opaque = 0x45000000
[ns_server:debug,2025-05-15T18:47:52.451Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x243 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 43
00 00 00 00
00 00 00 00
00 00 00 46

[rebalance:debug,2025-05-15T18:47:52.451Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 579, stream opaque = 0x46000000
[ns_server:debug,2025-05-15T18:47:52.451Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x244 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 44
00 00 00 00
00 00 00 00
00 00 00 47

[rebalance:debug,2025-05-15T18:47:52.451Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 580, stream opaque = 0x47000000
[ns_server:debug,2025-05-15T18:47:52.451Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x245 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 45
00 00 00 00
00 00 00 00
00 00 00 48

[rebalance:debug,2025-05-15T18:47:52.451Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 581, stream opaque = 0x48000000
[ns_server:debug,2025-05-15T18:47:52.451Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x246 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 46
00 00 00 00
00 00 00 00
00 00 00 49

[rebalance:debug,2025-05-15T18:47:52.451Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 582, stream opaque = 0x49000000
[ns_server:debug,2025-05-15T18:47:52.451Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x247 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 47
00 00 00 00
00 00 00 00
00 00 00 4A

[rebalance:debug,2025-05-15T18:47:52.451Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 583, stream opaque = 0x4A000000
[ns_server:debug,2025-05-15T18:47:52.451Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x248 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 48
00 00 00 00
00 00 00 00
00 00 00 4B

[rebalance:debug,2025-05-15T18:47:52.451Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 584, stream opaque = 0x4B000000
[ns_server:debug,2025-05-15T18:47:52.451Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x249 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 49
00 00 00 00
00 00 00 00
00 00 00 4C

[rebalance:debug,2025-05-15T18:47:52.452Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 585, stream opaque = 0x4C000000
[ns_server:debug,2025-05-15T18:47:52.452Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x24A status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 4A
00 00 00 00
00 00 00 00
00 00 00 4D

[rebalance:debug,2025-05-15T18:47:52.452Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 586, stream opaque = 0x4D000000
[ns_server:debug,2025-05-15T18:47:52.452Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x24B status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 4B
00 00 00 00
00 00 00 00
00 00 00 4E

[rebalance:debug,2025-05-15T18:47:52.452Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 587, stream opaque = 0x4E000000
[ns_server:debug,2025-05-15T18:47:52.452Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x24C status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 4C
00 00 00 00
00 00 00 00
00 00 00 4F

[rebalance:debug,2025-05-15T18:47:52.452Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 588, stream opaque = 0x4F000000
[ns_server:debug,2025-05-15T18:47:52.452Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x24D status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 4D
00 00 00 00
00 00 00 00
00 00 00 50

[rebalance:debug,2025-05-15T18:47:52.452Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 589, stream opaque = 0x50000000
[ns_server:debug,2025-05-15T18:47:52.453Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x24E status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 4E
00 00 00 00
00 00 00 00
00 00 00 51

[rebalance:debug,2025-05-15T18:47:52.453Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 590, stream opaque = 0x51000000
[ns_server:debug,2025-05-15T18:47:52.453Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x24F status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 4F
00 00 00 00
00 00 00 00
00 00 00 52

[rebalance:debug,2025-05-15T18:47:52.453Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 591, stream opaque = 0x52000000
[ns_server:debug,2025-05-15T18:47:52.453Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x250 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 50
00 00 00 00
00 00 00 00
00 00 00 53

[rebalance:debug,2025-05-15T18:47:52.453Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 592, stream opaque = 0x53000000
[ns_server:debug,2025-05-15T18:47:52.453Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x251 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 51
00 00 00 00
00 00 00 00
00 00 00 54

[rebalance:debug,2025-05-15T18:47:52.453Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 593, stream opaque = 0x54000000
[ns_server:debug,2025-05-15T18:47:52.453Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x252 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 52
00 00 00 00
00 00 00 00
00 00 00 55

[rebalance:debug,2025-05-15T18:47:52.453Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 594, stream opaque = 0x55000000
[ns_server:debug,2025-05-15T18:47:52.453Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x253 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 53
00 00 00 00
00 00 00 00
00 00 00 56

[rebalance:debug,2025-05-15T18:47:52.453Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 595, stream opaque = 0x56000000
[ns_server:debug,2025-05-15T18:47:52.453Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x254 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 54
00 00 00 00
00 00 00 00
00 00 00 57

[rebalance:debug,2025-05-15T18:47:52.453Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 596, stream opaque = 0x57000000
[ns_server:debug,2025-05-15T18:47:52.453Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x255 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 55
00 00 00 00
00 00 00 00
00 00 00 58

[rebalance:debug,2025-05-15T18:47:52.453Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 597, stream opaque = 0x58000000
[ns_server:debug,2025-05-15T18:47:52.453Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x256 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 56
00 00 00 00
00 00 00 00
00 00 00 59

[rebalance:debug,2025-05-15T18:47:52.454Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 598, stream opaque = 0x59000000
[ns_server:debug,2025-05-15T18:47:52.454Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x257 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 57
00 00 00 00
00 00 00 00
00 00 00 5A

[rebalance:debug,2025-05-15T18:47:52.454Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 599, stream opaque = 0x5A000000
[ns_server:debug,2025-05-15T18:47:52.454Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x258 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 58
00 00 00 00
00 00 00 00
00 00 00 5B

[rebalance:debug,2025-05-15T18:47:52.454Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 600, stream opaque = 0x5B000000
[ns_server:debug,2025-05-15T18:47:52.454Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x259 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 59
00 00 00 00
00 00 00 00
00 00 00 5C

[rebalance:debug,2025-05-15T18:47:52.454Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 601, stream opaque = 0x5C000000
[ns_server:debug,2025-05-15T18:47:52.454Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x25A status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 5A
00 00 00 00
00 00 00 00
00 00 00 5D

[rebalance:debug,2025-05-15T18:47:52.454Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 602, stream opaque = 0x5D000000
[ns_server:debug,2025-05-15T18:47:52.454Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x25B status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 5B
00 00 00 00
00 00 00 00
00 00 00 5E

[rebalance:debug,2025-05-15T18:47:52.454Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 603, stream opaque = 0x5E000000
[ns_server:debug,2025-05-15T18:47:52.457Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x25C status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 5C
00 00 00 00
00 00 00 00
00 00 00 5F

[rebalance:debug,2025-05-15T18:47:52.457Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 604, stream opaque = 0x5F000000
[ns_server:debug,2025-05-15T18:47:52.457Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x25D status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 5D
00 00 00 00
00 00 00 00
00 00 00 60

[rebalance:debug,2025-05-15T18:47:52.457Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 605, stream opaque = 0x60000000
[ns_server:debug,2025-05-15T18:47:52.457Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x25E status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 5E
00 00 00 00
00 00 00 00
00 00 00 61

[rebalance:debug,2025-05-15T18:47:52.457Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 606, stream opaque = 0x61000000
[ns_server:debug,2025-05-15T18:47:52.457Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x25F status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 5F
00 00 00 00
00 00 00 00
00 00 00 62

[rebalance:debug,2025-05-15T18:47:52.457Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 607, stream opaque = 0x62000000
[ns_server:debug,2025-05-15T18:47:52.457Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x260 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 60
00 00 00 00
00 00 00 00
00 00 00 63

[rebalance:debug,2025-05-15T18:47:52.457Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 608, stream opaque = 0x63000000
[ns_server:debug,2025-05-15T18:47:52.458Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x261 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 61
00 00 00 00
00 00 00 00
00 00 00 64

[rebalance:debug,2025-05-15T18:47:52.458Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 609, stream opaque = 0x64000000
[ns_server:debug,2025-05-15T18:47:52.458Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x262 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 62
00 00 00 00
00 00 00 00
00 00 00 65

[rebalance:debug,2025-05-15T18:47:52.458Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 610, stream opaque = 0x65000000
[ns_server:debug,2025-05-15T18:47:52.458Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x263 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 63
00 00 00 00
00 00 00 00
00 00 00 66

[rebalance:debug,2025-05-15T18:47:52.458Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 611, stream opaque = 0x66000000
[ns_server:debug,2025-05-15T18:47:52.458Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x264 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 64
00 00 00 00
00 00 00 00
00 00 00 67

[rebalance:debug,2025-05-15T18:47:52.458Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 612, stream opaque = 0x67000000
[ns_server:debug,2025-05-15T18:47:52.458Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x265 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 65
00 00 00 00
00 00 00 00
00 00 00 68

[rebalance:debug,2025-05-15T18:47:52.458Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 613, stream opaque = 0x68000000
[ns_server:debug,2025-05-15T18:47:52.459Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x266 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 66
00 00 00 00
00 00 00 00
00 00 00 69

[rebalance:debug,2025-05-15T18:47:52.459Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 614, stream opaque = 0x69000000
[ns_server:debug,2025-05-15T18:47:52.459Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x267 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 67
00 00 00 00
00 00 00 00
00 00 00 6A

[rebalance:debug,2025-05-15T18:47:52.459Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 615, stream opaque = 0x6A000000
[ns_server:debug,2025-05-15T18:47:52.459Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x268 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 68
00 00 00 00
00 00 00 00
00 00 00 6B

[rebalance:debug,2025-05-15T18:47:52.459Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 616, stream opaque = 0x6B000000
[ns_server:debug,2025-05-15T18:47:52.459Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x269 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 69
00 00 00 00
00 00 00 00
00 00 00 6C

[rebalance:debug,2025-05-15T18:47:52.459Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 617, stream opaque = 0x6C000000
[ns_server:debug,2025-05-15T18:47:52.459Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x26A status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 6A
00 00 00 00
00 00 00 00
00 00 00 6D

[rebalance:debug,2025-05-15T18:47:52.459Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 618, stream opaque = 0x6D000000
[ns_server:debug,2025-05-15T18:47:52.459Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x26B status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 6B
00 00 00 00
00 00 00 00
00 00 00 6E

[rebalance:debug,2025-05-15T18:47:52.459Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 619, stream opaque = 0x6E000000
[ns_server:debug,2025-05-15T18:47:52.459Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x26C status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 6C
00 00 00 00
00 00 00 00
00 00 00 6F

[rebalance:debug,2025-05-15T18:47:52.459Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 620, stream opaque = 0x6F000000
[ns_server:debug,2025-05-15T18:47:52.459Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x26D status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 6D
00 00 00 00
00 00 00 00
00 00 00 70

[rebalance:debug,2025-05-15T18:47:52.460Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 621, stream opaque = 0x70000000
[ns_server:debug,2025-05-15T18:47:52.460Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x26E status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 6E
00 00 00 00
00 00 00 00
00 00 00 71

[rebalance:debug,2025-05-15T18:47:52.460Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 622, stream opaque = 0x71000000
[ns_server:debug,2025-05-15T18:47:52.460Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x26F status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 6F
00 00 00 00
00 00 00 00
00 00 00 72

[rebalance:debug,2025-05-15T18:47:52.460Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 623, stream opaque = 0x72000000
[ns_server:debug,2025-05-15T18:47:52.460Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x270 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 70
00 00 00 00
00 00 00 00
00 00 00 73

[rebalance:debug,2025-05-15T18:47:52.460Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 624, stream opaque = 0x73000000
[ns_server:debug,2025-05-15T18:47:52.460Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x271 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 71
00 00 00 00
00 00 00 00
00 00 00 74

[rebalance:debug,2025-05-15T18:47:52.462Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 625, stream opaque = 0x74000000
[ns_server:debug,2025-05-15T18:47:52.463Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x272 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 72
00 00 00 00
00 00 00 00
00 00 00 75

[rebalance:debug,2025-05-15T18:47:52.463Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 626, stream opaque = 0x75000000
[ns_server:debug,2025-05-15T18:47:52.463Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x273 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 73
00 00 00 00
00 00 00 00
00 00 00 76

[rebalance:debug,2025-05-15T18:47:52.463Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 627, stream opaque = 0x76000000
[ns_server:debug,2025-05-15T18:47:52.463Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x274 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 74
00 00 00 00
00 00 00 00
00 00 00 77

[rebalance:debug,2025-05-15T18:47:52.463Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 628, stream opaque = 0x77000000
[ns_server:debug,2025-05-15T18:47:52.463Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x275 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 75
00 00 00 00
00 00 00 00
00 00 00 78

[rebalance:debug,2025-05-15T18:47:52.463Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 629, stream opaque = 0x78000000
[ns_server:debug,2025-05-15T18:47:52.464Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x276 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 76
00 00 00 00
00 00 00 00
00 00 00 79

[rebalance:debug,2025-05-15T18:47:52.464Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 630, stream opaque = 0x79000000
[ns_server:debug,2025-05-15T18:47:52.464Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x277 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 77
00 00 00 00
00 00 00 00
00 00 00 7A

[rebalance:debug,2025-05-15T18:47:52.464Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 631, stream opaque = 0x7A000000
[ns_server:debug,2025-05-15T18:47:52.464Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x278 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 78
00 00 00 00
00 00 00 00
00 00 00 7B

[rebalance:debug,2025-05-15T18:47:52.464Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 632, stream opaque = 0x7B000000
[ns_server:debug,2025-05-15T18:47:52.464Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x279 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 79
00 00 00 00
00 00 00 00
00 00 00 7C

[rebalance:debug,2025-05-15T18:47:52.464Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 633, stream opaque = 0x7C000000
[ns_server:debug,2025-05-15T18:47:52.464Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x27A status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 7A
00 00 00 00
00 00 00 00
00 00 00 7D

[rebalance:debug,2025-05-15T18:47:52.464Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 634, stream opaque = 0x7D000000
[ns_server:debug,2025-05-15T18:47:52.464Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x27B status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 7B
00 00 00 00
00 00 00 00
00 00 00 7E

[rebalance:debug,2025-05-15T18:47:52.464Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 635, stream opaque = 0x7E000000
[ns_server:debug,2025-05-15T18:47:52.464Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x27C status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 7C
00 00 00 00
00 00 00 00
00 00 00 7F

[rebalance:debug,2025-05-15T18:47:52.464Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 636, stream opaque = 0x7F000000
[ns_server:debug,2025-05-15T18:47:52.464Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x27D status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 7D
00 00 00 00
00 00 00 00
00 00 00 80

[rebalance:debug,2025-05-15T18:47:52.464Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 637, stream opaque = 0x80000000
[ns_server:debug,2025-05-15T18:47:52.464Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x27E status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 7E
00 00 00 00
00 00 00 00
00 00 00 81

[rebalance:debug,2025-05-15T18:47:52.464Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 638, stream opaque = 0x81000000
[ns_server:debug,2025-05-15T18:47:52.464Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x27F status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 7F
00 00 00 00
00 00 00 00
00 00 00 82

[rebalance:debug,2025-05-15T18:47:52.464Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 639, stream opaque = 0x82000000
[ns_server:debug,2025-05-15T18:47:52.465Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x280 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 80
00 00 00 00
00 00 00 00
00 00 00 83

[rebalance:debug,2025-05-15T18:47:52.465Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 640, stream opaque = 0x83000000
[ns_server:debug,2025-05-15T18:47:52.465Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x281 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 81
00 00 00 00
00 00 00 00
00 00 00 84

[rebalance:debug,2025-05-15T18:47:52.465Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 641, stream opaque = 0x84000000
[ns_server:debug,2025-05-15T18:47:52.465Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x282 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 82
00 00 00 00
00 00 00 00
00 00 00 85

[rebalance:debug,2025-05-15T18:47:52.465Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 642, stream opaque = 0x85000000
[ns_server:debug,2025-05-15T18:47:52.465Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x283 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 83
00 00 00 00
00 00 00 00
00 00 00 86

[rebalance:debug,2025-05-15T18:47:52.465Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 643, stream opaque = 0x86000000
[ns_server:debug,2025-05-15T18:47:52.465Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x284 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 84
00 00 00 00
00 00 00 00
00 00 00 87

[rebalance:debug,2025-05-15T18:47:52.465Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 644, stream opaque = 0x87000000
[ns_server:debug,2025-05-15T18:47:52.465Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x285 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 85
00 00 00 00
00 00 00 00
00 00 00 88

[rebalance:debug,2025-05-15T18:47:52.466Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 645, stream opaque = 0x88000000
[ns_server:debug,2025-05-15T18:47:52.466Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x286 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 86
00 00 00 00
00 00 00 00
00 00 00 89

[rebalance:debug,2025-05-15T18:47:52.466Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 646, stream opaque = 0x89000000
[ns_server:debug,2025-05-15T18:47:52.466Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x287 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 87
00 00 00 00
00 00 00 00
00 00 00 8A

[rebalance:debug,2025-05-15T18:47:52.468Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 647, stream opaque = 0x8A000000
[ns_server:debug,2025-05-15T18:47:52.470Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x288 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 88
00 00 00 00
00 00 00 00
00 00 00 8B

[rebalance:debug,2025-05-15T18:47:52.470Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 648, stream opaque = 0x8B000000
[ns_server:debug,2025-05-15T18:47:52.470Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x289 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 89
00 00 00 00
00 00 00 00
00 00 00 8C

[rebalance:debug,2025-05-15T18:47:52.470Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 649, stream opaque = 0x8C000000
[ns_server:debug,2025-05-15T18:47:52.470Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x28A status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 8A
00 00 00 00
00 00 00 00
00 00 00 8D

[rebalance:debug,2025-05-15T18:47:52.470Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 650, stream opaque = 0x8D000000
[ns_server:debug,2025-05-15T18:47:52.470Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x28B status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 8B
00 00 00 00
00 00 00 00
00 00 00 8E

[rebalance:debug,2025-05-15T18:47:52.470Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 651, stream opaque = 0x8E000000
[ns_server:debug,2025-05-15T18:47:52.471Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x28C status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 8C
00 00 00 00
00 00 00 00
00 00 00 8F

[rebalance:debug,2025-05-15T18:47:52.471Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 652, stream opaque = 0x8F000000
[ns_server:debug,2025-05-15T18:47:52.471Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x28D status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 8D
00 00 00 00
00 00 00 00
00 00 00 90

[rebalance:debug,2025-05-15T18:47:52.471Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 653, stream opaque = 0x90000000
[ns_server:debug,2025-05-15T18:47:52.471Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x28E status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 8E
00 00 00 00
00 00 00 00
00 00 00 91

[rebalance:debug,2025-05-15T18:47:52.472Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 654, stream opaque = 0x91000000
[ns_server:debug,2025-05-15T18:47:52.472Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x28F status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 8F
00 00 00 00
00 00 00 00
00 00 00 92

[rebalance:debug,2025-05-15T18:47:52.472Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 655, stream opaque = 0x92000000
[ns_server:debug,2025-05-15T18:47:52.472Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x290 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 90
00 00 00 00
00 00 00 00
00 00 00 93

[rebalance:debug,2025-05-15T18:47:52.472Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 656, stream opaque = 0x93000000
[ns_server:debug,2025-05-15T18:47:52.472Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x291 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 91
00 00 00 00
00 00 00 00
00 00 00 94

[rebalance:debug,2025-05-15T18:47:52.472Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 657, stream opaque = 0x94000000
[ns_server:debug,2025-05-15T18:47:52.472Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x292 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 92
00 00 00 00
00 00 00 00
00 00 00 95

[rebalance:debug,2025-05-15T18:47:52.472Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 658, stream opaque = 0x95000000
[ns_server:debug,2025-05-15T18:47:52.472Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x293 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 93
00 00 00 00
00 00 00 00
00 00 00 96

[rebalance:debug,2025-05-15T18:47:52.472Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 659, stream opaque = 0x96000000
[ns_server:debug,2025-05-15T18:47:52.472Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x294 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 94
00 00 00 00
00 00 00 00
00 00 00 97

[rebalance:debug,2025-05-15T18:47:52.472Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 660, stream opaque = 0x97000000
[ns_server:debug,2025-05-15T18:47:52.472Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x295 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 95
00 00 00 00
00 00 00 00
00 00 00 98

[rebalance:debug,2025-05-15T18:47:52.473Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 661, stream opaque = 0x98000000
[ns_server:debug,2025-05-15T18:47:52.473Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x296 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 96
00 00 00 00
00 00 00 00
00 00 00 99

[rebalance:debug,2025-05-15T18:47:52.473Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 662, stream opaque = 0x99000000
[ns_server:debug,2025-05-15T18:47:52.473Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x297 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 97
00 00 00 00
00 00 00 00
00 00 00 9A

[rebalance:debug,2025-05-15T18:47:52.473Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 663, stream opaque = 0x9A000000
[ns_server:debug,2025-05-15T18:47:52.473Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x298 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 98
00 00 00 00
00 00 00 00
00 00 00 9B

[rebalance:debug,2025-05-15T18:47:52.473Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 664, stream opaque = 0x9B000000
[ns_server:debug,2025-05-15T18:47:52.473Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x299 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 99
00 00 00 00
00 00 00 00
00 00 00 9C

[rebalance:debug,2025-05-15T18:47:52.473Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 665, stream opaque = 0x9C000000
[ns_server:debug,2025-05-15T18:47:52.473Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x29A status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 9A
00 00 00 00
00 00 00 00
00 00 00 9D

[rebalance:debug,2025-05-15T18:47:52.473Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 666, stream opaque = 0x9D000000
[ns_server:debug,2025-05-15T18:47:52.473Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x29B status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 9B
00 00 00 00
00 00 00 00
00 00 00 9E

[rebalance:debug,2025-05-15T18:47:52.473Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 667, stream opaque = 0x9E000000
[ns_server:debug,2025-05-15T18:47:52.473Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x29C status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 9C
00 00 00 00
00 00 00 00
00 00 00 9F

[rebalance:debug,2025-05-15T18:47:52.474Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 668, stream opaque = 0x9F000000
[ns_server:debug,2025-05-15T18:47:52.474Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x29D status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 9D
00 00 00 00
00 00 00 00
00 00 00 A0

[rebalance:debug,2025-05-15T18:47:52.474Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 669, stream opaque = 0xA0000000
[ns_server:debug,2025-05-15T18:47:52.474Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x29E status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 9E
00 00 00 00
00 00 00 00
00 00 00 A1

[rebalance:debug,2025-05-15T18:47:52.474Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 670, stream opaque = 0xA1000000
[ns_server:debug,2025-05-15T18:47:52.474Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x29F status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 9F
00 00 00 00
00 00 00 00
00 00 00 A2

[rebalance:debug,2025-05-15T18:47:52.474Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 671, stream opaque = 0xA2000000
[ns_server:debug,2025-05-15T18:47:52.474Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2A0 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 A0
00 00 00 00
00 00 00 00
00 00 00 A3

[rebalance:debug,2025-05-15T18:47:52.474Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 672, stream opaque = 0xA3000000
[ns_server:debug,2025-05-15T18:47:52.474Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2A1 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 A1
00 00 00 00
00 00 00 00
00 00 00 A4

[rebalance:debug,2025-05-15T18:47:52.474Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 673, stream opaque = 0xA4000000
[ns_server:debug,2025-05-15T18:47:52.474Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2A2 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 A2
00 00 00 00
00 00 00 00
00 00 00 A5

[rebalance:debug,2025-05-15T18:47:52.474Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 674, stream opaque = 0xA5000000
[ns_server:debug,2025-05-15T18:47:52.474Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2A3 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 A3
00 00 00 00
00 00 00 00
00 00 00 A6

[rebalance:debug,2025-05-15T18:47:52.474Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 675, stream opaque = 0xA6000000
[ns_server:debug,2025-05-15T18:47:52.474Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2A4 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 A4
00 00 00 00
00 00 00 00
00 00 00 A7

[rebalance:debug,2025-05-15T18:47:52.474Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 676, stream opaque = 0xA7000000
[ns_server:debug,2025-05-15T18:47:52.474Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2A5 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 A5
00 00 00 00
00 00 00 00
00 00 00 A8

[rebalance:debug,2025-05-15T18:47:52.475Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 677, stream opaque = 0xA8000000
[ns_server:debug,2025-05-15T18:47:52.475Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2A6 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 A6
00 00 00 00
00 00 00 00
00 00 00 A9

[rebalance:debug,2025-05-15T18:47:52.475Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 678, stream opaque = 0xA9000000
[ns_server:debug,2025-05-15T18:47:52.475Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2A7 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 A7
00 00 00 00
00 00 00 00
00 00 00 AA

[rebalance:debug,2025-05-15T18:47:52.475Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 679, stream opaque = 0xAA000000
[ns_server:debug,2025-05-15T18:47:52.475Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2A8 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 A8
00 00 00 00
00 00 00 00
00 00 00 AB

[rebalance:debug,2025-05-15T18:47:52.475Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 680, stream opaque = 0xAB000000
[ns_server:debug,2025-05-15T18:47:52.475Z,ns_1@db3.lan:<0.4235.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2A9 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 A9
00 00 00 00
00 00 00 00
00 00 00 AC

[rebalance:debug,2025-05-15T18:47:52.475Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 681, stream opaque = 0xAC000000
[ns_server:debug,2025-05-15T18:47:52.475Z,ns_1@db3.lan:<0.4235.0>:dcp_consumer_conn:maybe_reply_setup_streams:499]Setup stream request completed with ok. Moving to idle state
[ns_server:info,2025-05-15T18:47:52.476Z,ns_1@db3.lan:ns_memcached-doom-scrolling<0.4097.0>:ns_memcached:handle_call:370]Enabling traffic to bucket "doom-scrolling"
[ns_server:info,2025-05-15T18:47:52.476Z,ns_1@db3.lan:ns_memcached-doom-scrolling<0.4097.0>:ns_memcached:handle_call:374]Bucket "doom-scrolling" marked as warmed in 2 seconds
[ns_server:debug,2025-05-15T18:47:52.493Z,ns_1@db3.lan:dcp_traffic_monitor<0.3321.0>:dcp_traffic_monitor:update_bucket:148]Saw that bucket "doom-scrolling" became alive on node 'ns_1@db2.lan'
[ns_server:debug,2025-05-15T18:47:52.501Z,ns_1@db3.lan:chronicle_kv_log<0.2577.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@172.19.0.4',buckets_with_data}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                               51})
[{"doom-scrolling",<<"f70f8d64d14cf7513107bff35eb6561d">>}]
[ns_server:debug,2025-05-15T18:47:52.501Z,ns_1@db3.lan:chronicle_kv_log<0.2577.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@db3.lan',buckets_with_data}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                            52})
[{"doom-scrolling",<<"f70f8d64d14cf7513107bff35eb6561d">>}]
[ns_server:debug,2025-05-15T18:47:52.501Z,ns_1@db3.lan:chronicle_kv_log<0.2577.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@db2.lan',buckets_with_data}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                            53})
[{"doom-scrolling",<<"f70f8d64d14cf7513107bff35eb6561d">>}]
[ns_server:info,2025-05-15T18:47:52.701Z,ns_1@db3.lan:ns_doctor<0.2661.0>:ns_doctor:update_status:316]The following buckets became ready on node 'ns_1@172.19.0.4': ["doom-scrolling"]
[ns_server:info,2025-05-15T18:47:52.704Z,ns_1@db3.lan:ns_doctor<0.2661.0>:ns_doctor:update_status:316]The following buckets became ready on node 'ns_1@db3.lan': ["doom-scrolling"]
[ns_server:info,2025-05-15T18:47:52.710Z,ns_1@db3.lan:ns_doctor<0.2661.0>:ns_doctor:update_status:316]The following buckets became ready on node 'ns_1@db2.lan': ["doom-scrolling"]
[ns_server:debug,2025-05-15T18:47:54.488Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/query/migration/CBO_STATS/state">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554074}}]}|
 <<"{\"node\":\"172.19.0.4:8091\",\"state\":\"migrated\",\"when\":\"2025-05-15T18:47:54.475750678Z\"}">>]
[ns_server:debug,2025-05-15T18:47:54.489Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/query/migration/UDF/state">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554074}}]}|
 <<"{\"node\":\"172.19.0.4:8091\",\"state\":\"migrated\",\"when\":\"2025-05-15T18:47:54.475987011Z\"}">>]
[ns_server:debug,2025-05-15T18:47:54.827Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/indexing/ddl/commandToken/create/17514355370907628042/0">>} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554074}}]}|
 <<"{\"DefnId\":17514355370907628042,\"BucketUUID\":\"f70f8d64d14cf7513107bff35eb6561d\",\"ScopeId\":\"8\",\"CollectionId\":\"9\",\"Definitions\":{\"7a33d46fd1976e65466c5efc8b45ef2e\":[{\"defnId\":17514355370907628042,\"name\":\"#primary\",\"using\":\"GSI\",\"bucket\":\"doom-scrolling\",\"isPrimary\":true,\"exprType\":\"N1QL\",\"partitionScheme\":\"SINGLE\",\"numReplica\":1,\"NumReplica2\":{\"HasValue\":true,\"Base\":1,\"Incr\":0,\"Decr\":0},"...>>]
[ns_server:debug,2025-05-15T18:47:55.108Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/indexing/ddl/commandToken/create/17514355370907628042/0">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554075}},
             {<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554074}}]}|
 '_deleted']
[ns_server:debug,2025-05-15T18:47:57.858Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:debug,2025-05-15T18:47:57.860Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:47:57.861Z,ns_1@db3.lan:<0.4588.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:47:57.866Z,ns_1@db3.lan:<0.4590.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 779173, disk size is 5662753
[ns_server:debug,2025-05-15T18:47:57.867Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:47:57.867Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:47:57.869Z,ns_1@db3.lan:<0.4591.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:47:57.870Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:47:57.870Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:48:25.098Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{5,63914554105}}]}|
 <<"{\"indexer.plasma.backIndex.enableInMemoryCompression\":true,\"indexer.plasma.backIndex.enablePageBloomFilter\":true,\"indexer.plasma.mainIndex.enableInMemoryCompression\":true,\"indexer.settings.allow_large_keys\":true,\"indexer.settings.bufferPoolBlockSize\":16384,\"indexer.settings.build.batch_size\":5,\"indexer.settings.compaction.abort_exceed_interval\":false,\"indexer.settings.compaction.check_"...>>]
[ns_server:debug,2025-05-15T18:48:25.099Z,ns_1@db3.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/indexing/settings/config/features/PlasmaInMemoryCompression">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554105}}]}|
 <<"{}">>]
[ns_server:info,2025-05-15T18:48:27.373Z,ns_1@db3.lan:ns_config_rep<0.2618.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@172.19.0.4'
[user:info,2025-05-15T18:48:27.850Z,ns_1@db3.lan:<0.5936.0>:menelaus_web_alerts_srv:global_alert:227]Approaching full disk warning. Usage of disk "/opt/couchbase/var" on node "db3.lan" is around 92%.
[ns_server:debug,2025-05-15T18:48:27.869Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:48:27.870Z,ns_1@db3.lan:<0.5940.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:48:27.871Z,ns_1@db3.lan:<0.5942.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 779173, disk size is 5662753
[ns_server:debug,2025-05-15T18:48:27.871Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:debug,2025-05-15T18:48:27.872Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:48:27.872Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:48:27.876Z,ns_1@db3.lan:<0.5943.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:48:27.877Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:48:27.877Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:48:41.117Z,ns_1@db3.lan:ldap_auth_cache<0.2523.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[error_logger:info,2025-05-15T18:48:41.923Z,ns_1@db3.lan:alarm_handler<0.133.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
    alarm_handler: {set,{system_memory_high_watermark,[]}}
[ns_server:debug,2025-05-15T18:48:57.874Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:48:57.878Z,ns_1@db3.lan:<0.7300.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:48:57.878Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:debug,2025-05-15T18:48:57.880Z,ns_1@db3.lan:<0.7302.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 779173, disk size is 5662753
[ns_server:debug,2025-05-15T18:48:57.880Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:48:57.880Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:48:57.882Z,ns_1@db3.lan:<0.7303.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:48:57.883Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:48:57.883Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:49:18.507Z,ns_1@db3.lan:ns_config_rep<0.2618.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@db2.lan'
[ns_server:debug,2025-05-15T18:49:27.883Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:debug,2025-05-15T18:49:27.884Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:49:27.884Z,ns_1@db3.lan:<0.8665.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:49:27.885Z,ns_1@db3.lan:<0.8667.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 779173, disk size is 5662753
[ns_server:debug,2025-05-15T18:49:27.885Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:49:27.886Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:49:27.887Z,ns_1@db3.lan:<0.8668.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:49:27.887Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:49:27.887Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:49:56.119Z,ns_1@db3.lan:ldap_auth_cache<0.2523.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2025-05-15T18:49:57.886Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:49:57.886Z,ns_1@db3.lan:<0.10022.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:49:57.888Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:debug,2025-05-15T18:49:57.888Z,ns_1@db3.lan:<0.10024.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 779173, disk size is 5662753
[ns_server:debug,2025-05-15T18:49:57.890Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:49:57.890Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:49:57.895Z,ns_1@db3.lan:<0.10025.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:49:57.895Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:49:57.895Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:50:01.385Z,ns_1@db3.lan:ns_config_rep<0.2618.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@172.19.0.4'
[ns_server:info,2025-05-15T18:50:10.667Z,ns_1@db3.lan:ns_config_rep<0.2618.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@172.19.0.4'
[ns_server:debug,2025-05-15T18:50:27.893Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:50:27.893Z,ns_1@db3.lan:<0.11386.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:50:27.894Z,ns_1@db3.lan:<0.11388.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 779173, disk size is 5662753
[ns_server:debug,2025-05-15T18:50:27.894Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:50:27.894Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:50:27.897Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:50:27.900Z,ns_1@db3.lan:<0.11389.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:50:27.901Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:50:27.901Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:50:33.217Z,ns_1@db3.lan:ns_gc_runner<0.938.0>:ns_gc_runner:handle_info:125]GC populating new pid list of size=689, prevMaxGcDuration=11147 us
[ns_server:info,2025-05-15T18:50:39.548Z,ns_1@db3.lan:ns_config_rep<0.2618.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@db2.lan'
[ns_server:debug,2025-05-15T18:50:57.897Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:50:57.899Z,ns_1@db3.lan:<0.12744.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:50:57.901Z,ns_1@db3.lan:<0.12746.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 779173, disk size is 5662753
[ns_server:debug,2025-05-15T18:50:57.901Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:50:57.901Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:50:57.904Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:50:57.911Z,ns_1@db3.lan:<0.12747.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:50:57.912Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:50:57.912Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:51:11.122Z,ns_1@db3.lan:ldap_auth_cache<0.2523.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:info,2025-05-15T18:51:24.590Z,ns_1@db3.lan:ns_config_rep<0.2618.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@172.19.0.4'
[ns_server:debug,2025-05-15T18:51:27.904Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:51:27.912Z,ns_1@db3.lan:<0.14109.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:51:27.913Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:debug,2025-05-15T18:51:27.918Z,ns_1@db3.lan:<0.14111.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 779173, disk size is 5662753
[ns_server:debug,2025-05-15T18:51:27.918Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:51:27.919Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:51:27.923Z,ns_1@db3.lan:<0.14112.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:51:27.924Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:51:27.924Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:51:47.492Z,ns_1@db3.lan:ns_config_rep<0.2618.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@172.19.0.4'
[ns_server:debug,2025-05-15T18:51:57.931Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:debug,2025-05-15T18:51:57.943Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:51:57.943Z,ns_1@db3.lan:<0.15466.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:51:57.967Z,ns_1@db3.lan:<0.15468.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 779173, disk size is 5662753
[ns_server:debug,2025-05-15T18:51:57.968Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:51:57.968Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:51:57.973Z,ns_1@db3.lan:<0.15469.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:51:57.973Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:51:57.973Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:52:01.006Z,ns_1@db3.lan:ns_config_rep<0.2618.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@db2.lan'
[ns_server:debug,2025-05-15T18:52:22.324Z,ns_1@db3.lan:tombstone_keeper<0.277.0>:tombstone_keeper:handle_call:49]Refreshed with timestamps [{ns_config_vclock_ts,63914554042}]
[ns_server:debug,2025-05-15T18:52:22.325Z,ns_1@db3.lan:chronicle_kv_log<0.2577.0>:chronicle_kv_log:log:59]update (key: ns_config_vclock_ts, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                        65})
63914554042
[ns_server:debug,2025-05-15T18:52:22.325Z,ns_1@db3.lan:ns_config_rep<0.2618.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 51 us
[ns_server:debug,2025-05-15T18:52:22.325Z,ns_1@db3.lan:chronicle_kv_log<0.2577.0>:chronicle_kv_log:log:59]update (key: ns_config_purger, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                     65})
'ns_1@172.19.0.4'
[ns_server:debug,2025-05-15T18:52:22.329Z,ns_1@db3.lan:ns_config_rep<0.2618.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([rbac_upgrade,
                               {metakv,
                                   <<"/indexing/ddl/commandToken/create/17514355370907628042/0">>},
                               {metakv,
                                   <<"/indexing/rebalance/RebalanceToken">>}]..)
[ns_server:debug,2025-05-15T18:52:22.330Z,ns_1@db3.lan:ns_config_rep<0.2618.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 17 us
[ns_server:debug,2025-05-15T18:52:22.333Z,ns_1@db3.lan:ns_config_rep<0.2618.0>:ns_config_rep:handle_call:149]Got full synchronization request from 'ns_1@172.19.0.4'
[ns_server:debug,2025-05-15T18:52:22.333Z,ns_1@db3.lan:ns_config_rep<0.2618.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 15 us
[ns_server:debug,2025-05-15T18:52:22.359Z,ns_1@db3.lan:chronicle_kv_log<0.2577.0>:chronicle_kv_log:log:59]update (key: ns_config_purge_ts, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                       66})
63914554042
[ns_server:debug,2025-05-15T18:52:22.360Z,ns_1@db3.lan:tombstone_keeper<0.277.0>:tombstone_keeper:handle_call:49]Refreshed with timestamps [{ns_config_purge_ts,63914554042},
                           {ns_config_vclock_ts,63914554042}]
[ns_server:debug,2025-05-15T18:52:22.360Z,ns_1@db3.lan:tombstone_agent<0.2640.0>:tombstone_agent:purge:182]Purged 1 ns_config tombstone(s) up to timestamp 63914554042. Tombstones:
[rbac_upgrade]
[ns_server:debug,2025-05-15T18:52:26.123Z,ns_1@db3.lan:ldap_auth_cache<0.2523.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2025-05-15T18:52:27.974Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:52:27.976Z,ns_1@db3.lan:<0.16842.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:52:27.976Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:debug,2025-05-15T18:52:27.979Z,ns_1@db3.lan:<0.16844.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 779173, disk size is 5662753
[ns_server:debug,2025-05-15T18:52:27.979Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:52:27.980Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:52:27.981Z,ns_1@db3.lan:<0.16845.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:52:27.981Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:52:27.981Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:52:51.179Z,ns_1@db3.lan:ns_config_rep<0.2618.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@172.19.0.4'
[ns_server:debug,2025-05-15T18:52:57.983Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:debug,2025-05-15T18:52:57.985Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:52:57.985Z,ns_1@db3.lan:<0.18203.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:52:57.987Z,ns_1@db3.lan:<0.18205.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 779173, disk size is 5662753
[ns_server:debug,2025-05-15T18:52:57.987Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:52:57.987Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:52:57.993Z,ns_1@db3.lan:<0.18206.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:52:57.994Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:52:57.994Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:53:22.387Z,ns_1@db3.lan:tombstone_keeper<0.277.0>:tombstone_keeper:handle_call:49]Refreshed with timestamps [{ns_config_purge_ts,63914554042},
                           {ns_config_vclock_ts,63914554102}]
[ns_server:debug,2025-05-15T18:53:22.387Z,ns_1@db3.lan:ns_config_rep<0.2618.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 42 us
[ns_server:debug,2025-05-15T18:53:22.387Z,ns_1@db3.lan:chronicle_kv_log<0.2577.0>:chronicle_kv_log:log:59]update (key: ns_config_vclock_ts, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                        69})
63914554102
[ns_server:debug,2025-05-15T18:53:22.387Z,ns_1@db3.lan:chronicle_kv_log<0.2577.0>:chronicle_kv_log:log:59]update (key: ns_config_purger, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                     69})
'ns_1@172.19.0.4'
[ns_server:debug,2025-05-15T18:53:22.388Z,ns_1@db3.lan:ns_config_rep<0.2618.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([{metakv,
                                   <<"/indexing/ddl/commandToken/create/17514355370907628042/0">>},
                               {metakv,
                                   <<"/indexing/rebalance/RebalanceToken">>}]..)
[ns_server:debug,2025-05-15T18:53:22.389Z,ns_1@db3.lan:ns_config_rep<0.2618.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 25 us
[ns_server:debug,2025-05-15T18:53:22.395Z,ns_1@db3.lan:ns_config_rep<0.2618.0>:ns_config_rep:handle_call:149]Got full synchronization request from 'ns_1@172.19.0.4'
[ns_server:debug,2025-05-15T18:53:22.395Z,ns_1@db3.lan:ns_config_rep<0.2618.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 5 us
[ns_server:debug,2025-05-15T18:53:22.419Z,ns_1@db3.lan:chronicle_kv_log<0.2577.0>:chronicle_kv_log:log:59]update (key: ns_config_purge_ts, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                       70})
63914554102
[ns_server:debug,2025-05-15T18:53:22.419Z,ns_1@db3.lan:tombstone_keeper<0.277.0>:tombstone_keeper:handle_call:49]Refreshed with timestamps [{ns_config_purge_ts,63914554102},
                           {ns_config_vclock_ts,63914554102}]
[ns_server:debug,2025-05-15T18:53:22.420Z,ns_1@db3.lan:tombstone_agent<0.2640.0>:tombstone_agent:purge:182]Purged 2 ns_config tombstone(s) up to timestamp 63914554102. Tombstones:
[{metakv,<<"/indexing/rebalance/RebalanceToken">>},{metakv,<<"/indexing/ddl/commandToken/create/17514355370907628042/0">>}]
[ns_server:debug,2025-05-15T18:53:27.988Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:53:27.990Z,ns_1@db3.lan:<0.19580.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:53:27.992Z,ns_1@db3.lan:<0.19582.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 779173, disk size is 5662753
[ns_server:debug,2025-05-15T18:53:27.992Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:53:27.992Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:53:27.994Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:53:27.996Z,ns_1@db3.lan:<0.19583.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:53:27.997Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:53:27.997Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:53:28.142Z,ns_1@db3.lan:ns_config_rep<0.2618.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@db2.lan'
[ns_server:debug,2025-05-15T18:53:41.126Z,ns_1@db3.lan:ldap_auth_cache<0.2523.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:info,2025-05-15T18:53:43.330Z,ns_1@db3.lan:ns_config_rep<0.2618.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@172.19.0.4'
[ns_server:debug,2025-05-15T18:53:57.995Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:53:57.996Z,ns_1@db3.lan:<0.20942.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:53:57.997Z,ns_1@db3.lan:<0.20944.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 779173, disk size is 5662753
[ns_server:debug,2025-05-15T18:53:57.997Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:53:57.997Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:53:57.999Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:53:58.002Z,ns_1@db3.lan:<0.20945.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:53:58.002Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:53:58.002Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 29s
[ns_server:info,2025-05-15T18:54:18.562Z,ns_1@db3.lan:ns_config_rep<0.2618.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@172.19.0.4'
[ns_server:debug,2025-05-15T18:54:27.005Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:54:27.010Z,ns_1@db3.lan:<0.22267.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:54:27.011Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:54:27.011Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:54:28.000Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:54:28.002Z,ns_1@db3.lan:<0.22303.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:54:28.003Z,ns_1@db3.lan:<0.22305.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 779173, disk size is 5662753
[ns_server:debug,2025-05-15T18:54:28.003Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:54:28.003Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 29s
[ns_server:info,2025-05-15T18:54:31.695Z,ns_1@db3.lan:ns_config_rep<0.2618.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@db2.lan'
[ns_server:debug,2025-05-15T18:54:56.119Z,ns_1@db3.lan:roles_cache<0.2539.0>:active_cache:renew:238]Starting roles_cache cache renewal
[ns_server:debug,2025-05-15T18:54:56.120Z,ns_1@db3.lan:roles_cache<0.2539.0>:active_cache:renew:244]roles_cache cache renewal process originated 0 queries
[ns_server:debug,2025-05-15T18:54:56.129Z,ns_1@db3.lan:ldap_auth_cache<0.2523.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2025-05-15T18:54:57.007Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:54:57.010Z,ns_1@db3.lan:<0.23622.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:54:57.013Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:debug,2025-05-15T18:54:57.013Z,ns_1@db3.lan:<0.23624.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 779173, disk size is 5662753
[ns_server:debug,2025-05-15T18:54:57.014Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:54:57.015Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:54:57.019Z,ns_1@db3.lan:<0.23625.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:54:57.019Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:54:57.019Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:55:26.333Z,ns_1@db3.lan:ns_config_rep<0.2618.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@db2.lan'
[ns_server:debug,2025-05-15T18:55:27.018Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:debug,2025-05-15T18:55:27.024Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:55:27.024Z,ns_1@db3.lan:<0.24982.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:55:27.028Z,ns_1@db3.lan:<0.24984.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 779173, disk size is 5662753
[ns_server:debug,2025-05-15T18:55:27.028Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:55:27.028Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:55:27.030Z,ns_1@db3.lan:<0.24985.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:55:27.031Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:55:27.031Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:55:57.032Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:debug,2025-05-15T18:55:57.034Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:55:57.034Z,ns_1@db3.lan:<0.26344.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:55:57.036Z,ns_1@db3.lan:<0.26346.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 779173, disk size is 5662753
[ns_server:debug,2025-05-15T18:55:57.036Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:55:57.036Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:55:57.037Z,ns_1@db3.lan:<0.26347.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:55:57.038Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:55:57.038Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:55:57.490Z,ns_1@db3.lan:ns_config_rep<0.2618.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@db2.lan'
[ns_server:debug,2025-05-15T18:56:11.134Z,ns_1@db3.lan:ldap_auth_cache<0.2523.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2025-05-15T18:56:27.041Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:debug,2025-05-15T18:56:27.043Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:56:27.043Z,ns_1@db3.lan:<0.27704.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:56:27.045Z,ns_1@db3.lan:<0.27706.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 779173, disk size is 5662753
[ns_server:debug,2025-05-15T18:56:27.046Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:56:27.046Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:56:27.046Z,ns_1@db3.lan:<0.27703.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:56:27.047Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:56:27.047Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:56:42.997Z,ns_1@db3.lan:ns_config_rep<0.2618.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@172.19.0.4'
[ns_server:debug,2025-05-15T18:56:57.048Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:debug,2025-05-15T18:56:57.049Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:56:57.049Z,ns_1@db3.lan:<0.29067.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:56:57.050Z,ns_1@db3.lan:<0.29069.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 779173, disk size is 5662753
[ns_server:debug,2025-05-15T18:56:57.051Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:56:57.051Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:56:57.051Z,ns_1@db3.lan:<0.29070.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:56:57.052Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:56:57.052Z,ns_1@db3.lan:compaction_daemon<0.2826.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
