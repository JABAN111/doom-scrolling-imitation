[ns_server:info,2025-05-15T18:46:45.521Z,nonode@nohost:<0.154.0>:ns_server:init_logging:180]Started & configured logging
[ns_server:warn,2025-05-15T18:46:45.537Z,nonode@nohost:<0.154.0>:config_profile:load:123]Could not load profile file ("/etc/couchbase.d/config_profile") because it does not exist
[ns_server:debug,2025-05-15T18:46:45.538Z,nonode@nohost:<0.154.0>:ns_server:setup_server_profile:108]Using profile 'default': [{name,"default"},
                          {{indexer,disable_shard_affinity},true}]
[ns_server:warn,2025-05-15T18:46:45.540Z,nonode@nohost:<0.154.0>:ns_server:config_profile_continuity_checker:129]Writing config_profile '"default"' to disk.
[ns_server:info,2025-05-15T18:46:45.549Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]Static config terms:
[{error_logger_mf_dir,"/opt/couchbase/var/lib/couchbase/logs"},
 {path_config_bindir,"/opt/couchbase/bin"},
 {path_config_etcdir,"/opt/couchbase/etc/couchbase"},
 {path_config_libdir,"/opt/couchbase/lib"},
 {path_config_datadir,"/opt/couchbase/var/lib/couchbase"},
 {path_config_tmpdir,"/opt/couchbase/var/lib/couchbase/tmp"},
 {path_config_secdir,"/opt/couchbase/etc/security"},
 {nodefile,"/opt/couchbase/var/lib/couchbase/couchbase-server.node"},
 {loglevel_default,debug},
 {loglevel_couchdb,info},
 {loglevel_ns_server,debug},
 {loglevel_error_logger,debug},
 {loglevel_user,debug},
 {loglevel_menelaus,debug},
 {loglevel_ns_doctor,debug},
 {loglevel_stats,debug},
 {loglevel_rebalance,debug},
 {loglevel_cluster,debug},
 {loglevel_views,debug},
 {loglevel_mapreduce_errors,debug},
 {loglevel_xdcr,debug},
 {loglevel_access,info},
 {loglevel_cbas,debug},
 {disk_sink_opts,
     [{rotation,
          [{compress,true},
           {size,41943040},
           {num_files,10},
           {buffer_size_max,52428800}]}]},
 {disk_sink_opts_disk_json_rpc,
     [{rotation,
          [{compress,true},
           {size,41943040},
           {num_files,2},
           {buffer_size_max,52428800}]}]},
 {disk_sink_opts_disk_tls_key_log,
     [{rotation,
          [{compress,true},
           {size,10485760},
           {num_files,1},
           {buffer_size_max,13107200}]}]},
 {net_kernel_verbosity,10}]
[ns_server:warn,2025-05-15T18:46:45.550Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter error_logger_mf_dir, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.550Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter path_config_bindir, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.550Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter path_config_etcdir, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.550Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter path_config_libdir, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.550Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter path_config_datadir, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.550Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter path_config_tmpdir, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.550Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter path_config_secdir, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.550Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter nodefile, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.550Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_default, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.550Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_couchdb, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.550Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_ns_server, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.550Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_error_logger, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.550Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_user, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.550Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_menelaus, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.550Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_ns_doctor, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.550Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_stats, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.550Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_rebalance, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.550Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_cluster, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.550Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_views, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.550Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_mapreduce_errors, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.550Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_xdcr, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.550Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_access, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.550Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_cbas, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.551Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter disk_sink_opts, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.551Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter disk_sink_opts_disk_json_rpc, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.551Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter disk_sink_opts_disk_tls_key_log, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.551Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter net_kernel_verbosity, which is given from command line
[ns_server:info,2025-05-15T18:46:45.581Z,nonode@nohost:dist_manager<0.215.0>:dist_manager:read_address_config_from_path:83]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip_start"
[ns_server:info,2025-05-15T18:46:45.581Z,nonode@nohost:dist_manager<0.215.0>:dist_manager:read_address_config_from_path:83]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2025-05-15T18:46:45.581Z,nonode@nohost:dist_manager<0.215.0>:dist_manager:init:180]ip config not found. Looks like we're brand new node
[ns_server:info,2025-05-15T18:46:45.592Z,nonode@nohost:dist_manager<0.215.0>:dist_manager:bringup:246]Attempting to bring up net_kernel with name 'ns_1@cb.local'
[error_logger:info,2025-05-15T18:46:45.611Z,nonode@nohost:ssl_dist_admin_sup<0.218.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_admin_sup}
    started: [{pid,<0.219.0>},
              {id,ssl_pem_cache_dist},
              {mfargs,{ssl_pem_cache,start_link_dist,[[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.611Z,nonode@nohost:ssl_dist_admin_sup<0.218.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_admin_sup}
    started: [{pid,<0.220.0>},
              {id,ssl_dist_manager},
              {mfargs,{ssl_manager,start_link_dist,[[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.611Z,nonode@nohost:ssl_dist_sup<0.217.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_sup}
    started: [{pid,<0.218.0>},
              {id,ssl_dist_admin_sup},
              {mfargs,{ssl_dist_admin_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:45.617Z,nonode@nohost:tls_dist_sup<0.221.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_sup}
    started: [{pid,<0.222.0>},
              {id,dist_tls_connection_sup},
              {mfargs,{tls_connection_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:45.622Z,nonode@nohost:tls_dist_server_sup<0.223.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_server_sup}
    started: [{pid,<0.224.0>},
              {id,dist_ssl_listen_tracker_sup},
              {mfargs,{ssl_listen_tracker_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:45.623Z,nonode@nohost:tls_dist_server_sup<0.223.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_server_sup}
    started: [{pid,<0.225.0>},
              {id,dist_tls_server_session_ticket},
              {mfargs,{tls_server_session_ticket_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:45.623Z,nonode@nohost:tls_dist_server_sup<0.223.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_server_sup}
    started: [{pid,<0.226.0>},
              {id,dist_ssl_upgrade_server_session_cache_sup},
              {mfargs,
                  {ssl_upgrade_server_session_cache_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:45.623Z,nonode@nohost:tls_dist_sup<0.221.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_sup}
    started: [{pid,<0.223.0>},
              {id,tls_dist_server_sup},
              {mfargs,{tls_dist_server_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:45.623Z,nonode@nohost:ssl_dist_sup<0.217.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_sup}
    started: [{pid,<0.221.0>},
              {id,tls_dist_sup},
              {mfargs,{tls_dist_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:45.623Z,nonode@nohost:net_sup<0.216.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.217.0>},
              {id,ssl_dist_sup},
              {mfargs,{ssl_dist_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:45.624Z,nonode@nohost:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Starting cb_dist with config [{config_vsn,2}]
[error_logger:info,2025-05-15T18:46:45.624Z,nonode@nohost:net_sup<0.216.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.227.0>},
              {id,cb_dist},
              {mfargs,{cb_dist,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.626Z,nonode@nohost:net_sup<0.216.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.228.0>},
              {id,auth},
              {mfargs,{auth,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,2000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:45.628Z,nonode@nohost:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Initial protos: [{external,inet_tcp_dist}], required protos: [{external,
                                                                        inet_tcp_dist}]
[ns_server:debug,2025-05-15T18:46:45.628Z,nonode@nohost:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Starting {external,inet_tcp_dist} listener on 21100...
[ns_server:debug,2025-05-15T18:46:45.631Z,nonode@nohost:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Started listener: inet_tcp_dist
[ns_server:debug,2025-05-15T18:46:45.681Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated server ssl_dist_opts (keep_secrets) - false
[ns_server:debug,2025-05-15T18:46:45.681Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Started acceptor inet_tcp_dist: <0.231.0>
[ns_server:debug,2025-05-15T18:46:45.681Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Ensure config is going to change listeners. Will be stopped: [], will be started: [], will be restarted: []
[error_logger:info,2025-05-15T18:46:45.681Z,ns_1@cb.local:net_sup<0.216.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.229.0>},
              {id,net_kernel},
              {mfargs,{net_kernel,start_link,
                                  [#{clean_halt => false,
                                     name => 'ns_1@cb.local',
                                     name_domain => longnames,
                                     net_tickintensity => 4,
                                     net_ticktime => 60,
                                     supervisor => net_sup_dynamic}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,2000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.681Z,ns_1@cb.local:kernel_sup<0.49.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,kernel_sup}
    started: [{pid,<0.216.0>},
              {id,net_sup_dynamic},
              {mfargs,
                  {erl_distribution,start_link,
                      [#{clean_halt => false,name => 'ns_1@cb.local',
                         name_domain => longnames,net_tickintensity => 4,
                         net_ticktime => 60,supervisor => net_sup_dynamic}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,60000},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:45.687Z,ns_1@cb.local:dist_manager<0.215.0>:dist_manager:configure_net_kernel:302]Set net_kernel vebosity to 10 -> 0
[ns_server:info,2025-05-15T18:46:45.693Z,ns_1@cb.local:dist_manager<0.215.0>:dist_manager:save_node:160]saving node name '"ns_1@cb.local"' to "/opt/couchbase/var/lib/couchbase/couchbase-server.node"
[ns_server:debug,2025-05-15T18:46:45.704Z,ns_1@cb.local:dist_manager<0.215.0>:dist_manager:bringup:269]Attempted to save node name to disk: ok
[ns_server:debug,2025-05-15T18:46:45.704Z,ns_1@cb.local:dist_manager<0.215.0>:dist_manager:wait_for_node:276]Waiting for connection to node 'babysitter_of_ns_1@cb.local' to be established
[error_logger:info,2025-05-15T18:46:45.705Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{connect,normal,'babysitter_of_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:46:45.718Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'babysitter_of_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:46:45.719Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3735524962.3650355203.241140>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:46:45.719Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3735524962.3650355203.241140>,
                                  inet_tcp_dist,<0.233.0>,
                                  #Ref<0.3735524962.3650355203.241143>}
[ns_server:debug,2025-05-15T18:46:45.740Z,ns_1@cb.local:dist_manager<0.215.0>:dist_manager:wait_for_node:288]Observed node 'babysitter_of_ns_1@cb.local' to come up
[ns_server:info,2025-05-15T18:46:45.741Z,ns_1@cb.local:dist_manager<0.215.0>:dist_manager:save_address_config:147]Deleting irrelevant ip file "/opt/couchbase/var/lib/couchbase/ip_start": {error,
                                                                          enoent}
[ns_server:info,2025-05-15T18:46:45.741Z,ns_1@cb.local:dist_manager<0.215.0>:dist_manager:save_address_config:148]saving ip config to "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2025-05-15T18:46:45.746Z,ns_1@cb.local:dist_manager<0.215.0>:dist_manager:save_address_config:151]Persisted the address successfully
[error_logger:info,2025-05-15T18:46:45.751Z,ns_1@cb.local:root_sup<0.214.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,root_sup}
    started: [{pid,<0.215.0>},
              {id,dist_manager},
              {mfargs,{dist_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.758Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.236.0>},
              {id,local_tasks},
              {mfargs,{local_tasks,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:45.762Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:log_os_info:start_link:19]OS type: {unix,linux} Version: {6,10,14}
Runtime info: [{otp_release,"25"},
               {erl_version,"13.2.2.3"},
               {erl_version_long,
                   "Erlang/OTP 25 [erts-13.2.2.3] [source-15104f9619] [64-bit] [smp:10:10] [ds:10:10:10] [async-threads:16] [jit]\n"},
               {system_arch_raw,"aarch64-unknown-linux-gnu"},
               {system_arch,"aarch64-unknown-linux-gnu"},
               {localtime,{{2025,5,15},{18,46,45}}},
               {memory,
                   [{total,44123448},
                    {processes,9959840},
                    {processes_used,9954104},
                    {system,34163608},
                    {atom,540873},
                    {atom_used,522143},
                    {binary,136720},
                    {code,9676022},
                    {ets,2639608}]},
               {loaded,
                   [ns_info,log_os_info,local_tasks,restartable,
                    ns_server_cluster_sup,ns_cluster,dist_util,ns_node_disco,
                    crypto,inet_tcp,re,auth,tls_dist_server_sup,tls_dist_sup,
                    ssl_dist_admin_sup,ssl_dist_sup,inet_tls_dist,
                    inet_tcp_dist,cb_epmd,gen_udp,inet_hosts,dist_manager,
                    root_sup,cb_dist,path_config,config_profile,
                    ns_server_stats,calendar,ale_default_formatter,
                    'ale_logger-tls_key','ale_logger-metakv',
                    'ale_logger-rebalance','ale_logger-chronicle',
                    'ale_logger-ns_server_trace','ale_logger-menelaus',
                    'ale_logger-stats','ale_logger-json_rpc',
                    'ale_logger-access','ale_logger-ns_server',
                    'ale_logger-user','ale_logger-ns_doctor',
                    'ale_logger-cluster','ale_logger-xdcr',erl_bits,
                    otp_internal,cb_log_counter_sink,ns_log_sink,
                    ale_disk_sink,misc,couch_util,ns_server,esaml_util,esaml,
                    ale_error_logger_handler,timer,cpu_sup,filelib,memsup,
                    disksup,os_mon,unicode_util,string,io,release_handler,
                    alarm_handler,sasl,httpd_sup,httpc_handler_sup,
                    httpc_cookie,inets_trace,httpc_manager,httpc,
                    httpc_profile_sup,httpc_sup,inets_sup,inets_app,ssl,
                    lhttpc_manager,lhttpc_sup,lhttpc,
                    dtls_server_session_cache_sup,dtls_listener_sup,
                    dtls_server_sup,dtls_connection_sup,dtls_sup,
                    ssl_upgrade_server_session_cache_sup,
                    ssl_server_session_cache_sup,
                    tls_server_session_ticket_sup,ssl_listen_tracker_sup,
                    tls_server_sup,tls_connection_sup,tls_sup,
                    ssl_connection_sup,tls_client_ticket_store,
                    ssl_client_session_cache_db,ssl_config,ssl_manager,
                    ssl_pkix_db,ssl_pem_cache,ssl_admin_sup,ssl_sup,
                    logger_h_common,logger_std_h,ssl_logger,ssl_app,
                    'ale_logger-trace_logger','ale_logger-ale_logger',
                    'ale_logger-error_logger',beam_opcodes,beam_dict,beam_asm,
                    beam_z,beam_flatten,beam_trim,beam_clean,beam_block,
                    beam_utils,beam_jump,beam_a,beam_validator,
                    beam_ssa_codegen,beam_ssa_pre_codegen,beam_ssa_throw,
                    beam_ssa_dead,beam_call_types,beam_types,beam_ssa_type,
                    beam_ssa_bc_size,beam_ssa_opt,beam_ssa_bsm,beam_ssa_recv,
                    beam_ssa_share,beam_ssa_bool,beam_ssa,beam_kernel_to_ssa,
                    v3_kernel,sys_core_bsm,sys_core_alias,erl_bifs,
                    cerl_clauses,sys_core_fold,sys_core_inline,cerl_trees,
                    core_lib,cerl,sets,v3_core,erl_expand_records,sofs,
                    erl_internal,ordsets,compile,dynamic_compile,ale_utils,
                    io_lib_pretty,io_lib_format,io_lib,ale_codegen,dict,ale,
                    ale_dynamic_sup,ale_sup,ale_app,ns_bootstrap,child_erlang,
                    raw_file_io,orddict,c,erl_signal_handler,
                    logger_handler_watcher,logger_sup,kernel_refc,
                    kernel_config,user_io,user_sup,supervisor_bridge,
                    standard_error,erpc,global_group,erl_distribution,maps,
                    rand,net_kernel,global,rpc,epp,inet_parse,inet,inet_udp,
                    inet_config,inet_db,unicode,os,gb_trees,gb_sets,binary,
                    beam_lib,peer,erl_anno,erl_features,proplists,erl_scan,
                    queue,logger_olp,logger_proxy,application,
                    application_controller,error_handler,application_master,
                    file,error_logger,filename,heart,code,file_server,ets,
                    logger_filters,gen,logger_config,erl_eval,logger_simple_h,
                    logger_backend,kernel,logger_server,file_io_server,
                    code_server,gen_event,gen_server,logger,proc_lib,lists,
                    supervisor,erl_parse,erl_lint,persistent_term,counters,
                    atomics,erts_dirty_process_signal_handler,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,prim_net,prim_socket,
                    socket_registry,zlib,prim_file,prim_inet,prim_eval,
                    prim_buffer,init,erl_init,erts_code_purger]},
               {applications,
                   [{stdlib,"ERTS  CXC 138 10","4.3.1.2"},
                    {sasl,"SASL  CXC 138 11","4.2"},
                    {public_key,"Public key infrastructure","1.13.3.1"},
                    {crypto,"CRYPTO","5.1.4.1"},
                    {lhttpc,"Lightweight HTTP Client","1.3.0"},
                    {asn1,"The Erlang ASN1 compiler version 5.0.21","5.0.21"},
                    {ssl,"Erlang/OTP SSL application","10.9.1.2"},
                    {esaml,"SAML Server Provider library for erlang","4.4.0"},
                    {ale,"Another Logger for Erlang","0.0.0"},
                    {xmerl,"XML parser","1.3.31.1"},
                    {inets,"INETS  CXC 138 49","8.3.1.2"},
                    {os_mon,"CPO  CXC 138 46","2.8.2"},
                    {ns_server,"Couchbase server","7.6.2-3721-enterprise"},
                    {kernel,"ERTS  CXC 138 10","8.5.4.2"}]},
               {pre_loaded,
                   [persistent_term,counters,atomics,
                    erts_dirty_process_signal_handler,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,prim_net,prim_socket,
                    socket_registry,zlib,prim_file,prim_inet,prim_eval,
                    prim_buffer,init,erl_init,erts_code_purger]},
               {process_count,159},
               {node,'ns_1@cb.local'},
               {nodes,[]},
               {registered,
                   [ssl_connection_sup,standard_error,net_kernel,
                    ssl_upgrade_server_session_cache_sup,kernel_refc,
                    'sink-disk_debug',ale_sup,httpd_sup,
                    ssl_upgrade_server_session_cache_sup_dist,
                    tls_client_ticket_store,dtls_server_session_cache_sup,
                    'sink-disk_trace',inets_sup,tls_sup,erts_code_purger,
                    'sink-disk_tls_key_log',dist_manager,ssl_pem_cache_dist,
                    cb_dist,ale,erl_prim_loader,global_group,cpu_sup,init,
                    application_controller,ns_server_cluster_sup,
                    httpc_manager,erl_signal_server,ssl_dist_sup,
                    tls_dist_connection_sup,ssl_sup,esaml,logger_sup,
                    'sink-disk_error',tls_server_session_ticket_sup_dist,
                    ssl_listen_tracker_sup,os_mon_sup,'sink-disk_reports',
                    dtls_sup,rex,tls_dist_sup,tls_server_session_ticket_sup,
                    'sink-disk_metakv',disksup,ssl_manager,'sink-ns_log',
                    logger_proxy,'sink-disk_default',ale_dynamic_sup,
                    ssl_listen_tracker_sup_dist,auth,logger_std_h_ssl_handler,
                    standard_error_sup,'sink-disk_stats',httpc_profile_sup,
                    kernel_sup,ssl_admin_sup,inet_db,user,sasl_safe_sup,
                    release_handler,'sink-disk_access_int',
                    ssl_server_session_cache_sup,root_sup,logger,
                    alarm_handler,lhttpc_manager,dtls_server_sup,
                    logger_handler_watcher,httpc_sup,tls_dist_server_sup,
                    global_group_check,kernel_safe_sup,'sink-disk_json_rpc',
                    local_tasks,memsup,ssl_pem_cache,'sink-disk_xdcr',
                    dtls_connection_sup,global_name_server,file_server_2,
                    httpc_handler_sup,sasl_sup,tls_connection_sup,
                    ssl_manager_dist,lhttpc_sup,'sink-disk_access',
                    dtls_listener_sup,ale_stats_events,socket_registry,
                    code_server,net_sup,esaml_ets_table_owner,
                    'sink-cb_log_counter',tls_server_sup,ssl_dist_admin_sup]},
               {cookie,nocookie},
               {wordsize,8},
               {wall_clock,5}]
[ns_server:info,2025-05-15T18:46:45.766Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:log_os_info:start_link:21]Manifest:
["<?xml version=\"1.0\" encoding=\"UTF-8\"?>","<manifest>",
 "  <remote name=\"blevesearch\" fetch=\"https://github.com/blevesearch/\"/>",
 "  <remote name=\"couchbase\" fetch=\"https://github.com/couchbase/\" review=\"review.couchbase.org\"/>",
 "  <remote name=\"couchbase-priv\" fetch=\"ssh://git@github.com/couchbase/\" review=\"review.couchbase.org\"/>",
 "  <remote name=\"couchbasedeps\" fetch=\"https://github.com/couchbasedeps/\" review=\"review.couchbase.org\"/>",
 "  <remote name=\"couchbaselabs\" fetch=\"https://github.com/couchbaselabs/\" review=\"review.couchbase.org\"/>",
 "  ","  <default remote=\"couchbase\" revision=\"master\"/>","  ",
 "  <project name=\"HdrHistogram_c\" path=\"third_party/HdrHistogram_c\" remote=\"couchbasedeps\" revision=\"9ead6b88adbf8d6131e5ae7a3a699c477a3b4195\" groups=\"kv\"/>",
 "  <project name=\"analytics-dcp-client\" path=\"analytics/java-dcp-client\" revision=\"081d9934d4a28b4abdadcd13891792ea423416c0\" upstream=\"trinity\" dest-branch=\"trinity\" groups=\"notdefault,enterprise,analytics\"/>",
 "  <project name=\"asterixdb\" path=\"analytics/asterixdb\" revision=\"6a10f3f81db977c706447ece476b487cbe56414c\" groups=\"notdefault,enterprise,analytics\"/>",
 "  <project name=\"backup\" remote=\"couchbase-priv\" revision=\"192d7500ba2a7b5281d2c61af126c8027bbb858d\" groups=\"backup,notdefault,enterprise\"/>",
 "  <project name=\"build\" path=\"cbbuild\" revision=\"cedf9d4ec929eac7e61f8e86488aeac5402c8563\" groups=\"notdefault,build\">",
 "    <annotation name=\"RELEASE\" value=\"trinity\"/>",
 "    <annotation name=\"PRODUCT\" value=\"couchbase-server\"/>",
 "    <annotation name=\"BLD_NUM\" value=\"3721\"/>",
 "    <annotation name=\"VERSION\" value=\"7.6.2\"/>","  </project>",
 "  <project name=\"cbas\" path=\"goproj/src/github.com/couchbase/cbas\" remote=\"couchbase-priv\" revision=\"2db6eb59fd5af47a1ce81d53c8f4e58c7a14df3a\" groups=\"notdefault,enterprise,analytics\"/>",
 "  <project name=\"cbas-core\" path=\"analytics\" remote=\"couchbase-priv\" revision=\"2c1d23ee3aba4c80196d9d94ceaca3917b8ea8a7\" upstream=\"7.6.2\" dest-branch=\"7.6.2\" groups=\"notdefault,enterprise,analytics\"/>",
 "  <project name=\"cbas-ui\" revision=\"704db180d01de15f70cacc9fc11c5d8d8d4ff965\" groups=\"notdefault,enterprise,analytics\"/>",
 "  <project name=\"cbauth\" path=\"goproj/src/github.com/couchbase/cbauth\" revision=\"a9992170165a1d330cb5a9918a29d5bd417c5e46\" groups=\"backup\"/>",
 "  <project name=\"cbbs\" remote=\"couchbase-priv\" revision=\"f1d0272decc7f1b445b08e56e1f75e99f743aa90\" groups=\"backup,notdefault,enterprise\"/>",
 "  <project name=\"cbft\" revision=\"69d32cca4a8eca6e5aad5dad689795ab72ecdd6e\"/>",
 "  <project name=\"cbftx\" remote=\"couchbase-priv\" revision=\"258e3829db59f06a202ea2435c776a351a590eba\" groups=\"notdefault,enterprise\"/>",
 "  <project name=\"cbgt\" revision=\"b7dd01a11c5c56fbca88b9b950d8eca4dacce36f\"/>",
 "  <project name=\"cbsummary\" path=\"goproj/src/github.com/couchbase/cbsummary\" revision=\"fb656c91554a97318c44f58e3cc7f166f1eef4fc\"/>",
 "  <project name=\"chronicle\" path=\"ns_server/deps/chronicle\" revision=\"8d1feeb0d8b15e2b6a4c1a417addfd159b422a71\"/>",
 "  <project name=\"client_golang\" path=\"godeps/src/github.com/prometheus/client_golang\" remote=\"couchbasedeps\" revision=\"2e1c4818ccfdcf953ce399cadad615ff2bed968c\" upstream=\"refs/tags/v1.12.1\" dest-branch=\"refs/tags/v1.12.1\"/>",
 "  <project name=\"client_model\" path=\"godeps/src/github.com/prometheus/client_model\" remote=\"couchbasedeps\" revision=\"6dc836ede0b5b08c61893c3ffeb474498b18bb83\"/>",
 "  <project name=\"clog\" path=\"godeps/src/github.com/couchbase/clog\" revision=\"f935d1fdfc36541b505cf86fea4822e4067f9c39\"/>",
 "  <project name=\"common\" path=\"godeps/src/github.com/prometheus/common\" remote=\"couchbasedeps\" revision=\"902cb39e6c079571d32c2db8da220da13c11b562\" upstream=\"refs/tags/v0.33.0\" dest-branch=\"refs/tags/v0.33.0\"/>",
 "  <project name=\"couchbase-cli\" revision=\"941f6d7bbac8f8a42870c3f5459376b9f19ef1fd\" groups=\"kv\"/>",
 "  <project name=\"couchdb\" revision=\"3e5b8f248d77dd9317b36b50eed2567bcfb5f4cf\" dest-branch=\"unstable\"/>",
 "  <project name=\"couchdbx-app\" revision=\"702647dd015e7443de9cdb789806351774e85463\" groups=\"notdefault,packaging\"/>",
 "  <project name=\"couchstore\" revision=\"ce7305bab3feb64bd2504f34d24a1419008e8bda\" groups=\"kv\"/>",
 "  <project name=\"crypto\" path=\"godeps/src/golang.org/x/crypto\" remote=\"couchbasedeps\" revision=\"eb61739cd99fb244c7cd188d3c5bae54824e781d\" upstream=\"refs/tags/v0.15.0\" dest-branch=\"refs/tags/v0.15.0\"/>",
 "  <project name=\"docloader\" path=\"goproj/src/github.com/couchbase/docloader\" revision=\"cf3254d7dfb042192c9a23bd2e64a281c32a29d8\"/>",
 "  <project name=\"errors\" path=\"godeps/src/github.com/pkg/errors\" remote=\"couchbasedeps\" revision=\"30136e27e2ac8d167177e8a583aa4c3fea5be833\"/>",
 "  <project name=\"eventing\" path=\"goproj/src/github.com/couchbase/eventing\" revision=\"047b756132464b8f756cc35e02a15b5f498f80d5\" dest-branch=\"unstable\" groups=\"notdefault,enterprise\"/>",
 "  <project name=\"eventing-ee\" path=\"goproj/src/github.com/couchbase/eventing-ee\" remote=\"couchbase-priv\" revision=\"5425f180a0756868524081f889ab224cfc10b70d\" dest-branch=\"unstable\" groups=\"notdefault,enterprise\"/>",
 "  <project name=\"flatbuffers\" path=\"godeps/src/github.com/google/flatbuffers\" remote=\"couchbasedeps\" revision=\"1a8968225130caeddd16e227678e6f8af1926303\"/>",
 "  <project name=\"forestdb\" revision=\"9efe6d75d7d61e742af70fb47fe97ad1d04ba86f\" groups=\"backup\"/>",
 "  <project name=\"geocouch\" revision=\"68f3b9d36630682d17ca5232770f1693b9b8fa18\"/>",
 "  <project name=\"go-couchbase\" path=\"goproj/src/github.com/couchbase/go-couchbase\" revision=\"959eaf944140a6c660990f38b1db310ddd6d8e42\" groups=\"backup\"/>",
 "  <project name=\"go-metrics\" path=\"godeps/src/github.com/rcrowley/go-metrics\" remote=\"couchbasedeps\" revision=\"cf1acfcdf4751e0554ffa765d03e479ec491cad6\"/>",
 "  <project name=\"go_json\" path=\"goproj/src/github.com/couchbase/go_json\" revision=\"d6e17ad2b9a218e82569e09b761c226fa8df726a\"/>",
 "  <project name=\"gocb\" path=\"godeps/src/github.com/couchbase/gocb/v2\" revision=\"40020eef484873e507745107edbf99c421927a93\" upstream=\"refs/tags/v2.2.5\" dest-branch=\"refs/tags/v2.2.5\"/>",
 "  <project name=\"gocbcore\" path=\"godeps/src/github.com/couchbase/gocbcore/v9\" revision=\"0ece206041d8cf5f5fcd919767446603691bdb69\" upstream=\"refs/tags/v9.1.6\" dest-branch=\"refs/tags/v9.1.6\"/>",
 "  <project name=\"godbc\" path=\"goproj/src/github.com/couchbase/godbc\" revision=\"2d3ecc3de903a5e4d0bc9181adedb5e637f83435\"/>",
 "  <project name=\"gojsonsm\" path=\"godeps/src/github.com/couchbaselabs/gojsonsm\" remote=\"couchbaselabs\" revision=\"8db06ae62940835d35db4de075bd68f0e00ea6b7\" groups=\"bsl\"/>",
 "  <project name=\"golang\" remote=\"couchbaselabs\" revision=\"4dd1b189981c94835b61c1607ca765e88604ce5a\" groups=\"kv\"/>",
 "  <project name=\"golang-pkg-pcre\" path=\"godeps/src/github.com/glenn-brown/golang-pkg-pcre\" remote=\"couchbasedeps\" revision=\"48bb82a8b8ceea98f4e97825b43870f6ba1970d6\"/>",
 "  <project name=\"golang-snappy\" path=\"godeps/src/github.com/golang/snappy\" remote=\"couchbasedeps\" revision=\"723cc1e459b8eea2dea4583200fd60757d40097a\"/>",
 "  <project name=\"golang-tools\" path=\"godeps/src/golang.org/x/tools\" remote=\"couchbasedeps\" revision=\"a28dfb48e06b2296b66678872c2cb638f0304f20\"/>",
 "  <project name=\"golang_protobuf_extensions\" path=\"godeps/src/github.com/matttproud/golang_protobuf_extensions\" remote=\"couchbasedeps\" revision=\"c182affec369e30f25d3eb8cd8a478dee585ae7d\"/>",
 "  <project name=\"gomemcached\" path=\"goproj/src/github.com/couchbase/gomemcached\" revision=\"689b8f03386ba2e7bac304bfd3a525b1e1427675\" groups=\"backup\"/>",
 "  <project name=\"gometa\" path=\"goproj/src/github.com/couchbase/gometa\" revision=\"816f7d6346c9fc5473c4a11e3efe9ed29a2f7f72\"/>",
 "  <project name=\"goutils\" path=\"goproj/src/github.com/couchbase/goutils\" revision=\"30adfca73d8113b5b217097414d7c3adeeef849a\" groups=\"bsl\"/>",
 "  <project name=\"goxdcr\" path=\"goproj/src/github.com/couchbase/goxdcr\" revision=\"4c570a31e5a6f3e087e147edf781022352497f64\" groups=\"bsl\"/>",
 "  <project name=\"gsl-lite\" path=\"third_party/gsl-lite\" remote=\"couchbasedeps\" revision=\"e1c381746c2625a76227255f999ae9f14a062208\" upstream=\"refs/tags/v0.38.1\" dest-branch=\"refs/tags/v0.38.1\" groups=\"kv\"/>",
 "  <project name=\"hebrew\" remote=\"couchbase-priv\" revision=\"c57616b187889a5318688f49817ccaceb9c098b9\" groups=\"notdefault,enterprise\"/>",
 "  <project name=\"indexing\" path=\"goproj/src/github.com/couchbase/indexing\" revision=\"7e924978fef8498113ae2d6e2be8ccd27da70d2d\" groups=\"bsl\"/>",
 "  <project name=\"jsonschema\" path=\"godeps/src/github.com/santhosh-tekuri/jsonschema\" remote=\"couchbasedeps\" revision=\"137f44a49015e5060a447c331aa37de6e0f50267\"/>",
 "  <project name=\"kv_engine\" revision=\"cfff435cf5fe6efcf1a18aa5580993fb8a4b2010\" groups=\"kv,bsl\"/>",
 "  <project name=\"libcouchbase\" revision=\"684931e59cd87e0c6292e8142c2b18897be5b10c\"/>",
 "  <project name=\"magma\" remote=\"couchbase-priv\" revision=\"86c2233ab8780e7aa71e0199bb957dcda2cf6cd1\" groups=\"notdefault,enterprise,kv_ee\"/>",
 "  <project name=\"mux\" path=\"godeps/src/github.com/gorilla/mux\" remote=\"couchbasedeps\" revision=\"043ee6597c29786140136a5747b6a886364f5282\"/>",
 "  <project name=\"n1fty\" path=\"goproj/src/github.com/couchbase/n1fty\" revision=\"a1fc533c18e5094ce75262c9e711d7189d256cd2\" groups=\"bsl\"/>",
 "  <project name=\"nitro\" path=\"goproj/src/github.com/couchbase/nitro\" revision=\"b70d849f0207f7cfe7ebf32b2db35b534929e041\" groups=\"bsl\"/>",
 "  <project name=\"ns_server\" revision=\"6954b533143cf0a8f906ef9086a8337ee50004a6\" groups=\"bsl\"/>",
 "  <project name=\"participle\" path=\"godeps/src/github.com/alecthomas/participle\" remote=\"couchbasedeps\" revision=\"d638c6e1953ed899e05a34da3935146790c60e46\"/>",
 "  <project name=\"perks\" path=\"godeps/src/github.com/beorn7/perks\" remote=\"couchbasedeps\" revision=\"37c8de3658fcb183f997c4e13e8337516ab753e6\" upstream=\"refs/tags/v1.0.1\" dest-branch=\"refs/tags/v1.0.1\"/>",
 "  <project name=\"phosphor\" revision=\"c0a034fe407eec4723f2e01db2d72762efdbc276\" groups=\"bsl,kv\"/>",
 "  <project name=\"pkcs8\" path=\"godeps/src/github.com/youmark/pkcs8\" remote=\"couchbasedeps\" revision=\"1be2e3e5546da8a58903ff4adcfab015022538ea\" upstream=\"refs/tags/v1.1\" dest-branch=\"refs/tags/v1.1\"/>",
 "  <project name=\"plasma\" path=\"goproj/src/github.com/couchbase/plasma\" remote=\"couchbase-priv\" revision=\"34f14cf1d8cc543932e60d9780cc13d48fa7ea5c\" groups=\"bsl,notdefault,enterprise\"/>",
 "  <project name=\"platform\" revision=\"a158d359293665b6251973868fdc42c3b642474c\" groups=\"bsl,kv\"/>",
 "  <project name=\"procfs\" path=\"godeps/src/github.com/prometheus/procfs\" remote=\"couchbasedeps\" revision=\"76fc8b844e3a18c31bf689e4fe7efdd5a2f41298\"/>",
 "  <project name=\"product-metadata\" revision=\"1bd027c34f33919f7005ddae0ba032a3120fe776\" groups=\"notdefault,packaging\"/>",
 "  <project name=\"product-texts\" revision=\"ec39f811376df6d18e56c81873fd565093666505\" upstream=\"master\" dest-branch=\"master\"/>",
 "  <project name=\"protobuf\" path=\"godeps/src/github.com/golang/protobuf\" remote=\"couchbasedeps\" revision=\"d04d7b157bb510b1e0c10132224b616ac0e26b17\" upstream=\"refs/tags/v1.4.2\" dest-branch=\"refs/tags/v1.4.2\"/>",
 "  <project name=\"protobuf-go\" path=\"godeps/src/google.golang.org/protobuf\" remote=\"couchbasedeps\" revision=\"32051b4f86e54c2142c7c05362c6e96ae3454a1c\" upstream=\"refs/tags/v1.28.0\" dest-branch=\"refs/tags/v1.28.0\"/>",
 "  <project name=\"query\" path=\"goproj/src/github.com/couchbase/query\" revision=\"5da4cb71df5aa00aad1e01e9a3b9d5be0c4f2769\" groups=\"bsl\"/>",
 "  <project name=\"query-ee\" path=\"goproj/src/github.com/couchbase/query-ee\" remote=\"couchbase-priv\" revision=\"6924a352019351c746fe08a2cf9a1993b54093e8\" groups=\"notdefault,enterprise\"/>",
 "  <project name=\"query-ui\" revision=\"abcc90e091c46ad74a59bb2fe768b6f09864ddbf\" groups=\"bsl\"/>",
 "  <project name=\"regulator\" path=\"goproj/src/github.com/couchbase/regulator\" remote=\"couchbase-priv\" revision=\"4ef404748ecc34fd87bdebc56074ebe99d240464\" groups=\"notdefault,enterprise\"/>",
 "  <project name=\"sigar\" revision=\"2da0c123cfb45ae39e76e730bd960db8812e3f20\" groups=\"kv\"/>",
 "  <project name=\"simdutf\" path=\"third_party/simdutf\" remote=\"couchbasedeps\" revision=\"4a212616ba23c65c7048f9604faccbff5353300f\" upstream=\"refs/tags/v3.2.14\" dest-branch=\"refs/tags/v3.2.14\" groups=\"kv\"/>",
 "  <project name=\"subjson\" revision=\"a619faccb30e43a4bc0708ee11b1b24abb349f18\" groups=\"bsl,kv\"/>",
 "  <project name=\"sys\" path=\"godeps/src/golang.org/x/sys\" remote=\"couchbasedeps\" revision=\"d36c6a25d886e7c9975d5bf247ac24887ba6da37\"/>",
 "  <project name=\"testrunner\" revision=\"b2d76b82c7d75a75ae78f787a76dc8300c427adf\" upstream=\"trinity\" dest-branch=\"trinity\"/>",
 "  <project name=\"tlm\" revision=\"0c610d8e4738567440ffb1f557dfa15bff81b99d\" upstream=\"7.6.2\" dest-branch=\"7.6.2\" groups=\"bsl,kv\">",
 "    <copyfile src=\"Build.sh\" dest=\"Build.sh\"/>",
 "    <copyfile src=\"GNUmakefile\" dest=\"GNUmakefile\"/>",
 "    <copyfile src=\"Makefile\" dest=\"Makefile\"/>",
 "    <copyfile src=\"CMakeLists.txt\" dest=\"CMakeLists.txt\"/>",
 "    <copyfile src=\"dot-clang-format\" dest=\".clang-format\"/>",
 "    <copyfile src=\"dot-clang-tidy\" dest=\".clang-tidy\"/>",
 "    <copyfile src=\"third-party-CMakeLists.txt\" dest=\"third_party/CMakeLists.txt\"/>",
 "  </project>",
 "  <project name=\"udf-api\" path=\"goproj/src/github.com/couchbase/udf-api\" revision=\"b2788ae3d412356a330b36d7f38ad2c66edb5879\"/>",
 "  <project name=\"uuid\" path=\"godeps/src/github.com/google/uuid\" remote=\"couchbasedeps\" revision=\"dec09d789f3dba190787f8b4454c7d3c936fed9e\"/>",
 "  <project name=\"vbmap\" revision=\"6cce93c4af4497d8108c3ed31b84d7139321cc82\"/>",
 "  <project name=\"voltron\" remote=\"couchbase-priv\" revision=\"19881dacfffb6d834a7aaa4a6d1925a904ea387f\" groups=\"notdefault,packaging\"/>",
 "  <project name=\"xxhash\" path=\"goproj/src/github.com/cespare/xxhash\" remote=\"couchbasedeps\" revision=\"e7a6b52374f7e2abfb8abb27249d53a1997b09a7\" upstream=\"refs/tags/v2.1.2\" dest-branch=\"refs/tags/v2.1.2\"/>",
 "</manifest>"]

[error_logger:info,2025-05-15T18:46:45.772Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.237.0>},
              {id,timeout_diag_logger},
              {mfargs,{timeout_diag_logger,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.776Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.238.0>},
              {id,ns_cookie_manager},
              {mfargs,{ns_cookie_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:45.786Z,ns_1@cb.local:chronicle_local<0.239.0>:chronicle_local:init:54]Ensure chronicle is started
[error_logger:info,2025-05-15T18:46:45.807Z,ns_1@cb.local:chronicle_sup<0.244.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.245.0>},
              {id,chronicle_events},
              {mfargs,{chronicle_events,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.807Z,ns_1@cb.local:chronicle_sup<0.244.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.246.0>},
              {id,chronicle_external_events},
              {mfargs,{gen_event,start_link,
                                 [{local,chronicle_external_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.810Z,ns_1@cb.local:chronicle_sup<0.244.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.247.0>},
              {id,chronicle_ets},
              {mfargs,{chronicle_ets,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.814Z,ns_1@cb.local:chronicle_sup<0.244.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.248.0>},
              {id,chronicle_settings},
              {mfargs,{chronicle_settings,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.818Z,ns_1@cb.local:chronicle_agent_sup<0.249.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_agent_sup}
    started: [{pid,<0.250.0>},
              {id,chronicle_snapshot_mgr},
              {mfargs,{chronicle_snapshot_mgr,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.818Z,ns_1@cb.local:chronicle_agent_sup<0.249.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_agent_sup}
    started: [{pid,<0.251.0>},
              {id,chronicle_rsm_events},
              {mfargs,{chronicle_events,start_link,[chronicle_rsm_events]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[chronicle:info,2025-05-15T18:46:45.845Z,ns_1@cb.local:chronicle_agent<0.252.0>:chronicle_storage:open_logs:180]Reading log files [{0,
                    "/opt/couchbase/var/lib/couchbase/config/chronicle/logs/0.log"}]
[chronicle:info,2025-05-15T18:46:45.847Z,ns_1@cb.local:chronicle_agent<0.252.0>:chronicle_storage:open_current_log:232]Error while opening log file "/opt/couchbase/var/lib/couchbase/config/chronicle/logs/0.log": no_header
[chronicle:info,2025-05-15T18:46:45.847Z,ns_1@cb.local:chronicle_agent<0.252.0>:chronicle_storage:create_log:280]Creating log file /opt/couchbase/var/lib/couchbase/config/chronicle/logs/0.log (high seqno = 0)
[chronicle:info,2025-05-15T18:46:45.849Z,ns_1@cb.local:chronicle_agent<0.252.0>:chronicle_agent:maybe_seed_storage:2444]Found empty storage. Seeding it with default metadata:
#{committed_seqno => 0,history_id => <<"no-history">>,peer => nonode@nohost,
  peer_id => <<>>,pending_branch => undefined,state => not_provisioned,
  term => {0,nonode@nohost}}
[error_logger:info,2025-05-15T18:46:45.849Z,ns_1@cb.local:chronicle_agent_sup<0.249.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_agent_sup}
    started: [{pid,<0.252.0>},
              {id,chronicle_agent},
              {mfargs,{chronicle_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.849Z,ns_1@cb.local:chronicle_sup<0.244.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.249.0>},
              {id,chronicle_agent_sup},
              {mfargs,{chronicle_agent_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:45.852Z,ns_1@cb.local:chronicle_sup<0.244.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.254.0>},
              {id,chronicle_secondary_sup},
              {mfargs,{chronicle_secondary_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:45.852Z,ns_1@cb.local:application_controller<0.44.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    application: chronicle
    started_at: 'ns_1@cb.local'

[ns_server:debug,2025-05-15T18:46:45.854Z,ns_1@cb.local:chronicle_local<0.239.0>:chronicle_local:init:58]Chronicle state is: not_provisioned
[ns_server:debug,2025-05-15T18:46:45.854Z,ns_1@cb.local:chronicle_local<0.239.0>:chronicle_local:provision:139]Provision chronicle on this node
[chronicle:debug,2025-05-15T18:46:45.861Z,ns_1@cb.local:chronicle_agent<0.252.0>:chronicle_agent:handle_provision:1199]Provisioning with history <<"132dcb91b709843f36fcbbab0f42c041">>. Config:
{config,undefined,0,undefined,
        #{'ns_1@cb.local' =>
              #{id => <<"9a5e5be0711d2936f646a0892bd096e1">>,role => voter}},
        undefined,
        #{chronicle_config_rsm => {rsm_config,chronicle_config_rsm,[]},
          kv => {rsm_config,chronicle_kv,[]}},
        #{},undefined,
        [{<<"132dcb91b709843f36fcbbab0f42c041">>,0}]}
[error_logger:info,2025-05-15T18:46:45.864Z,ns_1@cb.local:<0.255.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.255.0>,dynamic_supervisor}
    started: [{pid,<0.256.0>},
              {id,chronicle_leader},
              {mfargs,{chronicle_leader,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.868Z,ns_1@cb.local:chronicle_secondary_restartable_sup<0.257.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_secondary_restartable_sup}
    started: [{pid,<0.258.0>},
              {id,chronicle_status},
              {mfargs,{chronicle_status,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.870Z,ns_1@cb.local:chronicle_secondary_restartable_sup<0.257.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_secondary_restartable_sup}
    started: [{pid,<0.259.0>},
              {id,chronicle_failover},
              {mfargs,{chronicle_failover,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.870Z,ns_1@cb.local:<0.255.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.255.0>,dynamic_supervisor}
    started: [{pid,<0.257.0>},
              {id,chronicle_secondary_restartable_sup},
              {mfargs,{chronicle_secondary_restartable_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:45.877Z,ns_1@cb.local:<0.255.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.255.0>,dynamic_supervisor}
    started: [{pid,<0.260.0>},
              {id,chronicle_server},
              {mfargs,{chronicle_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[chronicle:info,2025-05-15T18:46:45.892Z,ns_1@cb.local:chronicle_config_rsm<0.264.0>:chronicle_rsm:get_incarnation:1594]No incarnation file found at '/opt/couchbase/var/lib/couchbase/config/chronicle/rsms/chronicle_config_rsm/incarnation'
[chronicle:debug,2025-05-15T18:46:45.895Z,ns_1@cb.local:chronicle_server<0.260.0>:chronicle_server:handle_register_rsm:361]Registering RSM chronicle_config_rsm with pid <0.264.0>
[error_logger:info,2025-05-15T18:46:45.895Z,ns_1@cb.local:<0.263.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.263.0>,chronicle_single_rsm_sup}
    started: [{pid,<0.264.0>},
              {id,chronicle_config_rsm},
              {mfargs,{chronicle_rsm,start_link,
                                     [chronicle_config_rsm,
                                      <<"9a5e5be0711d2936f646a0892bd096e1">>,
                                      chronicle_config_rsm,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.895Z,ns_1@cb.local:<0.262.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.262.0>,dynamic_supervisor}
    started: [{pid,<0.263.0>},
              {id,chronicle_config_rsm},
              {mfargs,
                  {chronicle_single_rsm_sup,start_link,
                      [chronicle_config_rsm,
                       <<"9a5e5be0711d2936f646a0892bd096e1">>,
                       chronicle_config_rsm,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:45.897Z,ns_1@cb.local:<0.266.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.266.0>,chronicle_single_rsm_sup}
    started: [{pid,<0.267.0>},
              {id,'kv-events'},
              {mfargs,{gen_event,start_link,[{local,'kv-events'}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[chronicle:info,2025-05-15T18:46:45.898Z,ns_1@cb.local:kv<0.268.0>:chronicle_rsm:get_incarnation:1594]No incarnation file found at '/opt/couchbase/var/lib/couchbase/config/chronicle/rsms/kv/incarnation'
[chronicle:debug,2025-05-15T18:46:45.899Z,ns_1@cb.local:chronicle_server<0.260.0>:chronicle_server:handle_register_rsm:361]Registering RSM kv with pid <0.268.0>
[error_logger:info,2025-05-15T18:46:45.899Z,ns_1@cb.local:<0.266.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.266.0>,chronicle_single_rsm_sup}
    started: [{pid,<0.268.0>},
              {id,kv},
              {mfargs,{chronicle_rsm,start_link,
                                     [kv,
                                      <<"9a5e5be0711d2936f646a0892bd096e1">>,
                                      chronicle_kv,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.899Z,ns_1@cb.local:<0.262.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.262.0>,dynamic_supervisor}
    started: [{pid,<0.266.0>},
              {id,kv},
              {mfargs,
                  {chronicle_single_rsm_sup,start_link,
                      [kv,<<"9a5e5be0711d2936f646a0892bd096e1">>,chronicle_kv,
                       []]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:45.900Z,ns_1@cb.local:<0.255.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.255.0>,dynamic_supervisor}
    started: [{pid,<0.261.0>},
              {id,chronicle_rsm_sup},
              {mfargs,{chronicle_rsm_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:info,2025-05-15T18:46:45.902Z,ns_1@cb.local:chronicle_local<0.239.0>:chronicle_upgrade:initialize:70]Setup initial chronicle content [{set,counters,[]},
                                 {set,auto_reprovision_cfg,
                                  [{enabled,true},{max_nodes,1},{count,0}]},
                                 {set,bucket_names,[]},
                                 {set,nodes_wanted,['ns_1@cb.local']},
                                 {set,server_groups,
                                  [[{uuid,<<"0">>},
                                    {name,<<"Group 1">>},
                                    {nodes,['ns_1@cb.local']}]]},
                                 {set,
                                  {node,'ns_1@cb.local',membership},
                                  active},
                                 {set,autocompaction,
                                  [{database_fragmentation_threshold,
                                    {30,undefined}},
                                   {view_fragmentation_threshold,
                                    {30,undefined}},
                                   {magma_fragmentation_percentage,50}]}]
[chronicle:debug,2025-05-15T18:46:46.104Z,ns_1@cb.local:chronicle_leader<0.256.0>:chronicle_leader:handle_state_timeout:608]State timeout when state is: {observer,true,false}
[chronicle:info,2025-05-15T18:46:46.105Z,ns_1@cb.local:<0.270.0>:chronicle_leader:do_election_worker:892]Starting election.
History ID: <<"132dcb91b709843f36fcbbab0f42c041">>
Log position: {{1,'ns_1@cb.local'},1}
Peers: ['ns_1@cb.local']
Required quorum: {majority,{set,1,16,16,8,80,48,
                                {[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],
                                 []},
                                {{[],[],[],[],[],[],[],[],[],[],[],[],
                                  ['ns_1@cb.local'],
                                  [],[],[]}}}}
[chronicle:info,2025-05-15T18:46:46.105Z,ns_1@cb.local:<0.270.0>:chronicle_leader:do_election_worker:901]I'm the only peer, so I'm the leader.
[chronicle:info,2025-05-15T18:46:46.105Z,ns_1@cb.local:chronicle_leader<0.256.0>:chronicle_leader:handle_election_result:691]Going to become a leader in term {2,'ns_1@cb.local'} (history id <<"132dcb91b709843f36fcbbab0f42c041">>)
[chronicle:debug,2025-05-15T18:46:46.109Z,ns_1@cb.local:chronicle_agent<0.252.0>:chronicle_agent:handle_establish_term:1529]Accepted term {2,'ns_1@cb.local'} in history <<"132dcb91b709843f36fcbbab0f42c041">>
[chronicle:debug,2025-05-15T18:46:46.109Z,ns_1@cb.local:chronicle_proposer<0.271.0>:chronicle_proposer:establish_term_init:367]Going to establish term {2,'ns_1@cb.local'} (history id <<"132dcb91b709843f36fcbbab0f42c041">>).
Quorum peers: ['ns_1@cb.local']
Metadata:
{metadata,'ns_1@cb.local',<<"9a5e5be0711d2936f646a0892bd096e1">>,
          <<"132dcb91b709843f36fcbbab0f42c041">>,
          {1,'ns_1@cb.local'},
          {1,'ns_1@cb.local'},
          1,1,
          {log_entry,<<"132dcb91b709843f36fcbbab0f42c041">>,
                     {1,'ns_1@cb.local'},
                     1,
                     {config,undefined,0,undefined,
                             #{'ns_1@cb.local' =>
                                   #{id =>
                                         <<"9a5e5be0711d2936f646a0892bd096e1">>,
                                     role => voter}},
                             undefined,
                             #{chronicle_config_rsm =>
                                   {rsm_config,chronicle_config_rsm,[]},
                               kv => {rsm_config,chronicle_kv,[]}},
                             #{},undefined,
                             [{<<"132dcb91b709843f36fcbbab0f42c041">>,0}]}},
          {log_entry,<<"132dcb91b709843f36fcbbab0f42c041">>,
                     {1,'ns_1@cb.local'},
                     1,
                     {config,undefined,0,undefined,
                             #{'ns_1@cb.local' =>
                                   #{id =>
                                         <<"9a5e5be0711d2936f646a0892bd096e1">>,
                                     role => voter}},
                             undefined,
                             #{chronicle_config_rsm =>
                                   {rsm_config,chronicle_config_rsm,[]},
                               kv => {rsm_config,chronicle_kv,[]}},
                             #{},undefined,
                             [{<<"132dcb91b709843f36fcbbab0f42c041">>,0}]}},
          undefined}
[chronicle:debug,2025-05-15T18:46:46.110Z,ns_1@cb.local:chronicle_proposer<0.271.0>:chronicle_proposer:establish_term_maybe_transition:686]Established term {2,'ns_1@cb.local'} (history id <<"132dcb91b709843f36fcbbab0f42c041">>) successfully.
Votes: ['ns_1@cb.local']
[chronicle:debug,2025-05-15T18:46:46.110Z,ns_1@cb.local:chronicle_proposer<0.271.0>:chronicle_proposer:handle_state_enter:284]Starting recovery for term {2,'ns_1@cb.local'} in history <<"132dcb91b709843f36fcbbab0f42c041">>
[chronicle:debug,2025-05-15T18:46:46.115Z,ns_1@cb.local:chronicle_proposer<0.271.0>:chronicle_proposer:handle_state_enter:296]Proposer for term {2,'ns_1@cb.local'} in history <<"132dcb91b709843f36fcbbab0f42c041">> is ready. Committed seqno: 2
[chronicle:info,2025-05-15T18:46:46.115Z,ns_1@cb.local:chronicle_leader<0.256.0>:chronicle_leader:handle_note_term_status:596]Term {2,'ns_1@cb.local'} established.
[ns_server:info,2025-05-15T18:46:46.137Z,ns_1@cb.local:chronicle_local<0.239.0>:chronicle_upgrade:initialize:72]Chronicle content was initialized. Rev = {<<"132dcb91b709843f36fcbbab0f42c041">>,
                                          3}.
[error_logger:info,2025-05-15T18:46:46.138Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.239.0>},
              {id,chronicle_local},
              {mfargs,{chronicle_local,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:46.139Z,ns_1@cb.local:ns_cluster<0.273.0>:ns_cluster:handle_info:514]Chronicle state is: provisioned
[error_logger:info,2025-05-15T18:46:46.139Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.273.0>},
              {id,ns_cluster},
              {mfargs,{ns_cluster,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:46.141Z,ns_1@cb.local:sigar<0.275.0>:sigar:spawn_sigar:134]Spawning sigar process 'portsigar for ns_1@cb.local'("/opt/couchbase/bin/sigar_port") with babysitter pid: 41 and log file "/opt/couchbase/var/lib/couchbase/logs/sigar_port.log"
[error_logger:info,2025-05-15T18:46:46.142Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.275.0>},
              {id,sigar},
              {mfargs,{sigar,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:46.144Z,ns_1@cb.local:ns_config_sup<0.276.0>:ns_config_sup:init:26]loading static ns_config from "/opt/couchbase/etc/couchbase/config"
[error_logger:info,2025-05-15T18:46:46.145Z,ns_1@cb.local:ns_config_sup<0.276.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.277.0>},
              {id,tombstone_keeper},
              {mfargs,{tombstone_keeper,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:46.145Z,ns_1@cb.local:ns_config_sup<0.276.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.278.0>},
              {id,ns_config_events},
              {mfargs,{gen_event,start_link,[{local,ns_config_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:46.146Z,ns_1@cb.local:ns_config_sup<0.276.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.279.0>},
              {id,ns_config_events_local},
              {mfargs,{gen_event,start_link,[{local,ns_config_events_local}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:46.228Z,ns_1@cb.local:ns_config<0.280.0>:ns_config:load_config:1113]Loading static config from "/opt/couchbase/etc/couchbase/config"
[ns_server:info,2025-05-15T18:46:46.229Z,ns_1@cb.local:ns_config<0.280.0>:ns_config:load_config:1127]Loading dynamic config from "/opt/couchbase/var/lib/couchbase/config/config.dat"
[ns_server:info,2025-05-15T18:46:46.229Z,ns_1@cb.local:ns_config<0.280.0>:ns_config:load_config:1132]No dynamic config file found. Assuming we're brand new node
[ns_server:debug,2025-05-15T18:46:46.235Z,ns_1@cb.local:ns_config<0.280.0>:ns_config:load_config:1135]Here's full dynamic config we loaded:
[[]]
[ns_server:info,2025-05-15T18:46:46.239Z,ns_1@cb.local:ns_config<0.280.0>:ns_config:load_config:1157]Here's full dynamic config we loaded + static & default config:
[{resource_management,
  [{bucket,
    [{resident_ratio,
      [{enabled,false},{couchstore_minimum,1},{magma_minimum,0.2}]},
     {data_size,[{enabled,false},{couchstore_maximum,2},{magma_maximum,16}]}]},
   {index,[]},
   {cores_per_bucket,[{enabled,false},{minimum,0.4}]},
   {disk_usage,[{enabled,false},{maximum,96}]},
   {collections_per_quota,[{enabled,false},{maximum,1}]}]},
 {{node,'ns_1@cb.local',index_dir},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]},
   47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
   98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
 {{node,'ns_1@cb.local',database_dir},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]},
   47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
   98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
 {auto_failover_cfg,
  [{enabled,true},
   {timeout,120},
   {count,0},
   {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
   {failover_server_group,false},
   {max_count,1},
   {failed_over_server_groups,[]},
   {can_abort_rebalance,true}]},
 {retry_rebalance,[{enabled,false},{after_time_period,300},{max_attempts,1}]},
 {{node,'ns_1@cb.local',{project_intact,is_vulnerable}},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   false]},
 {{node,'ns_1@cb.local',ssl_capi_port},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   18092]},
 {{node,'ns_1@cb.local',capi_port},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   8092]},
 {{node,'ns_1@cb.local',backup_grpc_port},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   9124]},
 {{node,'ns_1@cb.local',backup_https_port},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   18097]},
 {{node,'ns_1@cb.local',backup_http_port},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   8097]},
 {{node,'ns_1@cb.local',prometheus_http_port},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   9123]},
 {{node,'ns_1@cb.local',cbas_debug_port},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|-1]},
 {{node,'ns_1@cb.local',cbas_parent_port},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   9122]},
 {{node,'ns_1@cb.local',cbas_metadata_port},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   9121]},
 {{node,'ns_1@cb.local',cbas_replication_port},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   9120]},
 {{node,'ns_1@cb.local',cbas_metadata_callback_port},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   9119]},
 {{node,'ns_1@cb.local',cbas_messaging_port},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   9118]},
 {{node,'ns_1@cb.local',cbas_result_port},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   9117]},
 {{node,'ns_1@cb.local',cbas_data_port},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   9116]},
 {{node,'ns_1@cb.local',cbas_cluster_port},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   9115]},
 {{node,'ns_1@cb.local',cbas_console_port},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   9114]},
 {{node,'ns_1@cb.local',cbas_cc_client_port},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   9113]},
 {{node,'ns_1@cb.local',cbas_cc_cluster_port},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   9112]},
 {{node,'ns_1@cb.local',cbas_cc_http_port},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   9111]},
 {{node,'ns_1@cb.local',cbas_admin_port},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   9110]},
 {{node,'ns_1@cb.local',cbas_ssl_port},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   18095]},
 {{node,'ns_1@cb.local',cbas_http_port},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   8095]},
 {{node,'ns_1@cb.local',eventing_https_port},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   18096]},
 {{node,'ns_1@cb.local',eventing_debug_port},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   9140]},
 {{node,'ns_1@cb.local',eventing_http_port},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   8096]},
 {{node,'ns_1@cb.local',fts_grpc_ssl_port},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   19130]},
 {{node,'ns_1@cb.local',fts_grpc_port},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   9130]},
 {{node,'ns_1@cb.local',fts_ssl_port},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   18094]},
 {{node,'ns_1@cb.local',fts_http_port},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   8094]},
 {{node,'ns_1@cb.local',indexer_https_port},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   19102]},
 {{node,'ns_1@cb.local',indexer_stmaint_port},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   9105]},
 {{node,'ns_1@cb.local',indexer_stcatchup_port},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   9104]},
 {{node,'ns_1@cb.local',indexer_stinit_port},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   9103]},
 {{node,'ns_1@cb.local',indexer_http_port},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   9102]},
 {{node,'ns_1@cb.local',indexer_scan_port},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   9101]},
 {{node,'ns_1@cb.local',indexer_admin_port},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   9100]},
 {{node,'ns_1@cb.local',ssl_query_port},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   18093]},
 {{node,'ns_1@cb.local',query_port},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   8093]},
 {{node,'ns_1@cb.local',projector_ssl_port},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   9999]},
 {{node,'ns_1@cb.local',projector_port},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   9999]},
 {{node,'ns_1@cb.local',memcached_prometheus},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   11280]},
 {{node,'ns_1@cb.local',memcached_dedicated_ssl_port},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   11206]},
 {{node,'ns_1@cb.local',xdcr_rest_port},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   9998]},
 {{node,'ns_1@cb.local',ssl_rest_port},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   18091]},
 {{node,'ns_1@cb.local',rest},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]},
   {port,8091},
   {port_meta,global}]},
 {rest,[{port,8091}]},
 {password_policy,[{min_length,6},{must_present,[]}]},
 {health_monitor_refresh_interval,[]},
 {service_orchestrator_weight,
  [{kv,10000},
   {index,1000},
   {fts,1000},
   {cbas,1000},
   {n1ql,100},
   {eventing,100},
   {backup,10}]},
 {log_redaction_default_cfg,[{redact_level,none}]},
 {replication,[{enabled,true}]},
 {alert_limits,
  [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
 {email_alerts,
  [{recipients,["root@localhost"]},
   {sender,"couchbase@localhost"},
   {enabled,false},
   {email_server,
    [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
   {alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     indexer_low_resident_percentage,ep_clock_cas_drift_threshold_exceeded,
     communication_issue,time_out_of_sync,disk_usage_analyzer_stuck,
     cert_expires_soon,cert_expired,memory_threshold,history_size_warning,
     stuck_rebalance,memcached_connections]},
   {pop_up_alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     indexer_low_resident_percentage,ep_clock_cas_drift_threshold_exceeded,
     communication_issue,time_out_of_sync,disk_usage_analyzer_stuck,
     cert_expires_soon,cert_expired,memory_threshold,history_size_warning,
     stuck_rebalance,memcached_connections]}]},
 {{node,'ns_1@cb.local',event_log},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/event_log"}]},
 {{node,'ns_1@cb.local',ns_log},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
 {{node,'ns_1@cb.local',port_servers},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}]},
 {secure_headers,[]},
 {buckets,[{configs,[]}]},
 {cbas_memory_quota,1549},
 {fts_memory_quota,512},
 {memory_quota,3406},
 {{node,'ns_1@cb.local',memcached_config},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   {[{interfaces,{memcached_config_mgr,get_interfaces,[]}},
     {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
     {connection_idle_time,connection_idle_time},
     {privilege_debug,privilege_debug},
     {breakpad,
      {[{enabled,breakpad_enabled},
        {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
     {deployment_model,{memcached_config_mgr,get_config_profile,[]}},
     {verbosity,verbosity},
     {audit_file,{"~s",[audit_file]}},
     {rbac_file,{"~s",[rbac_file]}},
     {dedupe_nmvb_maps,dedupe_nmvb_maps},
     {tracing_enabled,tracing_enabled},
     {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
     {xattr_enabled,true},
     {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
     {collections_enabled,true},
     {max_connections,max_connections},
     {system_connections,system_connections},
     {num_reader_threads,num_reader_threads},
     {num_writer_threads,num_writer_threads},
     {num_auxio_threads,num_auxio_threads},
     {num_nonio_threads,num_nonio_threads},
     {num_storage_threads,num_storage_threads},
     {logger,
      {[{filename,{"~s/~s",[log_path,log_prefix]}},
        {cyclesize,log_cyclesize}]}},
     {external_auth_service,
      {memcached_config_mgr,get_external_auth_service,[]}},
     {active_external_users_push_interval,
      {memcached_config_mgr,get_external_users_push_interval,[]}},
     {prometheus,{memcached_config_mgr,prometheus_cfg,[]}},
     {sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
     {ssl_sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
     {tcp_keepalive_idle,tcp_keepalive_idle},
     {tcp_keepalive_interval,tcp_keepalive_interval},
     {tcp_keepalive_probes,tcp_keepalive_probes},
     {tcp_user_timeout,tcp_user_timeout},
     {always_collect_trace_info,always_collect_trace_info},
     {connection_limit_mode,connection_limit_mode},
     {free_connection_pool_size,free_connection_pool_size},
     {max_client_connection_details,max_client_connection_details}]}]},
 {{node,'ns_1@cb.local',memcached},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]},
   {port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,11206},
   {ssl_port,11207},
   {admin_user,"@ns_server"},
   {other_users,
    ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing","@cbas",
     "@backup"]},
   {admin_pass,"*****"},
   {engines,
    [{membase,
      [{engine,"/opt/couchbase/lib/memcached/ep.so"},
       {static_config_string,"failpartialwarmup=false"}]},
     {memcached,
      [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
       {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_rotation_period,39003}]},
 {{node,'ns_1@cb.local',memcached_defaults},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]},
   {max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {je_malloc_conf,undefined},
   {tracing_enabled,true},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>},
   {num_auxio_threads,<<"default">>},
   {num_nonio_threads,<<"default">>},
   {num_storage_threads,<<"default">>},
   {tcp_keepalive_idle,360},
   {tcp_keepalive_interval,10},
   {tcp_keepalive_probes,3},
   {tcp_user_timeout,30},
   {always_collect_trace_info,true},
   {connection_limit_mode,<<"disconnect">>},
   {free_connection_pool_size,0},
   {max_client_connection_details,0}]},
 {memcached,[]},
 {{node,'ns_1@cb.local',audit},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}]},
 {audit,
  [{auditd_enabled,false},
   {rotate_interval,86400},
   {rotate_size,20971520},
   {prune_age,0},
   {disabled,[]},
   {enabled,[]},
   {disabled_users,[]},
   {sync,[]},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
 {{node,'ns_1@cb.local',isasl},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]},
   {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
 {remote_clusters,[]},
 {scramsha_fallback_salt,<<247,201,10,223,224,101,235,194,88,231,234,2>>},
 {client_cert_auth,[{state,"disable"},{prefixes,[]}]},
 {rest_creds,null},
 {{metakv,<<"/analytics/settings/config">>},
  <<"{\"analytics.settings.num_replicas\":0}">>},
 {{metakv,<<"/query/settings/config">>},
  <<"{\"cleanupclientattempts\":true,\"cleanuplostattempts\":true,\"cleanupwindow\":\"60s\",\"completed-limit\":4000,\"completed-threshold\":1000,\"loglevel\":\"info\",\"max-parallelism\":1,\"memory-quota\":0,\"n1ql-feat-ctrl\":76,\"numatrs\":1024,\"pipeline-batch\":16,\"pipeline-cap\":512,\"prepared-limit\":16384,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"query.settings.tmp_space_size\":5120,\"scan-cap\":512,\"timeout\":0,\"txtimeout\":\"0ms\",\"use-cbo\":true}">>},
 {{metakv,<<"/eventing/settings/config">>},<<"{\"ram_quota\":256}">>},
 {{metakv,<<"/indexing/settings/config">>},
  <<"{\"indexer.settings.compaction.abort_exceed_interval\":false,\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.enable_page_bloom_filter\":false,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.log_level\":\"info\",\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.num_replica\":0,\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.storage_mode\":\"\"}">>},
 {{couchdb,max_parallel_replica_indexers},2},
 {{couchdb,max_parallel_indexers},4},
 {quorum_nodes,['ns_1@cb.local']},
 {nodes_wanted,[]},
 {{node,'ns_1@cb.local',compaction_daemon},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]},
   {check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}]},
 {set_view_update_daemon,
  [{update_interval,5000},
   {update_min_changes,5000},
   {replica_update_min_changes,5000}]},
 {max_bucket_count,30},
 {index_aware_rebalance_disabled,false},
 {{node,'ns_1@cb.local',saslauthd_enabled},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   true]},
 {{node,'ns_1@cb.local',is_enterprise},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   true]},
 {{node,'ns_1@cb.local',config_version},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   {7,6}]},
 {{node,'ns_1@cb.local',uuid},
  [{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|
   <<"b0a71fbd3e8ac51d55b35452995f45cf">>]}]
[ns_server:debug,2025-05-15T18:46:46.246Z,ns_1@cb.local:tombstone_keeper<0.277.0>:tombstone_keeper:handle_call:49]Refreshed with timestamps []
[error_logger:info,2025-05-15T18:46:46.246Z,ns_1@cb.local:ns_config_sup<0.276.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.280.0>},
              {id,ns_config},
              {mfargs,{ns_config,start_link,
                                 ["/opt/couchbase/etc/couchbase/config",
                                  ns_config_default]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:46.247Z,ns_1@cb.local:ns_config_sup<0.276.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.283.0>},
              {id,ns_config_remote},
              {mfargs,{ns_config_replica,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:46.247Z,ns_1@cb.local:ns_config_sup<0.276.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.284.0>},
              {id,ns_config_log},
              {mfargs,{ns_config_log,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:46.247Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.276.0>},
              {id,ns_config_sup},
              {mfargs,{ns_config_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:46.249Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.286.0>},
              {id,netconfig_updater},
              {mfargs,{netconfig_updater,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:46.249Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',n2n_client_cert_auth} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|false]
[ns_server:debug,2025-05-15T18:46:46.250Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',erl_external_listeners} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]},
 {inet,false}]
[ns_server:debug,2025-05-15T18:46:46.250Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',node_encryption} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|false]
[ns_server:debug,2025-05-15T18:46:46.250Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',address_family} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]}|inet]
[error_logger:info,2025-05-15T18:46:46.255Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.289.0>},
              {id,json_rpc_connection_sup},
              {mfargs,{json_rpc_connection_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:46.272Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.292.0>},
              {id,chronicle_compat_events},
              {mfargs,{chronicle_compat_events,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:46.273Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.297.0>},
              {id,remote_monitors},
              {mfargs,{remote_monitors,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:46.275Z,ns_1@cb.local:menelaus_barrier<0.298.0>:one_shot_barrier:barrier_body:52]Barrier menelaus_barrier has started
[error_logger:info,2025-05-15T18:46:46.275Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.298.0>},
              {id,menelaus_barrier},
              {mfargs,{menelaus_sup,barrier_start_link,[]}},
              {restart_type,temporary},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:46.284Z,ns_1@cb.local:<0.301.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,
                                    rest_lhttpc_pool} with delay of 1000
[error_logger:info,2025-05-15T18:46:46.285Z,ns_1@cb.local:<0.302.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.302.0>,suppress_max_restart_intensity}
    started: [{pid,<0.303.0>},
              {id,rest_lhttpc_pool},
              {mfargs,{lhttpc_manager,start_link,
                                      [[{name,rest_lhttpc_pool},
                                        {connection_timeout,120000},
                                        {pool_size,20}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:46.285Z,ns_1@cb.local:<0.300.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.300.0>,suppress_max_restart_intensity}
    started: [{pid,<0.301.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      rest_lhttpc_pool}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,rest_lhttpc_pool},
                       1000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 1,id => rest_lhttpc_pool,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [lhttpc_manager],
                          restart => permanent,shutdown => 1000,
                          start =>
                              {lhttpc_manager,start_link,
                                  [[{name,rest_lhttpc_pool},
                                    {connection_timeout,120000},
                                    {pool_size,20}]]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:46.285Z,ns_1@cb.local:rest_lhttpc_pool_sup<0.299.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,rest_lhttpc_pool_sup}
    started: [{pid,<0.300.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,rest_lhttpc_pool}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 1,id => rest_lhttpc_pool,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [lhttpc_manager],
                         restart => permanent,shutdown => 1000,
                         start =>
                             {lhttpc_manager,start_link,
                                 [[{name,rest_lhttpc_pool},
                                   {connection_timeout,120000},
                                   {pool_size,20}]]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:46.285Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.299.0>},
              {id,rest_lhttpc_pool_sup},
              {mfargs,{rest_lhttpc_pool_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:46.289Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.304.0>},
              {id,memcached_refresh},
              {mfargs,{memcached_refresh,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:46.292Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.305.0>},
              {id,ns_secrets},
              {mfargs,{ns_secrets,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:46.294Z,ns_1@cb.local:ns_ssl_services_sup<0.306.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_ssl_services_sup}
    started: [{pid,<0.307.0>},
              {id,ssl_service_events},
              {mfargs,{gen_event,start_link,[{local,ssl_service_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:46.303Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:maybe_store_ca_certs:832]Considering to store CA certs
[ns_server:debug,2025-05-15T18:46:46.619Z,ns_1@cb.local:<0.312.0>:goport:handle_eof:592]Stream 'stderr' closed
[ns_server:debug,2025-05-15T18:46:46.619Z,ns_1@cb.local:<0.312.0>:goport:handle_eof:592]Stream 'stdout' closed
[ns_server:info,2025-05-15T18:46:46.619Z,ns_1@cb.local:<0.312.0>:goport:handle_process_exit:573]Port exited with status 0.
[ns_server:debug,2025-05-15T18:46:46.629Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_server_cert:generate_cert_and_pkey:155]Generated certificate and private key in 322371 us
[ns_server:debug,2025-05-15T18:46:46.699Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:maybe_store_ca_certs:847]Updating CA file with 1 certificates
[ns_server:info,2025-05-15T18:46:46.703Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:maybe_store_ca_certs:850]CA file updated: 1 cert(s) written
[ns_server:info,2025-05-15T18:46:46.703Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:should_regenerate_certs:899]Should regenerate node_cert because there are no certs on this node
[ns_server:debug,2025-05-15T18:46:46.741Z,ns_1@cb.local:<0.317.0>:goport:handle_eof:592]Stream 'stderr' closed
[ns_server:debug,2025-05-15T18:46:46.742Z,ns_1@cb.local:<0.317.0>:goport:handle_eof:592]Stream 'stdout' closed
[ns_server:info,2025-05-15T18:46:46.742Z,ns_1@cb.local:<0.317.0>:goport:handle_process_exit:573]Port exited with status 0.
[ns_server:info,2025-05-15T18:46:46.747Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:save_certs:968]New node_cert and pkey are written to tmp file
[ns_server:info,2025-05-15T18:46:46.760Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:save_certs_phase2:995]node_cert cert and pkey files updated
[ns_server:debug,2025-05-15T18:46:46.760Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',node_cert} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]},
 {subject,<<"CN=Couchbase Server Node (127.0.0.1)">>},
 {not_after,63985747606},
 {verified_with,<<128,148,229,239,235,30,169,7,14,184,223,98,33,217,4,94>>},
 {load_timestamp,63914554006},
 {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv7QbCrEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciAxNDYxY2MyMjAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgMTQ2MWNj\nMjIwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC1eALPemMHR7JjfP48\nA8J89K045QIJUgPOt1HHzMEhY1TyprxrDiVgWNH+8CDMk80tP3uhVWgJlhXC1d0T\nA01MvdBd6C8"...>>},
 {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDOTCCAiGgAwIBAgIIGD/Hv7mJZGEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciAxNDYxY2MyMjAeFw0yNTA1MTQxODQ2NDZaFw0y\nNzA4MTcxODQ2NDZaMCwxKjAoBgNVBAMTIUNvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nMTI3LjAuMC4xKTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAKgrtzyE\ny80R8HI6Itm+lmIwA0qGI6+1dHJPKG+3vMEK26xuMWIL3bGkDCtJcyrqrw3sPB+N\nQCZ6wOg"...>>},
 {pkey_passphrase_settings,[]},
 {certs_epoch,0},
 {type,generated},
 {hostname,"127.0.0.1"}]
[ns_server:info,2025-05-15T18:46:46.766Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:should_regenerate_certs:899]Should regenerate client_cert because there are no certs on this node
[ns_server:debug,2025-05-15T18:46:46.925Z,ns_1@cb.local:<0.323.0>:goport:handle_eof:592]Stream 'stderr' closed
[ns_server:debug,2025-05-15T18:46:46.926Z,ns_1@cb.local:<0.323.0>:goport:handle_eof:592]Stream 'stdout' closed
[ns_server:info,2025-05-15T18:46:46.926Z,ns_1@cb.local:<0.323.0>:goport:handle_process_exit:573]Port exited with status 0.
[ns_server:info,2025-05-15T18:46:46.929Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:save_certs:968]New client_cert and pkey are written to tmp file
[ns_server:info,2025-05-15T18:46:46.936Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:save_certs_phase2:995]client_cert cert and pkey files updated
[ns_server:debug,2025-05-15T18:46:46.936Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',client_cert} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554006}}]},
 {subject,<<"CN=Couchbase Internal Client (24468677)">>},
 {not_after,63985747606},
 {verified_with,<<128,148,229,239,235,30,169,7,14,184,223,98,33,217,4,94>>},
 {load_timestamp,63914554006},
 {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv7QbCrEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciAxNDYxY2MyMjAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgMTQ2MWNj\nMjIwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC1eALPemMHR7JjfP48\nA8J89K045QIJUgPOt1HHzMEhY1TyprxrDiVgWNH+8CDMk80tP3uhVWgJlhXC1d0T\nA01MvdBd6C8"...>>},
 {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDWTCCAkGgAwIBAgIIGD/Hv72gZO4wDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciAxNDYxY2MyMjAeFw0yNTA1MTQxODQ2NDZaFw0y\nNzA4MTcxODQ2NDZaMC8xLTArBgNVBAMTJENvdWNoYmFzZSBJbnRlcm5hbCBDbGll\nbnQgKDI0NDY4Njc3KTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAPDg\nBsm0UIJQjJdPvcnOePVLeL7u3fVmfPtrCQBTsTBweRlUncLCjGvUzFiZsvoVJHf8\nDEG9VwZ"...>>},
 {pkey_passphrase_settings,[]},
 {certs_epoch,0},
 {type,generated},
 {name,"@internal"}]
[ns_server:debug,2025-05-15T18:46:46.968Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:restart_regenerate_client_cert_timer:1447]Time left before client cert regeneration: 70588800000
[ns_server:info,2025-05-15T18:46:46.968Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:handle_info:764]cert_and_pkey changed
[error_logger:info,2025-05-15T18:46:46.968Z,ns_1@cb.local:ns_ssl_services_sup<0.306.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_ssl_services_sup}
    started: [{pid,<0.308.0>},
              {id,ns_ssl_services_setup},
              {mfargs,{ns_ssl_services_setup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:46.980Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:notify_services:1146]Going to notify following services: [capi_ssl_service,cb_dist_tls,
                                     client_cert_event,memcached,
                                     server_cert_event,ssl_service]
[ns_server:info,2025-05-15T18:46:46.980Z,ns_1@cb.local:<0.343.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service server_cert_event
[ns_server:debug,2025-05-15T18:46:46.980Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Restarting tls distribution protocols (if any)
[ns_server:info,2025-05-15T18:46:46.980Z,ns_1@cb.local:<0.345.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service client_cert_event
[ns_server:debug,2025-05-15T18:46:46.981Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Ensure config is going to change listeners. Will be stopped: [], will be started: [], will be restarted: []
[ns_server:info,2025-05-15T18:46:46.981Z,ns_1@cb.local:<0.341.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service cb_dist_tls
[error_logger:info,2025-05-15T18:46:46.987Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {2355611,#Ref<0.3735524962.3650486276.241707>}}}
[ns_server:debug,2025-05-15T18:46:46.987Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:46:46.987Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3735524962.3650355204.241713>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:46:46.987Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3735524962.3650355204.241713>,
                                  inet_tcp_dist,<0.346.0>,
                                  #Ref<0.3735524962.3650355204.241716>}
[error_logger:info,2025-05-15T18:46:46.987Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.346.0>,shutdown}}
[ns_server:debug,2025-05-15T18:46:46.987Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3735524962.3650355204.241713>,
                               inet_tcp_dist,<0.346.0>,
                               #Ref<0.3735524962.3650355204.241716>}
[error_logger:info,2025-05-15T18:46:46.987Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:46:46.987Z,ns_1@cb.local:<0.340.0>:ns_couchdb_api:rpc_couchdb_node:152]RPC to couchdb node failed for restart_capi_ssl_service with {badrpc,nodedown}
Stack: [{ns_couchdb_api,rpc_couchdb_node,4,
                        [{file,"src/ns_couchdb_api.erl"},{line,150}]},
        {ns_ssl_services_setup,do_notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1203}]},
        {ns_ssl_services_setup,notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1179}]},
        {async,'-async_init/4-fun-1-',3,[{file,"src/async.erl"},{line,199}]}]
[ns_server:warn,2025-05-15T18:46:46.988Z,ns_1@cb.local:<0.340.0>:ns_ssl_services_setup:notify_service:1184]Failed to notify service capi_ssl_service: {'EXIT',{error,{badrpc,nodedown}}}
[ns_server:warn,2025-05-15T18:46:46.989Z,ns_1@cb.local:<0.342.0>:ns_ssl_services_setup:notify_service:1184]Failed to notify service memcached: {error,no_proccess}
[ns_server:info,2025-05-15T18:46:46.996Z,ns_1@cb.local:<0.330.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2025-05-15T18:46:46.996Z,ns_1@cb.local:<0.330.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2025-05-15T18:46:46.996Z,ns_1@cb.local:<0.330.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2025-05-15T18:46:46.996Z,ns_1@cb.local:<0.330.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2025-05-15T18:46:46.996Z,ns_1@cb.local:<0.330.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[error_logger:info,2025-05-15T18:46:47.002Z,ns_1@cb.local:kernel_sup<0.49.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,kernel_sup}
    started: [{pid,<0.349.0>},
              {id,timer_server},
              {mfargs,{timer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:47.029Z,ns_1@cb.local:<0.330.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"0.0.0.0"},
 [{keyfile,"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem"},
  {certfile,"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem"},
  {versions,['tlsv1.2','tlsv1.3']},
  {cacerts,[<<48,130,3,12,48,130,1,244,160,3,2,1,2,2,8,24,63,199,191,180,27,
              10,177,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,
              48,32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,
              101,114,118,101,114,32,49,52,54,49,99,99,50,50,48,30,23,13,49,
              51,48,49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,
              51,53,57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,
              117,99,104,98,97,115,101,32,83,101,114,118,101,114,32,49,52,54,
              49,99,99,50,50,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,
              1,5,0,3,130,1,15,0,48,130,1,10,2,130,1,1,0,181,120,2,207,122,
              99,7,71,178,99,124,254,60,3,194,124,244,173,56,229,2,9,82,3,
              206,183,81,199,204,193,33,99,84,242,166,188,107,14,37,96,88,
              209,254,240,32,204,147,205,45,63,123,161,85,104,9,150,21,194,
              213,221,19,3,77,76,189,208,93,232,47,63,226,64,53,14,136,224,
              149,250,97,65,87,66,59,254,91,0,116,97,49,186,38,79,0,74,253,
              18,47,169,76,212,127,181,44,78,32,241,232,104,53,173,102,141,
              194,235,50,161,36,240,71,126,109,238,147,180,102,33,170,119,20,
              109,208,73,212,125,100,165,198,140,210,185,71,194,152,180,1,
              242,111,31,230,61,53,12,218,145,184,127,130,12,249,6,112,111,
              221,221,245,161,89,130,74,135,153,231,237,136,181,125,70,232,1,
              156,245,196,14,149,104,104,202,6,233,86,16,69,111,71,86,230,
              105,161,211,116,106,249,40,136,196,141,254,172,122,108,78,252,
              182,126,8,182,88,220,41,202,91,163,155,96,85,22,97,186,40,194,
              19,0,57,238,64,87,144,142,62,207,252,84,26,9,57,116,0,47,173,
              193,153,104,169,103,194,29,2,3,1,0,1,163,66,48,64,48,14,6,3,85,
              29,15,1,1,255,4,4,3,2,1,134,48,15,6,3,85,29,19,1,1,255,4,5,48,
              3,1,1,255,48,29,6,3,85,29,14,4,22,4,20,197,18,173,226,193,241,
              105,140,163,34,50,61,128,135,150,89,215,37,208,144,48,13,6,9,
              42,134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,26,225,231,0,217,
              118,14,29,57,113,249,186,176,21,106,214,129,105,172,105,111,87,
              138,154,56,94,191,209,103,130,18,48,137,158,179,148,24,195,213,
              203,137,49,145,82,189,69,7,8,202,85,205,192,230,21,61,35,22,75,
              67,182,222,215,159,17,57,66,131,43,49,33,238,11,249,32,157,128,
              15,148,89,192,30,239,133,216,181,147,123,109,25,188,184,206,
              174,31,156,91,27,236,254,110,113,28,32,72,48,13,152,101,138,
              146,182,79,72,46,205,1,91,50,143,226,51,117,20,113,109,46,246,
              61,157,43,14,204,7,253,73,198,19,234,127,131,141,44,230,60,185,
              139,136,236,213,87,10,80,32,254,169,231,241,155,193,145,255,
              244,60,35,164,87,65,41,173,37,98,26,217,243,211,181,172,244,80,
              39,175,148,102,124,187,49,169,81,217,58,84,217,38,11,118,142,9,
              6,20,24,204,189,93,103,187,215,0,101,236,40,186,4,21,106,168,
              193,83,29,52,82,106,30,74,125,41,250,77,5,118,196,217,211,201,
              19,214,171,213,81,174,93,190,173,157,199,251,234,252,74,62,46,
              85,78,135,95,193,68>>]},
  {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,
        10,118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,
        158,232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,
        66,211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,
        250,145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,
        104,159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,
        246,169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,
        110,167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,
        118,190,67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,
        74,8,205,174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,
        221,95,184,110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,
        76,187,66,211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,
        69,254,147,103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,
        202,133,173,72,6,69,167,89,112,174,40,229,171,2,1,2>>},
  {ciphers,[#{cipher => aes_256_gcm,key_exchange => any,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm_8,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_ecdsa,
              mac => aead,prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_rsa,
              mac => aead,prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_128_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf},
            #{cipher => aes_128_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf}]},
  {honor_cipher_order,true},
  {secure_renegotiate,true},
  {client_renegotiation,false},
  {password,"********"}],
 {ssl,true},
 {port,18091}]
[error_logger:info,2025-05-15T18:46:47.031Z,ns_1@cb.local:<0.330.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.330.0>,menelaus_web}
    started: [{pid,<0.350.0>},
              {id,menelaus_web_ipv4},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet},
                        {ssl,true},
                        {name,menelaus_web_ssl},
                        {ssl_opts_fun,#Fun<ns_ssl_services_setup.11.39330155>},
                        {port,18091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:47.032Z,ns_1@cb.local:<0.330.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2025-05-15T18:46:47.032Z,ns_1@cb.local:<0.330.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2025-05-15T18:46:47.033Z,ns_1@cb.local:<0.330.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2025-05-15T18:46:47.033Z,ns_1@cb.local:<0.330.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2025-05-15T18:46:47.033Z,ns_1@cb.local:<0.330.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2025-05-15T18:46:47.036Z,ns_1@cb.local:<0.330.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"::"},
 [{keyfile,"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem"},
  {certfile,"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem"},
  {versions,['tlsv1.2','tlsv1.3']},
  {cacerts,[<<48,130,3,12,48,130,1,244,160,3,2,1,2,2,8,24,63,199,191,180,27,
              10,177,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,
              48,32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,
              101,114,118,101,114,32,49,52,54,49,99,99,50,50,48,30,23,13,49,
              51,48,49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,
              51,53,57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,
              117,99,104,98,97,115,101,32,83,101,114,118,101,114,32,49,52,54,
              49,99,99,50,50,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,
              1,5,0,3,130,1,15,0,48,130,1,10,2,130,1,1,0,181,120,2,207,122,
              99,7,71,178,99,124,254,60,3,194,124,244,173,56,229,2,9,82,3,
              206,183,81,199,204,193,33,99,84,242,166,188,107,14,37,96,88,
              209,254,240,32,204,147,205,45,63,123,161,85,104,9,150,21,194,
              213,221,19,3,77,76,189,208,93,232,47,63,226,64,53,14,136,224,
              149,250,97,65,87,66,59,254,91,0,116,97,49,186,38,79,0,74,253,
              18,47,169,76,212,127,181,44,78,32,241,232,104,53,173,102,141,
              194,235,50,161,36,240,71,126,109,238,147,180,102,33,170,119,20,
              109,208,73,212,125,100,165,198,140,210,185,71,194,152,180,1,
              242,111,31,230,61,53,12,218,145,184,127,130,12,249,6,112,111,
              221,221,245,161,89,130,74,135,153,231,237,136,181,125,70,232,1,
              156,245,196,14,149,104,104,202,6,233,86,16,69,111,71,86,230,
              105,161,211,116,106,249,40,136,196,141,254,172,122,108,78,252,
              182,126,8,182,88,220,41,202,91,163,155,96,85,22,97,186,40,194,
              19,0,57,238,64,87,144,142,62,207,252,84,26,9,57,116,0,47,173,
              193,153,104,169,103,194,29,2,3,1,0,1,163,66,48,64,48,14,6,3,85,
              29,15,1,1,255,4,4,3,2,1,134,48,15,6,3,85,29,19,1,1,255,4,5,48,
              3,1,1,255,48,29,6,3,85,29,14,4,22,4,20,197,18,173,226,193,241,
              105,140,163,34,50,61,128,135,150,89,215,37,208,144,48,13,6,9,
              42,134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,26,225,231,0,217,
              118,14,29,57,113,249,186,176,21,106,214,129,105,172,105,111,87,
              138,154,56,94,191,209,103,130,18,48,137,158,179,148,24,195,213,
              203,137,49,145,82,189,69,7,8,202,85,205,192,230,21,61,35,22,75,
              67,182,222,215,159,17,57,66,131,43,49,33,238,11,249,32,157,128,
              15,148,89,192,30,239,133,216,181,147,123,109,25,188,184,206,
              174,31,156,91,27,236,254,110,113,28,32,72,48,13,152,101,138,
              146,182,79,72,46,205,1,91,50,143,226,51,117,20,113,109,46,246,
              61,157,43,14,204,7,253,73,198,19,234,127,131,141,44,230,60,185,
              139,136,236,213,87,10,80,32,254,169,231,241,155,193,145,255,
              244,60,35,164,87,65,41,173,37,98,26,217,243,211,181,172,244,80,
              39,175,148,102,124,187,49,169,81,217,58,84,217,38,11,118,142,9,
              6,20,24,204,189,93,103,187,215,0,101,236,40,186,4,21,106,168,
              193,83,29,52,82,106,30,74,125,41,250,77,5,118,196,217,211,201,
              19,214,171,213,81,174,93,190,173,157,199,251,234,252,74,62,46,
              85,78,135,95,193,68>>]},
  {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,
        10,118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,
        158,232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,
        66,211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,
        250,145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,
        104,159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,
        246,169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,
        110,167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,
        118,190,67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,
        74,8,205,174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,
        221,95,184,110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,
        76,187,66,211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,
        69,254,147,103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,
        202,133,173,72,6,69,167,89,112,174,40,229,171,2,1,2>>},
  {ciphers,[#{cipher => aes_256_gcm,key_exchange => any,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm_8,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_ecdsa,
              mac => aead,prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_rsa,
              mac => aead,prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_128_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf},
            #{cipher => aes_128_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf}]},
  {honor_cipher_order,true},
  {secure_renegotiate,true},
  {client_renegotiation,false},
  {password,"********"}],
 {ssl,true},
 {port,18091}]
[error_logger:info,2025-05-15T18:46:47.037Z,ns_1@cb.local:<0.330.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.330.0>,menelaus_web}
    started: [{pid,<0.369.0>},
              {id,menelaus_web_ipv6},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet6},
                        {ssl,true},
                        {name,menelaus_web_ssl},
                        {ssl_opts_fun,#Fun<ns_ssl_services_setup.11.39330155>},
                        {port,18091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:47.037Z,ns_1@cb.local:<0.328.0>:restartable:start_child:92]Started child process <0.330.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
[error_logger:info,2025-05-15T18:46:47.037Z,ns_1@cb.local:ns_ssl_services_sup<0.306.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_ssl_services_sup}
    started: [{pid,<0.328.0>},
              {id,ns_rest_ssl_service},
              {mfargs,
                  {restartable,start_link,
                      [{ns_ssl_services_setup,start_link_rest_service,[]},
                       1000]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:47.037Z,ns_1@cb.local:<0.328.0>:restartable:loop:65]Restarting child <0.330.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
  Shutdown policy: 1000
  Caller: {<0.344.0>,#Ref<0.3735524962.3650355202.241046>}
[error_logger:info,2025-05-15T18:46:47.038Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.306.0>},
              {id,ns_ssl_services_sup},
              {mfargs,{ns_ssl_services_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:47.038Z,ns_1@cb.local:<0.328.0>:restartable:shutdown_child:114]Successfully terminated process <0.330.0>
[ns_server:info,2025-05-15T18:46:47.045Z,ns_1@cb.local:<0.388.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2025-05-15T18:46:47.045Z,ns_1@cb.local:<0.388.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2025-05-15T18:46:47.045Z,ns_1@cb.local:<0.388.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2025-05-15T18:46:47.045Z,ns_1@cb.local:<0.388.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2025-05-15T18:46:47.045Z,ns_1@cb.local:<0.388.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2025-05-15T18:46:47.046Z,ns_1@cb.local:<0.388.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"0.0.0.0"},
 [{keyfile,"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem"},
  {certfile,"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem"},
  {versions,['tlsv1.2','tlsv1.3']},
  {cacerts,[<<48,130,3,12,48,130,1,244,160,3,2,1,2,2,8,24,63,199,191,180,27,
              10,177,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,
              48,32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,
              101,114,118,101,114,32,49,52,54,49,99,99,50,50,48,30,23,13,49,
              51,48,49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,
              51,53,57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,
              117,99,104,98,97,115,101,32,83,101,114,118,101,114,32,49,52,54,
              49,99,99,50,50,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,
              1,5,0,3,130,1,15,0,48,130,1,10,2,130,1,1,0,181,120,2,207,122,
              99,7,71,178,99,124,254,60,3,194,124,244,173,56,229,2,9,82,3,
              206,183,81,199,204,193,33,99,84,242,166,188,107,14,37,96,88,
              209,254,240,32,204,147,205,45,63,123,161,85,104,9,150,21,194,
              213,221,19,3,77,76,189,208,93,232,47,63,226,64,53,14,136,224,
              149,250,97,65,87,66,59,254,91,0,116,97,49,186,38,79,0,74,253,
              18,47,169,76,212,127,181,44,78,32,241,232,104,53,173,102,141,
              194,235,50,161,36,240,71,126,109,238,147,180,102,33,170,119,20,
              109,208,73,212,125,100,165,198,140,210,185,71,194,152,180,1,
              242,111,31,230,61,53,12,218,145,184,127,130,12,249,6,112,111,
              221,221,245,161,89,130,74,135,153,231,237,136,181,125,70,232,1,
              156,245,196,14,149,104,104,202,6,233,86,16,69,111,71,86,230,
              105,161,211,116,106,249,40,136,196,141,254,172,122,108,78,252,
              182,126,8,182,88,220,41,202,91,163,155,96,85,22,97,186,40,194,
              19,0,57,238,64,87,144,142,62,207,252,84,26,9,57,116,0,47,173,
              193,153,104,169,103,194,29,2,3,1,0,1,163,66,48,64,48,14,6,3,85,
              29,15,1,1,255,4,4,3,2,1,134,48,15,6,3,85,29,19,1,1,255,4,5,48,
              3,1,1,255,48,29,6,3,85,29,14,4,22,4,20,197,18,173,226,193,241,
              105,140,163,34,50,61,128,135,150,89,215,37,208,144,48,13,6,9,
              42,134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,26,225,231,0,217,
              118,14,29,57,113,249,186,176,21,106,214,129,105,172,105,111,87,
              138,154,56,94,191,209,103,130,18,48,137,158,179,148,24,195,213,
              203,137,49,145,82,189,69,7,8,202,85,205,192,230,21,61,35,22,75,
              67,182,222,215,159,17,57,66,131,43,49,33,238,11,249,32,157,128,
              15,148,89,192,30,239,133,216,181,147,123,109,25,188,184,206,
              174,31,156,91,27,236,254,110,113,28,32,72,48,13,152,101,138,
              146,182,79,72,46,205,1,91,50,143,226,51,117,20,113,109,46,246,
              61,157,43,14,204,7,253,73,198,19,234,127,131,141,44,230,60,185,
              139,136,236,213,87,10,80,32,254,169,231,241,155,193,145,255,
              244,60,35,164,87,65,41,173,37,98,26,217,243,211,181,172,244,80,
              39,175,148,102,124,187,49,169,81,217,58,84,217,38,11,118,142,9,
              6,20,24,204,189,93,103,187,215,0,101,236,40,186,4,21,106,168,
              193,83,29,52,82,106,30,74,125,41,250,77,5,118,196,217,211,201,
              19,214,171,213,81,174,93,190,173,157,199,251,234,252,74,62,46,
              85,78,135,95,193,68>>]},
  {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,
        10,118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,
        158,232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,
        66,211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,
        250,145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,
        104,159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,
        246,169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,
        110,167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,
        118,190,67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,
        74,8,205,174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,
        221,95,184,110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,
        76,187,66,211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,
        69,254,147,103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,
        202,133,173,72,6,69,167,89,112,174,40,229,171,2,1,2>>},
  {ciphers,[#{cipher => aes_256_gcm,key_exchange => any,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm_8,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_ecdsa,
              mac => aead,prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_rsa,
              mac => aead,prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_128_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf},
            #{cipher => aes_128_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf}]},
  {honor_cipher_order,true},
  {secure_renegotiate,true},
  {client_renegotiation,false},
  {password,"********"}],
 {ssl,true},
 {port,18091}]
[error_logger:info,2025-05-15T18:46:47.047Z,ns_1@cb.local:<0.388.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.388.0>,menelaus_web}
    started: [{pid,<0.389.0>},
              {id,menelaus_web_ipv4},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet},
                        {ssl,true},
                        {name,menelaus_web_ssl},
                        {ssl_opts_fun,#Fun<ns_ssl_services_setup.11.39330155>},
                        {port,18091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:47.049Z,ns_1@cb.local:<0.388.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2025-05-15T18:46:47.049Z,ns_1@cb.local:<0.388.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2025-05-15T18:46:47.049Z,ns_1@cb.local:<0.388.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2025-05-15T18:46:47.049Z,ns_1@cb.local:<0.388.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2025-05-15T18:46:47.049Z,ns_1@cb.local:<0.388.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[error_logger:info,2025-05-15T18:46:47.049Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.408.0>},
              {id,ldap_auth_cache},
              {mfargs,{ldap_auth_cache,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:47.051Z,ns_1@cb.local:cb_saml<0.429.0>:cb_saml:handle_info:316]Settings have changed or this is the first start
[ns_server:info,2025-05-15T18:46:47.050Z,ns_1@cb.local:<0.388.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"::"},
 [{keyfile,"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem"},
  {certfile,"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem"},
  {versions,['tlsv1.2','tlsv1.3']},
  {cacerts,[<<48,130,3,12,48,130,1,244,160,3,2,1,2,2,8,24,63,199,191,180,27,
              10,177,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,
              48,32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,
              101,114,118,101,114,32,49,52,54,49,99,99,50,50,48,30,23,13,49,
              51,48,49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,
              51,53,57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,
              117,99,104,98,97,115,101,32,83,101,114,118,101,114,32,49,52,54,
              49,99,99,50,50,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,
              1,5,0,3,130,1,15,0,48,130,1,10,2,130,1,1,0,181,120,2,207,122,
              99,7,71,178,99,124,254,60,3,194,124,244,173,56,229,2,9,82,3,
              206,183,81,199,204,193,33,99,84,242,166,188,107,14,37,96,88,
              209,254,240,32,204,147,205,45,63,123,161,85,104,9,150,21,194,
              213,221,19,3,77,76,189,208,93,232,47,63,226,64,53,14,136,224,
              149,250,97,65,87,66,59,254,91,0,116,97,49,186,38,79,0,74,253,
              18,47,169,76,212,127,181,44,78,32,241,232,104,53,173,102,141,
              194,235,50,161,36,240,71,126,109,238,147,180,102,33,170,119,20,
              109,208,73,212,125,100,165,198,140,210,185,71,194,152,180,1,
              242,111,31,230,61,53,12,218,145,184,127,130,12,249,6,112,111,
              221,221,245,161,89,130,74,135,153,231,237,136,181,125,70,232,1,
              156,245,196,14,149,104,104,202,6,233,86,16,69,111,71,86,230,
              105,161,211,116,106,249,40,136,196,141,254,172,122,108,78,252,
              182,126,8,182,88,220,41,202,91,163,155,96,85,22,97,186,40,194,
              19,0,57,238,64,87,144,142,62,207,252,84,26,9,57,116,0,47,173,
              193,153,104,169,103,194,29,2,3,1,0,1,163,66,48,64,48,14,6,3,85,
              29,15,1,1,255,4,4,3,2,1,134,48,15,6,3,85,29,19,1,1,255,4,5,48,
              3,1,1,255,48,29,6,3,85,29,14,4,22,4,20,197,18,173,226,193,241,
              105,140,163,34,50,61,128,135,150,89,215,37,208,144,48,13,6,9,
              42,134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,26,225,231,0,217,
              118,14,29,57,113,249,186,176,21,106,214,129,105,172,105,111,87,
              138,154,56,94,191,209,103,130,18,48,137,158,179,148,24,195,213,
              203,137,49,145,82,189,69,7,8,202,85,205,192,230,21,61,35,22,75,
              67,182,222,215,159,17,57,66,131,43,49,33,238,11,249,32,157,128,
              15,148,89,192,30,239,133,216,181,147,123,109,25,188,184,206,
              174,31,156,91,27,236,254,110,113,28,32,72,48,13,152,101,138,
              146,182,79,72,46,205,1,91,50,143,226,51,117,20,113,109,46,246,
              61,157,43,14,204,7,253,73,198,19,234,127,131,141,44,230,60,185,
              139,136,236,213,87,10,80,32,254,169,231,241,155,193,145,255,
              244,60,35,164,87,65,41,173,37,98,26,217,243,211,181,172,244,80,
              39,175,148,102,124,187,49,169,81,217,58,84,217,38,11,118,142,9,
              6,20,24,204,189,93,103,187,215,0,101,236,40,186,4,21,106,168,
              193,83,29,52,82,106,30,74,125,41,250,77,5,118,196,217,211,201,
              19,214,171,213,81,174,93,190,173,157,199,251,234,252,74,62,46,
              85,78,135,95,193,68>>]},
  {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,
        10,118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,
        158,232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,
        66,211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,
        250,145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,
        104,159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,
        246,169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,
        110,167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,
        118,190,67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,
        74,8,205,174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,
        221,95,184,110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,
        76,187,66,211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,
        69,254,147,103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,
        202,133,173,72,6,69,167,89,112,174,40,229,171,2,1,2>>},
  {ciphers,[#{cipher => aes_256_gcm,key_exchange => any,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm_8,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_ecdsa,
              mac => aead,prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_rsa,
              mac => aead,prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_128_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf},
            #{cipher => aes_128_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf}]},
  {honor_cipher_order,true},
  {secure_renegotiate,true},
  {client_renegotiation,false},
  {password,"********"}],
 {ssl,true},
 {port,18091}]
[error_logger:info,2025-05-15T18:46:47.051Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.429.0>},
              {id,cb_saml},
              {mfargs,{cb_saml,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:47.051Z,ns_1@cb.local:cb_saml<0.429.0>:cb_saml:restart_refresh_timer:586]Restarting refresh timer: 24359 ms
[error_logger:info,2025-05-15T18:46:47.052Z,ns_1@cb.local:<0.388.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.388.0>,menelaus_web}
    started: [{pid,<0.410.0>},
              {id,menelaus_web_ipv6},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet6},
                        {ssl,true},
                        {name,menelaus_web_ssl},
                        {ssl_opts_fun,#Fun<ns_ssl_services_setup.11.39330155>},
                        {port,18091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:47.052Z,ns_1@cb.local:<0.328.0>:restartable:start_child:92]Started child process <0.388.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
[ns_server:info,2025-05-15T18:46:47.052Z,ns_1@cb.local:<0.344.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service ssl_service
[ns_server:info,2025-05-15T18:46:47.052Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:notify_services:1162]Succesfully notified services [ssl_service,server_cert_event,
                               client_cert_event,cb_dist_tls]
[error_logger:info,2025-05-15T18:46:47.053Z,ns_1@cb.local:users_sup<0.431.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_sup}
    started: [{pid,<0.432.0>},
              {id,user_storage_events},
              {mfargs,{gen_event,start_link,[{local,user_storage_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:47.054Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:notify_services:1173]Failed to notify some services. Will retry in 5 sec, [{{error,no_proccess},
                                                       memcached},
                                                      {{'EXIT',
                                                        {error,
                                                         {badrpc,nodedown}}},
                                                       capi_ssl_service}]
[ns_server:debug,2025-05-15T18:46:47.054Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:restart_regenerate_client_cert_timer:1447]Time left before client cert regeneration: 70588799000
[ns_server:debug,2025-05-15T18:46:47.054Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:maybe_store_ca_certs:832]Considering to store CA certs
[ns_server:debug,2025-05-15T18:46:47.055Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:notify_services:1146]Going to notify following services: [capi_ssl_service,memcached]
[ns_server:warn,2025-05-15T18:46:47.056Z,ns_1@cb.local:<0.440.0>:ns_ssl_services_setup:notify_service:1184]Failed to notify service memcached: {error,no_proccess}
[error_logger:info,2025-05-15T18:46:47.056Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {2355612,#Ref<0.3735524962.3650486276.241707>}}}
[ns_server:debug,2025-05-15T18:46:47.056Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:46:47.056Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3735524962.3650355206.241401>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:46:47.056Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3735524962.3650355206.241401>,
                                  inet_tcp_dist,<0.441.0>,
                                  #Ref<0.3735524962.3650355206.241404>}
[ns_server:debug,2025-05-15T18:46:47.056Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3735524962.3650355206.241401>,
                               inet_tcp_dist,<0.441.0>,
                               #Ref<0.3735524962.3650355206.241404>}
[error_logger:info,2025-05-15T18:46:47.056Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.441.0>,shutdown}}
[error_logger:info,2025-05-15T18:46:47.056Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:46:47.056Z,ns_1@cb.local:<0.439.0>:ns_couchdb_api:rpc_couchdb_node:152]RPC to couchdb node failed for restart_capi_ssl_service with {badrpc,nodedown}
Stack: [{ns_couchdb_api,rpc_couchdb_node,4,
                        [{file,"src/ns_couchdb_api.erl"},{line,150}]},
        {ns_ssl_services_setup,do_notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1203}]},
        {ns_ssl_services_setup,notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1179}]},
        {async,'-async_init/4-fun-1-',3,[{file,"src/async.erl"},{line,199}]}]
[ns_server:warn,2025-05-15T18:46:47.056Z,ns_1@cb.local:<0.439.0>:ns_ssl_services_setup:notify_service:1184]Failed to notify service capi_ssl_service: {'EXIT',{error,{badrpc,nodedown}}}
[error_logger:info,2025-05-15T18:46:47.058Z,ns_1@cb.local:users_storage_sup<0.433.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_storage_sup}
    started: [{pid,<0.443.0>},
              {id,users_replicator},
              {mfargs,{menelaus_users,start_replicator,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:47.059Z,ns_1@cb.local:users_replicator<0.443.0>:replicated_storage:wait_for_startup:47]Start waiting for startup
[ns_server:info,2025-05-15T18:46:47.061Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:notify_services:1173]Failed to notify some services. Will retry in 5 sec, [{{error,no_proccess},
                                                       memcached},
                                                      {{'EXIT',
                                                        {error,
                                                         {badrpc,nodedown}}},
                                                       capi_ssl_service}]
[ns_server:debug,2025-05-15T18:46:47.061Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:restart_regenerate_client_cert_timer:1447]Time left before client cert regeneration: 70588799000
[ns_server:info,2025-05-15T18:46:47.062Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:validate_pkey:1032]Private key (node_cert) passphrase validation suceeded
[ns_server:info,2025-05-15T18:46:47.062Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:validate_pkey:1032]Private key (client_cert) passphrase validation suceeded
[ns_server:debug,2025-05-15T18:46:47.062Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:notify_services:1146]Going to notify following services: [memcached,capi_ssl_service]
[ns_server:warn,2025-05-15T18:46:47.063Z,ns_1@cb.local:<0.450.0>:ns_ssl_services_setup:notify_service:1184]Failed to notify service memcached: {error,no_proccess}
[error_logger:info,2025-05-15T18:46:47.063Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {2355613,#Ref<0.3735524962.3650486276.241707>}}}
[ns_server:debug,2025-05-15T18:46:47.063Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:46:47.063Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3735524962.3650355203.241318>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:46:47.063Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3735524962.3650355203.241318>,
                                  inet_tcp_dist,<0.452.0>,
                                  #Ref<0.3735524962.3650355203.241321>}
[ns_server:debug,2025-05-15T18:46:47.063Z,ns_1@cb.local:users_storage<0.444.0>:replicated_storage:announce_startup:61]Announce my startup to <0.443.0>
[ns_server:debug,2025-05-15T18:46:47.063Z,ns_1@cb.local:users_storage<0.444.0>:replicated_dets:open:177]Opening file "/opt/couchbase/var/lib/couchbase/config/users.dets"
[ns_server:debug,2025-05-15T18:46:47.064Z,ns_1@cb.local:users_replicator<0.443.0>:replicated_storage:wait_for_startup:50]Received replicated storage registration from <0.444.0>
[error_logger:info,2025-05-15T18:46:47.064Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.452.0>,shutdown}}
[ns_server:debug,2025-05-15T18:46:47.064Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3735524962.3650355203.241318>,
                               inet_tcp_dist,<0.452.0>,
                               #Ref<0.3735524962.3650355203.241321>}
[error_logger:info,2025-05-15T18:46:47.064Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:46:47.064Z,ns_1@cb.local:<0.451.0>:ns_couchdb_api:rpc_couchdb_node:152]RPC to couchdb node failed for restart_capi_ssl_service with {badrpc,nodedown}
Stack: [{ns_couchdb_api,rpc_couchdb_node,4,
                        [{file,"src/ns_couchdb_api.erl"},{line,150}]},
        {ns_ssl_services_setup,do_notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1203}]},
        {ns_ssl_services_setup,notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1179}]},
        {async,'-async_init/4-fun-1-',3,[{file,"src/async.erl"},{line,199}]}]
[error_logger:info,2025-05-15T18:46:47.064Z,ns_1@cb.local:users_storage_sup<0.433.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_storage_sup}
    started: [{pid,<0.444.0>},
              {id,users_storage},
              {mfargs,{menelaus_users,start_storage,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:warn,2025-05-15T18:46:47.064Z,ns_1@cb.local:<0.451.0>:ns_ssl_services_setup:notify_service:1184]Failed to notify service capi_ssl_service: {'EXIT',{error,{badrpc,nodedown}}}
[error_logger:info,2025-05-15T18:46:47.064Z,ns_1@cb.local:users_sup<0.431.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_sup}
    started: [{pid,<0.433.0>},
              {id,users_storage_sup},
              {mfargs,{users_storage_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:info,2025-05-15T18:46:47.068Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:notify_services:1173]Failed to notify some services. Will retry in 5 sec, [{{'EXIT',
                                                        {error,
                                                         {badrpc,nodedown}}},
                                                       capi_ssl_service},
                                                      {{error,no_proccess},
                                                       memcached}]
[ns_server:debug,2025-05-15T18:46:47.069Z,ns_1@cb.local:compiled_roles_cache<0.455.0>:versioned_cache:init:41]Starting versioned cache compiled_roles_cache
[error_logger:info,2025-05-15T18:46:47.070Z,ns_1@cb.local:users_sup<0.431.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_sup}
    started: [{pid,<0.455.0>},
              {id,compiled_roles_cache},
              {mfargs,{menelaus_roles,start_compiled_roles_cache,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:47.073Z,ns_1@cb.local:users_sup<0.431.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_sup}
    started: [{pid,<0.458.0>},
              {id,roles_cache},
              {mfargs,{roles_cache,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:47.073Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.431.0>},
              {id,users_sup},
              {mfargs,{users_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:47.073Z,ns_1@cb.local:kernel_safe_sup<0.67.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,kernel_safe_sup}
    started: [{pid,<0.462.0>},
              {id,dets_sup},
              {mfargs,{dets_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:47.074Z,ns_1@cb.local:kernel_safe_sup<0.67.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,kernel_safe_sup}
    started: [{pid,<0.463.0>},
              {id,dets},
              {mfargs,{dets_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,2000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:47.089Z,ns_1@cb.local:users_storage<0.444.0>:replicated_dets:select_from_table:312][dets] Starting select with {users_storage,
                                [{{docv2,'_','_','_'},[],['$_']}],
                                100}
[ns_server:debug,2025-05-15T18:46:47.089Z,ns_1@cb.local:users_storage<0.444.0>:replicated_dets:init_after_ack:170]Loading 0 items, 307 words took 25ms
[ns_server:debug,2025-05-15T18:46:47.089Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',cbas_dirs} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554007}}]},
 "/opt/couchbase/var/lib/couchbase/data"]
[ns_server:debug,2025-05-15T18:46:47.089Z,ns_1@cb.local:<0.468.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,
                                    start_couchdb_node} with delay of 5000
[ns_server:debug,2025-05-15T18:46:47.089Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',eventing_dir} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554007}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[error_logger:info,2025-05-15T18:46:47.103Z,ns_1@cb.local:<0.469.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.469.0>,suppress_max_restart_intensity}
    started: [{pid,<0.470.0>},
              {id,start_couchdb_node},
              {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,86400000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:47.103Z,ns_1@cb.local:<0.467.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.467.0>,suppress_max_restart_intensity}
    started: [{pid,<0.468.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      start_couchdb_node}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,start_couchdb_node},
                       5000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 5,id => start_couchdb_node,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [],restart => permanent,
                          shutdown => 86400000,
                          start => {ns_server_nodes_sup,start_couchdb_node,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:47.104Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.467.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,start_couchdb_node}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 5,id => start_couchdb_node,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [],restart => permanent,
                         shutdown => 86400000,
                         start => {ns_server_nodes_sup,start_couchdb_node,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:47.104Z,ns_1@cb.local:wait_link_to_couchdb_node<0.472.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:158]Waiting for ns_couchdb node to start
[error_logger:info,2025-05-15T18:46:47.104Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {2355614,#Ref<0.3735524962.3650486276.241707>}}}
[ns_server:debug,2025-05-15T18:46:47.104Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:46:47.104Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3735524962.3650355205.241583>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:46:47.104Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3735524962.3650355205.241583>,
                                  inet_tcp_dist,<0.474.0>,
                                  #Ref<0.3735524962.3650355205.241586>}
[ns_server:debug,2025-05-15T18:46:47.105Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3735524962.3650355205.241583>,
                               inet_tcp_dist,<0.474.0>,
                               #Ref<0.3735524962.3650355205.241586>}
[error_logger:info,2025-05-15T18:46:47.104Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.474.0>,shutdown}}
[error_logger:info,2025-05-15T18:46:47.105Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:46:47.105Z,ns_1@cb.local:<0.473.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-05-15T18:46:47.306Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {2355615,#Ref<0.3735524962.3650486276.241707>}}}
[ns_server:debug,2025-05-15T18:46:47.307Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:46:47.307Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3735524962.3650355205.241598>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:46:47.307Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3735524962.3650355205.241598>,
                                  inet_tcp_dist,<0.476.0>,
                                  #Ref<0.3735524962.3650355205.241601>}
[ns_server:debug,2025-05-15T18:46:47.307Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3735524962.3650355205.241598>,
                               inet_tcp_dist,<0.476.0>,
                               #Ref<0.3735524962.3650355205.241601>}
[error_logger:info,2025-05-15T18:46:47.308Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.476.0>,shutdown}}
[error_logger:info,2025-05-15T18:46:47.308Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:46:47.308Z,ns_1@cb.local:<0.473.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-05-15T18:46:47.509Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {2355616,#Ref<0.3735524962.3650486276.241707>}}}
[ns_server:debug,2025-05-15T18:46:47.510Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:46:47.510Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3735524962.3650355202.241097>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:46:47.510Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3735524962.3650355202.241097>,
                                  inet_tcp_dist,<0.478.0>,
                                  #Ref<0.3735524962.3650355202.241100>}
[error_logger:info,2025-05-15T18:46:47.510Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.478.0>,shutdown}}
[error_logger:info,2025-05-15T18:46:47.510Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:46:47.510Z,ns_1@cb.local:<0.473.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2025-05-15T18:46:47.510Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3735524962.3650355202.241097>,
                               inet_tcp_dist,<0.478.0>,
                               #Ref<0.3735524962.3650355202.241100>}
[error_logger:info,2025-05-15T18:46:47.712Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {2355617,#Ref<0.3735524962.3650486276.241707>}}}
[ns_server:debug,2025-05-15T18:46:47.712Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:46:47.712Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3735524962.3650355203.241372>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:46:47.712Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3735524962.3650355203.241372>,
                                  inet_tcp_dist,<0.480.0>,
                                  #Ref<0.3735524962.3650355203.241375>}
[error_logger:info,2025-05-15T18:46:47.765Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.480.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2025-05-15T18:46:47.765Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:46:47.765Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3735524962.3650355203.241372>,
                               inet_tcp_dist,<0.480.0>,
                               #Ref<0.3735524962.3650355203.241375>}
[ns_server:debug,2025-05-15T18:46:47.765Z,ns_1@cb.local:<0.473.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-05-15T18:46:47.966Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {2355618,#Ref<0.3735524962.3650486276.241707>}}}
[ns_server:debug,2025-05-15T18:46:47.966Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:46:47.966Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3735524962.3650355203.241386>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:46:47.966Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3735524962.3650355203.241386>,
                                  inet_tcp_dist,<0.482.0>,
                                  #Ref<0.3735524962.3650355203.241389>}
[ns_server:info,2025-05-15T18:46:47.972Z,ns_1@cb.local:ns_couchdb_port<0.470.0>:ns_port_server:log:226]ns_couchdb<0.470.0>: =ERROR REPORT==== 15-May-2025::18:46:47.764851 ===
ns_couchdb<0.470.0>: ** Connection attempt to/from node 'ns_1@cb.local' rejected. Cookie is not set. **
ns_couchdb<0.470.0>: 

[error_logger:info,2025-05-15T18:46:47.990Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.482.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2025-05-15T18:46:47.990Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:46:47.990Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3735524962.3650355203.241386>,
                               inet_tcp_dist,<0.482.0>,
                               #Ref<0.3735524962.3650355203.241389>}
[ns_server:debug,2025-05-15T18:46:47.990Z,ns_1@cb.local:<0.473.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-05-15T18:46:48.192Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {2355619,#Ref<0.3735524962.3650486276.241707>}}}
[ns_server:debug,2025-05-15T18:46:48.193Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:46:48.193Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3735524962.3650355203.241403>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:46:48.193Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3735524962.3650355203.241403>,
                                  inet_tcp_dist,<0.484.0>,
                                  #Ref<0.3735524962.3650355203.241406>}
[error_logger:info,2025-05-15T18:46:48.201Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.484.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2025-05-15T18:46:48.202Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:46:48.202Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3735524962.3650355203.241403>,
                               inet_tcp_dist,<0.484.0>,
                               #Ref<0.3735524962.3650355203.241406>}
[ns_server:debug,2025-05-15T18:46:48.202Z,ns_1@cb.local:<0.473.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-05-15T18:46:48.404Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {2355620,#Ref<0.3735524962.3650486276.241707>}}}
[ns_server:debug,2025-05-15T18:46:48.404Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:46:48.405Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3735524962.3650355203.241409>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:46:48.405Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3735524962.3650355203.241409>,
                                  inet_tcp_dist,<0.486.0>,
                                  #Ref<0.3735524962.3650355203.241412>}
[error_logger:info,2025-05-15T18:46:48.411Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.486.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2025-05-15T18:46:48.412Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:46:48.412Z,ns_1@cb.local:<0.473.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2025-05-15T18:46:48.412Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3735524962.3650355203.241409>,
                               inet_tcp_dist,<0.486.0>,
                               #Ref<0.3735524962.3650355203.241412>}
[error_logger:info,2025-05-15T18:46:48.613Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {2355621,#Ref<0.3735524962.3650486276.241707>}}}
[ns_server:debug,2025-05-15T18:46:48.613Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:46:48.614Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3735524962.3650355201.242410>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:46:48.614Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3735524962.3650355201.242410>,
                                  inet_tcp_dist,<0.488.0>,
                                  #Ref<0.3735524962.3650355201.242413>}
[error_logger:info,2025-05-15T18:46:48.614Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.488.0>,{recv_challenge_failed,{error,closed}}}}
[ns_server:debug,2025-05-15T18:46:48.614Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3735524962.3650355201.242410>,
                               inet_tcp_dist,<0.488.0>,
                               #Ref<0.3735524962.3650355201.242413>}
[error_logger:info,2025-05-15T18:46:48.615Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:46:48.615Z,ns_1@cb.local:<0.473.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-05-15T18:46:48.816Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {2355622,#Ref<0.3735524962.3650486276.241707>}}}
[ns_server:debug,2025-05-15T18:46:48.816Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:46:48.816Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3735524962.3650355201.242424>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:46:48.817Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3735524962.3650355201.242424>,
                                  inet_tcp_dist,<0.490.0>,
                                  #Ref<0.3735524962.3650355201.242427>}
[ns_server:debug,2025-05-15T18:46:48.817Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3735524962.3650355201.242424>,
                               inet_tcp_dist,<0.490.0>,
                               #Ref<0.3735524962.3650355201.242427>}
[error_logger:info,2025-05-15T18:46:48.817Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.490.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2025-05-15T18:46:48.817Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:46:48.817Z,ns_1@cb.local:<0.473.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-05-15T18:46:49.019Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {2355623,#Ref<0.3735524962.3650486276.241707>}}}
[ns_server:debug,2025-05-15T18:46:49.020Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:46:49.020Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3735524962.3650355201.242438>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:46:49.020Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3735524962.3650355201.242438>,
                                  inet_tcp_dist,<0.492.0>,
                                  #Ref<0.3735524962.3650355201.242440>}
[ns_server:debug,2025-05-15T18:46:49.027Z,ns_1@cb.local:<0.473.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: false
[ns_server:info,2025-05-15T18:46:49.416Z,ns_1@cb.local:ns_couchdb_port<0.470.0>:ns_port_server:log:226]ns_couchdb<0.470.0>: Apache CouchDB v4.5.1-330-g3e5b8f24 (LogLevel=info) is starting.
ns_couchdb<0.470.0>: Apache CouchDB has started. Time to relax.

[error_logger:info,2025-05-15T18:46:49.604Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.472.0>},
              {id,wait_for_couchdb_node},
              {mfargs,{erlang,apply,
                              [#Fun<ns_server_nodes_sup.0.120652322>,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:49.611Z,ns_1@cb.local:<0.498.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,ns_disksup} with delay of 4000
[error_logger:info,2025-05-15T18:46:49.614Z,ns_1@cb.local:<0.499.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.499.0>,suppress_max_restart_intensity}
    started: [{pid,<0.500.0>},
              {id,ns_disksup},
              {mfargs,{ns_disksup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.614Z,ns_1@cb.local:<0.497.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.497.0>,suppress_max_restart_intensity}
    started: [{pid,<0.498.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      ns_disksup}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,ns_disksup},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => ns_disksup,inherited_max_r => 20,
                          inherited_max_t => 10,modules => [],
                          restart => permanent,shutdown => 1000,
                          start => {ns_disksup,start_link,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:49.614Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.497.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,ns_disksup}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => ns_disksup,inherited_max_r => 20,
                         inherited_max_t => 10,modules => [],
                         restart => permanent,shutdown => 1000,
                         start => {ns_disksup,start_link,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:49.617Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.501.0>},
              {id,diag_handler_worker},
              {mfargs,{work_queue,start_link,[diag_handler_worker]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:49.622Z,ns_1@cb.local:ns_server_sup<0.496.0>:dir_size:start_link:33]Starting quick version of dir_size with program name: godu
[error_logger:info,2025-05-15T18:46:49.622Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.502.0>},
              {id,dir_size},
              {mfargs,{dir_size,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.623Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.503.0>},
              {id,request_tracker},
              {mfargs,{request_tracker,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.625Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.504.0>},
              {id,chronicle_kv_log},
              {mfargs,{chronicle_kv_log,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:warn,2025-05-15T18:46:49.632Z,ns_1@cb.local:ns_log<0.506.0>:gossip_replicator:read_logs:244]Couldn't load logs from "/opt/couchbase/var/lib/couchbase/ns_log" (perhaps it's first
                         startup): {error,enoent}
[error_logger:info,2025-05-15T18:46:49.632Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.506.0>},
              {id,ns_log},
              {mfargs,{ns_log,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.633Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.507.0>},
              {id,event_log_events},
              {mfargs,{gen_event,start_link,[{local,event_log_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:warn,2025-05-15T18:46:49.636Z,ns_1@cb.local:event_log_server<0.508.0>:gossip_replicator:read_logs:244]Couldn't load logs from "/opt/couchbase/var/lib/couchbase/event_log" (perhaps it's first
                         startup): {error,enoent}
[error_logger:info,2025-05-15T18:46:49.636Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.508.0>},
              {id,event_log_server},
              {mfargs,{event_log_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.647Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.510.0>},
              {id,initargs_updater},
              {mfargs,{initargs_updater,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.649Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.512.0>},
              {id,timer_lag_recorder},
              {mfargs,{timer_lag_recorder,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:49.649Z,ns_1@cb.local:<0.514.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,
                                    ns_babysitter_log_consumer} with delay of 4000
[error_logger:info,2025-05-15T18:46:49.649Z,ns_1@cb.local:<0.515.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.515.0>,suppress_max_restart_intensity}
    started: [{pid,<0.516.0>},
              {id,ns_babysitter_log_consumer},
              {mfargs,{ns_log,start_link_babysitter_log_consumer,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.649Z,ns_1@cb.local:<0.513.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.513.0>,suppress_max_restart_intensity}
    started: [{pid,<0.514.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      ns_babysitter_log_consumer}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,
                           ns_babysitter_log_consumer},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => ns_babysitter_log_consumer,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [],restart => permanent,shutdown => 1000,
                          start =>
                              {ns_log,start_link_babysitter_log_consumer,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:49.650Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.513.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,
                      ns_babysitter_log_consumer}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => ns_babysitter_log_consumer,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [],restart => permanent,shutdown => 1000,
                         start =>
                             {ns_log,start_link_babysitter_log_consumer,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:49.748Z,ns_1@cb.local:<0.521.0>:goport:handle_eof:592]Stream 'stdout' closed
[ns_server:debug,2025-05-15T18:46:49.748Z,ns_1@cb.local:<0.521.0>:goport:handle_eof:592]Stream 'stderr' closed
[ns_server:info,2025-05-15T18:46:49.748Z,ns_1@cb.local:<0.521.0>:goport:handle_process_exit:573]Port exited with status 0.
[ns_server:debug,2025-05-15T18:46:49.756Z,ns_1@cb.local:prometheus_cfg<0.517.0>:prometheus_cfg:ensure_prometheus_config:932]Updating prometheus config file: /opt/couchbase/var/lib/couchbase/config/prometheus.yml
[ns_server:debug,2025-05-15T18:46:49.765Z,ns_1@cb.local:prometheus_cfg<0.517.0>:prometheus_cfg:ensure_prometheus_config:932]Updating prometheus config file: /opt/couchbase/var/lib/couchbase/config/prometheus_rules.yml
[ns_server:debug,2025-05-15T18:46:49.791Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',prometheus_auth_info} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554009}}]}|
 {"@prometheus",
  {auth,
   [{<<"plain">>,
     {sanitized,<<"xBTyY5PxiYqTH827nt18KKF6OhLvQZ5uVWo6ebcDGoU=">>}},
    {<<"sha512">>,
     {[{<<"s">>,
        <<"Itx0t6hA05/7KWcOTtXrnhZl+Y8xzf9IGmkGb8R407uDgxsYo0DddcQs1F6HNOTeu4TRz5fSNU19nWnO6NiW7w==">>},
       {<<"h">>,
        {sanitized,<<"oNeGNifmDlDVKl51i5CH5/I/tdxe1qsKTef5l3A5aeU=">>}},
       {<<"i">>,15000}]}},
    {<<"sha256">>,
     {[{<<"s">>,<<"uBznjFHU64WukptNCxV32d+XyQh39VEIfw0HlAwfHDA=">>},
       {<<"h">>,
        {sanitized,<<"iwQSjwH7yaiTQaJDQMjx6nw4QbBFWo3FmhYZf30UYH4=">>}},
       {<<"i">>,15000}]}},
    {<<"sha1">>,
     {[{<<"s">>,<<"cwh0SZpyufFmgvqCRFWq142sK/M=">>},
       {<<"h">>,
        {sanitized,<<"rfyQthAbcbsd4sVGf7Yd5uNbpVCaWIg0/M/RFnN4T88=">>}},
       {<<"i">>,15000}]}}]}}]
[ns_server:debug,2025-05-15T18:46:49.795Z,ns_1@cb.local:prometheus_cfg<0.517.0>:prometheus_cfg:apply_config:724]Restarting Prometheus as the start specs have changed
[error_logger:info,2025-05-15T18:46:49.797Z,ns_1@cb.local:ale_dynamic_sup<0.78.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ale_dynamic_sup}
    started: [{pid,<0.524.0>},
              {id,'sink-prometheus'},
              {mfargs,
                  {ale_dynamic_sup,delay_death,
                      [{ale_disk_sink,start_link,
                           ['sink-prometheus',
                            "/opt/couchbase/var/lib/couchbase/logs/prometheus.log",
                            [{rotation,
                                 [{compress,true},
                                  {size,41943040},
                                  {num_files,10},
                                  {buffer_size_max,52428800}]}]]},
                       1000]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:49.805Z,ns_1@cb.local:ns_couchdb_port<0.470.0>:ns_port_server:log:226]ns_couchdb<0.470.0>: 218: Booted. Waiting for shutdown request
ns_couchdb<0.470.0>: 218: Booted. Waiting for shutdown request
ns_couchdb<0.470.0>: working as port

[error_logger:info,2025-05-15T18:46:49.921Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.517.0>},
              {id,prometheus_cfg},
              {mfargs,{prometheus_cfg,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:49.924Z,ns_1@cb.local:memcached_passwords<0.530.0>:memcached_cfg:init:60]Init config writer for memcached_passwords, "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2025-05-15T18:46:49.926Z,ns_1@cb.local:memcached_passwords<0.530.0>:memcached_cfg:write_cfg:156]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2025-05-15T18:46:49.938Z,ns_1@cb.local:memcached_passwords<0.530.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,{auth,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2025-05-15T18:46:49.939Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:handle_call:57]File rename from "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw" is requested
[ns_server:debug,2025-05-15T18:46:49.940Z,ns_1@cb.local:memcached_passwords<0.530.0>:memcached_cfg:rename_and_refresh:178]Successfully renamed "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2025-05-15T18:46:49.940Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:handle_cast:67]Refresh of isasl requested
[ns_server:debug,2025-05-15T18:46:49.940Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:update_refresh_state:137]Refresh of [isasl] skipped. Retry in 1000 ms.
[error_logger:info,2025-05-15T18:46:49.940Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.530.0>},
              {id,memcached_passwords},
              {mfargs,{memcached_passwords,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:49.945Z,ns_1@cb.local:memcached_permissions<0.533.0>:memcached_cfg:init:60]Init config writer for memcached_permissions, "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2025-05-15T18:46:49.953Z,ns_1@cb.local:memcached_permissions<0.533.0>:memcached_cfg:write_cfg:156]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2025-05-15T18:46:49.954Z,ns_1@cb.local:memcached_permissions<0.533.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,{user,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2025-05-15T18:46:49.955Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:handle_call:57]File rename from "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac" is requested
[ns_server:debug,2025-05-15T18:46:49.956Z,ns_1@cb.local:memcached_permissions<0.533.0>:memcached_cfg:rename_and_refresh:178]Successfully renamed "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2025-05-15T18:46:49.956Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:handle_cast:67]Refresh of rbac requested
[error_logger:info,2025-05-15T18:46:49.956Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.533.0>},
              {id,memcached_permissions},
              {mfargs,{memcached_permissions,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.959Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.541.0>},
              {id,ns_email_alert},
              {mfargs,{ns_email_alert,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.960Z,ns_1@cb.local:ns_node_disco_sup<0.543.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_node_disco_sup}
    started: [{pid,<0.544.0>},
              {id,ns_node_disco_events},
              {mfargs,{gen_event,start_link,[{local,ns_node_disco_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:49.960Z,ns_1@cb.local:ns_node_disco<0.545.0>:ns_node_disco:init:111]Initting ns_node_disco with []
[error_logger:info,2025-05-15T18:46:49.960Z,ns_1@cb.local:ns_node_disco_sup<0.543.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_node_disco_sup}
    started: [{pid,<0.545.0>},
              {id,ns_node_disco},
              {mfargs,{ns_node_disco,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:49.960Z,ns_1@cb.local:ns_cookie_manager<0.238.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[ns_server:debug,2025-05-15T18:46:49.961Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
otp ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554009}}]},
 {cookie,{sanitized,<<"laexClf6j/iiXLRHc+IgLQ5C2ku89YxpvAez8UPpQjA=">>}}]
[user:info,2025-05-15T18:46:49.961Z,ns_1@cb.local:ns_cookie_manager<0.238.0>:ns_cookie_manager:do_cookie_init:78]Initial otp cookie generated: {sanitized,
                                  <<"laexClf6j/iiXLRHc+IgLQ5C2ku89YxpvAez8UPpQjA=">>}
[ns_server:debug,2025-05-15T18:46:49.961Z,ns_1@cb.local:ns_cookie_manager<0.238.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[ns_server:debug,2025-05-15T18:46:49.961Z,ns_1@cb.local:<0.547.0>:ns_node_disco:do_nodes_wanted_updated_fun:210]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"laexClf6j/iiXLRHc+IgLQ5C2ku89YxpvAez8UPpQjA=">>}
[ns_server:debug,2025-05-15T18:46:49.961Z,ns_1@cb.local:<0.549.0>:ns_node_disco:do_nodes_wanted_updated_fun:210]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"laexClf6j/iiXLRHc+IgLQ5C2ku89YxpvAez8UPpQjA=">>}
[error_logger:info,2025-05-15T18:46:49.962Z,ns_1@cb.local:ns_node_disco_sup<0.543.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_node_disco_sup}
    started: [{pid,<0.550.0>},
              {id,ns_node_disco_log},
              {mfargs,{ns_node_disco_log,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:49.963Z,ns_1@cb.local:<0.549.0>:ns_node_disco:do_nodes_wanted_updated_fun:213]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"laexClf6j/iiXLRHc+IgLQ5C2ku89YxpvAez8UPpQjA=">>}
[ns_server:debug,2025-05-15T18:46:49.963Z,ns_1@cb.local:<0.547.0>:ns_node_disco:do_nodes_wanted_updated_fun:213]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"laexClf6j/iiXLRHc+IgLQ5C2ku89YxpvAez8UPpQjA=">>}
[error_logger:info,2025-05-15T18:46:49.968Z,ns_1@cb.local:ns_config_rep_sup<0.552.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_rep_sup}
    started: [{pid,<0.553.0>},
              {id,ns_config_rep_merger},
              {mfargs,{ns_config_rep,start_link_merger,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:49.969Z,ns_1@cb.local:ns_config_rep<0.554.0>:ns_config_rep:init:80]init pulling
[ns_server:debug,2025-05-15T18:46:49.969Z,ns_1@cb.local:ns_config_rep<0.554.0>:ns_config_rep:init:82]init pushing
[error_logger:info,2025-05-15T18:46:49.969Z,ns_1@cb.local:ns_config_rep_sup<0.552.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_rep_sup}
    started: [{pid,<0.554.0>},
              {id,ns_config_rep},
              {mfargs,{ns_config_rep,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.969Z,ns_1@cb.local:ns_node_disco_sup<0.543.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_node_disco_sup}
    started: [{pid,<0.552.0>},
              {id,ns_config_rep_sup},
              {mfargs,{ns_config_rep_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:49.970Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.543.0>},
              {id,ns_node_disco_sup},
              {mfargs,{ns_node_disco_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:49.970Z,ns_1@cb.local:tombstone_keeper<0.277.0>:tombstone_keeper:handle_call:49]Refreshed with timestamps []
[error_logger:info,2025-05-15T18:46:49.970Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.565.0>},
              {id,tombstone_agent},
              {mfargs,{tombstone_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.973Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.569.0>},
              {id,vbucket_map_mirror},
              {mfargs,{vbucket_map_mirror,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.975Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.572.0>},
              {id,capi_url_cache},
              {mfargs,{capi_url_cache,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.977Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.575.0>},
              {id,bucket_info_cache},
              {mfargs,{bucket_info_cache,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.977Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.579.0>},
              {id,ns_tick_event},
              {mfargs,{gen_event,start_link,[{local,ns_tick_event}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.978Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.580.0>},
              {id,buckets_events},
              {mfargs,{gen_event,start_link,[{local,buckets_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.978Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.581.0>},
              {id,ns_stats_event},
              {mfargs,{gen_event,start_link,[{local,ns_stats_event}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.980Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.583.0>},
              {id,samples_loader_tasks},
              {mfargs,{samples_loader_tasks,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.988Z,ns_1@cb.local:ns_heart_sup<0.584.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_heart_sup}
    started: [{pid,<0.585.0>},
              {id,ns_heart},
              {mfargs,{ns_heart,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.988Z,ns_1@cb.local:ns_heart_sup<0.584.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_heart_sup}
    started: [{pid,<0.587.0>},
              {id,ns_heart_slow_updater},
              {mfargs,{ns_heart,start_link_slow_updater,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.988Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.584.0>},
              {id,ns_heart_sup},
              {mfargs,{ns_heart_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:49.989Z,ns_1@cb.local:ns_doctor_sup<0.589.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_doctor_sup}
    started: [{pid,<0.590.0>},
              {id,ns_doctor_events},
              {mfargs,{gen_event,start_link,[{local,ns_doctor_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.997Z,ns_1@cb.local:ns_doctor_sup<0.589.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_doctor_sup}
    started: [{pid,<0.593.0>},
              {id,ns_doctor},
              {mfargs,{ns_doctor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:49.998Z,ns_1@cb.local:<0.588.0>:restartable:start_child:92]Started child process <0.589.0>
  MFA: {ns_doctor_sup,start_link,[]}
[error_logger:info,2025-05-15T18:46:49.998Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.588.0>},
              {id,ns_doctor_sup},
              {mfargs,{restartable,start_link,
                                   [{ns_doctor_sup,start_link,[]},infinity]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:49.998Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.596.0>},
              {id,master_activity_events},
              {mfargs,{gen_event,start_link,[{local,master_activity_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.005Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.597.0>},
              {id,xdcr_ckpt_store},
              {mfargs,{simple_store,start_link,[xdcr_ckpt_data]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.005Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.598.0>},
              {id,metakv_worker},
              {mfargs,{work_queue,start_link,[metakv_worker]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.005Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.599.0>},
              {id,index_events},
              {mfargs,{gen_event,start_link,[{local,index_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.006Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.600.0>},
              {id,index_settings_manager},
              {mfargs,{index_settings_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.007Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.602.0>},
              {id,query_settings_manager},
              {mfargs,{query_settings_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.007Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.604.0>},
              {id,eventing_settings_manager},
              {mfargs,{eventing_settings_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.007Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.606.0>},
              {id,analytics_settings_manager},
              {mfargs,{analytics_settings_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.007Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.608.0>},
              {id,audit_events},
              {mfargs,{gen_event,start_link,[{local,audit_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:50.007Z,ns_1@cb.local:<0.610.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,
                                    encryption_service} with delay of 1000
[ns_server:debug,2025-05-15T18:46:50.008Z,ns_1@cb.local:encryption_service<0.612.0>:encryption_service:recover:199]Config marker /opt/couchbase/var/lib/couchbase/config/sm_load_config_marker doesn't exist
[error_logger:info,2025-05-15T18:46:50.008Z,ns_1@cb.local:<0.611.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.611.0>,suppress_max_restart_intensity}
    started: [{pid,<0.612.0>},
              {id,encryption_service},
              {mfargs,{encryption_service,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.008Z,ns_1@cb.local:<0.609.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.609.0>,suppress_max_restart_intensity}
    started: [{pid,<0.610.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      encryption_service}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,encryption_service},
                       1000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 1,id => encryption_service,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [encryption_service],
                          restart => permanent,shutdown => 5000,
                          start => {encryption_service,start_link,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:50.008Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.609.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,encryption_service}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 1,id => encryption_service,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [encryption_service],
                         restart => permanent,shutdown => 5000,
                         start => {encryption_service,start_link,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:50.021Z,ns_1@cb.local:menelaus_sup<0.616.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.617.0>},
              {id,menelaus_ui_auth},
              {mfargs,{menelaus_ui_auth,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.022Z,ns_1@cb.local:menelaus_sup<0.616.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.619.0>},
              {id,scram_sha},
              {mfargs,{scram_sha,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.030Z,ns_1@cb.local:menelaus_sup<0.616.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.620.0>},
              {id,menelaus_local_auth},
              {mfargs,{menelaus_local_auth,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.043Z,ns_1@cb.local:menelaus_sup<0.616.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.621.0>},
              {id,menelaus_web_cache},
              {mfargs,{menelaus_web_cache,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.049Z,ns_1@cb.local:menelaus_sup<0.616.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.622.0>},
              {id,menelaus_stats_gatherer},
              {mfargs,{menelaus_stats_gatherer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.049Z,ns_1@cb.local:menelaus_sup<0.616.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.623.0>},
              {id,json_rpc_events},
              {mfargs,{gen_event,start_link,[{local,json_rpc_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.072Z,ns_1@cb.local:menelaus_web_sup<0.624.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_web_sup}
    started: [{pid,<0.626.0>},
              {id,menelaus_event},
              {mfargs,{menelaus_event,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.075Z,ns_1@cb.local:inet_gethost_native_sup<0.629.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,inet_gethost_native_sup}
    started: [{pid,<0.630.0>},{mfa,{inet_gethost_native,init,[[]]}}]

[error_logger:info,2025-05-15T18:46:50.075Z,ns_1@cb.local:kernel_safe_sup<0.67.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,kernel_safe_sup}
    started: [{pid,<0.629.0>},
              {id,inet_gethost_native_sup},
              {mfargs,{inet_gethost_native,start_link,[]}},
              {restart_type,temporary},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:50.075Z,ns_1@cb.local:<0.628.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2025-05-15T18:46:50.076Z,ns_1@cb.local:<0.628.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2025-05-15T18:46:50.076Z,ns_1@cb.local:<0.628.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2025-05-15T18:46:50.077Z,ns_1@cb.local:<0.628.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2025-05-15T18:46:50.078Z,ns_1@cb.local:<0.628.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2025-05-15T18:46:50.079Z,ns_1@cb.local:<0.628.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"0.0.0.0"},{port,8091}]
[error_logger:info,2025-05-15T18:46:50.079Z,ns_1@cb.local:<0.628.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.628.0>,menelaus_web}
    started: [{pid,<0.631.0>},
              {id,menelaus_web_ipv4},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet},
                        {name,menelaus_web},
                        {port,8091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:50.082Z,ns_1@cb.local:<0.628.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2025-05-15T18:46:50.083Z,ns_1@cb.local:<0.628.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2025-05-15T18:46:50.084Z,ns_1@cb.local:<0.628.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2025-05-15T18:46:50.084Z,ns_1@cb.local:<0.628.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2025-05-15T18:46:50.085Z,ns_1@cb.local:<0.628.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2025-05-15T18:46:50.085Z,ns_1@cb.local:<0.628.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"::"},{port,8091}]
[error_logger:info,2025-05-15T18:46:50.085Z,ns_1@cb.local:<0.628.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.628.0>,menelaus_web}
    started: [{pid,<0.648.0>},
              {id,menelaus_web_ipv6},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet6},
                        {name,menelaus_web},
                        {port,8091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:50.085Z,ns_1@cb.local:<0.627.0>:restartable:start_child:92]Started child process <0.628.0>
  MFA: {menelaus_web,start_link,[]}
[error_logger:info,2025-05-15T18:46:50.086Z,ns_1@cb.local:menelaus_web_sup<0.624.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_web_sup}
    started: [{pid,<0.627.0>},
              {id,menelaus_web},
              {mfargs,{restartable,start_link,
                                   [{menelaus_web,start_link,[]},infinity]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[user:info,2025-05-15T18:46:50.086Z,ns_1@cb.local:menelaus_sup<0.616.0>:menelaus_web_sup:start_link:38]Couchbase Server has started on web port 8091 on node 'ns_1@cb.local'. Version: "7.6.2-3721-enterprise".
[error_logger:info,2025-05-15T18:46:50.086Z,ns_1@cb.local:menelaus_sup<0.616.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.624.0>},
              {id,menelaus_web_sup},
              {mfargs,{menelaus_web_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:50.086Z,ns_1@cb.local:menelaus_sup<0.616.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.665.0>},
              {id,menelaus_web_alerts_srv},
              {mfargs,{menelaus_web_alerts_srv,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.095Z,ns_1@cb.local:menelaus_sup<0.616.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.667.0>},
              {id,guardrail_monitor},
              {mfargs,{guardrail_monitor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.100Z,ns_1@cb.local:menelaus_sup<0.616.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.668.0>},
              {id,menelaus_cbauth},
              {mfargs,{menelaus_cbauth,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.100Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.616.0>},
              {id,menelaus},
              {mfargs,{menelaus_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:50.100Z,ns_1@cb.local:<0.675.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,ns_ports_setup} with delay of 4000
[error_logger:info,2025-05-15T18:46:50.100Z,ns_1@cb.local:<0.676.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.676.0>,suppress_max_restart_intensity}
    started: [{pid,<0.677.0>},
              {id,ns_ports_setup},
              {mfargs,{ns_ports_setup,start,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.100Z,ns_1@cb.local:<0.674.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.674.0>,suppress_max_restart_intensity}
    started: [{pid,<0.675.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      ns_ports_setup}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,ns_ports_setup},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => ns_ports_setup,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [],restart => permanent,
                          shutdown => brutal_kill,
                          start => {ns_ports_setup,start,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:50.101Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.674.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,ns_ports_setup}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => ns_ports_setup,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [],restart => permanent,
                         shutdown => brutal_kill,
                         start => {ns_ports_setup,start,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:50.102Z,ns_1@cb.local:ns_ports_setup<0.677.0>:ns_ports_manager:set_dynamic_children:48]Setting children [memcached,saslauthd_port,goxdcr]
[error_logger:info,2025-05-15T18:46:50.107Z,ns_1@cb.local:service_agent_sup<0.680.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_agent_sup}
    started: [{pid,<0.681.0>},
              {id,service_agent_children_sup},
              {mfargs,{supervisor,start_link,
                                  [{local,service_agent_children_sup},
                                   service_agent_sup,child]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:50.107Z,ns_1@cb.local:service_agent_sup<0.680.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_agent_sup}
    started: [{pid,<0.682.0>},
              {id,service_agent_worker},
              {mfargs,{erlang,apply,[#Fun<service_agent_sup.0.125430937>,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.108Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.680.0>},
              {id,service_agent_sup},
              {mfargs,{service_agent_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:50.113Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.685.0>},
              {id,ns_memcached_sockets_pool},
              {mfargs,{ns_memcached_sockets_pool,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:50.119Z,ns_1@cb.local:memcached_auth_server<0.686.0>:memcached_auth_server:reconnect:239]Skipping creation of 'Auth provider' connection because external users are disabled
[error_logger:info,2025-05-15T18:46:50.119Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.686.0>},
              {id,memcached_auth_server},
              {mfargs,{memcached_auth_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:50.119Z,ns_1@cb.local:<0.689.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,ns_audit_cfg} with delay of 4000
[ns_server:debug,2025-05-15T18:46:50.120Z,ns_1@cb.local:ns_audit_cfg<0.691.0>:ns_audit_cfg:write_audit_json:238]Writing new content to "/opt/couchbase/var/lib/couchbase/config/audit.json", Params [{descriptors_path,
                                                                                      "/opt/couchbase/etc/security"},
                                                                                     {version,
                                                                                      2},
                                                                                     {uuid,
                                                                                      "33985209"},
                                                                                     {event_states,
                                                                                      {[]}},
                                                                                     {filtering_enabled,
                                                                                      true},
                                                                                     {disabled_userids,
                                                                                      []},
                                                                                     {auditd_enabled,
                                                                                      false},
                                                                                     {log_path,
                                                                                      "/opt/couchbase/var/lib/couchbase/logs"},
                                                                                     {prune_age,
                                                                                      0},
                                                                                     {rotate_interval,
                                                                                      86400},
                                                                                     {rotate_size,
                                                                                      20971520},
                                                                                     {sync,
                                                                                      []}]
[error_logger:info,2025-05-15T18:46:50.123Z,ns_1@cb.local:<0.690.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.690.0>,suppress_max_restart_intensity}
    started: [{pid,<0.691.0>},
              {id,ns_audit_cfg},
              {mfargs,{ns_audit_cfg,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.123Z,ns_1@cb.local:<0.688.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.688.0>,suppress_max_restart_intensity}
    started: [{pid,<0.689.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      ns_audit_cfg}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,ns_audit_cfg},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => ns_audit_cfg,inherited_max_r => 20,
                          inherited_max_t => 10,modules => [],
                          restart => permanent,shutdown => 1000,
                          start => {ns_audit_cfg,start_link,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:50.123Z,ns_1@cb.local:ns_audit_cfg<0.691.0>:ns_audit_cfg:notify_memcached:152]Instruct memcached to reload audit config
[error_logger:info,2025-05-15T18:46:50.123Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.688.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,ns_audit_cfg}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => ns_audit_cfg,inherited_max_r => 20,
                         inherited_max_t => 10,modules => [],
                         restart => permanent,shutdown => 1000,
                         start => {ns_audit_cfg,start_link,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:50.124Z,ns_1@cb.local:<0.694.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,ns_audit} with delay of 4000
[ns_server:warn,2025-05-15T18:46:50.125Z,ns_1@cb.local:<0.697.0>:ns_memcached:connect:1460]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}, retrying.
[error_logger:info,2025-05-15T18:46:50.128Z,ns_1@cb.local:<0.696.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.696.0>,suppress_max_restart_intensity}
    started: [{pid,<0.699.0>},
              {id,ns_audit},
              {mfargs,{ns_audit,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.128Z,ns_1@cb.local:<0.693.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.693.0>,suppress_max_restart_intensity}
    started: [{pid,<0.694.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      ns_audit}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,ns_audit},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => ns_audit,inherited_max_r => 20,
                          inherited_max_t => 10,modules => [],
                          restart => permanent,shutdown => 1000,
                          start => {ns_audit,start_link,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:50.128Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.693.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,ns_audit}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => ns_audit,inherited_max_r => 20,
                         inherited_max_t => 10,modules => [],
                         restart => permanent,shutdown => 1000,
                         start => {ns_audit,start_link,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:50.128Z,ns_1@cb.local:<0.702.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,
                                    memcached_config_mgr} with delay of 4000
[ns_server:debug,2025-05-15T18:46:50.129Z,ns_1@cb.local:memcached_config_mgr<0.704.0>:memcached_config_mgr:memcached_port_pid:154]waiting for completion of initial ns_ports_setup round
[error_logger:info,2025-05-15T18:46:50.129Z,ns_1@cb.local:<0.703.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.703.0>,suppress_max_restart_intensity}
    started: [{pid,<0.704.0>},
              {id,memcached_config_mgr},
              {mfargs,{memcached_config_mgr,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.129Z,ns_1@cb.local:<0.701.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.701.0>,suppress_max_restart_intensity}
    started: [{pid,<0.702.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      memcached_config_mgr}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,memcached_config_mgr},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => memcached_config_mgr,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [],restart => permanent,shutdown => 1000,
                          start => {memcached_config_mgr,start_link,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:50.129Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.701.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,memcached_config_mgr}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => memcached_config_mgr,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [],restart => permanent,shutdown => 1000,
                         start => {memcached_config_mgr,start_link,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:info,2025-05-15T18:46:50.134Z,ns_1@cb.local:<0.705.0>:ns_memcached_log_rotator:init:36]Starting log rotator on "/opt/couchbase/var/lib/couchbase/logs"/"memcached.log"* with an initial period of 39003ms
[error_logger:info,2025-05-15T18:46:50.134Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.705.0>},
              {id,ns_memcached_log_rotator},
              {mfargs,{ns_memcached_log_rotator,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.139Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.706.0>},
              {id,testconditions_store},
              {mfargs,{simple_store,start_link,[testconditions]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:50.144Z,ns_1@cb.local:<0.707.0>:memcached_config_mgr:memcached_port_pid:154]waiting for completion of initial ns_ports_setup round
[error_logger:info,2025-05-15T18:46:50.144Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.707.0>},
              {id,terse_cluster_info_uploader},
              {mfargs,{terse_cluster_info_uploader,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.147Z,ns_1@cb.local:ns_bucket_worker_sup<0.709.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_bucket_worker_sup}
    started: [{pid,<0.710.0>},
              {id,ns_bucket_sup},
              {mfargs,{ns_bucket_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:50.149Z,ns_1@cb.local:ns_bucket_worker_sup<0.709.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_bucket_worker_sup}
    started: [{pid,<0.711.0>},
              {id,ns_bucket_worker},
              {mfargs,{ns_bucket_worker,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.149Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.709.0>},
              {id,ns_bucket_worker_sup},
              {mfargs,{ns_bucket_worker_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:warn,2025-05-15T18:46:50.151Z,ns_1@cb.local:<0.714.0>:ns_memcached:connect:1460]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}, retrying.
[error_logger:info,2025-05-15T18:46:50.156Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.715.0>},
              {id,ns_server_stats},
              {mfargs,{ns_server_stats,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:50.162Z,ns_1@cb.local:<0.717.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ale_stats_events,<0.715.0>} exited with reason {noproc,
                                                                                {gen_statem,
                                                                                 call,
                                                                                 [mb_master,
                                                                                  master_node,
                                                                                  infinity]}}
[error_logger:info,2025-05-15T18:46:50.168Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.719.0>},
              {id,{stats_reader,"@system"}},
              {mfargs,{stats_reader,start_link,["@system"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.168Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.721.0>},
              {id,{stats_reader,"@system-processes"}},
              {mfargs,{stats_reader,start_link,["@system-processes"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.168Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.723.0>},
              {id,{stats_reader,"@query"}},
              {mfargs,{stats_reader,start_link,["@query"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.168Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.725.0>},
              {id,{stats_reader,"@global"}},
              {mfargs,{stats_reader,start_link,["@global"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.172Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.727.0>},
              {id,goxdcr_status_keeper},
              {mfargs,{goxdcr_status_keeper,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.176Z,ns_1@cb.local:services_stats_sup<0.728.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,services_stats_sup}
    started: [{pid,<0.729.0>},
              {id,service_stats_children_sup},
              {mfargs,{supervisor,start_link,
                                  [{local,service_stats_children_sup},
                                   services_stats_sup,child]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:50.178Z,ns_1@cb.local:service_status_keeper_sup<0.730.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_status_keeper_sup}
    started: [{pid,<0.731.0>},
              {id,service_status_keeper_worker},
              {mfargs,{work_queue,start_link,[service_status_keeper_worker]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.187Z,ns_1@cb.local:service_status_keeper_sup<0.730.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_status_keeper_sup}
    started: [{pid,<0.733.0>},
              {id,service_status_keeper_index},
              {mfargs,{service_index,start_keeper,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:50.189Z,ns_1@cb.local:goxdcr_status_keeper<0.727.0>:goxdcr_rest:get_from_goxdcr:139]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2025-05-15T18:46:50.191Z,ns_1@cb.local:goxdcr_status_keeper<0.727.0>:goxdcr_rest:get_from_goxdcr:139]Goxdcr is temporary not available. Return empty list.
[error_logger:info,2025-05-15T18:46:50.191Z,ns_1@cb.local:service_status_keeper_sup<0.730.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_status_keeper_sup}
    started: [{pid,<0.737.0>},
              {id,service_status_keeper_fts},
              {mfargs,{service_fts,start_keeper,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.192Z,ns_1@cb.local:service_status_keeper_sup<0.730.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_status_keeper_sup}
    started: [{pid,<0.740.0>},
              {id,service_status_keeper_eventing},
              {mfargs,{service_eventing,start_keeper,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.192Z,ns_1@cb.local:services_stats_sup<0.728.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,services_stats_sup}
    started: [{pid,<0.730.0>},
              {id,service_status_keeper_sup},
              {mfargs,{service_status_keeper_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:50.194Z,ns_1@cb.local:services_stats_sup<0.728.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,services_stats_sup}
    started: [{pid,<0.743.0>},
              {id,service_stats_worker},
              {mfargs,{erlang,apply,[#Fun<services_stats_sup.0.35188242>,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.194Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.728.0>},
              {id,services_stats_sup},
              {mfargs,{services_stats_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:50.194Z,ns_1@cb.local:<0.746.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,
                                    compaction_daemon} with delay of 4000
[ns_server:debug,2025-05-15T18:46:50.206Z,ns_1@cb.local:<0.750.0>:new_concurrency_throttle:init:109]init concurrent throttle process, pid: <0.750.0>, type: kv_throttle# of available token: 1
[ns_server:debug,2025-05-15T18:46:50.210Z,ns_1@cb.local:compaction_daemon<0.748.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_kv. Rescheduling compaction.
[error_logger:info,2025-05-15T18:46:50.210Z,ns_1@cb.local:<0.747.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.747.0>,suppress_max_restart_intensity}
    started: [{pid,<0.748.0>},
              {id,compaction_daemon},
              {mfargs,{compaction_daemon,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,86400000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:50.210Z,ns_1@cb.local:compaction_daemon<0.748.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:46:50.210Z,ns_1@cb.local:compaction_daemon<0.748.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2025-05-15T18:46:50.210Z,ns_1@cb.local:compaction_daemon<0.748.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:46:50.210Z,ns_1@cb.local:compaction_daemon<0.748.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2025-05-15T18:46:50.210Z,ns_1@cb.local:compaction_daemon<0.748.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_master too soon. Next run will be in 3600s
[error_logger:info,2025-05-15T18:46:50.210Z,ns_1@cb.local:<0.745.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.745.0>,suppress_max_restart_intensity}
    started: [{pid,<0.746.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      compaction_daemon}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,compaction_daemon},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => compaction_daemon,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [compaction_daemon],
                          restart => permanent,shutdown => 86400000,
                          start => {compaction_daemon,start_link,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:50.210Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.745.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,compaction_daemon}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => compaction_daemon,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [compaction_daemon],
                         restart => permanent,shutdown => 86400000,
                         start => {compaction_daemon,start_link,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:50.220Z,ns_1@cb.local:cluster_logs_sup<0.751.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,cluster_logs_sup}
    started: [{pid,<0.754.0>},
              {id,ets_holder},
              {mfargs,{cluster_logs_collection_task,start_link_ets_holder,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.221Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.751.0>},
              {id,cluster_logs_sup},
              {mfargs,{cluster_logs_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:50.221Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.755.0>},
              {id,collections},
              {mfargs,{collections,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.221Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.757.0>},
              {id,leader_events},
              {mfargs,{gen_event,start_link,[{local,leader_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:50.223Z,ns_1@cb.local:ns_heart<0.585.0>:goxdcr_rest:get_from_goxdcr:139]Goxdcr is temporary not available. Return empty list.
[error_logger:info,2025-05-15T18:46:50.226Z,ns_1@cb.local:leader_leases_sup<0.763.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_leases_sup}
    started: [{pid,<0.764.0>},
              {id,leader_activities},
              {mfargs,{leader_activities,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,10000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.228Z,ns_1@cb.local:leader_leases_sup<0.763.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_leases_sup}
    started: [{pid,<0.767.0>},
              {id,leader_lease_agent},
              {mfargs,{leader_lease_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.228Z,ns_1@cb.local:leader_services_sup<0.761.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_services_sup}
    started: [{pid,<0.763.0>},
              {id,leader_leases_sup},
              {mfargs,{leader_leases_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:50.236Z,ns_1@cb.local:leader_registry_sup<0.771.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_registry_sup}
    started: [{pid,<0.772.0>},
              {id,leader_registry},
              {mfargs,{leader_registry,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:50.236Z,ns_1@cb.local:leader_registry_sup<0.771.0>:mb_master:check_master_takeover_needed:183]Sending master node question to the following nodes: []
[ns_server:debug,2025-05-15T18:46:50.236Z,ns_1@cb.local:leader_registry_sup<0.771.0>:mb_master:check_master_takeover_needed:187]Got replies: []
[ns_server:debug,2025-05-15T18:46:50.236Z,ns_1@cb.local:leader_registry_sup<0.771.0>:mb_master:check_master_takeover_needed:193]Was unable to discover master, not going to force mastership takeover
[ns_server:debug,2025-05-15T18:46:50.237Z,ns_1@cb.local:mb_master<0.774.0>:mb_master:init:86]Heartbeat interval is 2000
[user:info,2025-05-15T18:46:50.237Z,ns_1@cb.local:mb_master<0.774.0>:mb_master:init:91]I'm the only node, so I'm the master.
[ns_server:debug,2025-05-15T18:46:50.237Z,ns_1@cb.local:leader_registry<0.772.0>:leader_registry:handle_new_leader:275]New leader is 'ns_1@cb.local'. Invalidating name cache.
[ns_server:debug,2025-05-15T18:46:50.247Z,ns_1@cb.local:mb_master<0.774.0>:master_activity_events:submit_cast:75]Failed to send master activity event {became_master,'ns_1@cb.local'}: {error,
                                                                       badarg}
[ns_server:debug,2025-05-15T18:46:50.252Z,ns_1@cb.local:ns_heart_slow_status_updater<0.587.0>:goxdcr_rest:get_from_goxdcr:139]Goxdcr is temporary not available. Return empty list.
[error_logger:info,2025-05-15T18:46:50.255Z,ns_1@cb.local:mb_master_sup<0.789.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.793.0>},
              {id,leader_lease_acquirer},
              {mfargs,{leader_lease_acquirer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,10000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:50.267Z,ns_1@cb.local:ns_ports_setup<0.677.0>:ns_ports_setup:set_children:65]Monitor ns_child_ports_sup <16971.133.0>
[ns_server:debug,2025-05-15T18:46:50.267Z,ns_1@cb.local:leader_quorum_nodes_manager<0.799.0>:leader_quorum_nodes_manager:pull_config:99]Attempting to pull config from nodes:
[]
[error_logger:info,2025-05-15T18:46:50.267Z,ns_1@cb.local:mb_master_sup<0.789.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.799.0>},
              {id,leader_quorum_nodes_manager},
              {mfargs,{leader_quorum_nodes_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:50.267Z,ns_1@cb.local:memcached_config_mgr<0.704.0>:memcached_config_mgr:memcached_port_pid:156]ns_ports_setup seems to be ready
[ns_server:debug,2025-05-15T18:46:50.267Z,ns_1@cb.local:<0.707.0>:memcached_config_mgr:memcached_port_pid:156]ns_ports_setup seems to be ready
[ns_server:debug,2025-05-15T18:46:50.268Z,ns_1@cb.local:leader_quorum_nodes_manager<0.799.0>:leader_quorum_nodes_manager:pull_config:104]Pulled config successfully.
[ns_server:debug,2025-05-15T18:46:50.268Z,ns_1@cb.local:memcached_config_mgr<0.704.0>:memcached_config_mgr:find_port_pid_loop:164]Found memcached port <16971.139.0>
[ns_server:debug,2025-05-15T18:46:50.268Z,ns_1@cb.local:<0.707.0>:memcached_config_mgr:find_port_pid_loop:164]Found memcached port <16971.139.0>
[ns_server:debug,2025-05-15T18:46:50.276Z,ns_1@cb.local:memcached_config_mgr<0.704.0>:memcached_config_mgr:init:93]wrote memcached config to /opt/couchbase/var/lib/couchbase/config/memcached.json. Will activate memcached port server
[ns_server:debug,2025-05-15T18:46:50.277Z,ns_1@cb.local:memcached_config_mgr<0.704.0>:memcached_config_mgr:init:97]activated memcached port server
[ns_server:info,2025-05-15T18:46:50.279Z,ns_1@cb.local:memcached_config_mgr<0.704.0>:memcached_config_mgr:push_tls_config:233]Pushing TLS config to memcached:
{[{<<"private key">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem">>},
  {<<"certificate chain">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem">>},
  {<<"CA file">>,<<"/opt/couchbase/var/lib/couchbase/config/certs/ca.pem">>},
  {<<"minimum version">>,<<"TLS 1.2">>},
  {<<"cipher list">>,
   {[{<<"TLS 1.2">>,<<"HIGH">>},
     {<<"TLS 1.3">>,
      <<"TLS_AES_256_GCM_SHA384:TLS_AES_128_GCM_SHA256:TLS_CHACHA20_POLY1305_SHA256">>}]}},
  {<<"cipher order">>,true},
  {<<"client cert auth">>,<<"disabled">>}]}
[ns_server:warn,2025-05-15T18:46:50.280Z,ns_1@cb.local:<0.808.0>:ns_memcached:connect:1460]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}, retrying.
[ns_server:debug,2025-05-15T18:46:50.281Z,ns_1@cb.local:leader_lease_agent<0.767.0>:leader_lease_agent:do_handle_acquire_lease:140]Granting lease to {lease_holder,<<"445e053598a21dcd845301efeef2f950">>,
                                'ns_1@cb.local'} for 15000ms
[ns_server:info,2025-05-15T18:46:50.291Z,ns_1@cb.local:mb_master_sup<0.789.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,ns_tick},
                                         ns_tick,[],[]]): started as <0.811.0> on 'ns_1@cb.local'

[error_logger:info,2025-05-15T18:46:50.291Z,ns_1@cb.local:mb_master_sup<0.789.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.811.0>},
              {id,ns_tick},
              {mfargs,{ns_tick,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,10},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:50.298Z,ns_1@cb.local:<0.798.0>:leader_lease_acquire_worker:handle_fresh_lease_acquired:296]Acquired lease from node 'ns_1@cb.local' (lease uuid: <<"445e053598a21dcd845301efeef2f950">>)
[ns_server:debug,2025-05-15T18:46:50.303Z,ns_1@cb.local:<0.707.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":11,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true,\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\"]}}">>
[ns_server:warn,2025-05-15T18:46:50.307Z,ns_1@cb.local:<0.813.0>:ns_memcached:connect:1460]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}, retrying.
[ns_server:debug,2025-05-15T18:46:50.318Z,ns_1@cb.local:<0.814.0>:chronicle_master:do_init:115]Starting with SelfRef = #Ref<0.3735524962.3650355201.242778>
[ns_server:info,2025-05-15T18:46:50.318Z,ns_1@cb.local:mb_master_sup<0.789.0>:misc:start_singleton:913]start_singleton(gen_server2, start_link, [{via,leader_registry,
                                           chronicle_master},
                                          chronicle_master,[],[]]): started as <0.814.0> on 'ns_1@cb.local'

[error_logger:info,2025-05-15T18:46:50.318Z,ns_1@cb.local:mb_master_sup<0.789.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.814.0>},
              {id,chronicle_master},
              {mfargs,{chronicle_master,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.330Z,ns_1@cb.local:ns_orchestrator_sup<0.816.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.817.0>},
              {id,compat_mode_events},
              {mfargs,{gen_event,start_link,[{local,compat_mode_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:50.338Z,ns_1@cb.local:compat_mode_manager<0.818.0>:cluster_compat_mode:do_upgrades:205]Initiating rbac upgrade due to version change from [7,1] to [7,6] (target version: [7,
                                                                                    6])
[ns_server:info,2025-05-15T18:46:50.338Z,ns_1@cb.local:compat_mode_manager<0.818.0>:menelaus_users:upgrade:1057]Upgrading users database to [7,6]
[ns_server:debug,2025-05-15T18:46:50.339Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
rbac_upgrade ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554010}}]}|
 started]
[ns_server:debug,2025-05-15T18:46:50.339Z,ns_1@cb.local:ns_config_rep<0.554.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([rbac_upgrade]..)
[ns_server:debug,2025-05-15T18:46:50.339Z,ns_1@cb.local:ns_config_rep<0.554.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 46 us
[ns_server:debug,2025-05-15T18:46:50.339Z,ns_1@cb.local:ns_config_rep<0.554.0>:ns_config_rep:handle_call:149]Got full synchronization request from 'ns_1@cb.local'
[ns_server:debug,2025-05-15T18:46:50.339Z,ns_1@cb.local:ns_config_rep<0.554.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 1 us
[ns_server:debug,2025-05-15T18:46:50.339Z,ns_1@cb.local:users_storage<0.444.0>:replicated_storage:handle_call:151]Received sync_to_me with timeout = 60000, nodes = ['ns_1@cb.local']
[ns_server:debug,2025-05-15T18:46:50.339Z,ns_1@cb.local:users_replicator<0.443.0>:doc_replicator:loop:100]Received sync_to_me with timeout = 60000, nodes = ['ns_1@cb.local']
[ns_server:debug,2025-05-15T18:46:50.339Z,ns_1@cb.local:users_storage<0.444.0>:replicated_storage:handle_call:146]Received sync_token from {<0.834.0>,
                          [alias|#Ref<0.3735524962.3650420737.242850>]}
[ns_server:debug,2025-05-15T18:46:50.339Z,ns_1@cb.local:users_replicator<0.443.0>:doc_replicator:loop:95]Received sync_token from {<0.834.0>,
                          [alias|#Ref<0.3735524962.3650420737.242850>]}
[ns_server:debug,2025-05-15T18:46:50.340Z,ns_1@cb.local:<0.831.0>:replicated_storage:handle_call:157]sync_to_me reply: ok
[ns_server:debug,2025-05-15T18:46:50.340Z,ns_1@cb.local:users_storage<0.444.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,'_','_','_'},[],['$_']}],
                               100}
[ns_server:info,2025-05-15T18:46:50.340Z,ns_1@cb.local:compat_mode_manager<0.818.0>:menelaus_users:upgrade:1071]Users database was upgraded to [7,6]
[ns_server:debug,2025-05-15T18:46:50.340Z,ns_1@cb.local:users_storage<0.444.0>:replicated_storage:handle_call:151]Received sync_to_me with timeout = 60000, nodes = ['ns_1@cb.local']
[ns_server:debug,2025-05-15T18:46:50.340Z,ns_1@cb.local:users_replicator<0.443.0>:doc_replicator:loop:100]Received sync_to_me with timeout = 60000, nodes = ['ns_1@cb.local']
[ns_server:debug,2025-05-15T18:46:50.340Z,ns_1@cb.local:users_storage<0.444.0>:replicated_storage:handle_call:146]Received sync_token from {<0.839.0>,
                          [alias|#Ref<0.3735524962.3650420737.242869>]}
[ns_server:debug,2025-05-15T18:46:50.340Z,ns_1@cb.local:users_replicator<0.443.0>:doc_replicator:loop:95]Received sync_token from {<0.839.0>,
                          [alias|#Ref<0.3735524962.3650420737.242869>]}
[ns_server:debug,2025-05-15T18:46:50.340Z,ns_1@cb.local:<0.836.0>:replicated_storage:handle_call:157]sync_to_me reply: ok
[ns_server:info,2025-05-15T18:46:50.341Z,ns_1@cb.local:compat_mode_manager<0.818.0>:menelaus_users:upgrade:1073]Users database upgrade was delivered to ['ns_1@cb.local']
[ns_server:info,2025-05-15T18:46:50.341Z,ns_1@cb.local:kv<0.268.0>:chronicle_upgrade:upgrade_loop:114]Upgrading chronicle from [7,1]. Final version = [7,6]
[ns_server:info,2025-05-15T18:46:50.341Z,ns_1@cb.local:kv<0.268.0>:chronicle_upgrade:upgrade_loop:114]Upgrading chronicle from [7,2]. Final version = [7,6]
[ns_server:info,2025-05-15T18:46:50.365Z,ns_1@cb.local:ns_config<0.280.0>:ns_config:do_upgrade_config:733]Upgrading config by changes:
[{set,cluster_compat_version,[7,1]}]

[ns_server:info,2025-05-15T18:46:50.365Z,ns_1@cb.local:ns_config<0.280.0>:ns_online_config_upgrader:do_upgrade_config:59]Performing online config upgrade to [7,2]
[ns_server:info,2025-05-15T18:46:50.366Z,ns_1@cb.local:ns_config<0.280.0>:ns_config:do_upgrade_config:733]Upgrading config by changes:
[{set,cluster_compat_version,[7,2]},
 {set,auto_failover_cfg,
      [{enabled,true},
       {timeout,120},
       {count,0},
       {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
       {failover_server_group,false},
       {max_count,1},
       {failed_over_server_groups,[]},
       {can_abort_rebalance,true},
       {failover_preserve_durability_majority,false}]}]

[ns_server:debug,2025-05-15T18:46:50.367Z,ns_1@cb.local:memcached_permissions<0.533.0>:memcached_permissions:producer:512]Skipping update during users upgrade
[ns_server:debug,2025-05-15T18:46:50.367Z,ns_1@cb.local:ns_cookie_manager<0.238.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[ns_server:debug,2025-05-15T18:46:50.367Z,ns_1@cb.local:chronicle_kv_log<0.504.0>:chronicle_kv_log:log:59]update (key: cluster_compat_version, rev: {<<"132dcb91b709843f36fcbbab0f42c041">>,
                                           5})
[7,6]
[ns_server:debug,2025-05-15T18:46:50.367Z,ns_1@cb.local:roles_cache<0.458.0>:active_cache:clean:181]Clearing the roles_cache cache
[ns_server:debug,2025-05-15T18:46:50.367Z,ns_1@cb.local:tombstone_keeper<0.277.0>:tombstone_keeper:handle_call:49]Refreshed with timestamps []
[ns_server:debug,2025-05-15T18:46:50.367Z,ns_1@cb.local:compiled_roles_cache<0.455.0>:versioned_cache:handle_info:89]Flushing cache compiled_roles_cache due to version change from undefined to {[7,
                                                                              6],
                                                                             {0,
                                                                              2972779260},
                                                                             {0,
                                                                              2972779260},
                                                                             false,
                                                                             []}
[ns_server:debug,2025-05-15T18:46:50.367Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:maybe_store_ca_certs:832]Considering to store CA certs
[ns_server:debug,2025-05-15T18:46:50.367Z,ns_1@cb.local:<0.840.0>:ns_node_disco:do_nodes_wanted_updated_fun:210]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"laexClf6j/iiXLRHc+IgLQ5C2ku89YxpvAez8UPpQjA=">>}
[ns_server:debug,2025-05-15T18:46:50.369Z,ns_1@cb.local:<0.840.0>:ns_node_disco:do_nodes_wanted_updated_fun:213]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"laexClf6j/iiXLRHc+IgLQ5C2ku89YxpvAez8UPpQjA=">>}
[ns_server:debug,2025-05-15T18:46:50.379Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:notify_services:1146]Going to notify following services: [capi_ssl_service,memcached]
[ns_server:info,2025-05-15T18:46:50.380Z,ns_1@cb.local:<0.849.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service memcached
[ns_server:info,2025-05-15T18:46:50.388Z,ns_1@cb.local:ns_config<0.280.0>:ns_online_config_upgrader:do_upgrade_config:59]Performing online config upgrade to [7,6]
[ns_server:info,2025-05-15T18:46:50.404Z,ns_1@cb.local:ns_config<0.280.0>:ns_config:do_upgrade_config:733]Upgrading config by changes:
[{set,cluster_compat_version,[7,6]},
 {set,audit_decriptors,
      [{8243,
        [{name,<<"mutate document">>},
         {description,<<"Document was mutated via the REST API">>},
         {enabled,true},
         {module,ns_server}]},
       {8255,
        [{name,<<"read document">>},
         {description,<<"Document was read via the REST API">>},
         {enabled,false},
         {module,ns_server}]},
       {8257,
        [{name,<<"alert email sent">>},
         {description,<<"An alert email was successfully sent">>},
         {enabled,true},
         {module,ns_server}]},
       {8265,
        [{name,<<"RBAC information retrieved">>},
         {description,<<"RBAC information was retrieved">>},
         {enabled,true},
         {module,ns_server}]},
       {16384,
        [{name,<<"remote cluster ref creation">>},
         {description,<<"created remote cluster ref">>},
         {enabled,true},
         {module,xdcr}]},
       {16385,
        [{name,<<"remote cluster ref update">>},
         {description,<<"updated remote cluster ref">>},
         {enabled,true},
         {module,xdcr}]},
       {16386,
        [{name,<<"remote cluster ref deletion">>},
         {description,<<"deleted remote cluster ref">>},
         {enabled,true},
         {module,xdcr}]},
       {16387,
        [{name,<<"replication creation">>},
         {description,<<"created replication">>},
         {enabled,true},
         {module,xdcr}]},
       {16388,
        [{name,<<"replication pause">>},
         {description,<<"paused replication">>},
         {enabled,true},
         {module,xdcr}]},
       {16389,
        [{name,<<"replication resume">>},
         {description,<<"resumed replication">>},
         {enabled,true},
         {module,xdcr}]},
       {16390,
        [{name,<<"replication cancellation">>},
         {description,<<"canceled replication">>},
         {enabled,true},
         {module,xdcr}]},
       {16391,
        [{name,<<"default replication settings update">>},
         {description,<<"updated default replication settings">>},
         {enabled,true},
         {module,xdcr}]},
       {16392,
        [{name,<<"individual replication settings update">>},
         {description,<<"updated individual replication settings">>},
         {enabled,true},
         {module,xdcr}]},
       {16393,
        [{name,<<"bucket settings update">>},
         {description,<<"updated bucket settings">>},
         {enabled,true},
         {module,xdcr}]},
       {16394,
        [{name,<<"authorization failure while adding remote cluster ref">>},
         {description,<<"failed to add remote cluster ref because of authorization failure">>},
         {enabled,true},
         {module,xdcr}]},
       {16395,
        [{name,<<"authorization failure while updating remote cluster ref">>},
         {description,<<"failed to update remote cluster ref because of authorization failure">>},
         {enabled,true},
         {module,xdcr}]},
       {16396,
        [{name,<<"access denied">>},
         {description,<<"access denied">>},
         {enabled,true},
         {module,xdcr}]},
       {20480,
        [{name,<<"opened DCP connection">>},
         {description,<<"opened DCP connection">>},
         {enabled,true},
         {module,memcached}]},
       {20482,
        [{name,<<"external memcached bucket flush">>},
         {description,<<"External user flushed the content of a memcached bucket">>},
         {enabled,true},
         {module,memcached}]},
       {20483,
        [{name,<<"invalid packet">>},
         {description,<<"Rejected an invalid packet">>},
         {enabled,true},
         {module,memcached}]},
       {20485,
        [{name,<<"authentication succeeded">>},
         {description,<<"Authentication to the cluster succeeded">>},
         {enabled,false},
         {module,memcached}]},
       {20488,
        [{name,<<"document read">>},
         {description,<<"Document was read">>},
         {enabled,false},
         {module,memcached}]},
       {20489,
        [{name,<<"document locked">>},
         {description,<<"Document was locked">>},
         {enabled,false},
         {module,memcached}]},
       {20490,
        [{name,<<"document modify">>},
         {description,<<"Document was modified">>},
         {enabled,false},
         {module,memcached}]},
       {20491,
        [{name,<<"document delete">>},
         {description,<<"Document was deleted">>},
         {enabled,false},
         {module,memcached}]},
       {20492,
        [{name,<<"select bucket">>},
         {description,<<"The specified bucket was selected">>},
         {enabled,true},
         {module,memcached}]},
       {20493,
        [{name,<<"session terminated">>},
         {description,<<"Session to the cluster has terminated">>},
         {enabled,false},
         {module,memcached}]},
       {20494,
        [{name,<<"tenant rate limited">>},
         {description,<<"The given tenant was rate limited">>},
         {enabled,true},
         {module,memcached}]},
       {24576,
        [{name,<<"Delete index">>},
         {description,<<"FTS index was deleted">>},
         {enabled,true},
         {module,fts}]},
       {24577,
        [{name,<<"Create/Update index">>},
         {description,<<"FTS index was created/Updated">>},
         {enabled,true},
         {module,fts}]},
       {24579,
        [{name,<<"Control index">>},
         {description,<<"FTS index control command was issued">>},
         {enabled,true},
         {module,fts}]},
       {24582,
        [{name,<<"GC run">>},
         {description,<<"GC run was triggered">>},
         {enabled,true},
         {module,fts}]},
       {24583,
        [{name,<<"CPU profile">>},
         {description,<<"CPU profiling was started">>},
         {enabled,true},
         {module,fts}]},
       {24584,
        [{name,<<"Memory profile">>},
         {description,<<"Memory profiling was started">>},
         {enabled,true},
         {module,fts}]},
       {28672,
        [{name,<<"SELECT statement">>},
         {description,<<"A N1QL SELECT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28673,
        [{name,<<"EXPLAIN statement">>},
         {description,<<"A N1QL EXPLAIN statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28674,
        [{name,<<"PREPARE statement">>},
         {description,<<"A N1QL PREPARE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28675,
        [{name,<<"INFER statement">>},
         {description,<<"A N1QL INFER statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28676,
        [{name,<<"INSERT statement">>},
         {description,<<"A N1QL INSERT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28677,
        [{name,<<"UPSERT statement">>},
         {description,<<"A N1QL UPSERT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28678,
        [{name,<<"DELETE statement">>},
         {description,<<"A N1QL DELETE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28679,
        [{name,<<"UPDATE statement">>},
         {description,<<"A N1QL UPDATE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28680,
        [{name,<<"MERGE statement">>},
         {description,<<"A N1QL MERGE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28681,
        [{name,<<"CREATE INDEX statement">>},
         {description,<<"A N1QL CREATE INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28682,
        [{name,<<"DROP INDEX statement">>},
         {description,<<"A N1QL DROP INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28683,
        [{name,<<"ALTER INDEX statement">>},
         {description,<<"A N1QL ALTER INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28684,
        [{name,<<"BUILD INDEX statement">>},
         {description,<<"A N1QL BUILD INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28685,
        [{name,<<"GRANT ROLE statement">>},
         {description,<<"A N1QL GRANT ROLE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28686,
        [{name,<<"REVOKE ROLE statement">>},
         {description,<<"A N1QL REVOKE ROLE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28687,
        [{name,<<"UNRECOGNIZED statement">>},
         {description,<<"An unrecognized statement was received by the N1QL query engine">>},
         {enabled,false},
         {module,n1ql}]},
       {28688,
        [{name,<<"CREATE PRIMARY INDEX statement">>},
         {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28689,
        [{name,<<"/admin/stats API request">>},
         {description,<<"An HTTP request was made to the API at /admin/stats.">>},
         {enabled,false},
         {module,n1ql}]},
       {28690,
        [{name,<<"/admin/vitals API request">>},
         {description,<<"An HTTP request was made to the API at /admin/vitals.">>},
         {enabled,false},
         {module,n1ql}]},
       {28691,
        [{name,<<"/admin/prepareds API request">>},
         {description,<<"An HTTP request was made to the API at /admin/prepareds.">>},
         {enabled,false},
         {module,n1ql}]},
       {28692,
        [{name,<<"/admin/active_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/active_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28693,
        [{name,<<"/admin/indexes/prepareds API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
         {enabled,false},
         {module,n1ql}]},
       {28694,
        [{name,<<"/admin/indexes/active_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28695,
        [{name,<<"/admin/indexes/completed_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28697,
        [{name,<<"/admin/ping API request">>},
         {description,<<"An HTTP request was made to the API at /admin/ping.">>},
         {enabled,false},
         {module,n1ql}]},
       {28698,
        [{name,<<"/admin/config API request">>},
         {description,<<"An HTTP request was made to the API at /admin/config.">>},
         {enabled,false},
         {module,n1ql}]},
       {28699,
        [{name,<<"/admin/ssl_cert API request">>},
         {description,<<"An HTTP request was made to the API at /admin/ssl_cert.">>},
         {enabled,false},
         {module,n1ql}]},
       {28700,
        [{name,<<"/admin/settings API request">>},
         {description,<<"An HTTP request was made to the API at /admin/settings.">>},
         {enabled,false},
         {module,n1ql}]},
       {28701,
        [{name,<<"/admin/clusters API request">>},
         {description,<<"An HTTP request was made to the API at /admin/clusters.">>},
         {enabled,false},
         {module,n1ql}]},
       {28702,
        [{name,<<"/admin/completed_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/completed_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28704,
        [{name,<<"/admin/functions API request">>},
         {description,<<"An HTTP request was made to the API at /admin/functions.">>},
         {enabled,false},
         {module,n1ql}]},
       {28705,
        [{name,<<"/admin/indexes/functions API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/functions.">>},
         {enabled,false},
         {module,n1ql}]},
       {28706,
        [{name,<<"CREATE FUNCTION statement">>},
         {description,<<"A N1QL CREATE FUNCTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28707,
        [{name,<<"DROP FUNCTION statement">>},
         {description,<<"A N1QL DROP FUNCTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28708,
        [{name,<<"EXECUTE FUNCTION statement">>},
         {description,<<"A N1QL EXECUTE FUNCTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28709,
        [{name,<<"/admin/tasks API request">>},
         {description,<<"An HTTP request was made to the API at /admin/tasks.">>},
         {enabled,false},
         {module,n1ql}]},
       {28710,
        [{name,<<"/admin/indexes/tasks API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/tasks.">>},
         {enabled,false},
         {module,n1ql}]},
       {28711,
        [{name,<<"/admin/dictionary_cache API request">>},
         {description,<<"An HTTP request was made to the API at /admin/dictionary_cache.">>},
         {enabled,false},
         {module,n1ql}]},
       {28712,
        [{name,<<"/admin/indexes/dictionary_cache API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/dictionary_cache.">>},
         {enabled,false},
         {module,n1ql}]},
       {28713,
        [{name,<<"CREATE SCOPE statement">>},
         {description,<<"A N1QL CREATE SCOPE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28714,
        [{name,<<"DROP SCOPE statement">>},
         {description,<<"A N1QL DROP SCOPE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28715,
        [{name,<<"CREATE COLLECTION statement">>},
         {description,<<"A N1QL CREATE COLLECTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28716,
        [{name,<<"DROP COLLECTION statement">>},
         {description,<<"A N1QL DROP COLLECTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28717,
        [{name,<<"FLUSH COLLECTION statement">>},
         {description,<<"A N1QL FLUSH COLLECTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28718,
        [{name,<<"UPDATE STATISTICS statement">>},
         {description,<<"A N1QL UPDATE STATISTICS statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28719,
        [{name,<<"ADVISE statement">>},
         {description,<<"A N1QL ADVISE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28720,
        [{name,<<"START TRANSACTION statement">>},
         {description,<<"A N1QL START TRANSACTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28721,
        [{name,<<"COMMIT TRANSACTION statement">>},
         {description,<<"A N1QL COMMIT TRANSACTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28722,
        [{name,<<"ROLLBACK TRANSACTION statement">>},
         {description,<<"A N1QL ROLLBACK TRANSACTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28723,
        [{name,<<"ROLLBACK TRANSACTION TO SAVEPOINT statement">>},
         {description,<<"A N1QL ROLLBACK TRANSACTION TO SAVEPOINT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28724,
        [{name,<<"SET TRANSACTION ISOLATION statement">>},
         {description,<<"A N1QL SET TRANSACTION ISOLATION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28725,
        [{name,<<"SAVEPOINT statement">>},
         {description,<<"A N1QL SAVEPOINT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28726,
        [{name,<<"/admin/transactions API request">>},
         {description,<<"An HTTP request was made to the API at /admin/transactions.">>},
         {enabled,false},
         {module,n1ql}]},
       {28727,
        [{name,<<"/admin/indexes/transactions API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/transactions.">>},
         {enabled,false},
         {module,n1ql}]},
       {28728,
        [{name,<<"N1QL backup / restore API request">>},
         {description,<<"An HTTP request was made to archive or restore N1QL metadata">>},
         {enabled,false},
         {module,n1ql}]},
       {28729,
        [{name,<<"/admin/shutdown API request">>},
         {description,<<"An HTTP request was made to initiate graceful shutdown">>},
         {enabled,false},
         {module,n1ql}]},
       {28730,
        [{name,<<"/admin/gc API request">>},
         {description,<<"An HTTP request was made to run garbage collection">>},
         {enabled,false},
         {module,n1ql}]},
       {28731,
        [{name,<<"/admin/ffdc API request">>},
         {description,<<"An HTTP request was made to run an FFDC collection">>},
         {enabled,false},
         {module,n1ql}]},
       {28732,
        [{name,<<"/admin/log/ API request">>},
         {description,<<"An HTTP request was made to access diagnostic logs">>},
         {enabled,false},
         {module,n1ql}]},
       {28733,
        [{name,<<"/admin/sequences_cache API request">>},
         {description,<<"An HTTP request was made to access sequences">>},
         {enabled,false},
         {module,n1ql}]},
       {28734,
        [{name,<<"CREATE SEQUENCE statement">>},
         {description,<<"A N1QL CREATE SEQUENCE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28735,
        [{name,<<"ALTER SEQUENCE statement">>},
         {description,<<"A N1QL ALTER SEQUENCE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28736,
        [{name,<<"DROP SEQUENCE statement">>},
         {description,<<"A N1QL DROP SEQUENCE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28737,
        [{name,<<"Migration abort">>},
         {description,<<"Migration was aborted">>},
         {enabled,false},
         {module,n1ql}]},
       {32768,
        [{name,<<"Create Function">>},
         {description,<<"Request to create or update eventing function definition">>},
         {enabled,true},
         {module,eventing}]},
       {32769,
        [{name,<<"Delete Function">>},
         {description,<<"Request to delete eventing function definition">>},
         {enabled,true},
         {module,eventing}]},
       {32770,
        [{name,<<"Fetch Functions">>},
         {description,<<"Request to fetch eventing function definition">>},
         {enabled,false},
         {module,eventing}]},
       {32771,
        [{name,<<"List Deployed">>},
         {description,<<"Request to fetch eventing deployed functions list">>},
         {enabled,false},
         {module,eventing}]},
       {32772,
        [{name,<<"Fetch Drafts">>},
         {description,<<"Request to fetch eventing function draft definitions">>},
         {enabled,false},
         {module,eventing}]},
       {32773,
        [{name,<<"Delete Drafts">>},
         {description,<<"Request to delete eventing function draft definitions">>},
         {enabled,true},
         {module,eventing}]},
       {32774,
        [{name,<<"Save Draft">>},
         {description,<<"Request to save a draft definition">>},
         {enabled,true},
         {module,eventing}]},
       {32775,
        [{name,<<"Start Debug">>},
         {description,<<"Request to start eventing function debugger">>},
         {enabled,true},
         {module,eventing}]},
       {32776,
        [{name,<<"Stop Debug">>},
         {description,<<"Request to stop eventing function debugger">>},
         {enabled,true},
         {module,eventing}]},
       {32777,
        [{name,<<"Start Tracing">>},
         {description,<<"Request to start tracing eventing function execution">>},
         {enabled,true},
         {module,eventing}]},
       {32778,
        [{name,<<"Stop Tracing">>},
         {description,<<"Request to stop tracing eventing function execution">>},
         {enabled,true},
         {module,eventing}]},
       {32779,
        [{name,<<"Set Settings">>},
         {description,<<"Request to save settings for an eventing function">>},
         {enabled,true},
         {module,eventing}]},
       {32780,
        [{name,<<"Fetch Config">>},
         {description,<<"Request to fetch eventing config">>},
         {enabled,false},
         {module,eventing}]},
       {32781,
        [{name,<<"Save Config">>},
         {description,<<"Request to save eventing config">>},
         {enabled,true},
         {module,eventing}]},
       {32783,
        [{name,<<"Get Settings">>},
         {description,<<"Request to fetch eventing function settings">>},
         {enabled,false},
         {module,eventing}]},
       {32784,
        [{name,<<"Import Functions">>},
         {description,<<"Request to import one or more eventing functions">>},
         {enabled,true},
         {module,eventing}]},
       {32785,
        [{name,<<"Export Functions">>},
         {description,<<"Request to export all eventing functions">>},
         {enabled,false},
         {module,eventing}]},
       {32786,
        [{name,<<"List Running">>},
         {description,<<"Request to fetch eventing running function list">>},
         {enabled,false},
         {module,eventing}]},
       {32789,
        [{name,<<"Deploy Function">>},
         {description,<<"Request to deploy eventing function">>},
         {enabled,true},
         {module,eventing}]},
       {32790,
        [{name,<<"Undeploy Function">>},
         {description,<<"Request to undeploy eventing function">>},
         {enabled,true},
         {module,eventing}]},
       {32791,
        [{name,<<"Pause Function">>},
         {description,<<"Request to pause eventing function">>},
         {enabled,true},
         {module,eventing}]},
       {32792,
        [{name,<<"Resume Function">>},
         {description,<<"Request to resume eventing function">>},
         {enabled,true},
         {module,eventing}]},
       {32793,
        [{name,<<"Backup Functions">>},
         {description,<<"Request to backup one or more eventing functions">>},
         {enabled,false},
         {module,eventing}]},
       {32794,
        [{name,<<"Restore Functions">>},
         {description,<<"Request to restore one or more eventing functions from a backup">>},
         {enabled,true},
         {module,eventing}]},
       {32795,
        [{name,<<"List Function">>},
         {description,<<"Request to fetch eventing functions">>},
         {enabled,false},
         {module,eventing}]},
       {32796,
        [{name,<<"Function Status">>},
         {description,<<"Request to fetch eventing function status">>},
         {enabled,false},
         {module,eventing}]},
       {32797,
        [{name,<<"Clear Stats">>},
         {description,<<"Request to reset eventing function stats">>},
         {enabled,true},
         {module,eventing}]},
       {32798,
        [{name,<<"Fetch Stats">>},
         {description,<<"Request to fetch eventing function stats">>},
         {enabled,false},
         {module,eventing}]},
       {32799,
        [{name,<<"Eventing Cluster Stats">>},
         {description,<<"Request to fetch eventing cluster stats">>},
         {enabled,false},
         {module,eventing}]},
       {32801,
        [{name,<<"Eventing System Event">>},
         {description,<<"Request to execute eventing node related functions">>},
         {enabled,false},
         {module,eventing}]},
       {32802,
        [{name,<<"Get User Info">>},
         {description,<<"Request to get real_userid eventing permissions">>},
         {enabled,false},
         {module,eventing}]},
       {36865,
        [{name,<<"Service configuration change">>},
         {description,<<"A successful service configuration change was made.">>},
         {enabled,true},
         {module,analytics}]},
       {36866,
        [{name,<<"Node configuration change">>},
         {description,<<"A successful node configuration change was made.">>},
         {enabled,true},
         {module,analytics}]},
       {36867,
        [{name,<<"SELECT statement">>},
         {description,<<"A N1QL SELECT statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36868,
        [{name,<<"CREATE DATAVERSE statement">>},
         {description,<<"A N1QL CREATE DATAVERSE statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36869,
        [{name,<<"DROP DATAVERSE statement">>},
         {description,<<"A N1QL DROP DATAVERSE statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36870,
        [{name,<<"CREATE DATASET statement">>},
         {description,<<"A N1QL CREATE DATASET statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36871,
        [{name,<<"DROP DATASET statement">>},
         {description,<<"A N1QL DROP DATASET statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36872,
        [{name,<<"CREATE INDEX statement">>},
         {description,<<"A N1QL CREATE INDEX statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36873,
        [{name,<<"DROP INDEX statement">>},
         {description,<<"A N1QL DROP INDEX statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36877,
        [{name,<<"CONNECT LINK statement">>},
         {description,<<"A N1QL CONNECT LINK statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36878,
        [{name,<<"DISCONNECT LINK statement">>},
         {description,<<"A N1QL DISCONNECT LINK statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36879,
        [{name,<<"UNRECOGNIZED statement">>},
         {description,<<"An UNRECOGNIZED N1QL statement was encountered">>},
         {enabled,false},
         {module,analytics}]},
       {36880,
        [{name,<<"ALTER COLLECTION statement">>},
         {description,<<"A N1QL ALTER COLLECTION statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {40960,
        [{name,<<"Create Design Doc">>},
         {description,<<"Design Doc is Created">>},
         {enabled,true},
         {module,view_engine}]},
       {40961,
        [{name,<<"Delete Design Doc">>},
         {description,<<"Design Doc is Deleted">>},
         {enabled,true},
         {module,view_engine}]},
       {40962,
        [{name,<<"Query DDoc Meta Data">>},
         {description,<<"Design Doc Meta Data Query Request">>},
         {enabled,true},
         {module,view_engine}]},
       {40963,
        [{name,<<"View Query">>},
         {description,<<"View Query Request">>},
         {enabled,false},
         {module,view_engine}]},
       {40964,
        [{name,<<"Update Design Doc">>},
         {description,<<"Design Doc is Updated">>},
         {enabled,true},
         {module,view_engine}]},
       {40966,
        [{name,<<"Access denied">>},
         {description,<<"Access denied to the REST API due to invalid permissions or credentials">>},
         {enabled,true},
         {module,view_engine}]},
       {45056,
        [{name,<<"Modify configuration">>},
         {description,<<"Backup service configuration was modified">>},
         {enabled,true},
         {module,backup}]},
       {45057,
        [{name,<<"Fetch configuration">>},
         {description,<<"Backup service configuration was retrieved">>},
         {enabled,false},
         {module,backup}]},
       {45058,
        [{name,<<"Add plan">>},
         {description,<<"A new backup plan was added">>},
         {enabled,true},
         {module,backup}]},
       {45059,
        [{name,<<"Modify plan">>},
         {description,<<"Existing backup plan was modified">>},
         {enabled,true},
         {module,backup}]},
       {45060,
        [{name,<<"Delete plan">>},
         {description,<<"A backup plan was removed">>},
         {enabled,true},
         {module,backup}]},
       {45061,
        [{name,<<"Fetch plan">>},
         {description,<<"One or more backup plans where fetched">>},
         {enabled,false},
         {module,backup}]},
       {45062,
        [{name,<<"Add repository">>},
         {description,<<"A new active backup repository was added">>},
         {enabled,true},
         {module,backup}]},
       {45063,
        [{name,<<"Archive repository">>},
         {description,<<"An active repository was archived">>},
         {enabled,true},
         {module,backup}]},
       {45064,
        [{name,<<"Pause repository">>},
         {description,<<"An active repository was paused">>},
         {enabled,true},
         {module,backup}]},
       {45065,
        [{name,<<"Resume repository">>},
         {description,<<"An active repository was resumed">>},
         {enabled,true},
         {module,backup}]},
       {45066,
        [{name,<<"Fetch repository">>},
         {description,<<"A repository was fetched">>},
         {enabled,false},
         {module,backup}]},
       {45067,
        [{name,<<"Restore repository">>},
         {description,<<"The repository data was restored">>},
         {enabled,true},
         {module,backup}]},
       {45068,
        [{name,<<"Backup repository">>},
         {description,<<"A manual backup was triggered on an active repository">>},
         {enabled,true},
         {module,backup}]},
       {45069,
        [{name,<<"Merge repository">>},
         {description,<<"A manual merge was triggered on an active repository">>},
         {enabled,true},
         {module,backup}]},
       {45070,
        [{name,<<"Info repository">>},
         {description,<<"Information about the structure and contents of the backup repository was fetched.">>},
         {enabled,false},
         {module,backup}]},
       {45071,
        [{name,<<"Examine repository">>},
         {description,<<"A document was retrieved from the repository backups">>},
         {enabled,true},
         {module,backup}]},
       {45072,
        [{name,<<"Delete repository">>},
         {description,<<"A repository was deleted">>},
         {enabled,true},
         {module,backup}]},
       {45073,
        [{name,<<"Delete backup">>},
         {description,<<"An active repository backup was deleted">>},
         {enabled,true},
         {module,backup}]},
       {45074,
        [{name,<<"Access denied">>},
         {description,<<"A user has been denied access to the REST API due to invalid permissions or credentials">>},
         {enabled,true},
         {module,backup}]}]},
 {delete,rbac_upgrade},
 {set,auto_failover_cfg,
      [{disable_max_count,false},
       {enabled,true},
       {timeout,120},
       {count,0},
       {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
       {failover_server_group,false},
       {max_count,1},
       {failed_over_server_groups,[]},
       {can_abort_rebalance,true},
       {failover_preserve_durability_majority,false}]},
 {delete,memory_alert_email},
 {delete,memory_alert_popup},
 {delete,popup_alerts_auto_failover_upgrade_70_fixed},
 {set,{metakv,<<"/indexing/settings/config">>},
      <<"{\"indexer.settings.compaction.abort_exceed_interval\":false,\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.enable_page_bloom_filter\":false,\"indexer.settings.enable_shard_affinity\":false,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.log_level\":\"info\",\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.num_replica\":0,\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.rebalance.blob_storage_bucket\":\"\",\"indexer.settings.rebalance.blob_storage_prefix\":\"\",\"indexer.settings.rebalance.blob_storage_region\":\"\",\"indexer.settings.rebalance.blob_storage_scheme\":\"\",\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.thresholds.mem_high\":70,\"indexer.settings.thresholds.mem_low\":50,\"indexer.settings.thresholds.units_high\":60,\"indexer.settings.thresholds.units_low\":40}">>},
 {set,{metakv,<<"/query/settings/config">>},
      <<"{\"cleanupclientattempts\":true,\"cleanuplostattempts\":true,\"cleanupwindow\":\"60s\",\"completed-limit\":4000,\"completed-max-plan-size\":262144,\"completed-threshold\":1000,\"loglevel\":\"info\",\"max-parallelism\":1,\"memory-quota\":0,\"n1ql-feat-ctrl\":76,\"node-quota\":0,\"node-quota-val-percent\":67,\"num-cpus\":0,\"numatrs\":1024,\"pipeline-batch\":16,\"pipeline-cap\":512,\"prepared-limit\":16384,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"query.settings.tmp_space_size\":5120,\"scan-cap\":512,\"timeout\":0,\"txtimeout\":\"0ms\",\"use-cbo\":true,\"use-replica\":\"unset\"}">>},
 {set,{metakv,<<"/analytics/settings/config">>},
      <<"{\"analytics.settings.blob_storage_bucket\":\"\",\"analytics.settings.blob_storage_prefix\":\"\",\"analytics.settings.blob_storage_region\":\"\",\"analytics.settings.blob_storage_scheme\":\"\",\"analytics.settings.num_replicas\":0}">>},
 {delete,mb33750_workaround_enabled},
 {delete,cert_and_pkey},
 {set,resource_management,
      [{bucket,[{resident_ratio,[{enabled,false},
                                 {couchstore_minimum,1},
                                 {magma_minimum,0.2}]},
                {data_size,[{enabled,false},
                            {couchstore_maximum,2},
                            {magma_maximum,16}]}]},
       {index,[]},
       {cores_per_bucket,[{enabled,false},{minimum,0.4}]},
       {disk_usage,[{enabled,false},{maximum,96}]},
       {collections_per_quota,[{enabled,false},{maximum,1}]}]}]

[ns_server:debug,2025-05-15T18:46:50.419Z,ns_1@cb.local:ns_config_rep<0.554.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([audit_decriptors,auto_failover_cfg,
                               cluster_compat_version,rbac_upgrade,
                               resource_management,
                               {metakv,<<"/analytics/settings/config">>},
                               {metakv,<<"/indexing/settings/config">>},
                               {metakv,<<"/query/settings/config">>}]..)
[ns_server:debug,2025-05-15T18:46:50.419Z,ns_1@cb.local:roles_cache<0.458.0>:active_cache:clean:181]Clearing the roles_cache cache
[ns_server:debug,2025-05-15T18:46:50.419Z,ns_1@cb.local:tombstone_keeper<0.277.0>:tombstone_keeper:handle_call:49]Refreshed with timestamps []
[ns_server:debug,2025-05-15T18:46:50.421Z,ns_1@cb.local:memcached_passwords<0.530.0>:memcached_cfg:write_cfg:156]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2025-05-15T18:46:50.421Z,ns_1@cb.local:ns_cookie_manager<0.238.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[ns_server:debug,2025-05-15T18:46:50.422Z,ns_1@cb.local:<0.853.0>:ns_node_disco:do_nodes_wanted_updated_fun:210]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"laexClf6j/iiXLRHc+IgLQ5C2ku89YxpvAez8UPpQjA=">>}
[ns_server:debug,2025-05-15T18:46:50.422Z,ns_1@cb.local:<0.853.0>:ns_node_disco:do_nodes_wanted_updated_fun:213]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"laexClf6j/iiXLRHc+IgLQ5C2ku89YxpvAez8UPpQjA=">>}
[ns_server:debug,2025-05-15T18:46:50.423Z,ns_1@cb.local:ns_config_rep<0.554.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 43 us
[ns_server:debug,2025-05-15T18:46:50.423Z,ns_1@cb.local:ns_config_rep<0.554.0>:ns_config_rep:handle_call:149]Got full synchronization request from 'ns_1@cb.local'
[ns_server:debug,2025-05-15T18:46:50.423Z,ns_1@cb.local:ns_config_rep<0.554.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 2 us
[user:warn,2025-05-15T18:46:50.423Z,ns_1@cb.local:compat_mode_manager<0.818.0>:compat_mode_manager:handle_consider_switching_compat_mode:43]Changed cluster compat mode from [7,1] to [7,6]
[error_logger:info,2025-05-15T18:46:50.423Z,ns_1@cb.local:ns_orchestrator_sup<0.816.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.818.0>},
              {id,compat_mode_manager},
              {mfargs,{compat_mode_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:50.424Z,ns_1@cb.local:memcached_permissions<0.533.0>:memcached_cfg:write_cfg:156]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2025-05-15T18:46:50.426Z,ns_1@cb.local:memcached_permissions<0.533.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,{user,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2025-05-15T18:46:50.430Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:handle_call:57]File rename from "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac" is requested
[ns_server:info,2025-05-15T18:46:50.433Z,ns_1@cb.local:<0.848.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service capi_ssl_service
[ns_server:info,2025-05-15T18:46:50.433Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:notify_services:1162]Succesfully notified services [memcached,capi_ssl_service]
[ns_server:debug,2025-05-15T18:46:50.436Z,ns_1@cb.local:prometheus_cfg<0.517.0>:prometheus_cfg:maybe_apply_new_settings:704]Settings didn't change, ignoring update
[ns_server:debug,2025-05-15T18:46:50.436Z,ns_1@cb.local:ns_config_rep<0.554.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([{node,'ns_1@cb.local',prometheus_auth_info}]..)
[ns_server:debug,2025-05-15T18:46:50.431Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
audit_decriptors ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554010}}]},
 {8243,
  [{name,<<"mutate document">>},
   {description,<<"Document was mutated via the REST API">>},
   {enabled,true},
   {module,ns_server}]},
 {8255,
  [{name,<<"read document">>},
   {description,<<"Document was read via the REST API">>},
   {enabled,false},
   {module,ns_server}]},
 {8257,
  [{name,<<"alert email sent">>},
   {description,<<"An alert email was successfully sent">>},
   {enabled,true},
   {module,ns_server}]},
 {8265,
  [{name,<<"RBAC information retrieved">>},
   {description,<<"RBAC information was retrieved">>},
   {enabled,true},
   {module,ns_server}]},
 {16384,
  [{name,<<"remote cluster ref creation">>},
   {description,<<"created remote cluster ref">>},
   {enabled,true},
   {module,xdcr}]},
 {16385,
  [{name,<<"remote cluster ref update">>},
   {description,<<"updated remote cluster ref">>},
   {enabled,true},
   {module,xdcr}]},
 {16386,
  [{name,<<"remote cluster ref deletion">>},
   {description,<<"deleted remote cluster ref">>},
   {enabled,true},
   {module,xdcr}]},
 {16387,
  [{name,<<"replication creation">>},
   {description,<<"created replication">>},
   {enabled,true},
   {module,xdcr}]},
 {16388,
  [{name,<<"replication pause">>},
   {description,<<"paused replication">>},
   {enabled,true},
   {module,xdcr}]},
 {16389,
  [{name,<<"replication resume">>},
   {description,<<"resumed replication">>},
   {enabled,true},
   {module,xdcr}]},
 {16390,
  [{name,<<"replication cancellation">>},
   {description,<<"canceled replication">>},
   {enabled,true},
   {module,xdcr}]},
 {16391,
  [{name,<<"default replication settings update">>},
   {description,<<"updated default replication settings">>},
   {enabled,true},
   {module,xdcr}]},
 {16392,
  [{name,<<"individual replication settings update">>},
   {description,<<"updated individual replication settings">>},
   {enabled,true},
   {module,xdcr}]},
 {16393,
  [{name,<<"bucket settings update">>},
   {description,<<"updated bucket settings">>},
   {enabled,true},
   {module,xdcr}]},
 {16394,
  [{name,<<"authorization failure while adding remote cluster ref">>},
   {description,<<"failed to add remote cluster ref because of authorization failure">>},
   {enabled,true},
   {module,xdcr}]},
 {16395,
  [{name,<<"authorization failure while updating remote cluster ref">>},
   {description,<<"failed to update remote cluster ref because of authorization failure">>},
   {enabled,true},
   {module,xdcr}]},
 {16396,
  [{name,<<"access denied">>},
   {description,<<"access denied">>},
   {enabled,true},
   {module,xdcr}]},
 {20480,
  [{name,<<"opened DCP connection">>},
   {description,<<"opened DCP connection">>},
   {enabled,true},
   {module,memcached}]},
 {20482,
  [{name,<<"external memcached bucket flush">>},
   {description,<<"External user flushed the content of a memcached bucket">>},
   {enabled,true},
   {module,memcached}]},
 {20483,
  [{name,<<"invalid packet">>},
   {description,<<"Rejected an invalid packet">>},
   {enabled,true},
   {module,memcached}]},
 {20485,
  [{name,<<"authentication succeeded">>},
   {description,<<"Authentication to the cluster succeeded">>},
   {enabled,false},
   {module,memcached}]},
 {20488,
  [{name,<<"document read">>},
   {description,<<"Document was read">>},
   {enabled,false},
   {module,memcached}]},
 {20489,
  [{name,<<"document locked">>},
   {description,<<"Document was locked">>},
   {enabled,false},
   {module,memcached}]},
 {20490,
  [{name,<<"document modify">>},
   {description,<<"Document was modified">>},
   {enabled,false},
   {module,memcached}]},
 {20491,
  [{name,<<"document delete">>},
   {description,<<"Document was deleted">>},
   {enabled,false},
   {module,memcached}]},
 {20492,
  [{name,<<"select bucket">>},
   {description,<<"The specified bucket was selected">>},
   {enabled,true},
   {module,memcached}]},
 {20493,
  [{name,<<"session terminated">>},
   {description,<<"Session to the cluster has terminated">>},
   {enabled,false},
   {module,memcached}]},
 {20494,
  [{name,<<"tenant rate limited">>},
   {description,<<"The given tenant was rate limited">>},
   {enabled,true},
   {module,memcached}]},
 {24576,
  [{name,<<"Delete index">>},
   {description,<<"FTS index was deleted">>},
   {enabled,true},
   {module,fts}]},
 {24577,
  [{name,<<"Create/Update index">>},
   {description,<<"FTS index was created/Updated">>},
   {enabled,true},
   {module,fts}]},
 {24579,
  [{name,<<"Control index">>},
   {description,<<"FTS index control command was issued">>},
   {enabled,true},
   {module,fts}]},
 {24582,
  [{name,<<"GC run">>},
   {description,<<"GC run was triggered">>},
   {enabled,true},
   {module,fts}]},
 {24583,
  [{name,<<"CPU profile">>},
   {description,<<"CPU profiling was started">>},
   {enabled,true},
   {module,fts}]},
 {24584,
  [{name,<<"Memory profile">>},
   {description,<<"Memory profiling was started">>},
   {enabled,true},
   {module,fts}]},
 {28672,
  [{name,<<"SELECT statement">>},
   {description,<<"A N1QL SELECT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28673,
  [{name,<<"EXPLAIN statement">>},
   {description,<<"A N1QL EXPLAIN statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28674,
  [{name,<<"PREPARE statement">>},
   {description,<<"A N1QL PREPARE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28675,
  [{name,<<"INFER statement">>},
   {description,<<"A N1QL INFER statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28676,
  [{name,<<"INSERT statement">>},
   {description,<<"A N1QL INSERT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28677,
  [{name,<<"UPSERT statement">>},
   {description,<<"A N1QL UPSERT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28678,
  [{name,<<"DELETE statement">>},
   {description,<<"A N1QL DELETE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28679,
  [{name,<<"UPDATE statement">>},
   {description,<<"A N1QL UPDATE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28680,
  [{name,<<"MERGE statement">>},
   {description,<<"A N1QL MERGE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28681,
  [{name,<<"CREATE INDEX statement">>},
   {description,<<"A N1QL CREATE INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28682,
  [{name,<<"DROP INDEX statement">>},
   {description,<<"A N1QL DROP INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28683,
  [{name,<<"ALTER INDEX statement">>},
   {description,<<"A N1QL ALTER INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28684,
  [{name,<<"BUILD INDEX statement">>},
   {description,<<"A N1QL BUILD INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28685,
  [{name,<<"GRANT ROLE statement">>},
   {description,<<"A N1QL GRANT ROLE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28686,
  [{name,<<"REVOKE ROLE statement">>},
   {description,<<"A N1QL REVOKE ROLE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28687,
  [{name,<<"UNRECOGNIZED statement">>},
   {description,<<"An unrecognized statement was received by the N1QL query engine">>},
   {enabled,false},
   {module,n1ql}]},
 {28688,
  [{name,<<"CREATE PRIMARY INDEX statement">>},
   {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28689,
  [{name,<<"/admin/stats API request">>},
   {description,<<"An HTTP request was made to the API at /admin/stats.">>},
   {enabled,false},
   {module,n1ql}]},
 {28690,
  [{name,<<"/admin/vitals API request">>},
   {description,<<"An HTTP request was made to the API at /admin/vitals.">>},
   {enabled,false},
   {module,n1ql}]},
 {28691,
  [{name,<<"/admin/prepareds API request">>},
   {description,<<"An HTTP request was made to the API at /admin/prepareds.">>},
   {enabled,false},
   {module,n1ql}]},
 {28692,
  [{name,<<"/admin/active_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/active_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28693,
  [{name,<<"/admin/indexes/prepareds API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
   {enabled,false},
   {module,n1ql}]},
 {28694,
  [{name,<<"/admin/indexes/active_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28695,
  [{name,<<"/admin/indexes/completed_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28697,
  [{name,<<"/admin/ping API request">>},
   {description,<<"An HTTP request was made to the API at /admin/ping.">>},
   {enabled,false},
   {module,n1ql}]},
 {28698,
  [{name,<<"/admin/config API request">>},
   {description,<<"An HTTP request was made to the API at /admin/config.">>},
   {enabled,false},
   {module,n1ql}]},
 {28699,
  [{name,<<"/admin/ssl_cert API request">>},
   {description,<<"An HTTP request was made to the API at /admin/ssl_cert.">>},
   {enabled,false},
   {module,n1ql}]},
 {28700,
  [{name,<<"/admin/settings API request">>},
   {description,<<"An HTTP request was made to the API at /admin/settings.">>},
   {enabled,false},
   {module,n1ql}]},
 {28701,
  [{name,<<"/admin/clusters API request">>},
   {description,<<"An HTTP request was made to the API at /admin/clusters.">>},
   {enabled,false},
   {module,n1ql}]},
 {28702,
  [{name,<<"/admin/completed_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/completed_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28704,
  [{name,<<"/admin/functions API request">>},
   {description,<<"An HTTP request was made to the API at /admin/functions.">>},
   {enabled,false},
   {module,n1ql}]},
 {28705,
  [{name,<<"/admin/indexes/functions API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/functions.">>},
   {enabled,false},
   {module,n1ql}]},
 {28706,
  [{name,<<"CREATE FUNCTION statement">>},
   {description,<<"A N1QL CREATE FUNCTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28707,
  [{name,<<"DROP FUNCTION statement">>},
   {description,<<"A N1QL DROP FUNCTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28708,
  [{name,<<"EXECUTE FUNCTION statement">>},
   {description,<<"A N1QL EXECUTE FUNCTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28709,
  [{name,<<"/admin/tasks API request">>},
   {description,<<"An HTTP request was made to the API at /admin/tasks.">>},
   {enabled,false},
   {module,n1ql}]},
 {28710,
  [{name,<<"/admin/indexes/tasks API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/tasks.">>},
   {enabled,false},
   {module,n1ql}]},
 {28711,
  [{name,<<"/admin/dictionary_cache API request">>},
   {description,<<"An HTTP request was made to the API at /admin/dictionary_cache.">>},
   {enabled,false},
   {module,n1ql}]},
 {28712,
  [{name,<<"/admin/indexes/dictionary_cache API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/dictionary_cache.">>},
   {enabled,false},
   {module,n1ql}]},
 {28713,
  [{name,<<"CREATE SCOPE statement">>},
   {description,<<"A N1QL CREATE SCOPE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28714,
  [{name,<<"DROP SCOPE statement">>},
   {description,<<"A N1QL DROP SCOPE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28715,
  [{name,<<"CREATE COLLECTION statement">>},
   {description,<<"A N1QL CREATE COLLECTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28716,
  [{name,<<"DROP COLLECTION statement">>},
   {description,<<"A N1QL DROP COLLECTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28717,
  [{name,<<"FLUSH COLLECTION statement">>},
   {description,<<"A N1QL FLUSH COLLECTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28718,
  [{name,<<"UPDATE STATISTICS statement">>},
   {description,<<"A N1QL UPDATE STATISTICS statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28719,
  [{name,<<"ADVISE statement">>},
   {description,<<"A N1QL ADVISE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28720,
  [{name,<<"START TRANSACTION statement">>},
   {description,<<"A N1QL START TRANSACTION statement was execu"...>>},
   {enabled,false},
   {module,n1ql}]},
 {28721,
  [{name,<<"COMMIT TRANSACTION statement">>},
   {description,<<"A N1QL COMMIT TRANSACTION statement was "...>>},
   {enabled,false},
   {module,n1ql}]},
 {28722,
  [{name,<<"ROLLBACK TRANSACTION statement">>},
   {description,<<"A N1QL ROLLBACK TRANSACTION statemen"...>>},
   {enabled,false},
   {module,n1ql}]},
 {28723,
  [{name,<<"ROLLBACK TRANSACTION TO SAVEPOINT st"...>>},
   {description,<<"A N1QL ROLLBACK TRANSACTION TO S"...>>},
   {enabled,false},
   {module,n1ql}]},
 {28724,
  [{name,<<"SET TRANSACTION ISOLATION statem"...>>},
   {description,<<"A N1QL SET TRANSACTION ISOLA"...>>},
   {enabled,false},
   {module,n1ql}]},
 {28725,
  [{name,<<"SAVEPOINT statement">>},
   {description,<<"A N1QL SAVEPOINT stateme"...>>},
   {enabled,false},
   {module,n1ql}]},
 {28726,
  [{name,<<"/admin/transactions API "...>>},
   {description,<<"An HTTP request was "...>>},
   {enabled,false},
   {module,n1ql}]},
 {28727,
  [{name,<<"/admin/indexes/trans"...>>},
   {description,<<"An HTTP request "...>>},
   {enabled,false},
   {module,n1ql}]},
 {28728,
  [{name,<<"N1QL backup / re"...>>},
   {description,<<"An HTTP requ"...>>},
   {enabled,false},
   {module,n1ql}]},
 {28729,
  [{name,<<"/admin/shutd"...>>},
   {description,<<"An HTTP "...>>},
   {enabled,false},
   {module,n1ql}]},
 {28730,
  [{name,<<"/admin/g"...>>},
   {description,<<"An H"...>>},
   {enabled,false},
   {module,...}]},
 {28731,[{name,<<"/adm"...>>},{description,<<...>>},{enabled,...},{...}]},
 {28732,[{name,<<...>>},{description,...},{...}|...]},
 {28733,[{name,...},{...}|...]},
 {28734,[{...}|...]},
 {28735,[...]},
 {28736,...},
 {...}|...]
[ns_server:debug,2025-05-15T18:46:50.437Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
cluster_compat_version ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{3,63914554010}}]},7,6]
[ns_server:debug,2025-05-15T18:46:50.437Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
rbac_upgrade ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554010}}]}|
 '_deleted']
[ns_server:debug,2025-05-15T18:46:50.437Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
auto_failover_cfg ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554010}}]},
 {disable_max_count,false},
 {enabled,true},
 {timeout,120},
 {count,0},
 {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
 {failover_server_group,false},
 {max_count,1},
 {failed_over_server_groups,[]},
 {can_abort_rebalance,true},
 {failover_preserve_durability_majority,false}]
[ns_server:debug,2025-05-15T18:46:50.437Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
resource_management ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554010}}]},
 {bucket,[{resident_ratio,[{enabled,false},
                           {couchstore_minimum,1},
                           {magma_minimum,0.2}]},
          {data_size,[{enabled,false},
                      {couchstore_maximum,2},
                      {magma_maximum,16}]}]},
 {index,[]},
 {cores_per_bucket,[{enabled,false},{minimum,0.4}]},
 {disk_usage,[{enabled,false},{maximum,96}]},
 {collections_per_quota,[{enabled,false},{maximum,1}]}]
[ns_server:debug,2025-05-15T18:46:50.437Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/analytics/settings/config">>} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554010}}]}|
 <<"{\"analytics.settings.blob_storage_bucket\":\"\",\"analytics.settings.blob_storage_prefix\":\"\",\"analytics.settings.blob_storage_region\":\"\",\"analytics.settings.blob_storage_scheme\":\"\",\"analytics.settings.num_replicas\":0}">>]
[ns_server:debug,2025-05-15T18:46:50.437Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554010}}]}|
 <<"{\"indexer.settings.compaction.abort_exceed_interval\":false,\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.enable_page_bloom_filter\":false,\"indexer.settings.enable_"...>>]
[ns_server:debug,2025-05-15T18:46:50.437Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/query/settings/config">>} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554010}}]}|
 <<"{\"cleanupclientattempts\":true,\"cleanuplostattempts\":true,\"cleanupwindow\":\"60s\",\"completed-limit\":4000,\"completed-max-plan-size\":262144,\"completed-threshold\":1000,\"loglevel\":\"info\",\"max-parallelism\":1,\"memory-quota\":0,\"n1ql-feat-ctrl\":76,\"node-quota\":0,\"node-quota-val-percent\":67,\"num-cpus\":0,\"numatrs\":1024,\"pipeline-batch\":16,\"pipeline-cap\":512,\"prepared-limit\":16384,\"query.settings.cu"...>>]
[ns_server:debug,2025-05-15T18:46:50.437Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',prometheus_auth_info} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554010}}]}|
 {"@prometheus",
  {auth,
   [{<<"hash">>,
     {[{<<"hashes">>,
        {sanitized,<<"HmfAF2EJsiMXLpFCMXkwI34vHpKDSuQ9OLt/WXmflzM=">>}},
       {<<"algorithm">>,<<"argon2id">>},
       {<<"salt">>,<<"uKWlA91cJAr0Ue2ihPum/w==">>},
       {<<"parallelism">>,1},
       {<<"time">>,3},
       {<<"memory">>,524288}]}},
    {<<"scram-sha-512">>,
     {[{<<"salt">>,
        <<"B9qHYrF6B8PUh2HylXGlWHK8C5mZNOug98FyWD9UiLqElSnKTzdCIZzgFR8u69cjhSjtbD3VEM/dcZbWXNTBGw==">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"Jr1d3/BDp9K+TGL8YrZCBCGqzGbFloqcMPVzFi39KC0=">>}}]}},
    {<<"scram-sha-256">>,
     {[{<<"salt">>,<<"1nOjbJ6vT1sVuqxQCaa4EQHYdQk5w+2NYTeK1TUhKmY=">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"x86lsx8NdTsLHhvU9aNAyIeE61P+Yt6GyVOHrh2MFQo=">>}}]}},
    {<<"scram-sha-1">>,
     {[{<<"salt">>,<<"2rk59nwO1XOIg7dCVQ5eTg7p0y8=">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"4dXFSggAV9gFKuhCJ2pKs13gZZUKMc03Uwl7Sc+puO0=">>}}]}}]}}]
[ns_server:debug,2025-05-15T18:46:50.439Z,ns_1@cb.local:prometheus_cfg<0.517.0>:prometheus_cfg:maybe_apply_new_settings:704]Settings didn't change, ignoring update
[ns_server:debug,2025-05-15T18:46:50.441Z,ns_1@cb.local:memcached_permissions<0.533.0>:memcached_cfg:rename_and_refresh:178]Successfully renamed "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2025-05-15T18:46:50.441Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:handle_cast:67]Refresh of rbac requested
[ns_server:debug,2025-05-15T18:46:50.442Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:restart_regenerate_client_cert_timer:1447]Time left before client cert regeneration: 70588796000
[ns_server:debug,2025-05-15T18:46:50.442Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:maybe_store_ca_certs:832]Considering to store CA certs
[ns_server:debug,2025-05-15T18:46:50.444Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:restart_regenerate_client_cert_timer:1447]Time left before client cert regeneration: 70588796000
[ns_server:debug,2025-05-15T18:46:50.445Z,ns_1@cb.local:memcached_passwords<0.530.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,{auth,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2025-05-15T18:46:50.447Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:handle_call:57]File rename from "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw" is requested
[ns_server:debug,2025-05-15T18:46:50.448Z,ns_1@cb.local:memcached_passwords<0.530.0>:memcached_cfg:rename_and_refresh:178]Successfully renamed "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2025-05-15T18:46:50.448Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:handle_cast:67]Refresh of isasl requested
[error_logger:info,2025-05-15T18:46:50.448Z,ns_1@cb.local:ns_orchestrator_child_sup<0.875.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.876.0>},
              {id,ns_janitor_server},
              {mfargs,{ns_janitor_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:50.449Z,ns_1@cb.local:memcached_passwords<0.530.0>:memcached_cfg:write_cfg:156]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:info,2025-05-15T18:46:50.465Z,ns_1@cb.local:ns_orchestrator_child_sup<0.875.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          auto_reprovision},
                                         auto_reprovision,[],[]]): started as <0.877.0> on 'ns_1@cb.local'

[error_logger:info,2025-05-15T18:46:50.465Z,ns_1@cb.local:ns_orchestrator_child_sup<0.875.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.877.0>},
              {id,auto_reprovision},
              {mfargs,{auto_reprovision,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:50.465Z,ns_1@cb.local:memcached_passwords<0.530.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,{auth,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2025-05-15T18:46:50.466Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:handle_call:57]File rename from "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw" is requested
[ns_server:info,2025-05-15T18:46:50.467Z,ns_1@cb.local:ns_orchestrator_child_sup<0.875.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,auto_rebalance},
                                         auto_rebalance,[],[]]): started as <0.878.0> on 'ns_1@cb.local'

[error_logger:info,2025-05-15T18:46:50.467Z,ns_1@cb.local:ns_orchestrator_child_sup<0.875.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.878.0>},
              {id,auto_rebalance},
              {mfargs,{auto_rebalance,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:50.467Z,ns_1@cb.local:memcached_passwords<0.530.0>:memcached_cfg:rename_and_refresh:178]Successfully renamed "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:info,2025-05-15T18:46:50.468Z,ns_1@cb.local:ns_orchestrator_child_sup<0.875.0>:misc:start_singleton:913]start_singleton(gen_statem, start_link, [{via,leader_registry,ns_orchestrator},
                                         ns_orchestrator,[],[]]): started as <0.879.0> on 'ns_1@cb.local'

[error_logger:info,2025-05-15T18:46:50.468Z,ns_1@cb.local:ns_orchestrator_child_sup<0.875.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.879.0>},
              {id,ns_orchestrator},
              {mfargs,{ns_orchestrator,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.468Z,ns_1@cb.local:ns_orchestrator_sup<0.816.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.875.0>},
              {id,ns_orchestrator_child_sup},
              {mfargs,{ns_orchestrator_child_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:50.468Z,ns_1@cb.local:<0.881.0>:auto_failover:init:223]init auto_failover.
[ns_server:debug,2025-05-15T18:46:50.468Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:handle_cast:67]Refresh of isasl requested
[user:info,2025-05-15T18:46:50.471Z,ns_1@cb.local:<0.881.0>:auto_failover:enable_auto_failover:460]Enabled auto-failover with timeout 120 and max count 1
[ns_server:debug,2025-05-15T18:46:50.471Z,ns_1@cb.local:<0.881.0>:auto_failover:update_and_save_auto_failover_state:479]No change in timeout 120
[ns_server:debug,2025-05-15T18:46:50.471Z,ns_1@cb.local:<0.881.0>:auto_failover:update_and_save_auto_failover_state:487]No change in max count 1
[ns_server:debug,2025-05-15T18:46:50.474Z,ns_1@cb.local:<0.881.0>:auto_failover:init_logic_state:249]Using auto-failover logic state {state,[],
                                    [{service_state,kv,nil,false},
                                     {service_state,n1ql,nil,false},
                                     {service_state,index,nil,false},
                                     {service_state,fts,nil,false},
                                     {service_state,cbas,nil,false},
                                     {service_state,eventing,nil,false},
                                     {service_state,backup,nil,false}],
                                    118}
[ns_server:debug,2025-05-15T18:46:50.475Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
auto_failover_cfg ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{3,63914554010}}]},
 {enabled,true},
 {timeout,120},
 {count,0},
 {max_count,1},
 {disable_max_count,false},
 {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
 {failover_server_group,false},
 {failed_over_server_groups,[]},
 {can_abort_rebalance,true},
 {failover_preserve_durability_majority,false}]
[ns_server:info,2025-05-15T18:46:50.475Z,ns_1@cb.local:ns_orchestrator_sup<0.816.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,auto_failover},
                                         auto_failover,[],[]]): started as <0.881.0> on 'ns_1@cb.local'

[ns_server:debug,2025-05-15T18:46:50.476Z,ns_1@cb.local:ns_config_rep<0.554.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([auto_failover_cfg]..)
[error_logger:info,2025-05-15T18:46:50.476Z,ns_1@cb.local:ns_orchestrator_sup<0.816.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.881.0>},
              {id,auto_failover},
              {mfargs,{auto_failover,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.476Z,ns_1@cb.local:mb_master_sup<0.789.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.816.0>},
              {id,ns_orchestrator_sup},
              {mfargs,{ns_orchestrator_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:info,2025-05-15T18:46:50.480Z,ns_1@cb.local:mb_master_sup<0.789.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          tombstone_purger},
                                         tombstone_purger,[],[]]): started as <0.893.0> on 'ns_1@cb.local'

[error_logger:info,2025-05-15T18:46:50.480Z,ns_1@cb.local:mb_master_sup<0.789.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.893.0>},
              {id,tombstone_purger},
              {mfargs,{tombstone_purger,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.480Z,ns_1@cb.local:mb_master_sup<0.789.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.894.0>},
              {id,global_tasks},
              {mfargs,{global_tasks,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.483Z,ns_1@cb.local:mb_master_sup<0.789.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.895.0>},
              {id,guardrail_enforcer},
              {mfargs,{guardrail_enforcer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:50.484Z,ns_1@cb.local:guardrail_enforcer<0.895.0>:guardrail_enforcer:maybe_notify_services:137]Changed Statuses: #{}
[ns_server:debug,2025-05-15T18:46:50.488Z,ns_1@cb.local:<0.897.0>:license_reporting:init:60]Starting license_reporting server
[ns_server:info,2025-05-15T18:46:50.488Z,ns_1@cb.local:mb_master_sup<0.789.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          license_reporting},
                                         license_reporting,[],[]]): started as <0.897.0> on 'ns_1@cb.local'

[error_logger:info,2025-05-15T18:46:50.488Z,ns_1@cb.local:mb_master_sup<0.789.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.897.0>},
              {id,license_reporting},
              {mfargs,{license_reporting,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.488Z,ns_1@cb.local:leader_registry_sup<0.771.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_registry_sup}
    started: [{pid,<0.774.0>},
              {id,mb_master},
              {mfargs,{mb_master,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:50.488Z,ns_1@cb.local:leader_services_sup<0.761.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_services_sup}
    started: [{pid,<0.771.0>},
              {id,leader_registry_sup},
              {mfargs,{leader_registry_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:50.489Z,ns_1@cb.local:<0.758.0>:restartable:start_child:92]Started child process <0.761.0>
  MFA: {leader_services_sup,start_link,[]}
[error_logger:info,2025-05-15T18:46:50.489Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.758.0>},
              {id,leader_services_sup},
              {mfargs,{restartable,start_link,
                                   [{leader_services_sup,start_link,[]},
                                    infinity]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:50.490Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.899.0>},
              {id,ns_tick_agent},
              {mfargs,{ns_tick_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.490Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.901.0>},
              {id,master_activity_events_ingress},
              {mfargs,{gen_event,start_link,
                                 [{local,master_activity_events_ingress}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.490Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.902.0>},
              {id,master_activity_events_timestamper},
              {mfargs,{master_activity_events,start_link_timestamper,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.492Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.903.0>},
              {id,master_activity_events_pids_watcher},
              {mfargs,{master_activity_events_pids_watcher,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.494Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.904.0>},
              {id,master_activity_events_keeper},
              {mfargs,{master_activity_events_keeper,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.502Z,ns_1@cb.local:health_monitor_sup<0.906.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.907.0>},
              {id,ns_server_monitor},
              {mfargs,{ns_server_monitor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.502Z,ns_1@cb.local:health_monitor_sup<0.906.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.909.0>},
              {id,service_monitor_children_sup},
              {mfargs,{supervisor,start_link,
                                  [{local,service_monitor_children_sup},
                                   health_monitor_sup,child]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:50.502Z,ns_1@cb.local:health_monitor_sup<0.906.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.910.0>},
              {id,service_monitor_worker},
              {mfargs,{erlang,apply,[#Fun<health_monitor_sup.0.30770475>,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:50.503Z,ns_1@cb.local:chronicle_kv_log<0.504.0>:chronicle_kv_log:log:59]update (key: tasks, rev: {<<"132dcb91b709843f36fcbbab0f42c041">>,6})
[]
[error_logger:info,2025-05-15T18:46:50.504Z,ns_1@cb.local:health_monitor_sup<0.906.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.916.0>},
              {id,node_monitor},
              {mfargs,{node_monitor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.506Z,ns_1@cb.local:health_monitor_sup<0.906.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.923.0>},
              {id,node_status_analyzer},
              {mfargs,{node_status_analyzer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.506Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.906.0>},
              {id,health_monitor_sup},
              {mfargs,{health_monitor_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:50.508Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.926.0>},
              {id,rebalance_agent},
              {mfargs,{rebalance_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.510Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.927.0>},
              {id,kv_hibernation_agent},
              {mfargs,{kv_hibernation_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.516Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.928.0>},
              {id,ns_rebalance_report_manager},
              {mfargs,{ns_rebalance_report_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:50.518Z,ns_1@cb.local:cb_creds_rotation<0.930.0>:cb_creds_rotation:start_rotate_timer:214]Starting creds rotation timer (1800000ms)
[error_logger:info,2025-05-15T18:46:50.518Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.930.0>},
              {id,creds_rotation},
              {mfargs,{cb_creds_rotation,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.518Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.496.0>},
              {id,ns_server_sup},
              {mfargs,{ns_server_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:50.518Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:one_shot_barrier:notify:21]Notifying on barrier menelaus_barrier
[error_logger:error,2025-05-15T18:46:50.518Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {local,ns_server_sup}
    errorContext: child_terminated
    reason: {noproc,{gen_statem,call,[mb_master,master_node,infinity]}}
    offender: [{pid,<0.715.0>},
               {id,ns_server_stats},
               {mfargs,{ns_server_stats,start_link,[]}},
               {restart_type,permanent},
               {significant,false},
               {shutdown,1000},
               {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:50.518Z,ns_1@cb.local:menelaus_barrier<0.298.0>:one_shot_barrier:barrier_body:56]Barrier menelaus_barrier got notification from <0.291.0>
[ns_server:debug,2025-05-15T18:46:50.519Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:one_shot_barrier:notify:26]Successfuly notified on barrier menelaus_barrier
[ns_server:debug,2025-05-15T18:46:50.519Z,ns_1@cb.local:<0.290.0>:restartable:start_child:92]Started child process <0.291.0>
  MFA: {ns_server_nodes_sup,start_link,[]}
[error_logger:info,2025-05-15T18:46:50.519Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.290.0>},
              {id,ns_server_nodes_sup},
              {mfargs,{restartable,start_link,
                                   [{ns_server_nodes_sup,start_link,[]},
                                    infinity]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:50.519Z,ns_1@cb.local:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.932.0>},
              {id,ns_server_stats},
              {mfargs,{ns_server_stats,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:50.521Z,ns_1@cb.local:compiled_roles_cache<0.455.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@",admin}
[error_logger:info,2025-05-15T18:46:50.522Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.936.0>},
              {id,remote_api},
              {mfargs,{remote_api,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.533Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.937.0>},
              {id,ns_gc_runner},
              {mfargs,{ns_gc_runner,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:50.533Z,ns_1@cb.local:root_sup<0.214.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,root_sup}
    started: [{pid,<0.235.0>},
              {id,ns_server_cluster_sup},
              {mfargs,{ns_server_cluster_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:50.533Z,ns_1@cb.local:application_controller<0.44.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    application: ns_server
    started_at: 'ns_1@cb.local'

[ns_server:debug,2025-05-15T18:46:50.533Z,ns_1@cb.local:<0.9.0>:child_erlang:child_loop:147]118: Entered child_loop
[ns_server:debug,2025-05-15T18:46:50.533Z,ns_1@cb.local:<0.9.0>:child_erlang:child_loop:147]118: Entered child_loop
[ns_server:debug,2025-05-15T18:46:50.540Z,ns_1@cb.local:compiled_roles_cache<0.455.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@ns_server",admin}
[ns_server:debug,2025-05-15T18:46:50.541Z,ns_1@cb.local:json_rpc_connection-goxdcr-cbauth<0.940.0>:json_rpc_connection:init:71]Observed revrpc connection: label "goxdcr-cbauth", handling process <0.940.0>
[ns_server:debug,2025-05-15T18:46:50.541Z,ns_1@cb.local:json_rpc_connection-saslauthd-saslauthd-port<0.941.0>:json_rpc_connection:init:71]Observed revrpc connection: label "saslauthd-saslauthd-port", handling process <0.941.0>
[ns_server:debug,2025-05-15T18:46:50.541Z,ns_1@cb.local:menelaus_cbauth<0.668.0>:menelaus_cbauth:handle_cast:203]Observed json rpc process {"goxdcr-cbauth",[{internal,true}],<0.940.0>} started
[ns_server:debug,2025-05-15T18:46:50.545Z,ns_1@cb.local:compiled_roles_cache<0.455.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@goxdcr",admin}
[ns_server:debug,2025-05-15T18:46:50.547Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/query/sequences_cache/revision">>} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554010}}]}|
 <<"0">>]
[ns_server:debug,2025-05-15T18:46:50.548Z,ns_1@cb.local:ns_config_rep<0.554.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([{metakv,<<"/query/sequences_cache/revision">>}]..)
[ns_server:debug,2025-05-15T18:46:50.950Z,ns_1@cb.local:<0.962.0>:memcached_refresh:do_refresh:153]Successfully connected to memcached, Trying to refresh [rbac,isasl]
[ns_server:debug,2025-05-15T18:46:50.953Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:update_refresh_state:116]Refresh of [rbac,isasl] succeeded
[ns_server:info,2025-05-15T18:46:51.290Z,ns_1@cb.local:memcached_config_mgr<0.704.0>:memcached_config_mgr:push_tls_config:237]Successfully pushed TLS config to memcached
[ns_server:info,2025-05-15T18:46:51.290Z,ns_1@cb.local:memcached_config_mgr<0.704.0>:memcached_config_mgr:push_tls_config:233]Pushing TLS config to memcached:
{[{<<"private key">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem">>},
  {<<"certificate chain">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem">>},
  {<<"CA file">>,<<"/opt/couchbase/var/lib/couchbase/config/certs/ca.pem">>},
  {<<"minimum version">>,<<"TLS 1.2">>},
  {<<"cipher list">>,
   {[{<<"TLS 1.2">>,<<"HIGH">>},
     {<<"TLS 1.3">>,
      <<"TLS_AES_256_GCM_SHA384:TLS_AES_128_GCM_SHA256:TLS_CHACHA20_POLY1305_SHA256">>}]}},
  {<<"cipher order">>,true},
  {<<"client cert auth">>,<<"disabled">>}]}
[ns_server:info,2025-05-15T18:46:51.292Z,ns_1@cb.local:memcached_config_mgr<0.704.0>:memcached_config_mgr:push_tls_config:237]Successfully pushed TLS config to memcached
[ns_server:debug,2025-05-15T18:46:51.313Z,ns_1@cb.local:<0.707.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":18,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true,\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[ns_server:debug,2025-05-15T18:46:51.473Z,ns_1@cb.local:<0.881.0>:auto_failover_logic:log_master_activity:130]Transitioned node {'ns_1@cb.local',<<"b0a71fbd3e8ac51d55b35452995f45cf">>} state new -> up
[ns_server:debug,2025-05-15T18:46:51.537Z,ns_1@cb.local:ns_gc_runner<0.937.0>:ns_gc_runner:handle_info:125]GC populating new pid list of size=591, prevMaxGcDuration=0 us
[ns_server:debug,2025-05-15T18:47:02.668Z,ns_1@cb.local:compiled_roles_cache<0.455.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@prometheus",stats_reader}
[ns_server:debug,2025-05-15T18:47:11.417Z,ns_1@cb.local:cb_saml<0.429.0>:cb_saml:refresh_metadata:526]Refreshing metadata
[ns_server:debug,2025-05-15T18:47:11.418Z,ns_1@cb.local:cb_saml<0.429.0>:cb_saml:restart_refresh_timer:583]Disabling refresh timer
[ns_server:debug,2025-05-15T18:47:20.212Z,ns_1@cb.local:compaction_daemon<0.748.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2025-05-15T18:47:20.213Z,ns_1@cb.local:compaction_daemon<0.748.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:47:20.214Z,ns_1@cb.local:compaction_daemon<0.748.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2025-05-15T18:47:20.214Z,ns_1@cb.local:compaction_daemon<0.748.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:47:22.687Z,ns_1@cb.local:compiled_roles_cache<0.455.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"<ud>jaba_admin</ud>",admin}
[cluster:debug,2025-05-15T18:47:22.692Z,ns_1@cb.local:<0.1852.0>:ns_cluster:engage_cluster:97]Processing engage cluster request with [{<<"requestedTargetNodeHostname">>,
                                         <<"db2.lan">>},
                                        {<<"requestedServices">>,
                                         [<<"index">>,<<"kv">>,<<"n1ql">>]},
                                        {<<"isDeveloperPreview">>,false},
                                        {<<"availableStorage">>,
                                         {[{<<"hdd">>,
                                            [{[{<<"path">>,<<"/">>},
                                               {<<"sizeKBytes">>,1056557396},
                                               {<<"usagePercent">>,2}]},
                                             {[{<<"path">>,<<"/dev">>},
                                               {<<"sizeKBytes">>,65536},
                                               {<<"usagePercent">>,0}]},
                                             {[{<<"path">>,<<"/dev/shm">>},
                                               {<<"sizeKBytes">>,65536},
                                               {<<"usagePercent">>,0}]},
                                             {[{<<"path">>,
                                                <<"/etc/resolv.conf">>},
                                               {<<"sizeKBytes">>,1056557396},
                                               {<<"usagePercent">>,2}]},
                                             {[{<<"path">>,
                                                <<"/etc/hostname">>},
                                               {<<"sizeKBytes">>,1056557396},
                                               {<<"usagePercent">>,2}]},
                                             {[{<<"path">>,<<"/etc/hosts">>},
                                               {<<"sizeKBytes">>,1056557396},
                                               {<<"usagePercent">>,2}]},
                                             {[{<<"path">>,
                                                <<"/opt/couchbase/var">>},
                                               {<<"sizeKBytes">>,482797652},
                                               {<<"usagePercent">>,92}]},
                                             {[{<<"path">>,<<"/proc/kcore">>},
                                               {<<"sizeKBytes">>,65536},
                                               {<<"usagePercent">>,0}]},
                                             {[{<<"path">>,<<"/proc/keys">>},
                                               {<<"sizeKBytes">>,65536},
                                               {<<"usagePercent">>,0}]},
                                             {[{<<"path">>,
                                                <<"/proc/timer_list">>},
                                               {<<"sizeKBytes">>,65536},
                                               {<<"usagePercent">>,0}]},
                                             {[{<<"path">>,<<"/proc/scsi">>},
                                               {<<"sizeKBytes">>,4012680},
                                               {<<"usagePercent">>,0}]},
                                             {[{<<"path">>,
                                                <<"/sys/firmware">>},
                                               {<<"sizeKBytes">>,4012680},
                                               {<<"usagePercent">>,0}]}]}]}},
                                        {<<"storageTotals">>,
                                         {[{<<"ram">>,
                                            {[{<<"total">>,8217968640},
                                              {<<"quotaTotal">>,3221225472},
                                              {<<"quotaUsed">>,0},
                                              {<<"used">>,4251119616},
                                              {<<"usedByData">>,0},
                                              {<<"quotaUsedPerNode">>,0},
                                              {<<"quotaTotalPerNode">>,
                                               3221225472}]}},
                                           {<<"hdd">>,
                                            {[{<<"total">>,494384795648},
                                              {<<"quotaTotal">>,494384795648},
                                              {<<"used">>,454834011996},
                                              {<<"usedByData">>,0},
                                              {<<"free">>,39550783652}]}}]}},
                                        {<<"storage">>,
                                         {[{<<"ssd">>,[]},
                                           {<<"hdd">>,
                                            [{[{<<"path">>,
                                                <<"/opt/couchbase/var/lib/couchbase/data">>},
                                               {<<"index_path">>,
                                                <<"/opt/couchbase/var/lib/couchbase/data">>},
                                               {<<"cbas_dirs">>,
                                                [<<"/opt/couchbase/var/lib/couchbase/data">>]},
                                               {<<"eventing_path">>,
                                                <<"/opt/couchbase/var/lib/couchbase/data">>},
                                               {<<"java_home">>,<<>>},
                                               {<<"quotaMb">>,<<"none">>},
                                               {<<"state">>,<<"ok">>}]}]}]}},
                                        {<<"clusterMembership">>,<<"active">>},
                                        {<<"recoveryType">>,<<"none">>},
                                        {<<"status">>,<<"healthy">>},
                                        {<<"otpNode">>,<<"ns_1@172.19.0.4">>},
                                        {<<"thisNode">>,true},
                                        {<<"hostname">>,<<"172.19.0.4:8091">>},
                                        {<<"nodeUUID">>,
                                         <<"28569ac00b9c1d7c50e39741027d428c">>},
                                        {<<"clusterCompatibility">>,458758},
                                        {<<"version">>,
                                         <<"7.6.2-3721-enterprise">>},
                                        {<<"os">>,
                                         <<"aarch64-unknown-linux-gnu">>},
                                        {<<"cpuCount">>,10},
                                        {<<"ports">>,
                                         {[{<<"direct">>,11210},
                                           {<<"httpsMgmt">>,18091},
                                           {<<"httpsCAPI">>,18092},
                                           {<<"distTCP">>,21100},
                                           {<<"distTLS">>,21150}]}},
                                        {<<"services">>,
                                         [<<"index">>,<<"kv">>,<<"n1ql">>]},
                                        {<<"nodeEncryption">>,false},
                                        {<<"nodeEncryptionClientCertVerification">>,
                                         false},
                                        {<<"addressFamilyOnly">>,false},
                                        {<<"configuredHostname">>,
                                         <<"172.19.0.4:8091">>},
                                        {<<"addressFamily">>,<<"inet">>},
                                        {<<"externalListeners">>,
                                         [{[{<<"afamily">>,<<"inet">>},
                                            {<<"nodeEncryption">>,false}]}]},
                                        {<<"serverGroup">>,<<"Group 1">>},
                                        {<<"couchApiBase">>,
                                         <<"http://172.19.0.4:8092/">>},
                                        {<<"couchApiBaseHTTPS">>,
                                         <<"https://172.19.0.4:18092/">>},
                                        {<<"nodeHash">>,10838665},
                                        {<<"systemStats">>,
                                         {[{<<"cpu_utilization_rate">>,
                                            3.89883035089008},
                                           {<<"cpu_stolen_rate">>,0},
                                           {<<"swap_total">>,1073737728},
                                           {<<"swap_used">>,3956736},
                                           {<<"mem_total">>,8217968640},
                                           {<<"mem_free">>,4805804032},
                                           {<<"mem_limit">>,8217968640},
                                           {<<"cpu_cores_available">>,10},
                                           {<<"allocstall">>,17}]}},
                                        {<<"interestingStats">>,{[]}},
                                        {<<"uptime">>,<<"41">>},
                                        {<<"memoryTotal">>,8217968640},
                                        {<<"memoryFree">>,4805804032},
                                        {<<"mcdMemoryReserved">>,6269},
                                        {<<"mcdMemoryAllocated">>,6269},
                                        {<<"memoryQuota">>,3072},
                                        {<<"queryMemoryQuota">>,0},
                                        {<<"indexMemoryQuota">>,512},
                                        {<<"ftsMemoryQuota">>,512},
                                        {<<"cbasMemoryQuota">>,1024},
                                        {<<"eventingMemoryQuota">>,256},
                                        {<<"autogeneratedCA">>,
                                         <<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQCuy0dFqLPrJ6xY21RJ5ttVPR78ssxxXSe9JdTj/IsUczkwyxfZfwK\npBszlGYuHO0Utnwq1K4ruMPYFYfpLacJSfsb3KWEmZwAoNuRpjvE24AsH3tvj7cB\nB5v49HJ0axvNAFm75GWDyUr7T7UwcTJaFIFtfy+J0Umt2TjFNY0DPpt1WEzkKFKx\nOdBs+7cNc35ZG4rwVu/EJ14OXhQMXkpbUpYJxS8jzkzSI+1Eb67kH4j5QhnR9H+w\nw2S9AgMBAAGjQjBAMA4GA1UdDwEB/wQEAwIBhjAPBgNVHRMBAf8EBTADAQH/MB0G\nA1UdDgQWBBS8ASq2Mkh0UsZe4fIvzs4mCSfTmjANBgkqhkiG9w0BAQsFAAOCAQEA\nB3q1mzdJc3VY17JDCaRPKY/Veni3oZ6zGJ9r9LnWKE7IO9AxBAKYGMELHMf9httQ\nmRZrXiGQi/tYXLlFFk7IcIID6iOOZLLagDhCQJPi52dGhBB4a2EnBvGF9OJI4zW9\nJw04HNR5cF1cxWIrRdGJAX/kbs/M9jz0STlXziVNwiAYeGIxkjq/fwKk6weai8iI\nwQVu6GsbtSBL8f4FRCk2vK6a1MD54BQDasRShdi3k/DJAEtY92muu13M0jc48mCM\ndEVeXxM8rIrG6LavziTwtO+jkFyWjlRyNbSr6il+neSWJ7ZspTo+ZYFy55CAh5ws\nwk6Ljo2wlQgfGflJbKcxAw==\n-----END CERTIFICATE-----\n">>},
                                        {<<"autogeneratedClientCert">>,
                                         <<"-----BEGIN CERTIFICATE-----\nMIIDWjCCAkKgAwIBAgIIGD/HyA7ClpEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjJaFw0y\nNzA4MTcxODQ3MjJaMDAxLjAsBgNVBAMTJUNvdWNoYmFzZSBJbnRlcm5hbCBDbGll\nbnQgKDExNjU1MDk1MikwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDg\nAaBAyJnrm54fDG0xlsEQH9E51WTLpkwtleiz980c498XKRPsgRWnjY920KTUIH2/\ndQcryFyvGTggbCNUUsAqs6qWzDX6iZW6uhcXUh9ujNASr1VNBMLPY60ejukP9Jbq\nxlTfAYvoidlCAZ43nBN17IbZjObU9wEL5AJVX+DLfxX9K0uTI37qWIX6lODZMnek\nj8dGQcmpjyB0ktQaFPLCfzIyXDoA0XE4zsxXW+qX4Xji5cwFwgUgJ3E1IYamZJZv\nF3WkWl7ai8xuos3gTqe41d4rvGemY5Pxao60ClgbgFPaimvOFT7Is2h4TzVQI8f7\nonIen4hIAiwE18Iz4WRJAgMBAAGjgYMwgYAwDgYDVR0PAQH/BAQDAgWgMBMGA1Ud\nJQQMMAoGCCsGAQUFBwMCMAwGA1UdEwEB/wQCMAAwHwYDVR0jBBgwFoAUvAEqtjJI\ndFLGXuHyL87OJgkn05owKgYDVR0RBCMwIYEfaW50ZXJuYWxAaW50ZXJuYWwuY291\nY2hiYXNlLmNvbTANBgkqhkiG9w0BAQsFAAOCAQEAjrpupGzgLn2L0Zf3xDsp7paD\nNHbJDPJeJAeG3u4nAur7tcZqAQbUgzkIPe9IFvz1dfdS9CRfyUYLzjcGK7M6Nzyt\nhgAwmdTOjAxTtcjVstxSdyhSxLORZnyMQcFpzJH+weMhVe5khi/T3nDWyR9yqwW0\nw9iReXBJhh/QyE+EBLMYJ58gXG5qQOHiOVw0ePQmQtoQhkjEM2jWVJegD+boNmOK\nKoCXNSxWZB2dC1MOQxtvA5ggysGIVOYH1ASMyXyZszSCRjKfGDzTYyJSfnBACijg\nWXt8NyZIaucN/aXR9H4CzrC077tJz9hihz0Ld4uhaIbVddLOY13TGq/V/0H6cg==\n-----END CERTIFICATE-----\n">>},
                                        {<<"autogeneratedClientKey">>,
                                         <<"********">>},
                                        {<<"autogeneratedCert">>,
                                         <<"-----BEGIN CERTIFICATE-----\nMIIDOjCCAiKgAwIBAgIIGD/HyAMALgMwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjJaFw0y\nNzA4MTcxODQ3MjJaMCoxKDAmBgNVBAMTH0NvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nZGIyLmxhbikwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC63r7EIhIk\nOyMwE9mw7bzVJfstC9FfdZvk0bP8vxZkuhNt/bVcND8p00ihMP8FCbfsa5d8VgWC\nDoao/+R+8KpeXDZeXWTzOfGPz6ySJpDMUbPsj/pymSkYBvwsl0iBlPrBYDVlHe6C\nIFfKfnfHOsS9itHFANxppJ+spLeJP7Na1p/rOGFELp9Y0pYnDvAccES6DCwgzMKr\n4lk/1l1L4jJkyHzK0hQ5UutHe7jGBhOxRoIyTCPO5wf6miZJNlKxA/1zwCJuLaft\ny4N6DDBMIs9R/dS02i1IrhKLDvC/7OnK/BkXi4HKLKY/wE3YNBmgMNXEwxUMeqZk\n2IUILCZn4W9fAgMBAAGjajBoMA4GA1UdDwEB/wQEAwIFoDATBgNVHSUEDDAKBggr\nBgEFBQcDATAMBgNVHRMBAf8EAjAAMB8GA1UdIwQYMBaAFLwBKrYySHRSxl7h8i/O\nziYJJ9OaMBIGA1UdEQQLMAmCB2RiMi5sYW4wDQYJKoZIhvcNAQELBQADggEBADkD\n1qqpKjJY4cyeoPzdD5mW7I12+dWN/oW0yUQ4heVH9TZzgThchsucMQvFg3vpZ9j5\nSbxsH9h23M26Cv8PxOjpu8lmlYAtENDDGnagAiwDJuMtSeBkKw36xzLrB4jj8UZ+\n8k+5CLVUzncPt9J9o7Lgm+wgavlprqSMJLUGfAc+O0qGzN+mQk1UeM2a8++Ty97Q\neks+wPVQCPbprWQU7k7F8mSepoAWHglkhfkBjBXgHGvGMIMGDv68l7yfNBJ9yRoJ\nO5cAQgoNiHZDzZ9clPCCGeE+JQnxNQCMOIxq7xhnTrYgBeU9hWu0hBcrHR6ZwO+m\n6MNtblxGbkNee9HO7LY=\n-----END CERTIFICATE-----\n">>},
                                        {<<"autogeneratedKey">>,
                                         <<"********">>}].
[ns_server:debug,2025-05-15T18:47:22.697Z,ns_1@cb.local:<0.1852.0>:ns_ssl_services_setup:set_certs:632]Setting certificates
[ns_server:debug,2025-05-15T18:47:22.721Z,ns_1@cb.local:chronicle_kv_log<0.504.0>:chronicle_kv_log:log:59]update (key: root_cert_and_pkey, rev: {<<"132dcb91b709843f36fcbbab0f42c041">>,
                                       8})
{sanitized,<<"oMbeHkcMXIA/ths1QH3yRdqP7VoqICiFcjKji48Mlic=">>}
[ns_server:debug,2025-05-15T18:47:22.748Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:maybe_store_ca_certs:832]Considering to store CA certs
[ns_server:debug,2025-05-15T18:47:22.748Z,ns_1@cb.local:chronicle_kv_log<0.504.0>:chronicle_kv_log:log:59]update (key: ca_certificates, rev: {<<"132dcb91b709843f36fcbbab0f42c041">>,9})
[[{id,1},
  {load_timestamp,63914554042},
  {subject,<<"CN=Couchbase Server c64d914e">>},
  {not_before,63524217600},
  {not_after,64691827199},
  {type,generated},
  {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl"...>>}],
 [{id,0},
  {load_timestamp,63914554006},
  {subject,<<"CN=Couchbase Server 1461cc22">>},
  {not_before,63524217600},
  {not_after,64691827199},
  {type,generated},
  {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv7QbCrEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciAxNDYxY2MyMjAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgMTQ2MWNj\nMjIwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC1eALPemMHR7JjfP48\nA8J89K045QIJUgPOt1HHzMEhY1TyprxrDiVgWNH+8CDMk80tP3uhVWgJlhXC1d0T"...>>}]]
[ns_server:debug,2025-05-15T18:47:22.748Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:maybe_store_ca_certs:847]Updating CA file with 2 certificates
[ns_server:info,2025-05-15T18:47:22.751Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:maybe_store_ca_certs:850]CA file updated: 2 cert(s) written
[ns_server:info,2025-05-15T18:47:22.753Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:save_certs:968]New node_cert and pkey are written to tmp file
[ns_server:info,2025-05-15T18:47:22.757Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:save_certs_phase2:995]node_cert cert and pkey files updated
[ns_server:debug,2025-05-15T18:47:22.757Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',node_cert} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]},
 {subject,<<"CN=Couchbase Server Node (db2.lan)">>},
 {not_after,63985747642},
 {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
 {load_timestamp,63914554042},
 {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQ"...>>},
 {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDOjCCAiKgAwIBAgIIGD/HyAMALgMwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjJaFw0y\nNzA4MTcxODQ3MjJaMCoxKDAmBgNVBAMTH0NvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nZGIyLmxhbikwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC63r7EIhIk\nOyMwE9mw7bzVJfstC9FfdZvk0bP8vxZkuhNt/bVcND8p00ihMP8FCbfsa5d8VgWC\nDoao/+R"...>>},
 {pkey_passphrase_settings,[]},
 {certs_epoch,0},
 {type,generated},
 {hostname,"db2.lan"}]
[ns_server:debug,2025-05-15T18:47:22.757Z,ns_1@cb.local:ns_config_rep<0.554.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([{node,'ns_1@cb.local',node_cert}]..)
[ns_server:info,2025-05-15T18:47:22.763Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:save_certs:968]New client_cert and pkey are written to tmp file
[ns_server:info,2025-05-15T18:47:22.769Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:save_certs_phase2:995]client_cert cert and pkey files updated
[ns_server:debug,2025-05-15T18:47:22.769Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',client_cert} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]},
 {subject,<<"CN=Couchbase Internal Client (116550952)">>},
 {not_after,63985747642},
 {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
 {load_timestamp,63914554042},
 {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQ"...>>},
 {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDWjCCAkKgAwIBAgIIGD/HyA7ClpEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjJaFw0y\nNzA4MTcxODQ3MjJaMDAxLjAsBgNVBAMTJUNvdWNoYmFzZSBJbnRlcm5hbCBDbGll\nbnQgKDExNjU1MDk1MikwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDg\nAaBAyJnrm54fDG0xlsEQH9E51WTLpkwtleiz980c498XKRPsgRWnjY920KTUIH2/\ndQcryFy"...>>},
 {pkey_passphrase_settings,[]},
 {certs_epoch,0},
 {type,generated},
 {name,"@internal"}]
[ns_server:debug,2025-05-15T18:47:22.769Z,ns_1@cb.local:ns_config_rep<0.554.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([{node,'ns_1@cb.local',client_cert}]..)
[ns_server:debug,2025-05-15T18:47:22.774Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:notify_services:1146]Going to notify following services: [capi_ssl_service,cb_dist_tls,
                                     client_cert_event,memcached,
                                     server_cert_event,ssl_service]
[ns_server:info,2025-05-15T18:47:22.774Z,ns_1@cb.local:<0.2106.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service memcached
[ns_server:info,2025-05-15T18:47:22.774Z,ns_1@cb.local:<0.2105.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service client_cert_event
[ns_server:debug,2025-05-15T18:47:22.774Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Restarting tls distribution protocols (if any)
[ns_server:info,2025-05-15T18:47:22.774Z,ns_1@cb.local:<0.2107.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service server_cert_event
[ns_server:debug,2025-05-15T18:47:22.775Z,ns_1@cb.local:<0.328.0>:restartable:loop:65]Restarting child <0.388.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
  Shutdown policy: 1000
  Caller: {<0.2108.0>,#Ref<0.3735524962.3650355201.245667>}
[ns_server:debug,2025-05-15T18:47:22.775Z,ns_1@cb.local:<0.328.0>:restartable:shutdown_child:114]Successfully terminated process <0.388.0>
[ns_server:info,2025-05-15T18:47:22.775Z,ns_1@cb.local:memcached_config_mgr<0.704.0>:memcached_config_mgr:push_tls_config:233]Pushing TLS config to memcached:
{[{<<"private key">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem">>},
  {<<"certificate chain">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem">>},
  {<<"CA file">>,<<"/opt/couchbase/var/lib/couchbase/config/certs/ca.pem">>},
  {<<"minimum version">>,<<"TLS 1.2">>},
  {<<"cipher list">>,
   {[{<<"TLS 1.2">>,<<"HIGH">>},
     {<<"TLS 1.3">>,
      <<"TLS_AES_256_GCM_SHA384:TLS_AES_128_GCM_SHA256:TLS_CHACHA20_POLY1305_SHA256">>}]}},
  {<<"cipher order">>,true},
  {<<"client cert auth">>,<<"disabled">>}]}
[ns_server:debug,2025-05-15T18:47:22.778Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Ensure config is going to change listeners. Will be stopped: [], will be started: [], will be restarted: []
[ns_server:info,2025-05-15T18:47:22.778Z,ns_1@cb.local:<0.2104.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service cb_dist_tls
[ns_server:info,2025-05-15T18:47:22.778Z,ns_1@cb.local:<0.2109.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2025-05-15T18:47:22.778Z,ns_1@cb.local:<0.2109.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2025-05-15T18:47:22.779Z,ns_1@cb.local:<0.2109.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2025-05-15T18:47:22.779Z,ns_1@cb.local:<0.2109.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2025-05-15T18:47:22.779Z,ns_1@cb.local:<0.2109.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2025-05-15T18:47:22.781Z,ns_1@cb.local:<0.2109.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"0.0.0.0"},
 [{keyfile,"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem"},
  {certfile,"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem"},
  {versions,['tlsv1.2','tlsv1.3']},
  {cacerts,[<<48,130,3,12,48,130,1,244,160,3,2,1,2,2,8,24,63,199,191,180,27,10,
              177,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,48,32,
              6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,101,114,
              118,101,114,32,49,52,54,49,99,99,50,50,48,30,23,13,49,51,48,49,
              48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,51,53,57,
              53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,117,99,104,98,
              97,115,101,32,83,101,114,118,101,114,32,49,52,54,49,99,99,50,50,
              48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,1,5,0,3,130,1,15,
              0,48,130,1,10,2,130,1,1,0,181,120,2,207,122,99,7,71,178,99,124,
              254,60,3,194,124,244,173,56,229,2,9,82,3,206,183,81,199,204,193,
              33,99,84,242,166,188,107,14,37,96,88,209,254,240,32,204,147,205,
              45,63,123,161,85,104,9,150,21,194,213,221,19,3,77,76,189,208,93,
              232,47,63,226,64,53,14,136,224,149,250,97,65,87,66,59,254,91,0,
              116,97,49,186,38,79,0,74,253,18,47,169,76,212,127,181,44,78,32,
              241,232,104,53,173,102,141,194,235,50,161,36,240,71,126,109,238,
              147,180,102,33,170,119,20,109,208,73,212,125,100,165,198,140,210,
              185,71,194,152,180,1,242,111,31,230,61,53,12,218,145,184,127,130,
              12,249,6,112,111,221,221,245,161,89,130,74,135,153,231,237,136,
              181,125,70,232,1,156,245,196,14,149,104,104,202,6,233,86,16,69,
              111,71,86,230,105,161,211,116,106,249,40,136,196,141,254,172,122,
              108,78,252,182,126,8,182,88,220,41,202,91,163,155,96,85,22,97,
              186,40,194,19,0,57,238,64,87,144,142,62,207,252,84,26,9,57,116,0,
              47,173,193,153,104,169,103,194,29,2,3,1,0,1,163,66,48,64,48,14,6,
              3,85,29,15,1,1,255,4,4,3,2,1,134,48,15,6,3,85,29,19,1,1,255,4,5,
              48,3,1,1,255,48,29,6,3,85,29,14,4,22,4,20,197,18,173,226,193,241,
              105,140,163,34,50,61,128,135,150,89,215,37,208,144,48,13,6,9,42,
              134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,26,225,231,0,217,118,14,
              29,57,113,249,186,176,21,106,214,129,105,172,105,111,87,138,154,
              56,94,191,209,103,130,18,48,137,158,179,148,24,195,213,203,137,
              49,145,82,189,69,7,8,202,85,205,192,230,21,61,35,22,75,67,182,
              222,215,159,17,57,66,131,43,49,33,238,11,249,32,157,128,15,148,
              89,192,30,239,133,216,181,147,123,109,25,188,184,206,174,31,156,
              91,27,236,254,110,113,28,32,72,48,13,152,101,138,146,182,79,72,
              46,205,1,91,50,143,226,51,117,20,113,109,46,246,61,157,43,14,204,
              7,253,73,198,19,234,127,131,141,44,230,60,185,139,136,236,213,87,
              10,80,32,254,169,231,241,155,193,145,255,244,60,35,164,87,65,41,
              173,37,98,26,217,243,211,181,172,244,80,39,175,148,102,124,187,
              49,169,81,217,58,84,217,38,11,118,142,9,6,20,24,204,189,93,103,
              187,215,0,101,236,40,186,4,21,106,168,193,83,29,52,82,106,30,74,
              125,41,250,77,5,118,196,217,211,201,19,214,171,213,81,174,93,190,
              173,157,199,251,234,252,74,62,46,85,78,135,95,193,68>>,
            <<48,130,3,12,48,130,1,244,160,3,2,1,2,2,8,24,63,199,191,156,197,
              82,17,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,48,
              32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,101,
              114,118,101,114,32,99,54,52,100,57,49,52,101,48,30,23,13,49,51,
              48,49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,51,
              53,57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,117,
              99,104,98,97,115,101,32,83,101,114,118,101,114,32,99,54,52,100,
              57,49,52,101,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,1,
              5,0,3,130,1,15,0,48,130,1,10,2,130,1,1,0,189,71,7,138,162,116,
              202,173,172,80,169,207,183,229,131,28,103,231,118,113,23,184,
              163,118,132,50,140,197,100,9,14,13,222,41,184,111,89,132,2,102,
              81,20,185,124,217,165,68,88,93,96,211,184,188,36,17,35,176,167,
              109,82,1,80,201,124,250,21,203,22,244,2,187,45,29,22,162,207,
              172,158,177,99,109,81,39,155,109,84,244,123,242,203,49,197,116,
              158,244,151,83,143,242,44,81,204,228,195,44,95,101,252,10,164,
              27,51,148,102,46,28,237,20,182,124,42,212,174,43,184,195,216,
              21,135,233,45,167,9,73,251,27,220,165,132,153,156,0,160,219,
              145,166,59,196,219,128,44,31,123,111,143,183,1,7,155,248,244,
              114,116,107,27,205,0,89,187,228,101,131,201,74,251,79,181,48,
              113,50,90,20,129,109,127,47,137,209,73,173,217,56,197,53,141,3,
              62,155,117,88,76,228,40,82,177,57,208,108,251,183,13,115,126,
              89,27,138,240,86,239,196,39,94,14,94,20,12,94,74,91,82,150,9,
              197,47,35,206,76,210,35,237,68,111,174,228,31,136,249,66,25,
              209,244,127,176,195,100,189,2,3,1,0,1,163,66,48,64,48,14,6,3,
              85,29,15,1,1,255,4,4,3,2,1,134,48,15,6,3,85,29,19,1,1,255,4,5,
              48,3,1,1,255,48,29,6,3,85,29,14,4,22,4,20,188,1,42,182,50,72,
              116,82,198,94,225,242,47,206,206,38,9,39,211,154,48,13,6,9,42,
              134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,7,122,181,155,55,73,
              115,117,88,215,178,67,9,164,79,41,143,213,122,120,183,161,158,
              179,24,159,107,244,185,214,40,78,200,59,208,49,4,2,152,24,193,
              11,28,199,253,134,219,80,153,22,107,94,33,144,139,251,88,92,
              185,69,22,78,200,112,130,3,234,35,142,100,178,218,128,56,66,64,
              147,226,231,103,70,132,16,120,107,97,39,6,241,133,244,226,72,
              227,53,189,39,13,56,28,212,121,112,93,92,197,98,43,69,209,137,
              1,127,228,110,207,204,246,60,244,73,57,87,206,37,77,194,32,24,
              120,98,49,146,58,191,127,2,164,235,7,154,139,200,136,193,5,110,
              232,107,27,181,32,75,241,254,5,68,41,54,188,174,154,212,192,
              249,224,20,3,106,196,82,133,216,183,147,240,201,0,75,88,247,
              105,174,187,93,204,210,55,56,242,96,140,116,69,94,95,19,60,172,
              138,198,232,182,175,206,36,240,180,239,163,144,92,150,142,84,
              114,53,180,171,234,41,126,157,228,150,39,182,108,165,58,62,101,
              129,114,231,144,128,135,156,44,194,78,139,142,141,176,149,8,31,
              25,249,73,108,167,49,3>>]},
  {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,
        10,118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,
        158,232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,
        66,211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,
        250,145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,
        104,159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,
        246,169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,
        110,167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,
        118,190,67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,
        74,8,205,174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,
        221,95,184,110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,
        76,187,66,211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,
        69,254,147,103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,
        202,133,173,72,6,69,167,89,112,174,40,229,171,2,1,2>>},
  {ciphers,[#{cipher => aes_256_gcm,key_exchange => any,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm_8,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_ecdsa,
              mac => aead,prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_rsa,
              mac => aead,prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_128_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf},
            #{cipher => aes_128_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf}]},
  {honor_cipher_order,true},
  {secure_renegotiate,true},
  {client_renegotiation,false},
  {password,"********"}],
 {ssl,true},
 {port,18091}]
[error_logger:info,2025-05-15T18:47:22.785Z,ns_1@cb.local:<0.2109.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2109.0>,menelaus_web}
    started: [{pid,<0.2112.0>},
              {id,menelaus_web_ipv4},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet},
                        {ssl,true},
                        {name,menelaus_web_ssl},
                        {ssl_opts_fun,#Fun<ns_ssl_services_setup.11.39330155>},
                        {port,18091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:47:22.788Z,ns_1@cb.local:<0.2109.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2025-05-15T18:47:22.788Z,ns_1@cb.local:<0.2103.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service capi_ssl_service
[ns_server:info,2025-05-15T18:47:22.789Z,ns_1@cb.local:<0.2109.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2025-05-15T18:47:22.789Z,ns_1@cb.local:<0.2109.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2025-05-15T18:47:22.789Z,ns_1@cb.local:<0.2109.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2025-05-15T18:47:22.789Z,ns_1@cb.local:memcached_config_mgr<0.704.0>:memcached_config_mgr:push_tls_config:237]Successfully pushed TLS config to memcached
[ns_server:info,2025-05-15T18:47:22.789Z,ns_1@cb.local:<0.2109.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2025-05-15T18:47:22.790Z,ns_1@cb.local:<0.2109.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"::"},
 [{keyfile,"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem"},
  {certfile,"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem"},
  {versions,['tlsv1.2','tlsv1.3']},
  {cacerts,[<<48,130,3,12,48,130,1,244,160,3,2,1,2,2,8,24,63,199,191,180,27,10,
              177,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,48,32,
              6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,101,114,
              118,101,114,32,49,52,54,49,99,99,50,50,48,30,23,13,49,51,48,49,
              48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,51,53,57,
              53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,117,99,104,98,
              97,115,101,32,83,101,114,118,101,114,32,49,52,54,49,99,99,50,50,
              48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,1,5,0,3,130,1,15,
              0,48,130,1,10,2,130,1,1,0,181,120,2,207,122,99,7,71,178,99,124,
              254,60,3,194,124,244,173,56,229,2,9,82,3,206,183,81,199,204,193,
              33,99,84,242,166,188,107,14,37,96,88,209,254,240,32,204,147,205,
              45,63,123,161,85,104,9,150,21,194,213,221,19,3,77,76,189,208,93,
              232,47,63,226,64,53,14,136,224,149,250,97,65,87,66,59,254,91,0,
              116,97,49,186,38,79,0,74,253,18,47,169,76,212,127,181,44,78,32,
              241,232,104,53,173,102,141,194,235,50,161,36,240,71,126,109,238,
              147,180,102,33,170,119,20,109,208,73,212,125,100,165,198,140,210,
              185,71,194,152,180,1,242,111,31,230,61,53,12,218,145,184,127,130,
              12,249,6,112,111,221,221,245,161,89,130,74,135,153,231,237,136,
              181,125,70,232,1,156,245,196,14,149,104,104,202,6,233,86,16,69,
              111,71,86,230,105,161,211,116,106,249,40,136,196,141,254,172,122,
              108,78,252,182,126,8,182,88,220,41,202,91,163,155,96,85,22,97,
              186,40,194,19,0,57,238,64,87,144,142,62,207,252,84,26,9,57,116,0,
              47,173,193,153,104,169,103,194,29,2,3,1,0,1,163,66,48,64,48,14,6,
              3,85,29,15,1,1,255,4,4,3,2,1,134,48,15,6,3,85,29,19,1,1,255,4,5,
              48,3,1,1,255,48,29,6,3,85,29,14,4,22,4,20,197,18,173,226,193,241,
              105,140,163,34,50,61,128,135,150,89,215,37,208,144,48,13,6,9,42,
              134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,26,225,231,0,217,118,14,
              29,57,113,249,186,176,21,106,214,129,105,172,105,111,87,138,154,
              56,94,191,209,103,130,18,48,137,158,179,148,24,195,213,203,137,
              49,145,82,189,69,7,8,202,85,205,192,230,21,61,35,22,75,67,182,
              222,215,159,17,57,66,131,43,49,33,238,11,249,32,157,128,15,148,
              89,192,30,239,133,216,181,147,123,109,25,188,184,206,174,31,156,
              91,27,236,254,110,113,28,32,72,48,13,152,101,138,146,182,79,72,
              46,205,1,91,50,143,226,51,117,20,113,109,46,246,61,157,43,14,204,
              7,253,73,198,19,234,127,131,141,44,230,60,185,139,136,236,213,87,
              10,80,32,254,169,231,241,155,193,145,255,244,60,35,164,87,65,41,
              173,37,98,26,217,243,211,181,172,244,80,39,175,148,102,124,187,
              49,169,81,217,58,84,217,38,11,118,142,9,6,20,24,204,189,93,103,
              187,215,0,101,236,40,186,4,21,106,168,193,83,29,52,82,106,30,74,
              125,41,250,77,5,118,196,217,211,201,19,214,171,213,81,174,93,190,
              173,157,199,251,234,252,74,62,46,85,78,135,95,193,68>>,
            <<48,130,3,12,48,130,1,244,160,3,2,1,2,2,8,24,63,199,191,156,197,
              82,17,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,48,
              32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,101,
              114,118,101,114,32,99,54,52,100,57,49,52,101,48,30,23,13,49,51,
              48,49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,51,
              53,57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,117,
              99,104,98,97,115,101,32,83,101,114,118,101,114,32,99,54,52,100,
              57,49,52,101,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,1,
              5,0,3,130,1,15,0,48,130,1,10,2,130,1,1,0,189,71,7,138,162,116,
              202,173,172,80,169,207,183,229,131,28,103,231,118,113,23,184,
              163,118,132,50,140,197,100,9,14,13,222,41,184,111,89,132,2,102,
              81,20,185,124,217,165,68,88,93,96,211,184,188,36,17,35,176,167,
              109,82,1,80,201,124,250,21,203,22,244,2,187,45,29,22,162,207,
              172,158,177,99,109,81,39,155,109,84,244,123,242,203,49,197,116,
              158,244,151,83,143,242,44,81,204,228,195,44,95,101,252,10,164,
              27,51,148,102,46,28,237,20,182,124,42,212,174,43,184,195,216,
              21,135,233,45,167,9,73,251,27,220,165,132,153,156,0,160,219,
              145,166,59,196,219,128,44,31,123,111,143,183,1,7,155,248,244,
              114,116,107,27,205,0,89,187,228,101,131,201,74,251,79,181,48,
              113,50,90,20,129,109,127,47,137,209,73,173,217,56,197,53,141,3,
              62,155,117,88,76,228,40,82,177,57,208,108,251,183,13,115,126,
              89,27,138,240,86,239,196,39,94,14,94,20,12,94,74,91,82,150,9,
              197,47,35,206,76,210,35,237,68,111,174,228,31,136,249,66,25,
              209,244,127,176,195,100,189,2,3,1,0,1,163,66,48,64,48,14,6,3,
              85,29,15,1,1,255,4,4,3,2,1,134,48,15,6,3,85,29,19,1,1,255,4,5,
              48,3,1,1,255,48,29,6,3,85,29,14,4,22,4,20,188,1,42,182,50,72,
              116,82,198,94,225,242,47,206,206,38,9,39,211,154,48,13,6,9,42,
              134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,7,122,181,155,55,73,
              115,117,88,215,178,67,9,164,79,41,143,213,122,120,183,161,158,
              179,24,159,107,244,185,214,40,78,200,59,208,49,4,2,152,24,193,
              11,28,199,253,134,219,80,153,22,107,94,33,144,139,251,88,92,
              185,69,22,78,200,112,130,3,234,35,142,100,178,218,128,56,66,64,
              147,226,231,103,70,132,16,120,107,97,39,6,241,133,244,226,72,
              227,53,189,39,13,56,28,212,121,112,93,92,197,98,43,69,209,137,
              1,127,228,110,207,204,246,60,244,73,57,87,206,37,77,194,32,24,
              120,98,49,146,58,191,127,2,164,235,7,154,139,200,136,193,5,110,
              232,107,27,181,32,75,241,254,5,68,41,54,188,174,154,212,192,
              249,224,20,3,106,196,82,133,216,183,147,240,201,0,75,88,247,
              105,174,187,93,204,210,55,56,242,96,140,116,69,94,95,19,60,172,
              138,198,232,182,175,206,36,240,180,239,163,144,92,150,142,84,
              114,53,180,171,234,41,126,157,228,150,39,182,108,165,58,62,101,
              129,114,231,144,128,135,156,44,194,78,139,142,141,176,149,8,31,
              25,249,73,108,167,49,3>>]},
  {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,
        10,118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,
        158,232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,
        66,211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,
        250,145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,
        104,159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,
        246,169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,
        110,167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,
        118,190,67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,
        74,8,205,174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,
        221,95,184,110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,
        76,187,66,211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,
        69,254,147,103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,
        202,133,173,72,6,69,167,89,112,174,40,229,171,2,1,2>>},
  {ciphers,[#{cipher => aes_256_gcm,key_exchange => any,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm_8,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_ecdsa,
              mac => aead,prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_rsa,
              mac => aead,prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_128_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf},
            #{cipher => aes_128_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf}]},
  {honor_cipher_order,true},
  {secure_renegotiate,true},
  {client_renegotiation,false},
  {password,"********"}],
 {ssl,true},
 {port,18091}]
[error_logger:info,2025-05-15T18:47:22.792Z,ns_1@cb.local:<0.2109.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2109.0>,menelaus_web}
    started: [{pid,<0.2132.0>},
              {id,menelaus_web_ipv6},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet6},
                        {ssl,true},
                        {name,menelaus_web_ssl},
                        {ssl_opts_fun,#Fun<ns_ssl_services_setup.11.39330155>},
                        {port,18091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:22.792Z,ns_1@cb.local:<0.328.0>:restartable:start_child:92]Started child process <0.2109.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
[ns_server:info,2025-05-15T18:47:22.792Z,ns_1@cb.local:<0.2108.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service ssl_service
[ns_server:info,2025-05-15T18:47:22.792Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:notify_services:1162]Succesfully notified services [ssl_service,server_cert_event,memcached,
                               client_cert_event,cb_dist_tls,capi_ssl_service]
[ns_server:debug,2025-05-15T18:47:22.793Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:restart_regenerate_client_cert_timer:1447]Time left before client cert regeneration: 70588800000
[ns_server:info,2025-05-15T18:47:22.793Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:handle_info:764]cert_and_pkey changed
[ns_server:info,2025-05-15T18:47:22.794Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:should_regenerate_certs:894]Should regenerate node_cert because CA or name in the certificate has changed
[ns_server:warn,2025-05-15T18:47:22.794Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:maybe_generate_certs:913]Node doesn't have private key, skipping node_cert generation
[ns_server:debug,2025-05-15T18:47:22.796Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:restart_regenerate_client_cert_timer:1447]Time left before client cert regeneration: 70588800000
[ns_server:debug,2025-05-15T18:47:22.796Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:maybe_store_ca_certs:832]Considering to store CA certs
[ns_server:debug,2025-05-15T18:47:22.796Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:restart_regenerate_client_cert_timer:1447]Time left before client cert regeneration: 70588800000
[ns_server:info,2025-05-15T18:47:22.797Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:validate_pkey:1032]Private key (node_cert) passphrase validation suceeded
[ns_server:info,2025-05-15T18:47:22.797Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:validate_pkey:1032]Private key (client_cert) passphrase validation suceeded
[ns_server:info,2025-05-15T18:47:22.797Z,ns_1@cb.local:<0.1852.0>:ns_cluster:engage_cluster_apply_certs:167]Generated certificate was loaded on the node before joining. Cert: <<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQCuy0dFqLPrJ6xY21RJ5ttVPR78ssxxXSe9JdTj/IsUczkwyxfZfwK\npBszlGYuHO0Utnwq1K4ruMPYFYfpLacJSfsb3KWEmZwAoNuRpjvE24AsH3tvj7cB\nB5v49HJ0axvNAFm75GWDyUr7T7UwcTJaFIFtfy+J0Umt2TjFNY0DPpt1WEzkKFKx\nOdBs+7cNc35ZG4rwVu/EJ14OXhQMXkpbUpYJxS8jzkzSI+1Eb67kH4j5QhnR9H+w\nw2S9AgMBAAGjQjBAMA4GA1UdDwEB/wQEAwIBhjAPBgNVHRMBAf8EBTADAQH/MB0G\nA1UdDgQWBBS8ASq2Mkh0UsZe4fIvzs4mCSfTmjANBgkqhkiG9w0BAQsFAAOCAQEA\nB3q1mzdJc3VY17JDCaRPKY/Veni3oZ6zGJ9r9LnWKE7IO9AxBAKYGMELHMf9httQ\nmRZrXiGQi/tYXLlFFk7IcIID6iOOZLLagDhCQJPi52dGhBB4a2EnBvGF9OJI4zW9\nJw04HNR5cF1cxWIrRdGJAX/kbs/M9jz0STlXziVNwiAYeGIxkjq/fwKk6weai8iI\nwQVu6GsbtSBL8f4FRCk2vK6a1MD54BQDasRShdi3k/DJAEtY92muu13M0jc48mCM\ndEVeXxM8rIrG6LavziTwtO+jkFyWjlRyNbSr6il+neSWJ7ZspTo+ZYFy55CAh5ws\nwk6Ljo2wlQgfGflJbKcxAw==\n-----END CERTIFICATE-----\n">>
[ns_server:info,2025-05-15T18:47:22.798Z,ns_1@cb.local:<0.1852.0>:ns_cluster:apply_net_config:251]Applying net config. AFamily: inet, AFamilyOnly: false, NEncryption: false, DistProtos: [{inet,
                                                                                          false}]ClientCert: false
[cluster:debug,2025-05-15T18:47:22.800Z,ns_1@cb.local:ns_cluster<0.273.0>:ns_cluster:handle_call:424]handling engage_cluster([{<<"requestedTargetNodeHostname">>,<<"db2.lan">>},
                         {<<"requestedServices">>,
                          [<<"index">>,<<"kv">>,<<"n1ql">>]},
                         {<<"isDeveloperPreview">>,false},
                         {<<"availableStorage">>,
                          {[{<<"hdd">>,
                             [{[{<<"path">>,<<"/">>},
                                {<<"sizeKBytes">>,1056557396},
                                {<<"usagePercent">>,2}]},
                              {[{<<"path">>,<<"/dev">>},
                                {<<"sizeKBytes">>,65536},
                                {<<"usagePercent">>,0}]},
                              {[{<<"path">>,<<"/dev/shm">>},
                                {<<"sizeKBytes">>,65536},
                                {<<"usagePercent">>,0}]},
                              {[{<<"path">>,<<"/etc/resolv.conf">>},
                                {<<"sizeKBytes">>,1056557396},
                                {<<"usagePercent">>,2}]},
                              {[{<<"path">>,<<"/etc/hostname">>},
                                {<<"sizeKBytes">>,1056557396},
                                {<<"usagePercent">>,2}]},
                              {[{<<"path">>,<<"/etc/hosts">>},
                                {<<"sizeKBytes">>,1056557396},
                                {<<"usagePercent">>,2}]},
                              {[{<<"path">>,<<"/opt/couchbase/var">>},
                                {<<"sizeKBytes">>,482797652},
                                {<<"usagePercent">>,92}]},
                              {[{<<"path">>,<<"/proc/kcore">>},
                                {<<"sizeKBytes">>,65536},
                                {<<"usagePercent">>,0}]},
                              {[{<<"path">>,<<"/proc/keys">>},
                                {<<"sizeKBytes">>,65536},
                                {<<"usagePercent">>,0}]},
                              {[{<<"path">>,<<"/proc/timer_list">>},
                                {<<"sizeKBytes">>,65536},
                                {<<"usagePercent">>,0}]},
                              {[{<<"path">>,<<"/proc/scsi">>},
                                {<<"sizeKBytes">>,4012680},
                                {<<"usagePercent">>,0}]},
                              {[{<<"path">>,<<"/sys/firmware">>},
                                {<<"sizeKBytes">>,4012680},
                                {<<"usagePercent">>,0}]}]}]}},
                         {<<"storageTotals">>,
                          {[{<<"ram">>,
                             {[{<<"total">>,8217968640},
                               {<<"quotaTotal">>,3221225472},
                               {<<"quotaUsed">>,0},
                               {<<"used">>,4251119616},
                               {<<"usedByData">>,0},
                               {<<"quotaUsedPerNode">>,0},
                               {<<"quotaTotalPerNode">>,3221225472}]}},
                            {<<"hdd">>,
                             {[{<<"total">>,494384795648},
                               {<<"quotaTotal">>,494384795648},
                               {<<"used">>,454834011996},
                               {<<"usedByData">>,0},
                               {<<"free">>,39550783652}]}}]}},
                         {<<"storage">>,
                          {[{<<"ssd">>,[]},
                            {<<"hdd">>,
                             [{[{<<"path">>,
                                 <<"/opt/couchbase/var/lib/couchbase/data">>},
                                {<<"index_path">>,
                                 <<"/opt/couchbase/var/lib/couchbase/data">>},
                                {<<"cbas_dirs">>,
                                 [<<"/opt/couchbase/var/lib/couchbase/data">>]},
                                {<<"eventing_path">>,
                                 <<"/opt/couchbase/var/lib/couchbase/data">>},
                                {<<"java_home">>,<<>>},
                                {<<"quotaMb">>,<<"none">>},
                                {<<"state">>,<<"ok">>}]}]}]}},
                         {<<"clusterMembership">>,<<"active">>},
                         {<<"recoveryType">>,<<"none">>},
                         {<<"status">>,<<"healthy">>},
                         {<<"otpNode">>,<<"ns_1@172.19.0.4">>},
                         {<<"thisNode">>,true},
                         {<<"hostname">>,<<"172.19.0.4:8091">>},
                         {<<"nodeUUID">>,
                          <<"28569ac00b9c1d7c50e39741027d428c">>},
                         {<<"clusterCompatibility">>,458758},
                         {<<"version">>,<<"7.6.2-3721-enterprise">>},
                         {<<"os">>,<<"aarch64-unknown-linux-gnu">>},
                         {<<"cpuCount">>,10},
                         {<<"ports">>,
                          {[{<<"direct">>,11210},
                            {<<"httpsMgmt">>,18091},
                            {<<"httpsCAPI">>,18092},
                            {<<"distTCP">>,21100},
                            {<<"distTLS">>,21150}]}},
                         {<<"services">>,[<<"index">>,<<"kv">>,<<"n1ql">>]},
                         {<<"nodeEncryption">>,false},
                         {<<"nodeEncryptionClientCertVerification">>,false},
                         {<<"addressFamilyOnly">>,false},
                         {<<"configuredHostname">>,<<"172.19.0.4:8091">>},
                         {<<"addressFamily">>,<<"inet">>},
                         {<<"externalListeners">>,
                          [{[{<<"afamily">>,<<"inet">>},
                             {<<"nodeEncryption">>,false}]}]},
                         {<<"serverGroup">>,<<"Group 1">>},
                         {<<"couchApiBase">>,<<"http://172.19.0.4:8092/">>},
                         {<<"couchApiBaseHTTPS">>,
                          <<"https://172.19.0.4:18092/">>},
                         {<<"nodeHash">>,10838665},
                         {<<"systemStats">>,
                          {[{<<"cpu_utilization_rate">>,3.89883035089008},
                            {<<"cpu_stolen_rate">>,0},
                            {<<"swap_total">>,1073737728},
                            {<<"swap_used">>,3956736},
                            {<<"mem_total">>,8217968640},
                            {<<"mem_free">>,4805804032},
                            {<<"mem_limit">>,8217968640},
                            {<<"cpu_cores_available">>,10},
                            {<<"allocstall">>,17}]}},
                         {<<"interestingStats">>,{[]}},
                         {<<"uptime">>,<<"41">>},
                         {<<"memoryTotal">>,8217968640},
                         {<<"memoryFree">>,4805804032},
                         {<<"mcdMemoryReserved">>,6269},
                         {<<"mcdMemoryAllocated">>,6269},
                         {<<"memoryQuota">>,3072},
                         {<<"queryMemoryQuota">>,0},
                         {<<"indexMemoryQuota">>,512},
                         {<<"ftsMemoryQuota">>,512},
                         {<<"cbasMemoryQuota">>,1024},
                         {<<"eventingMemoryQuota">>,256},
                         {<<"autogeneratedCA">>,
                          <<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQCuy0dFqLPrJ6xY21RJ5ttVPR78ssxxXSe9JdTj/IsUczkwyxfZfwK\npBszlGYuHO0Utnwq1K4ruMPYFYfpLacJSfsb3KWEmZwAoNuRpjvE24AsH3tvj7cB\nB5v49HJ0axvNAFm75GWDyUr7T7UwcTJaFIFtfy+J0Umt2TjFNY0DPpt1WEzkKFKx\nOdBs+7cNc35ZG4rwVu/EJ14OXhQMXkpbUpYJxS8jzkzSI+1Eb67kH4j5QhnR9H+w\nw2S9AgMBAAGjQjBAMA4GA1UdDwEB/wQEAwIBhjAPBgNVHRMBAf8EBTADAQH/MB0G\nA1UdDgQWBBS8ASq2Mkh0UsZe4fIvzs4mCSfTmjANBgkqhkiG9w0BAQsFAAOCAQEA\nB3q1mzdJc3VY17JDCaRPKY/Veni3oZ6zGJ9r9LnWKE7IO9AxBAKYGMELHMf9httQ\nmRZrXiGQi/tYXLlFFk7IcIID6iOOZLLagDhCQJPi52dGhBB4a2EnBvGF9OJI4zW9\nJw04HNR5cF1cxWIrRdGJAX/kbs/M9jz0STlXziVNwiAYeGIxkjq/fwKk6weai8iI\nwQVu6GsbtSBL8f4FRCk2vK6a1MD54BQDasRShdi3k/DJAEtY92muu13M0jc48mCM\ndEVeXxM8rIrG6LavziTwtO+jkFyWjlRyNbSr6il+neSWJ7ZspTo+ZYFy55CAh5ws\nwk6Ljo2wlQgfGflJbKcxAw==\n-----END CERTIFICATE-----\n">>},
                         {<<"autogeneratedClientCert">>,
                          <<"-----BEGIN CERTIFICATE-----\nMIIDWjCCAkKgAwIBAgIIGD/HyA7ClpEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjJaFw0y\nNzA4MTcxODQ3MjJaMDAxLjAsBgNVBAMTJUNvdWNoYmFzZSBJbnRlcm5hbCBDbGll\nbnQgKDExNjU1MDk1MikwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDg\nAaBAyJnrm54fDG0xlsEQH9E51WTLpkwtleiz980c498XKRPsgRWnjY920KTUIH2/\ndQcryFyvGTggbCNUUsAqs6qWzDX6iZW6uhcXUh9ujNASr1VNBMLPY60ejukP9Jbq\nxlTfAYvoidlCAZ43nBN17IbZjObU9wEL5AJVX+DLfxX9K0uTI37qWIX6lODZMnek\nj8dGQcmpjyB0ktQaFPLCfzIyXDoA0XE4zsxXW+qX4Xji5cwFwgUgJ3E1IYamZJZv\nF3WkWl7ai8xuos3gTqe41d4rvGemY5Pxao60ClgbgFPaimvOFT7Is2h4TzVQI8f7\nonIen4hIAiwE18Iz4WRJAgMBAAGjgYMwgYAwDgYDVR0PAQH/BAQDAgWgMBMGA1Ud\nJQQMMAoGCCsGAQUFBwMCMAwGA1UdEwEB/wQCMAAwHwYDVR0jBBgwFoAUvAEqtjJI\ndFLGXuHyL87OJgkn05owKgYDVR0RBCMwIYEfaW50ZXJuYWxAaW50ZXJuYWwuY291\nY2hiYXNlLmNvbTANBgkqhkiG9w0BAQsFAAOCAQEAjrpupGzgLn2L0Zf3xDsp7paD\nNHbJDPJeJAeG3u4nAur7tcZqAQbUgzkIPe9IFvz1dfdS9CRfyUYLzjcGK7M6Nzyt\nhgAwmdTOjAxTtcjVstxSdyhSxLORZnyMQcFpzJH+weMhVe5khi/T3nDWyR9yqwW0\nw9iReXBJhh/QyE+EBLMYJ58gXG5qQOHiOVw0ePQmQtoQhkjEM2jWVJegD+boNmOK\nKoCXNSxWZB2dC1MOQxtvA5ggysGIVOYH1ASMyXyZszSCRjKfGDzTYyJSfnBACijg\nWXt8NyZIaucN/aXR9H4CzrC077tJz9hihz0Ld4uhaIbVddLOY13TGq/V/0H6cg==\n-----END CERTIFICATE-----\n">>},
                         {<<"autogeneratedClientKey">>,<<"********">>},
                         {<<"autogeneratedCert">>,
                          <<"-----BEGIN CERTIFICATE-----\nMIIDOjCCAiKgAwIBAgIIGD/HyAMALgMwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjJaFw0y\nNzA4MTcxODQ3MjJaMCoxKDAmBgNVBAMTH0NvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nZGIyLmxhbikwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC63r7EIhIk\nOyMwE9mw7bzVJfstC9FfdZvk0bP8vxZkuhNt/bVcND8p00ihMP8FCbfsa5d8VgWC\nDoao/+R+8KpeXDZeXWTzOfGPz6ySJpDMUbPsj/pymSkYBvwsl0iBlPrBYDVlHe6C\nIFfKfnfHOsS9itHFANxppJ+spLeJP7Na1p/rOGFELp9Y0pYnDvAccES6DCwgzMKr\n4lk/1l1L4jJkyHzK0hQ5UutHe7jGBhOxRoIyTCPO5wf6miZJNlKxA/1zwCJuLaft\ny4N6DDBMIs9R/dS02i1IrhKLDvC/7OnK/BkXi4HKLKY/wE3YNBmgMNXEwxUMeqZk\n2IUILCZn4W9fAgMBAAGjajBoMA4GA1UdDwEB/wQEAwIFoDATBgNVHSUEDDAKBggr\nBgEFBQcDATAMBgNVHRMBAf8EAjAAMB8GA1UdIwQYMBaAFLwBKrYySHRSxl7h8i/O\nziYJJ9OaMBIGA1UdEQQLMAmCB2RiMi5sYW4wDQYJKoZIhvcNAQELBQADggEBADkD\n1qqpKjJY4cyeoPzdD5mW7I12+dWN/oW0yUQ4heVH9TZzgThchsucMQvFg3vpZ9j5\nSbxsH9h23M26Cv8PxOjpu8lmlYAtENDDGnagAiwDJuMtSeBkKw36xzLrB4jj8UZ+\n8k+5CLVUzncPt9J9o7Lgm+wgavlprqSMJLUGfAc+O0qGzN+mQk1UeM2a8++Ty97Q\neks+wPVQCPbprWQU7k7F8mSepoAWHglkhfkBjBXgHGvGMIMGDv68l7yfNBJ9yRoJ\nO5cAQgoNiHZDzZ9clPCCGeE+JQnxNQCMOIxq7xhnTrYgBeU9hWu0hBcrHR6ZwO+m\n6MNtblxGbkNee9HO7LY=\n-----END CERTIFICATE-----\n">>},
                         {<<"autogeneratedKey">>,<<"********">>}])
[cluster:debug,2025-05-15T18:47:22.802Z,ns_1@cb.local:ns_cluster<0.273.0>:ns_cluster:get_port_from_epmd:1093]port_please('ns_1@172.19.0.4', inet, false) = {ok,21100}
[ns_server:debug,2025-05-15T18:47:22.802Z,ns_1@cb.local:ns_cluster<0.273.0>:ns_cluster:check_host_port_connectivity:656]Successfully checked TCP connectivity to "172.19.0.4":21100
[cluster:info,2025-05-15T18:47:22.803Z,ns_1@cb.local:ns_cluster<0.273.0>:ns_cluster:do_change_address:715]Change of address to "db2.lan" is requested.
[ns_server:debug,2025-05-15T18:47:22.803Z,ns_1@cb.local:ns_node_disco<0.545.0>:ns_node_disco:maybe_monitor_rename_txn:201]Monitor node renaming transaction. Pid = <0.2159.0>, MRef = #Ref<0.3735524962.3650355207.241365>
[ns_server:debug,2025-05-15T18:47:22.803Z,ns_1@cb.local:remote_monitors<0.297.0>:remote_monitors:maybe_monitor_rename_txn:159]Monitor node renaming transaction. Pid = <0.2159.0>, MRef = #Ref<0.3735524962.3650355207.241372>
[ns_server:debug,2025-05-15T18:47:22.803Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Closing listener {external,inet_tcp_dist}
[ns_server:debug,2025-05-15T18:47:22.803Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Full list of processes expected to stop: [<0.231.0>]
[ns_server:debug,2025-05-15T18:47:22.803Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Down from <0.231.0>
[error_logger:info,2025-05-15T18:47:22.803Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,959,nodedown,'ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:47:22.804Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3735524962.3650355203.241140>,
                               inet_tcp_dist,<0.233.0>,
                               #Ref<0.3735524962.3650355203.241143>}
[ns_server:debug,2025-05-15T18:47:22.804Z,ns_1@cb.local:<0.516.0>:misc:delaying_crash:1810]Delaying crash exit:{{nodedown,'babysitter_of_ns_1@cb.local'},
                     {gen_server,call,
                         [{ns_babysitter_log,'babysitter_of_ns_1@cb.local'},
                          consume,infinity]}} by 1000ms
Stacktrace: [{gen_server,call,3,[{file,"gen_server.erl"},{line,385}]},
             {ns_log,babysitter_log_consumption_loop,0,
                     [{file,"src/ns_log.erl"},{line,66}]},
             {misc,delaying_crash,2,[{file,"src/misc.erl"},{line,1808}]},
             {proc_lib,init_p,3,[{file,"proc_lib.erl"},{line,225}]}]
[user:warn,2025-05-15T18:47:22.804Z,nonode@nohost:ns_node_disco<0.545.0>:ns_node_disco:handle_info:169]Node nonode@nohost saw that node 'ns_1@cb.local' went down. Details: [{nodedown_reason,
                                                                       net_kernel_terminated}]
[ns_server:debug,2025-05-15T18:47:22.804Z,nonode@nohost:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3735524962.3650355201.242438>,
                               inet_tcp_dist,<0.492.0>,
                               #Ref<0.3735524962.3650355201.242440>}
[chronicle:info,2025-05-15T18:47:22.804Z,nonode@nohost:chronicle_proposer<0.271.0>:chronicle_proposer:handle_nodedown:1135]Peer 'ns_1@cb.local' went down: [{nodedown_reason,net_kernel_terminated}]
[error_logger:error,2025-05-15T18:47:22.804Z,nonode@nohost:cb_dist<0.227.0>:ale_error_logger_handler:do_log:101]cb_dist: terminating with reason: shutdown
[ns_server:debug,2025-05-15T18:47:22.805Z,nonode@nohost:<0.2161.0>:dist_manager:teardown:315]Got nodedown msg {nodedown,'ns_1@cb.local',
                           [{nodedown_reason,net_kernel_terminated}]} after terminating net kernel
[ns_server:info,2025-05-15T18:47:22.805Z,nonode@nohost:<0.2159.0>:dist_manager:do_adjust_address:357]Adjusted IP to "db2.lan"
[ns_server:info,2025-05-15T18:47:22.805Z,nonode@nohost:<0.2159.0>:dist_manager:bringup:246]Attempting to bring up net_kernel with name 'ns_1@db2.lan'
[error_logger:info,2025-05-15T18:47:22.806Z,nonode@nohost:ssl_dist_admin_sup<0.2164.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_admin_sup}
    started: [{pid,<0.2165.0>},
              {id,ssl_pem_cache_dist},
              {mfargs,{ssl_pem_cache,start_link_dist,[[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:22.807Z,nonode@nohost:ssl_dist_admin_sup<0.2164.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_admin_sup}
    started: [{pid,<0.2166.0>},
              {id,ssl_dist_manager},
              {mfargs,{ssl_manager,start_link_dist,[[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:22.807Z,nonode@nohost:ssl_dist_sup<0.2163.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_sup}
    started: [{pid,<0.2164.0>},
              {id,ssl_dist_admin_sup},
              {mfargs,{ssl_dist_admin_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:22.807Z,nonode@nohost:tls_dist_sup<0.2167.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_sup}
    started: [{pid,<0.2168.0>},
              {id,dist_tls_connection_sup},
              {mfargs,{tls_connection_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:22.807Z,nonode@nohost:tls_dist_server_sup<0.2169.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_server_sup}
    started: [{pid,<0.2170.0>},
              {id,dist_ssl_listen_tracker_sup},
              {mfargs,{ssl_listen_tracker_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:22.807Z,nonode@nohost:tls_dist_server_sup<0.2169.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_server_sup}
    started: [{pid,<0.2171.0>},
              {id,dist_tls_server_session_ticket},
              {mfargs,{tls_server_session_ticket_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:22.808Z,nonode@nohost:tls_dist_server_sup<0.2169.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_server_sup}
    started: [{pid,<0.2172.0>},
              {id,dist_ssl_upgrade_server_session_cache_sup},
              {mfargs,
                  {ssl_upgrade_server_session_cache_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:22.808Z,nonode@nohost:tls_dist_sup<0.2167.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_sup}
    started: [{pid,<0.2169.0>},
              {id,tls_dist_server_sup},
              {mfargs,{tls_dist_server_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:22.808Z,nonode@nohost:ssl_dist_sup<0.2163.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_sup}
    started: [{pid,<0.2167.0>},
              {id,tls_dist_sup},
              {mfargs,{tls_dist_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:22.808Z,nonode@nohost:net_sup<0.2162.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.2163.0>},
              {id,ssl_dist_sup},
              {mfargs,{ssl_dist_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:47:22.808Z,nonode@nohost:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Starting cb_dist with config [{config_vsn,2}]
[error_logger:info,2025-05-15T18:47:22.810Z,nonode@nohost:net_sup<0.2162.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.2173.0>},
              {id,cb_dist},
              {mfargs,{cb_dist,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:22.810Z,nonode@nohost:net_sup<0.2162.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.2174.0>},
              {id,auth},
              {mfargs,{auth,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,2000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:22.813Z,nonode@nohost:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Initial protos: [{external,inet_tcp_dist}], required protos: [{external,
                                                                        inet_tcp_dist}]
[ns_server:debug,2025-05-15T18:47:22.813Z,nonode@nohost:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Starting {external,inet_tcp_dist} listener on 21100...
[ns_server:debug,2025-05-15T18:47:22.814Z,nonode@nohost:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Started listener: inet_tcp_dist
[chronicle:info,2025-05-15T18:47:22.814Z,ns_1@db2.lan:chronicle_proposer<0.271.0>:chronicle_proposer:handle_nodeup:1093]Peer 'ns_1@db2.lan' came up
[user:info,2025-05-15T18:47:22.814Z,ns_1@db2.lan:ns_node_disco<0.545.0>:ns_node_disco:handle_info:163]Node 'ns_1@db2.lan' saw that node 'ns_1@db2.lan' came up. Tags: []
[ns_server:debug,2025-05-15T18:47:22.814Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Updated server ssl_dist_opts (keep_secrets) - false
[chronicle:info,2025-05-15T18:47:22.814Z,ns_1@db2.lan:chronicle_proposer<0.271.0>:chronicle_proposer:handle_nodeup:1127]Peer 'ns_1@db2.lan' is not in peers: ['ns_1@cb.local']
[ns_server:debug,2025-05-15T18:47:22.814Z,ns_1@db2.lan:<0.454.0>:doc_replicator:nodeup_monitoring_loop:145]got nodeup event. Considering ddocs replication
[ns_server:debug,2025-05-15T18:47:22.814Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Started acceptor inet_tcp_dist: <0.2176.0>
[ns_server:debug,2025-05-15T18:47:22.814Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Ensure config is going to change listeners. Will be stopped: [], will be started: [], will be restarted: []
[chronicle:debug,2025-05-15T18:47:22.814Z,ns_1@db2.lan:chronicle_leader<0.256.0>:chronicle_leader:handle_note_term_status:603]Ignoring stale term status {<<"132dcb91b709843f36fcbbab0f42c041">>,
                            {2,'ns_1@cb.local'},
                            finished}: {error,{not_a_leader,follower}}
[chronicle:info,2025-05-15T18:47:22.814Z,ns_1@db2.lan:chronicle_proposer<0.271.0>:chronicle_proposer:handle_stop:1269]Proposer for term {2,'ns_1@cb.local'} in history <<"132dcb91b709843f36fcbbab0f42c041">> is terminating.
[chronicle:debug,2025-05-15T18:47:22.814Z,ns_1@db2.lan:chronicle_agent<0.252.0>:chronicle_agent:handle_local_mark_committed:1997]Marked seqno 9 committed
[error_logger:info,2025-05-15T18:47:22.814Z,ns_1@db2.lan:net_sup<0.2162.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.2175.0>},
              {id,net_kernel},
              {mfargs,{net_kernel,start_link,
                                  [#{clean_halt => false,
                                     name => 'ns_1@db2.lan',
                                     name_domain => longnames,
                                     net_tickintensity => 4,
                                     net_ticktime => 60,
                                     supervisor => net_sup_dynamic}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,2000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:22.814Z,ns_1@db2.lan:net_kernel<0.2175.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'ns_1@cb.local' using inet_tcp_dist
[chronicle:info,2025-05-15T18:47:22.814Z,ns_1@db2.lan:chronicle_server<0.260.0>:chronicle_proposer:stop:136]Proposer <0.271.0> stopped: {shutdown,stop}
[ns_server:debug,2025-05-15T18:47:22.814Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3735524962.3650355207.241408>,
                               inet_tcp_dist,undefined,undefined}
[error_logger:info,2025-05-15T18:47:22.814Z,ns_1@db2.lan:kernel_sup<0.49.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,kernel_sup}
    started: [{pid,<0.2162.0>},
              {id,net_sup_dynamic},
              {mfargs,
                  {erl_distribution,start_link,
                      [#{clean_halt => false,name => 'ns_1@db2.lan',
                         name_domain => longnames,net_tickintensity => 4,
                         net_ticktime => 60,supervisor => net_sup_dynamic}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,60000},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:47:22.815Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3735524962.3650355207.241408>,
                                  inet_tcp_dist,<0.2180.0>,
                                  #Ref<0.3735524962.3650355201.245714>}
[ns_server:debug,2025-05-15T18:47:22.815Z,ns_1@db2.lan:<0.2159.0>:dist_manager:configure_net_kernel:302]Set net_kernel vebosity to 10 -> 0
[ns_server:debug,2025-05-15T18:47:22.815Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Accepted new connection from <0.2176.0> DistCtrl #Port<0.81>: {con,
                                                                        #Ref<0.3735524962.3650355205.242852>,
                                                                        inet_tcp_dist,
                                                                        undefined,
                                                                        undefined}
[ns_server:debug,2025-05-15T18:47:22.815Z,ns_1@db2.lan:net_kernel<0.2175.0>:cb_dist:info_msg:1098]cb_dist: Accepting connection from <0.2176.0> using module inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:22.815Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3735524962.3650355205.242852>,
                                  inet_tcp_dist,<0.2182.0>,
                                  #Ref<0.3735524962.3650355204.243012>}
[error_logger:error,2025-05-15T18:47:22.816Z,ns_1@db2.lan:net_kernel<0.2175.0>:ale_error_logger_handler:do_log:101]
=========================ERROR REPORT=========================

** Cannot get connection id for node 'ns_1@db2.lan'

[ns_server:debug,2025-05-15T18:47:22.816Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Accepted new connection from <0.2176.0> DistCtrl #Port<0.82>: {con,
                                                                        #Ref<0.3735524962.3650355208.241792>,
                                                                        inet_tcp_dist,
                                                                        undefined,
                                                                        undefined}
[error_logger:info,2025-05-15T18:47:22.816Z,ns_1@db2.lan:net_kernel<0.2175.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.2182.0>,shutdown}}
[ns_server:debug,2025-05-15T18:47:22.816Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3735524962.3650355205.242852>,
                               inet_tcp_dist,<0.2182.0>,
                               #Ref<0.3735524962.3650355204.243012>}
[ns_server:debug,2025-05-15T18:47:22.816Z,ns_1@db2.lan:net_kernel<0.2175.0>:cb_dist:info_msg:1098]cb_dist: Accepting connection from <0.2176.0> using module inet_tcp_dist
[ns_server:info,2025-05-15T18:47:22.816Z,ns_1@db2.lan:<0.2159.0>:dist_manager:save_node:160]saving node name '"ns_1@db2.lan"' to "/opt/couchbase/var/lib/couchbase/couchbase-server.node"
[ns_server:debug,2025-05-15T18:47:22.816Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3735524962.3650355208.241792>,
                                  inet_tcp_dist,<0.2184.0>,
                                  #Ref<0.3735524962.3650355206.242435>}
[ns_server:debug,2025-05-15T18:47:22.816Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3735524962.3650355208.241792>,
                               inet_tcp_dist,<0.2184.0>,
                               #Ref<0.3735524962.3650355206.242435>}
[error_logger:info,2025-05-15T18:47:22.816Z,ns_1@db2.lan:net_kernel<0.2175.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.2184.0>,{recv_challenge_reply_failed,{error,closed}}}}
[ns_server:debug,2025-05-15T18:47:22.818Z,ns_1@db2.lan:<0.2159.0>:dist_manager:bringup:269]Attempted to save node name to disk: ok
[ns_server:debug,2025-05-15T18:47:22.818Z,ns_1@db2.lan:<0.2159.0>:dist_manager:wait_for_node:276]Waiting for connection to node 'babysitter_of_ns_1@cb.local' to be established
[error_logger:info,2025-05-15T18:47:22.818Z,ns_1@db2.lan:net_kernel<0.2175.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{connect,normal,'babysitter_of_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:47:22.818Z,ns_1@db2.lan:net_kernel<0.2175.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'babysitter_of_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:22.818Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3735524962.3650355210.241132>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:47:22.818Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3735524962.3650355210.241132>,
                                  inet_tcp_dist,<0.2186.0>,
                                  #Ref<0.3735524962.3650355210.241135>}
[ns_server:debug,2025-05-15T18:47:22.820Z,ns_1@db2.lan:<0.2159.0>:dist_manager:wait_for_node:288]Observed node 'babysitter_of_ns_1@cb.local' to come up
[ns_server:info,2025-05-15T18:47:22.820Z,ns_1@db2.lan:<0.2159.0>:dist_manager:do_adjust_address:361]Re-setting cookie {{sanitized,<<"laexClf6j/iiXLRHc+IgLQ5C2ku89YxpvAez8UPpQjA=">>},
                   'ns_1@db2.lan'}
[ns_server:info,2025-05-15T18:47:22.822Z,ns_1@db2.lan:<0.2159.0>:dist_manager:save_address_config:147]Deleting irrelevant ip file "/opt/couchbase/var/lib/couchbase/ip": ok
[ns_server:info,2025-05-15T18:47:22.822Z,ns_1@db2.lan:<0.2159.0>:dist_manager:save_address_config:148]saving ip config to "/opt/couchbase/var/lib/couchbase/ip_start"
[ns_server:info,2025-05-15T18:47:22.823Z,ns_1@db2.lan:<0.2159.0>:dist_manager:save_address_config:151]Persisted the address successfully
[ns_server:debug,2025-05-15T18:47:22.823Z,ns_1@db2.lan:<0.2159.0>:dist_manager:rename_node_in_configs:442]Renaming node from 'ns_1@cb.local' to 'ns_1@db2.lan' in config
[ns_server:debug,2025-05-15T18:47:22.823Z,ns_1@db2.lan:chronicle_local<0.239.0>:chronicle_local:handle_rename:177]Handle renaming from 'ns_1@cb.local' to 'ns_1@db2.lan'
[chronicle:debug,2025-05-15T18:47:22.824Z,ns_1@db2.lan:chronicle_agent<0.252.0>:chronicle_agent:handle_reprovision:1140]Reprovisioning peer with config:
{log_entry,<<"132dcb91b709843f36fcbbab0f42c041">>,
           {3,'ns_1@db2.lan'},
           10,
           {config,undefined,0,undefined,
                   #{'ns_1@db2.lan' =>
                         #{id => <<"9a5e5be0711d2936f646a0892bd096e1">>,
                           role => voter}},
                   undefined,
                   #{chronicle_config_rsm =>
                         {rsm_config,chronicle_config_rsm,[]},
                     kv => {rsm_config,chronicle_kv,[]}},
                   #{},undefined,
                   [{<<"132dcb91b709843f36fcbbab0f42c041">>,0}]}}
[chronicle:info,2025-05-15T18:47:22.824Z,ns_1@db2.lan:chronicle_leader<0.256.0>:chronicle_leader:handle_reprovisioned:498]System reprovisioned.
[chronicle:info,2025-05-15T18:47:22.824Z,ns_1@db2.lan:<0.2189.0>:chronicle_leader:do_election_worker:892]Starting election.
History ID: <<"132dcb91b709843f36fcbbab0f42c041">>
Log position: {{3,'ns_1@db2.lan'},10}
Peers: ['ns_1@db2.lan']
Required quorum: {majority,{set,1,16,16,8,80,48,
                                {[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],
                                 []},
                                {{[],[],[],[],[],[],[],[],[],[],[],[],[],[],
                                  ['ns_1@db2.lan'],
                                  []}}}}
[chronicle:info,2025-05-15T18:47:22.824Z,ns_1@db2.lan:<0.2189.0>:chronicle_leader:do_election_worker:901]I'm the only peer, so I'm the leader.
[chronicle:info,2025-05-15T18:47:22.824Z,ns_1@db2.lan:chronicle_leader<0.256.0>:chronicle_leader:handle_election_result:691]Going to become a leader in term {4,'ns_1@db2.lan'} (history id <<"132dcb91b709843f36fcbbab0f42c041">>)
[chronicle:debug,2025-05-15T18:47:22.825Z,ns_1@db2.lan:chronicle_agent<0.252.0>:chronicle_agent:handle_establish_term:1529]Accepted term {4,'ns_1@db2.lan'} in history <<"132dcb91b709843f36fcbbab0f42c041">>
[chronicle:debug,2025-05-15T18:47:22.825Z,ns_1@db2.lan:chronicle_proposer<0.2190.0>:chronicle_proposer:establish_term_init:367]Going to establish term {4,'ns_1@db2.lan'} (history id <<"132dcb91b709843f36fcbbab0f42c041">>).
Quorum peers: ['ns_1@db2.lan']
Metadata:
{metadata,'ns_1@db2.lan',<<"9a5e5be0711d2936f646a0892bd096e1">>,
          <<"132dcb91b709843f36fcbbab0f42c041">>,
          {3,'ns_1@db2.lan'},
          {3,'ns_1@db2.lan'},
          10,10,
          {log_entry,<<"132dcb91b709843f36fcbbab0f42c041">>,
                     {3,'ns_1@db2.lan'},
                     10,
                     {config,undefined,0,undefined,
                             #{'ns_1@db2.lan' =>
                                   #{id =>
                                         <<"9a5e5be0711d2936f646a0892bd096e1">>,
                                     role => voter}},
                             undefined,
                             #{chronicle_config_rsm =>
                                   {rsm_config,chronicle_config_rsm,[]},
                               kv => {rsm_config,chronicle_kv,[]}},
                             #{},undefined,
                             [{<<"132dcb91b709843f36fcbbab0f42c041">>,0}]}},
          {log_entry,<<"132dcb91b709843f36fcbbab0f42c041">>,
                     {3,'ns_1@db2.lan'},
                     10,
                     {config,undefined,0,undefined,
                             #{'ns_1@db2.lan' =>
                                   #{id =>
                                         <<"9a5e5be0711d2936f646a0892bd096e1">>,
                                     role => voter}},
                             undefined,
                             #{chronicle_config_rsm =>
                                   {rsm_config,chronicle_config_rsm,[]},
                               kv => {rsm_config,chronicle_kv,[]}},
                             #{},undefined,
                             [{<<"132dcb91b709843f36fcbbab0f42c041">>,0}]}},
          undefined}
[chronicle:debug,2025-05-15T18:47:22.825Z,ns_1@db2.lan:chronicle_proposer<0.2190.0>:chronicle_proposer:establish_term_maybe_transition:686]Established term {4,'ns_1@db2.lan'} (history id <<"132dcb91b709843f36fcbbab0f42c041">>) successfully.
Votes: ['ns_1@db2.lan']
[chronicle:debug,2025-05-15T18:47:22.825Z,ns_1@db2.lan:chronicle_proposer<0.2190.0>:chronicle_proposer:handle_state_enter:284]Starting recovery for term {4,'ns_1@db2.lan'} in history <<"132dcb91b709843f36fcbbab0f42c041">>
[chronicle:debug,2025-05-15T18:47:22.825Z,ns_1@db2.lan:chronicle_proposer<0.2190.0>:chronicle_proposer:handle_state_enter:296]Proposer for term {4,'ns_1@db2.lan'} in history <<"132dcb91b709843f36fcbbab0f42c041">> is ready. Committed seqno: 11
[chronicle:info,2025-05-15T18:47:22.825Z,ns_1@db2.lan:chronicle_leader<0.256.0>:chronicle_leader:handle_note_term_status:596]Term {4,'ns_1@db2.lan'} established.
[ns_server:debug,2025-05-15T18:47:22.847Z,ns_1@db2.lan:chronicle_kv_log<0.504.0>:chronicle_kv_log:handle_info:42]delete (key: {node,'ns_1@cb.local',membership}, rev: {<<"132dcb91b709843f36fcbbab0f42c041">>,
                                                      12})
[ns_server:debug,2025-05-15T18:47:22.847Z,ns_1@db2.lan:chronicle_kv_log<0.504.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@db2.lan',membership}, rev: {<<"132dcb91b709843f36fcbbab0f42c041">>,
                                                     12})
active
[ns_server:debug,2025-05-15T18:47:22.847Z,ns_1@db2.lan:chronicle_kv_log<0.504.0>:chronicle_kv_log:log:59]update (key: server_groups, rev: {<<"132dcb91b709843f36fcbbab0f42c041">>,12})
[[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@db2.lan']}]]
[ns_server:debug,2025-05-15T18:47:22.847Z,ns_1@db2.lan:mb_master<0.774.0>:mb_master:update_peers:543]List of peers has changed from ['ns_1@cb.local'] to ['ns_1@db2.lan']
[ns_server:debug,2025-05-15T18:47:22.848Z,ns_1@db2.lan:chronicle_kv_log<0.504.0>:chronicle_kv_log:log:59]update (key: nodes_wanted, rev: {<<"132dcb91b709843f36fcbbab0f42c041">>,12})
['ns_1@db2.lan']
[ns_server:debug,2025-05-15T18:47:22.850Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',client_cert} -> {node,
                                                          'ns_1@db2.lan',
                                                          client_cert}:
  [{subject,<<"CN=Couchbase Internal Client (116550952)">>},
   {not_after,63985747642},
   {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
   {load_timestamp,63914554042},
   {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQCuy0dFqLPrJ6xY21RJ5ttVPR78ssxxXSe9JdTj/IsUczkwyxfZfwK\npBszlGYuHO0Utnwq1K4ruMPYFYfpLacJSfsb3KWEmZwAoNuRpjvE24AsH3tvj7cB\nB5v49HJ0axvNAFm75GWDyUr7T7UwcTJaFIFtfy+J0Umt2TjFNY0DPpt1WEzkKFKx\nOdBs+7cNc35ZG4rwVu/EJ14OXhQMXkpbUpYJxS8jzkzSI+1Eb67kH4j5QhnR9H+w\nw2S9AgMBAAGjQjBAMA4GA1UdDwEB/wQEAwIBhjAPBgNVHRMBAf8EBTADAQH/MB0G\nA1UdDgQWBBS8ASq2Mkh0UsZe4fIvzs4mCSfTmjANBgkqhkiG9w0BAQsFAAOCAQEA\nB3q1mzdJc3VY17JDCaRPKY/Veni3oZ6zGJ9r9LnWKE7IO9AxBAKYGMELHMf9httQ\nmRZrXiGQi/tYXLlFFk7IcIID6iOOZLLagDhCQJPi52dGhBB4a2EnBvGF9OJI4zW9\nJw04HNR5cF1cxWIrRdGJAX/kbs/M9jz0STlXziVNwiAYeGIxkjq/fwKk6weai8iI\nwQVu6GsbtSBL8f4FRCk2vK6a1MD54BQDasRShdi3k/DJAEtY92muu13M0jc48mCM\ndEVeXxM8rIrG6LavziTwtO+jkFyWjlRyNbSr6il+neSWJ7ZspTo+ZYFy55CAh5ws\nwk6Ljo2wlQgfGflJbKcxAw==\n-----END CERTIFICATE-----\n">>},
   {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDWjCCAkKgAwIBAgIIGD/HyA7ClpEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjJaFw0y\nNzA4MTcxODQ3MjJaMDAxLjAsBgNVBAMTJUNvdWNoYmFzZSBJbnRlcm5hbCBDbGll\nbnQgKDExNjU1MDk1MikwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDg\nAaBAyJnrm54fDG0xlsEQH9E51WTLpkwtleiz980c498XKRPsgRWnjY920KTUIH2/\ndQcryFyvGTggbCNUUsAqs6qWzDX6iZW6uhcXUh9ujNASr1VNBMLPY60ejukP9Jbq\nxlTfAYvoidlCAZ43nBN17IbZjObU9wEL5AJVX+DLfxX9K0uTI37qWIX6lODZMnek\nj8dGQcmpjyB0ktQaFPLCfzIyXDoA0XE4zsxXW+qX4Xji5cwFwgUgJ3E1IYamZJZv\nF3WkWl7ai8xuos3gTqe41d4rvGemY5Pxao60ClgbgFPaimvOFT7Is2h4TzVQI8f7\nonIen4hIAiwE18Iz4WRJAgMBAAGjgYMwgYAwDgYDVR0PAQH/BAQDAgWgMBMGA1Ud\nJQQMMAoGCCsGAQUFBwMCMAwGA1UdEwEB/wQCMAAwHwYDVR0jBBgwFoAUvAEqtjJI\ndFLGXuHyL87OJgkn05owKgYDVR0RBCMwIYEfaW50ZXJuYWxAaW50ZXJuYWwuY291\nY2hiYXNlLmNvbTANBgkqhkiG9w0BAQsFAAOCAQEAjrpupGzgLn2L0Zf3xDsp7paD\nNHbJDPJeJAeG3u4nAur7tcZqAQbUgzkIPe9IFvz1dfdS9CRfyUYLzjcGK7M6Nzyt\nhgAwmdTOjAxTtcjVstxSdyhSxLORZnyMQcFpzJH+weMhVe5khi/T3nDWyR9yqwW0\nw9iReXBJhh/QyE+EBLMYJ58gXG5qQOHiOVw0ePQmQtoQhkjEM2jWVJegD+boNmOK\nKoCXNSxWZB2dC1MOQxtvA5ggysGIVOYH1ASMyXyZszSCRjKfGDzTYyJSfnBACijg\nWXt8NyZIaucN/aXR9H4CzrC077tJz9hihz0Ld4uhaIbVddLOY13TGq/V/0H6cg==\n-----END CERTIFICATE-----\n">>},
   {pkey_passphrase_settings,[]},
   {certs_epoch,0},
   {type,generated},
   {name,"@internal"}] ->
  [{subject,<<"CN=Couchbase Internal Client (116550952)">>},
   {not_after,63985747642},
   {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
   {load_timestamp,63914554042},
   {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQCuy0dFqLPrJ6xY21RJ5ttVPR78ssxxXSe9JdTj/IsUczkwyxfZfwK\npBszlGYuHO0Utnwq1K4ruMPYFYfpLacJSfsb3KWEmZwAoNuRpjvE24AsH3tvj7cB\nB5v49HJ0axvNAFm75GWDyUr7T7UwcTJaFIFtfy+J0Umt2TjFNY0DPpt1WEzkKFKx\nOdBs+7cNc35ZG4rwVu/EJ14OXhQMXkpbUpYJxS8jzkzSI+1Eb67kH4j5QhnR9H+w\nw2S9AgMBAAGjQjBAMA4GA1UdDwEB/wQEAwIBhjAPBgNVHRMBAf8EBTADAQH/MB0G\nA1UdDgQWBBS8ASq2Mkh0UsZe4fIvzs4mCSfTmjANBgkqhkiG9w0BAQsFAAOCAQEA\nB3q1mzdJc3VY17JDCaRPKY/Veni3oZ6zGJ9r9LnWKE7IO9AxBAKYGMELHMf9httQ\nmRZrXiGQi/tYXLlFFk7IcIID6iOOZLLagDhCQJPi52dGhBB4a2EnBvGF9OJI4zW9\nJw04HNR5cF1cxWIrRdGJAX/kbs/M9jz0STlXziVNwiAYeGIxkjq/fwKk6weai8iI\nwQVu6GsbtSBL8f4FRCk2vK6a1MD54BQDasRShdi3k/DJAEtY92muu13M0jc48mCM\ndEVeXxM8rIrG6LavziTwtO+jkFyWjlRyNbSr6il+neSWJ7ZspTo+ZYFy55CAh5ws\nwk6Ljo2wlQgfGflJbKcxAw==\n-----END CERTIFICATE-----\n">>},
   {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDWjCCAkKgAwIBAgIIGD/HyA7ClpEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjJaFw0y\nNzA4MTcxODQ3MjJaMDAxLjAsBgNVBAMTJUNvdWNoYmFzZSBJbnRlcm5hbCBDbGll\nbnQgKDExNjU1MDk1MikwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDg\nAaBAyJnrm54fDG0xlsEQH9E51WTLpkwtleiz980c498XKRPsgRWnjY920KTUIH2/\ndQcryFyvGTggbCNUUsAqs6qWzDX6iZW6uhcXUh9ujNASr1VNBMLPY60ejukP9Jbq\nxlTfAYvoidlCAZ43nBN17IbZjObU9wEL5AJVX+DLfxX9K0uTI37qWIX6lODZMnek\nj8dGQcmpjyB0ktQaFPLCfzIyXDoA0XE4zsxXW+qX4Xji5cwFwgUgJ3E1IYamZJZv\nF3WkWl7ai8xuos3gTqe41d4rvGemY5Pxao60ClgbgFPaimvOFT7Is2h4TzVQI8f7\nonIen4hIAiwE18Iz4WRJAgMBAAGjgYMwgYAwDgYDVR0PAQH/BAQDAgWgMBMGA1Ud\nJQQMMAoGCCsGAQUFBwMCMAwGA1UdEwEB/wQCMAAwHwYDVR0jBBgwFoAUvAEqtjJI\ndFLGXuHyL87OJgkn05owKgYDVR0RBCMwIYEfaW50ZXJuYWxAaW50ZXJuYWwuY291\nY2hiYXNlLmNvbTANBgkqhkiG9w0BAQsFAAOCAQEAjrpupGzgLn2L0Zf3xDsp7paD\nNHbJDPJeJAeG3u4nAur7tcZqAQbUgzkIPe9IFvz1dfdS9CRfyUYLzjcGK7M6Nzyt\nhgAwmdTOjAxTtcjVstxSdyhSxLORZnyMQcFpzJH+weMhVe5khi/T3nDWyR9yqwW0\nw9iReXBJhh/QyE+EBLMYJ58gXG5qQOHiOVw0ePQmQtoQhkjEM2jWVJegD+boNmOK\nKoCXNSxWZB2dC1MOQxtvA5ggysGIVOYH1ASMyXyZszSCRjKfGDzTYyJSfnBACijg\nWXt8NyZIaucN/aXR9H4CzrC077tJz9hihz0Ld4uhaIbVddLOY13TGq/V/0H6cg==\n-----END CERTIFICATE-----\n">>},
   {pkey_passphrase_settings,[]},
   {certs_epoch,0},
   {type,generated},
   {name,"@internal"}]
[ns_server:debug,2025-05-15T18:47:22.852Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',node_cert} -> {node,'ns_1@db2.lan',
                                                        node_cert}:
  [{subject,<<"CN=Couchbase Server Node (db2.lan)">>},
   {not_after,63985747642},
   {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
   {load_timestamp,63914554042},
   {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQCuy0dFqLPrJ6xY21RJ5ttVPR78ssxxXSe9JdTj/IsUczkwyxfZfwK\npBszlGYuHO0Utnwq1K4ruMPYFYfpLacJSfsb3KWEmZwAoNuRpjvE24AsH3tvj7cB\nB5v49HJ0axvNAFm75GWDyUr7T7UwcTJaFIFtfy+J0Umt2TjFNY0DPpt1WEzkKFKx\nOdBs+7cNc35ZG4rwVu/EJ14OXhQMXkpbUpYJxS8jzkzSI+1Eb67kH4j5QhnR9H+w\nw2S9AgMBAAGjQjBAMA4GA1UdDwEB/wQEAwIBhjAPBgNVHRMBAf8EBTADAQH/MB0G\nA1UdDgQWBBS8ASq2Mkh0UsZe4fIvzs4mCSfTmjANBgkqhkiG9w0BAQsFAAOCAQEA\nB3q1mzdJc3VY17JDCaRPKY/Veni3oZ6zGJ9r9LnWKE7IO9AxBAKYGMELHMf9httQ\nmRZrXiGQi/tYXLlFFk7IcIID6iOOZLLagDhCQJPi52dGhBB4a2EnBvGF9OJI4zW9\nJw04HNR5cF1cxWIrRdGJAX/kbs/M9jz0STlXziVNwiAYeGIxkjq/fwKk6weai8iI\nwQVu6GsbtSBL8f4FRCk2vK6a1MD54BQDasRShdi3k/DJAEtY92muu13M0jc48mCM\ndEVeXxM8rIrG6LavziTwtO+jkFyWjlRyNbSr6il+neSWJ7ZspTo+ZYFy55CAh5ws\nwk6Ljo2wlQgfGflJbKcxAw==\n-----END CERTIFICATE-----\n">>},
   {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDOjCCAiKgAwIBAgIIGD/HyAMALgMwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjJaFw0y\nNzA4MTcxODQ3MjJaMCoxKDAmBgNVBAMTH0NvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nZGIyLmxhbikwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC63r7EIhIk\nOyMwE9mw7bzVJfstC9FfdZvk0bP8vxZkuhNt/bVcND8p00ihMP8FCbfsa5d8VgWC\nDoao/+R+8KpeXDZeXWTzOfGPz6ySJpDMUbPsj/pymSkYBvwsl0iBlPrBYDVlHe6C\nIFfKfnfHOsS9itHFANxppJ+spLeJP7Na1p/rOGFELp9Y0pYnDvAccES6DCwgzMKr\n4lk/1l1L4jJkyHzK0hQ5UutHe7jGBhOxRoIyTCPO5wf6miZJNlKxA/1zwCJuLaft\ny4N6DDBMIs9R/dS02i1IrhKLDvC/7OnK/BkXi4HKLKY/wE3YNBmgMNXEwxUMeqZk\n2IUILCZn4W9fAgMBAAGjajBoMA4GA1UdDwEB/wQEAwIFoDATBgNVHSUEDDAKBggr\nBgEFBQcDATAMBgNVHRMBAf8EAjAAMB8GA1UdIwQYMBaAFLwBKrYySHRSxl7h8i/O\nziYJJ9OaMBIGA1UdEQQLMAmCB2RiMi5sYW4wDQYJKoZIhvcNAQELBQADggEBADkD\n1qqpKjJY4cyeoPzdD5mW7I12+dWN/oW0yUQ4heVH9TZzgThchsucMQvFg3vpZ9j5\nSbxsH9h23M26Cv8PxOjpu8lmlYAtENDDGnagAiwDJuMtSeBkKw36xzLrB4jj8UZ+\n8k+5CLVUzncPt9J9o7Lgm+wgavlprqSMJLUGfAc+O0qGzN+mQk1UeM2a8++Ty97Q\neks+wPVQCPbprWQU7k7F8mSepoAWHglkhfkBjBXgHGvGMIMGDv68l7yfNBJ9yRoJ\nO5cAQgoNiHZDzZ9clPCCGeE+JQnxNQCMOIxq7xhnTrYgBeU9hWu0hBcrHR6ZwO+m\n6MNtblxGbkNee9HO7LY=\n-----END CERTIFICATE-----\n">>},
   {pkey_passphrase_settings,[]},
   {certs_epoch,0},
   {type,generated},
   {hostname,"db2.lan"}] ->
  [{subject,<<"CN=Couchbase Server Node (db2.lan)">>},
   {not_after,63985747642},
   {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
   {load_timestamp,63914554042},
   {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQCuy0dFqLPrJ6xY21RJ5ttVPR78ssxxXSe9JdTj/IsUczkwyxfZfwK\npBszlGYuHO0Utnwq1K4ruMPYFYfpLacJSfsb3KWEmZwAoNuRpjvE24AsH3tvj7cB\nB5v49HJ0axvNAFm75GWDyUr7T7UwcTJaFIFtfy+J0Umt2TjFNY0DPpt1WEzkKFKx\nOdBs+7cNc35ZG4rwVu/EJ14OXhQMXkpbUpYJxS8jzkzSI+1Eb67kH4j5QhnR9H+w\nw2S9AgMBAAGjQjBAMA4GA1UdDwEB/wQEAwIBhjAPBgNVHRMBAf8EBTADAQH/MB0G\nA1UdDgQWBBS8ASq2Mkh0UsZe4fIvzs4mCSfTmjANBgkqhkiG9w0BAQsFAAOCAQEA\nB3q1mzdJc3VY17JDCaRPKY/Veni3oZ6zGJ9r9LnWKE7IO9AxBAKYGMELHMf9httQ\nmRZrXiGQi/tYXLlFFk7IcIID6iOOZLLagDhCQJPi52dGhBB4a2EnBvGF9OJI4zW9\nJw04HNR5cF1cxWIrRdGJAX/kbs/M9jz0STlXziVNwiAYeGIxkjq/fwKk6weai8iI\nwQVu6GsbtSBL8f4FRCk2vK6a1MD54BQDasRShdi3k/DJAEtY92muu13M0jc48mCM\ndEVeXxM8rIrG6LavziTwtO+jkFyWjlRyNbSr6il+neSWJ7ZspTo+ZYFy55CAh5ws\nwk6Ljo2wlQgfGflJbKcxAw==\n-----END CERTIFICATE-----\n">>},
   {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDOjCCAiKgAwIBAgIIGD/HyAMALgMwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjJaFw0y\nNzA4MTcxODQ3MjJaMCoxKDAmBgNVBAMTH0NvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nZGIyLmxhbikwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC63r7EIhIk\nOyMwE9mw7bzVJfstC9FfdZvk0bP8vxZkuhNt/bVcND8p00ihMP8FCbfsa5d8VgWC\nDoao/+R+8KpeXDZeXWTzOfGPz6ySJpDMUbPsj/pymSkYBvwsl0iBlPrBYDVlHe6C\nIFfKfnfHOsS9itHFANxppJ+spLeJP7Na1p/rOGFELp9Y0pYnDvAccES6DCwgzMKr\n4lk/1l1L4jJkyHzK0hQ5UutHe7jGBhOxRoIyTCPO5wf6miZJNlKxA/1zwCJuLaft\ny4N6DDBMIs9R/dS02i1IrhKLDvC/7OnK/BkXi4HKLKY/wE3YNBmgMNXEwxUMeqZk\n2IUILCZn4W9fAgMBAAGjajBoMA4GA1UdDwEB/wQEAwIFoDATBgNVHSUEDDAKBggr\nBgEFBQcDATAMBgNVHRMBAf8EAjAAMB8GA1UdIwQYMBaAFLwBKrYySHRSxl7h8i/O\nziYJJ9OaMBIGA1UdEQQLMAmCB2RiMi5sYW4wDQYJKoZIhvcNAQELBQADggEBADkD\n1qqpKjJY4cyeoPzdD5mW7I12+dWN/oW0yUQ4heVH9TZzgThchsucMQvFg3vpZ9j5\nSbxsH9h23M26Cv8PxOjpu8lmlYAtENDDGnagAiwDJuMtSeBkKw36xzLrB4jj8UZ+\n8k+5CLVUzncPt9J9o7Lgm+wgavlprqSMJLUGfAc+O0qGzN+mQk1UeM2a8++Ty97Q\neks+wPVQCPbprWQU7k7F8mSepoAWHglkhfkBjBXgHGvGMIMGDv68l7yfNBJ9yRoJ\nO5cAQgoNiHZDzZ9clPCCGeE+JQnxNQCMOIxq7xhnTrYgBeU9hWu0hBcrHR6ZwO+m\n6MNtblxGbkNee9HO7LY=\n-----END CERTIFICATE-----\n">>},
   {pkey_passphrase_settings,[]},
   {certs_epoch,0},
   {type,generated},
   {hostname,"db2.lan"}]
[ns_server:debug,2025-05-15T18:47:22.852Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',prometheus_auth_info} -> {node,
                                                                   'ns_1@db2.lan',
                                                                   prometheus_auth_info}:
  {"@prometheus",
   {auth,[{<<"hash">>,
           {[{<<"hashes">>,
              {sanitized,<<"HmfAF2EJsiMXLpFCMXkwI34vHpKDSuQ9OLt/WXmflzM=">>}},
             {<<"algorithm">>,<<"argon2id">>},
             {<<"salt">>,<<"uKWlA91cJAr0Ue2ihPum/w==">>},
             {<<"parallelism">>,1},
             {<<"time">>,3},
             {<<"memory">>,524288}]}},
          {<<"scram-sha-512">>,
           {[{<<"salt">>,
              <<"B9qHYrF6B8PUh2HylXGlWHK8C5mZNOug98FyWD9UiLqElSnKTzdCIZzgFR8u69cjhSjtbD3VEM/dcZbWXNTBGw==">>},
             {<<"iterations">>,15000},
             {<<"hashes">>,
              {sanitized,<<"Jr1d3/BDp9K+TGL8YrZCBCGqzGbFloqcMPVzFi39KC0=">>}}]}},
          {<<"scram-sha-256">>,
           {[{<<"salt">>,<<"1nOjbJ6vT1sVuqxQCaa4EQHYdQk5w+2NYTeK1TUhKmY=">>},
             {<<"iterations">>,15000},
             {<<"hashes">>,
              {sanitized,<<"x86lsx8NdTsLHhvU9aNAyIeE61P+Yt6GyVOHrh2MFQo=">>}}]}},
          {<<"scram-sha-1">>,
           {[{<<"salt">>,<<"2rk59nwO1XOIg7dCVQ5eTg7p0y8=">>},
             {<<"iterations">>,15000},
             {<<"hashes">>,
              {sanitized,<<"4dXFSggAV9gFKuhCJ2pKs13gZZUKMc03Uwl7Sc+puO0=">>}}]}}]}} ->
  {"@prometheus",
   {auth,[{<<"hash">>,
           {[{<<"hashes">>,
              {sanitized,<<"HmfAF2EJsiMXLpFCMXkwI34vHpKDSuQ9OLt/WXmflzM=">>}},
             {<<"algorithm">>,<<"argon2id">>},
             {<<"salt">>,<<"uKWlA91cJAr0Ue2ihPum/w==">>},
             {<<"parallelism">>,1},
             {<<"time">>,3},
             {<<"memory">>,524288}]}},
          {<<"scram-sha-512">>,
           {[{<<"salt">>,
              <<"B9qHYrF6B8PUh2HylXGlWHK8C5mZNOug98FyWD9UiLqElSnKTzdCIZzgFR8u69cjhSjtbD3VEM/dcZbWXNTBGw==">>},
             {<<"iterations">>,15000},
             {<<"hashes">>,
              {sanitized,<<"Jr1d3/BDp9K+TGL8YrZCBCGqzGbFloqcMPVzFi39KC0=">>}}]}},
          {<<"scram-sha-256">>,
           {[{<<"salt">>,<<"1nOjbJ6vT1sVuqxQCaa4EQHYdQk5w+2NYTeK1TUhKmY=">>},
             {<<"iterations">>,15000},
             {<<"hashes">>,
              {sanitized,<<"x86lsx8NdTsLHhvU9aNAyIeE61P+Yt6GyVOHrh2MFQo=">>}}]}},
          {<<"scram-sha-1">>,
           {[{<<"salt">>,<<"2rk59nwO1XOIg7dCVQ5eTg7p0y8=">>},
             {<<"iterations">>,15000},
             {<<"hashes">>,
              {sanitized,<<"4dXFSggAV9gFKuhCJ2pKs13gZZUKMc03Uwl7Sc+puO0=">>}}]}}]}}
[ns_server:debug,2025-05-15T18:47:22.853Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',eventing_dir} -> {node,
                                                           'ns_1@db2.lan',
                                                           eventing_dir}:
  "/opt/couchbase/var/lib/couchbase/data" ->
  "/opt/couchbase/var/lib/couchbase/data"
[ns_server:debug,2025-05-15T18:47:22.853Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_dirs} -> {node,'ns_1@db2.lan',
                                                        cbas_dirs}:
  ["/opt/couchbase/var/lib/couchbase/data"] ->
  ["/opt/couchbase/var/lib/couchbase/data"]
[ns_server:debug,2025-05-15T18:47:22.853Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',n2n_client_cert_auth} -> {node,
                                                                   'ns_1@db2.lan',
                                                                   n2n_client_cert_auth}:
  false ->
  false
[ns_server:debug,2025-05-15T18:47:22.854Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',erl_external_listeners} -> {node,
                                                                     'ns_1@db2.lan',
                                                                     erl_external_listeners}:
  [{inet,false}] ->
  [{inet,false}]
[ns_server:debug,2025-05-15T18:47:22.854Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',node_encryption} -> {node,
                                                              'ns_1@db2.lan',
                                                              node_encryption}:
  false ->
  false
[ns_server:debug,2025-05-15T18:47:22.854Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',address_family} -> {node,
                                                             'ns_1@db2.lan',
                                                             address_family}:
  inet ->
  inet
[ns_server:debug,2025-05-15T18:47:22.854Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf quorum_nodes -> quorum_nodes:
  ['ns_1@cb.local'] ->
  ['ns_1@db2.lan']
[ns_server:debug,2025-05-15T18:47:22.854Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',audit} -> {node,'ns_1@db2.lan',audit}:
  [] ->
  []
[ns_server:debug,2025-05-15T18:47:22.854Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',backup_grpc_port} -> {node,
                                                               'ns_1@db2.lan',
                                                               backup_grpc_port}:
  9124 ->
  9124
[ns_server:debug,2025-05-15T18:47:22.854Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',backup_http_port} -> {node,
                                                               'ns_1@db2.lan',
                                                               backup_http_port}:
  8097 ->
  8097
[ns_server:debug,2025-05-15T18:47:22.854Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',backup_https_port} -> {node,
                                                                'ns_1@db2.lan',
                                                                backup_https_port}:
  18097 ->
  18097
[ns_server:debug,2025-05-15T18:47:22.854Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',capi_port} -> {node,'ns_1@db2.lan',
                                                        capi_port}:
  8092 ->
  8092
[ns_server:debug,2025-05-15T18:47:22.854Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_admin_port} -> {node,
                                                              'ns_1@db2.lan',
                                                              cbas_admin_port}:
  9110 ->
  9110
[ns_server:debug,2025-05-15T18:47:22.854Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_cc_client_port} -> {node,
                                                                  'ns_1@db2.lan',
                                                                  cbas_cc_client_port}:
  9113 ->
  9113
[ns_server:debug,2025-05-15T18:47:22.854Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_cc_cluster_port} -> {node,
                                                                   'ns_1@db2.lan',
                                                                   cbas_cc_cluster_port}:
  9112 ->
  9112
[ns_server:debug,2025-05-15T18:47:22.854Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_cc_http_port} -> {node,
                                                                'ns_1@db2.lan',
                                                                cbas_cc_http_port}:
  9111 ->
  9111
[ns_server:debug,2025-05-15T18:47:22.854Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_cluster_port} -> {node,
                                                                'ns_1@db2.lan',
                                                                cbas_cluster_port}:
  9115 ->
  9115
[ns_server:debug,2025-05-15T18:47:22.854Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_console_port} -> {node,
                                                                'ns_1@db2.lan',
                                                                cbas_console_port}:
  9114 ->
  9114
[ns_server:debug,2025-05-15T18:47:22.854Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_data_port} -> {node,
                                                             'ns_1@db2.lan',
                                                             cbas_data_port}:
  9116 ->
  9116
[ns_server:debug,2025-05-15T18:47:22.854Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_debug_port} -> {node,
                                                              'ns_1@db2.lan',
                                                              cbas_debug_port}:
  -1 ->
  -1
[ns_server:debug,2025-05-15T18:47:22.854Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_http_port} -> {node,
                                                             'ns_1@db2.lan',
                                                             cbas_http_port}:
  8095 ->
  8095
[ns_server:debug,2025-05-15T18:47:22.854Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_messaging_port} -> {node,
                                                                  'ns_1@db2.lan',
                                                                  cbas_messaging_port}:
  9118 ->
  9118
[ns_server:debug,2025-05-15T18:47:22.854Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_metadata_callback_port} -> {node,
                                                                          'ns_1@db2.lan',
                                                                          cbas_metadata_callback_port}:
  9119 ->
  9119
[ns_server:debug,2025-05-15T18:47:22.855Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_metadata_port} -> {node,
                                                                 'ns_1@db2.lan',
                                                                 cbas_metadata_port}:
  9121 ->
  9121
[ns_server:debug,2025-05-15T18:47:22.855Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_parent_port} -> {node,
                                                               'ns_1@db2.lan',
                                                               cbas_parent_port}:
  9122 ->
  9122
[ns_server:debug,2025-05-15T18:47:22.855Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_replication_port} -> {node,
                                                                    'ns_1@db2.lan',
                                                                    cbas_replication_port}:
  9120 ->
  9120
[ns_server:debug,2025-05-15T18:47:22.855Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_result_port} -> {node,
                                                               'ns_1@db2.lan',
                                                               cbas_result_port}:
  9117 ->
  9117
[ns_server:debug,2025-05-15T18:47:22.855Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_ssl_port} -> {node,
                                                            'ns_1@db2.lan',
                                                            cbas_ssl_port}:
  18095 ->
  18095
[ns_server:debug,2025-05-15T18:47:22.855Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',compaction_daemon} -> {node,
                                                                'ns_1@db2.lan',
                                                                compaction_daemon}:
  [{check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}] ->
  [{check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}]
[ns_server:debug,2025-05-15T18:47:22.855Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',config_version} -> {node,
                                                             'ns_1@db2.lan',
                                                             config_version}:
  {7,6} ->
  {7,6}
[ns_server:debug,2025-05-15T18:47:22.855Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',database_dir} -> {node,
                                                           'ns_1@db2.lan',
                                                           database_dir}:
  "/opt/couchbase/var/lib/couchbase/data" ->
  "/opt/couchbase/var/lib/couchbase/data"
[ns_server:debug,2025-05-15T18:47:22.855Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',event_log} -> {node,'ns_1@db2.lan',
                                                        event_log}:
  [{filename,"/opt/couchbase/var/lib/couchbase/event_log"}] ->
  [{filename,"/opt/couchbase/var/lib/couchbase/event_log"}]
[ns_server:debug,2025-05-15T18:47:22.855Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',eventing_debug_port} -> {node,
                                                                  'ns_1@db2.lan',
                                                                  eventing_debug_port}:
  9140 ->
  9140
[ns_server:debug,2025-05-15T18:47:22.855Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',eventing_http_port} -> {node,
                                                                 'ns_1@db2.lan',
                                                                 eventing_http_port}:
  8096 ->
  8096
[ns_server:debug,2025-05-15T18:47:22.855Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',eventing_https_port} -> {node,
                                                                  'ns_1@db2.lan',
                                                                  eventing_https_port}:
  18096 ->
  18096
[ns_server:debug,2025-05-15T18:47:22.855Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',fts_grpc_port} -> {node,
                                                            'ns_1@db2.lan',
                                                            fts_grpc_port}:
  9130 ->
  9130
[ns_server:debug,2025-05-15T18:47:22.855Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',fts_grpc_ssl_port} -> {node,
                                                                'ns_1@db2.lan',
                                                                fts_grpc_ssl_port}:
  19130 ->
  19130
[ns_server:debug,2025-05-15T18:47:22.855Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',fts_http_port} -> {node,
                                                            'ns_1@db2.lan',
                                                            fts_http_port}:
  8094 ->
  8094
[ns_server:debug,2025-05-15T18:47:22.855Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',fts_ssl_port} -> {node,
                                                           'ns_1@db2.lan',
                                                           fts_ssl_port}:
  18094 ->
  18094
[ns_server:debug,2025-05-15T18:47:22.855Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',index_dir} -> {node,'ns_1@db2.lan',
                                                        index_dir}:
  "/opt/couchbase/var/lib/couchbase/data" ->
  "/opt/couchbase/var/lib/couchbase/data"
[ns_server:debug,2025-05-15T18:47:22.855Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',indexer_admin_port} -> {node,
                                                                 'ns_1@db2.lan',
                                                                 indexer_admin_port}:
  9100 ->
  9100
[ns_server:debug,2025-05-15T18:47:22.855Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',indexer_http_port} -> {node,
                                                                'ns_1@db2.lan',
                                                                indexer_http_port}:
  9102 ->
  9102
[ns_server:debug,2025-05-15T18:47:22.855Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',indexer_https_port} -> {node,
                                                                 'ns_1@db2.lan',
                                                                 indexer_https_port}:
  19102 ->
  19102
[ns_server:debug,2025-05-15T18:47:22.855Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',indexer_scan_port} -> {node,
                                                                'ns_1@db2.lan',
                                                                indexer_scan_port}:
  9101 ->
  9101
[ns_server:debug,2025-05-15T18:47:22.855Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',indexer_stcatchup_port} -> {node,
                                                                     'ns_1@db2.lan',
                                                                     indexer_stcatchup_port}:
  9104 ->
  9104
[ns_server:debug,2025-05-15T18:47:22.855Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',indexer_stinit_port} -> {node,
                                                                  'ns_1@db2.lan',
                                                                  indexer_stinit_port}:
  9103 ->
  9103
[ns_server:debug,2025-05-15T18:47:22.855Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',indexer_stmaint_port} -> {node,
                                                                   'ns_1@db2.lan',
                                                                   indexer_stmaint_port}:
  9105 ->
  9105
[ns_server:debug,2025-05-15T18:47:22.855Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',is_enterprise} -> {node,
                                                            'ns_1@db2.lan',
                                                            is_enterprise}:
  true ->
  true
[ns_server:debug,2025-05-15T18:47:22.856Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',isasl} -> {node,'ns_1@db2.lan',isasl}:
  [{path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}] ->
  [{path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]
[ns_server:debug,2025-05-15T18:47:22.856Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',memcached} -> {node,'ns_1@db2.lan',
                                                        memcached}:
  [{port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,11206},
   {ssl_port,11207},
   {admin_user,"@ns_server"},
   {other_users,["@cbq-engine","@projector","@goxdcr","@index","@fts",
                 "@eventing","@cbas","@backup"]},
   {admin_pass,"*****"},
   {engines,[{membase,[{engine,"/opt/couchbase/lib/memcached/ep.so"},
                       {static_config_string,"failpartialwarmup=false"}]},
             {memcached,[{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
                         {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_rotation_period,39003}] ->
  [{port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,11206},
   {ssl_port,11207},
   {admin_user,"@ns_server"},
   {other_users,["@cbq-engine","@projector","@goxdcr","@index","@fts",
                 "@eventing","@cbas","@backup"]},
   {admin_pass,"*****"},
   {engines,[{membase,[{engine,"/opt/couchbase/lib/memcached/ep.so"},
                       {static_config_string,"failpartialwarmup=false"}]},
             {memcached,[{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
                         {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_rotation_period,39003}]
[ns_server:debug,2025-05-15T18:47:22.856Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',memcached_config} -> {node,
                                                               'ns_1@db2.lan',
                                                               memcached_config}:
  {[{interfaces,{memcached_config_mgr,get_interfaces,[]}},
    {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
    {connection_idle_time,connection_idle_time},
    {privilege_debug,privilege_debug},
    {breakpad,
        {[{enabled,breakpad_enabled},
          {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
    {deployment_model,{memcached_config_mgr,get_config_profile,[]}},
    {verbosity,verbosity},
    {audit_file,{"~s",[audit_file]}},
    {rbac_file,{"~s",[rbac_file]}},
    {dedupe_nmvb_maps,dedupe_nmvb_maps},
    {tracing_enabled,tracing_enabled},
    {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
    {xattr_enabled,true},
    {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
    {collections_enabled,true},
    {max_connections,max_connections},
    {system_connections,system_connections},
    {num_reader_threads,num_reader_threads},
    {num_writer_threads,num_writer_threads},
    {num_auxio_threads,num_auxio_threads},
    {num_nonio_threads,num_nonio_threads},
    {num_storage_threads,num_storage_threads},
    {logger,
        {[{filename,{"~s/~s",[log_path,log_prefix]}},
          {cyclesize,log_cyclesize}]}},
    {external_auth_service,
        {memcached_config_mgr,get_external_auth_service,[]}},
    {active_external_users_push_interval,
        {memcached_config_mgr,get_external_users_push_interval,[]}},
    {prometheus,{memcached_config_mgr,prometheus_cfg,[]}},
    {sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
    {ssl_sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
    {tcp_keepalive_idle,tcp_keepalive_idle},
    {tcp_keepalive_interval,tcp_keepalive_interval},
    {tcp_keepalive_probes,tcp_keepalive_probes},
    {tcp_user_timeout,tcp_user_timeout},
    {always_collect_trace_info,always_collect_trace_info},
    {connection_limit_mode,connection_limit_mode},
    {free_connection_pool_size,free_connection_pool_size},
    {max_client_connection_details,max_client_connection_details}]} ->
  {[{interfaces,{memcached_config_mgr,get_interfaces,[]}},
    {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
    {connection_idle_time,connection_idle_time},
    {privilege_debug,privilege_debug},
    {breakpad,
        {[{enabled,breakpad_enabled},
          {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
    {deployment_model,{memcached_config_mgr,get_config_profile,[]}},
    {verbosity,verbosity},
    {audit_file,{"~s",[audit_file]}},
    {rbac_file,{"~s",[rbac_file]}},
    {dedupe_nmvb_maps,dedupe_nmvb_maps},
    {tracing_enabled,tracing_enabled},
    {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
    {xattr_enabled,true},
    {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
    {collections_enabled,true},
    {max_connections,max_connections},
    {system_connections,system_connections},
    {num_reader_threads,num_reader_threads},
    {num_writer_threads,num_writer_threads},
    {num_auxio_threads,num_auxio_threads},
    {num_nonio_threads,num_nonio_threads},
    {num_storage_threads,num_storage_threads},
    {logger,
        {[{filename,{"~s/~s",[log_path,log_prefix]}},
          {cyclesize,log_cyclesize}]}},
    {external_auth_service,
        {memcached_config_mgr,get_external_auth_service,[]}},
    {active_external_users_push_interval,
        {memcached_config_mgr,get_external_users_push_interval,[]}},
    {prometheus,{memcached_config_mgr,prometheus_cfg,[]}},
    {sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
    {ssl_sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
    {tcp_keepalive_idle,tcp_keepalive_idle},
    {tcp_keepalive_interval,tcp_keepalive_interval},
    {tcp_keepalive_probes,tcp_keepalive_probes},
    {tcp_user_timeout,tcp_user_timeout},
    {always_collect_trace_info,always_collect_trace_info},
    {connection_limit_mode,connection_limit_mode},
    {free_connection_pool_size,free_connection_pool_size},
    {max_client_connection_details,max_client_connection_details}]}
[ns_server:debug,2025-05-15T18:47:22.856Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',memcached_dedicated_ssl_port} -> {node,
                                                                           'ns_1@db2.lan',
                                                                           memcached_dedicated_ssl_port}:
  11206 ->
  11206
[ns_server:debug,2025-05-15T18:47:22.856Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',memcached_defaults} -> {node,
                                                                 'ns_1@db2.lan',
                                                                 memcached_defaults}:
  [{max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {je_malloc_conf,undefined},
   {tracing_enabled,true},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>},
   {num_auxio_threads,<<"default">>},
   {num_nonio_threads,<<"default">>},
   {num_storage_threads,<<"default">>},
   {tcp_keepalive_idle,360},
   {tcp_keepalive_interval,10},
   {tcp_keepalive_probes,3},
   {tcp_user_timeout,30},
   {always_collect_trace_info,true},
   {connection_limit_mode,<<"disconnect">>},
   {free_connection_pool_size,0},
   {max_client_connection_details,0}] ->
  [{max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {je_malloc_conf,undefined},
   {tracing_enabled,true},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>},
   {num_auxio_threads,<<"default">>},
   {num_nonio_threads,<<"default">>},
   {num_storage_threads,<<"default">>},
   {tcp_keepalive_idle,360},
   {tcp_keepalive_interval,10},
   {tcp_keepalive_probes,3},
   {tcp_user_timeout,30},
   {always_collect_trace_info,true},
   {connection_limit_mode,<<"disconnect">>},
   {free_connection_pool_size,0},
   {max_client_connection_details,0}]
[ns_server:debug,2025-05-15T18:47:22.857Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',memcached_prometheus} -> {node,
                                                                   'ns_1@db2.lan',
                                                                   memcached_prometheus}:
  11280 ->
  11280
[ns_server:debug,2025-05-15T18:47:22.857Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',ns_log} -> {node,'ns_1@db2.lan',
                                                     ns_log}:
  [{filename,"/opt/couchbase/var/lib/couchbase/ns_log"}] ->
  [{filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]
[ns_server:debug,2025-05-15T18:47:22.857Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',port_servers} -> {node,
                                                           'ns_1@db2.lan',
                                                           port_servers}:
  [] ->
  []
[ns_server:debug,2025-05-15T18:47:22.857Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',projector_port} -> {node,
                                                             'ns_1@db2.lan',
                                                             projector_port}:
  9999 ->
  9999
[ns_server:debug,2025-05-15T18:47:22.857Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',projector_ssl_port} -> {node,
                                                                 'ns_1@db2.lan',
                                                                 projector_ssl_port}:
  9999 ->
  9999
[ns_server:debug,2025-05-15T18:47:22.857Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',prometheus_http_port} -> {node,
                                                                   'ns_1@db2.lan',
                                                                   prometheus_http_port}:
  9123 ->
  9123
[ns_server:debug,2025-05-15T18:47:22.857Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',query_port} -> {node,'ns_1@db2.lan',
                                                         query_port}:
  8093 ->
  8093
[ns_server:debug,2025-05-15T18:47:22.857Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',rest} -> {node,'ns_1@db2.lan',rest}:
  [{port,8091},{port_meta,global}] ->
  [{port,8091},{port_meta,global}]
[ns_server:debug,2025-05-15T18:47:22.857Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',saslauthd_enabled} -> {node,
                                                                'ns_1@db2.lan',
                                                                saslauthd_enabled}:
  true ->
  true
[ns_server:debug,2025-05-15T18:47:22.857Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',ssl_capi_port} -> {node,
                                                            'ns_1@db2.lan',
                                                            ssl_capi_port}:
  18092 ->
  18092
[ns_server:debug,2025-05-15T18:47:22.857Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',ssl_query_port} -> {node,
                                                             'ns_1@db2.lan',
                                                             ssl_query_port}:
  18093 ->
  18093
[ns_server:debug,2025-05-15T18:47:22.857Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',ssl_rest_port} -> {node,
                                                            'ns_1@db2.lan',
                                                            ssl_rest_port}:
  18091 ->
  18091
[ns_server:debug,2025-05-15T18:47:22.857Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',uuid} -> {node,'ns_1@db2.lan',uuid}:
  <<"b0a71fbd3e8ac51d55b35452995f45cf">> ->
  <<"b0a71fbd3e8ac51d55b35452995f45cf">>
[ns_server:debug,2025-05-15T18:47:22.857Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',xdcr_rest_port} -> {node,
                                                             'ns_1@db2.lan',
                                                             xdcr_rest_port}:
  9998 ->
  9998
[ns_server:debug,2025-05-15T18:47:22.857Z,ns_1@db2.lan:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',{project_intact,is_vulnerable}} -> {node,
                                                                             'ns_1@db2.lan',
                                                                             {project_intact,
                                                                              is_vulnerable}}:
  false ->
  false
[ns_server:debug,2025-05-15T18:47:22.857Z,ns_1@db2.lan:<0.2159.0>:dist_manager:wait_for_node:276]Waiting for connection to node 'couchdb_ns_1@cb.local' to be established
[error_logger:info,2025-05-15T18:47:22.858Z,ns_1@db2.lan:net_kernel<0.2175.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:warn,2025-05-15T18:47:22.858Z,ns_1@db2.lan:leader_quorum_nodes_manager<0.799.0>:leader_quorum_nodes_manager:handle_quorum_nodes_updated:155]Somebody else updated the quorum nodes when we are the master node.
Our quorum nodes: ['ns_1@cb.local']
Their quorum nodes: ['ns_1@db2.lan']
[ns_server:debug,2025-05-15T18:47:22.858Z,ns_1@db2.lan:net_kernel<0.2175.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:info,2025-05-15T18:47:22.858Z,ns_1@db2.lan:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:handle_info:764]cert_and_pkey changed
[ns_server:debug,2025-05-15T18:47:22.858Z,ns_1@db2.lan:<0.803.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.799.0>} exited with reason {shutdown,
                                                                                {quorum_nodes_update_conflict,
                                                                                 ['ns_1@cb.local'],
                                                                                 ['ns_1@db2.lan']}}
[ns_server:debug,2025-05-15T18:47:22.858Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3735524962.3650355205.242898>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:47:22.858Z,ns_1@db2.lan:<0.707.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":27,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true,\"hostname\":\"db2.lan\",\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[ns_server:debug,2025-05-15T18:47:22.858Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',{project_intact,is_vulnerable}} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|false]
[error_logger:error,2025-05-15T18:47:22.858Z,ns_1@db2.lan:mb_master_sup<0.789.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {local,mb_master_sup}
    errorContext: child_terminated
    reason: {shutdown,
                {quorum_nodes_update_conflict,
                    ['ns_1@cb.local'],
                    ['ns_1@db2.lan']}}
    offender: [{pid,<0.799.0>},
               {id,leader_quorum_nodes_manager},
               {mfargs,{leader_quorum_nodes_manager,start_link,[]}},
               {restart_type,permanent},
               {significant,false},
               {shutdown,1000},
               {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:22.858Z,ns_1@db2.lan:prometheus_cfg<0.517.0>:prometheus_cfg:maybe_apply_new_settings:704]Settings didn't change, ignoring update
[ns_server:debug,2025-05-15T18:47:22.858Z,ns_1@db2.lan:leader_activities<0.764.0>:leader_activities:handle_internal_process_down:440]Process {quorum_nodes_manager,<0.799.0>} terminated with reason {shutdown,
                                                                 {quorum_nodes_update_conflict,
                                                                  ['ns_1@cb.local'],
                                                                  ['ns_1@db2.lan']}}
[ns_server:debug,2025-05-15T18:47:22.858Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3735524962.3650355205.242898>,
                                  inet_tcp_dist,<0.2193.0>,
                                  #Ref<0.3735524962.3650355205.242904>}
[ns_server:debug,2025-05-15T18:47:22.858Z,ns_1@db2.lan:leader_quorum_nodes_manager<0.2194.0>:leader_quorum_nodes_manager:pull_config:99]Attempting to pull config from nodes:
[]
[ns_server:debug,2025-05-15T18:47:22.858Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',xdcr_rest_port} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|9998]
[error_logger:info,2025-05-15T18:47:22.858Z,ns_1@db2.lan:mb_master_sup<0.789.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.2194.0>},
              {id,leader_quorum_nodes_manager},
              {mfargs,{leader_quorum_nodes_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:22.858Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',uuid} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|
 <<"b0a71fbd3e8ac51d55b35452995f45cf">>]
[ns_server:debug,2025-05-15T18:47:22.858Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',ssl_rest_port} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|18091]
[ns_server:debug,2025-05-15T18:47:22.858Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',ssl_query_port} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|18093]
[ns_server:debug,2025-05-15T18:47:22.858Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',ssl_capi_port} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|18092]
[ns_server:debug,2025-05-15T18:47:22.858Z,ns_1@db2.lan:ns_config_rep<0.554.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([quorum_nodes,
                               {node,'ns_1@db2.lan',address_family},
                               {node,'ns_1@db2.lan',audit},
                               {node,'ns_1@db2.lan',backup_grpc_port},
                               {node,'ns_1@db2.lan',backup_http_port},
                               {node,'ns_1@db2.lan',backup_https_port},
                               {node,'ns_1@db2.lan',capi_port},
                               {node,'ns_1@db2.lan',cbas_admin_port},
                               {node,'ns_1@db2.lan',cbas_cc_client_port},
                               {node,'ns_1@db2.lan',cbas_cc_cluster_port},
                               {node,'ns_1@db2.lan',cbas_cc_http_port},
                               {node,'ns_1@db2.lan',cbas_cluster_port},
                               {node,'ns_1@db2.lan',cbas_console_port},
                               {node,'ns_1@db2.lan',cbas_data_port},
                               {node,'ns_1@db2.lan',cbas_debug_port},
                               {node,'ns_1@db2.lan',cbas_dirs},
                               {node,'ns_1@db2.lan',cbas_http_port},
                               {node,'ns_1@db2.lan',cbas_messaging_port},
                               {node,'ns_1@db2.lan',
                                     cbas_metadata_callback_port},
                               {node,'ns_1@db2.lan',cbas_metadata_port},
                               {node,'ns_1@db2.lan',cbas_parent_port},
                               {node,'ns_1@db2.lan',cbas_replication_port},
                               {node,'ns_1@db2.lan',cbas_result_port},
                               {node,'ns_1@db2.lan',cbas_ssl_port},
                               {node,'ns_1@db2.lan',client_cert},
                               {node,'ns_1@db2.lan',compaction_daemon},
                               {node,'ns_1@db2.lan',config_version},
                               {node,'ns_1@db2.lan',database_dir},
                               {node,'ns_1@db2.lan',erl_external_listeners},
                               {node,'ns_1@db2.lan',event_log},
                               {node,'ns_1@db2.lan',eventing_debug_port},
                               {node,'ns_1@db2.lan',eventing_dir},
                               {node,'ns_1@db2.lan',eventing_http_port},
                               {node,'ns_1@db2.lan',eventing_https_port},
                               {node,'ns_1@db2.lan',fts_grpc_port},
                               {node,'ns_1@db2.lan',fts_grpc_ssl_port},
                               {node,'ns_1@db2.lan',fts_http_port},
                               {node,'ns_1@db2.lan',fts_ssl_port},
                               {node,'ns_1@db2.lan',index_dir},
                               {node,'ns_1@db2.lan',indexer_admin_port},
                               {node,'ns_1@db2.lan',indexer_http_port},
                               {node,'ns_1@db2.lan',indexer_https_port},
                               {node,'ns_1@db2.lan',indexer_scan_port},
                               {node,'ns_1@db2.lan',indexer_stcatchup_port},
                               {node,'ns_1@db2.lan',indexer_stinit_port},
                               {node,'ns_1@db2.lan',indexer_stmaint_port},
                               {node,'ns_1@db2.lan',is_enterprise},
                               {node,'ns_1@db2.lan',isasl},
                               {node,'ns_1@db2.lan',memcached},
                               {node,'ns_1@db2.lan',memcached_config},
                               {node,'ns_1@db2.lan',
                                     memcached_dedicated_ssl_port},
                               {node,'ns_1@db2.lan',memcached_defaults},
                               {node,'ns_1@db2.lan',memcached_prometheus},
                               {node,'ns_1@db2.lan',n2n_client_cert_auth},
                               {node,'ns_1@db2.lan',node_cert},
                               {node,'ns_1@db2.lan',node_encryption},
                               {node,'ns_1@db2.lan',ns_log},
                               {node,'ns_1@db2.lan',port_servers},
                               {node,'ns_1@db2.lan',projector_port},
                               {node,'ns_1@db2.lan',projector_ssl_port},
                               {node,'ns_1@db2.lan',prometheus_auth_info},
                               {node,'ns_1@db2.lan',prometheus_http_port},
                               {node,'ns_1@db2.lan',query_port}]..)
[ns_server:debug,2025-05-15T18:47:22.858Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',saslauthd_enabled} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|true]
[ns_server:debug,2025-05-15T18:47:22.858Z,ns_1@db2.lan:prometheus_cfg<0.517.0>:prometheus_cfg:maybe_apply_new_settings:704]Settings didn't change, ignoring update
[ns_server:debug,2025-05-15T18:47:22.858Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',rest} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]},
 {port,8091},
 {port_meta,global}]
[ns_server:debug,2025-05-15T18:47:22.858Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',query_port} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|8093]
[ns_server:debug,2025-05-15T18:47:22.859Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',prometheus_http_port} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|9123]
[ns_server:debug,2025-05-15T18:47:22.859Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',projector_ssl_port} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|9999]
[ns_server:debug,2025-05-15T18:47:22.859Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',projector_port} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|9999]
[ns_server:debug,2025-05-15T18:47:22.859Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',port_servers} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}]
[ns_server:debug,2025-05-15T18:47:22.859Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',ns_log} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]},
 {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]
[ns_server:debug,2025-05-15T18:47:22.859Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',memcached_prometheus} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|11280]
[ns_server:debug,2025-05-15T18:47:22.859Z,ns_1@db2.lan:prometheus_cfg<0.517.0>:prometheus_cfg:maybe_apply_new_settings:704]Settings didn't change, ignoring update
[ns_server:debug,2025-05-15T18:47:22.859Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',memcached_defaults} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]},
 {max_connections,65000},
 {system_connections,5000},
 {connection_idle_time,0},
 {verbosity,0},
 {privilege_debug,false},
 {breakpad_enabled,true},
 {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
 {dedupe_nmvb_maps,false},
 {je_malloc_conf,undefined},
 {tracing_enabled,true},
 {datatype_snappy,true},
 {num_reader_threads,<<"default">>},
 {num_writer_threads,<<"default">>},
 {num_auxio_threads,<<"default">>},
 {num_nonio_threads,<<"default">>},
 {num_storage_threads,<<"default">>},
 {tcp_keepalive_idle,360},
 {tcp_keepalive_interval,10},
 {tcp_keepalive_probes,3},
 {tcp_user_timeout,30},
 {always_collect_trace_info,true},
 {connection_limit_mode,<<"disconnect">>},
 {free_connection_pool_size,0},
 {max_client_connection_details,0}]
[ns_server:debug,2025-05-15T18:47:22.859Z,ns_1@db2.lan:leader_quorum_nodes_manager<0.2194.0>:leader_quorum_nodes_manager:pull_config:104]Pulled config successfully.
[ns_server:debug,2025-05-15T18:47:22.859Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',memcached_dedicated_ssl_port} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|11206]
[ns_server:debug,2025-05-15T18:47:22.859Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',memcached_config} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|
 {[{interfaces,{memcached_config_mgr,get_interfaces,[]}},
   {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
   {connection_idle_time,connection_idle_time},
   {privilege_debug,privilege_debug},
   {breakpad,
    {[{enabled,breakpad_enabled},
      {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
   {deployment_model,{memcached_config_mgr,get_config_profile,[]}},
   {verbosity,verbosity},
   {audit_file,{"~s",[audit_file]}},
   {rbac_file,{"~s",[rbac_file]}},
   {dedupe_nmvb_maps,dedupe_nmvb_maps},
   {tracing_enabled,tracing_enabled},
   {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
   {xattr_enabled,true},
   {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
   {collections_enabled,true},
   {max_connections,max_connections},
   {system_connections,system_connections},
   {num_reader_threads,num_reader_threads},
   {num_writer_threads,num_writer_threads},
   {num_auxio_threads,num_auxio_threads},
   {num_nonio_threads,num_nonio_threads},
   {num_storage_threads,num_storage_threads},
   {logger,
    {[{filename,{"~s/~s",[log_path,log_prefix]}},{cyclesize,log_cyclesize}]}},
   {external_auth_service,{memcached_config_mgr,get_external_auth_service,[]}},
   {active_external_users_push_interval,
    {memcached_config_mgr,get_external_users_push_interval,[]}},
   {prometheus,{memcached_config_mgr,prometheus_cfg,[]}},
   {sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
   {ssl_sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
   {tcp_keepalive_idle,tcp_keepalive_idle},
   {tcp_keepalive_interval,tcp_keepalive_interval},
   {tcp_keepalive_probes,tcp_keepalive_probes},
   {tcp_user_timeout,tcp_user_timeout},
   {always_collect_trace_info,always_collect_trace_info},
   {connection_limit_mode,connection_limit_mode},
   {free_connection_pool_size,free_connection_pool_size},
   {max_client_connection_details,max_client_connection_details}]}]
[ns_server:debug,2025-05-15T18:47:22.859Z,ns_1@db2.lan:<0.707.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":27,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true,\"hostname\":\"db2.lan\",\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[ns_server:debug,2025-05-15T18:47:22.860Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',memcached} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]},
 {port,11210},
 {dedicated_port,11209},
 {dedicated_ssl_port,11206},
 {ssl_port,11207},
 {admin_user,"@ns_server"},
 {other_users,["@cbq-engine","@projector","@goxdcr","@index","@fts",
               "@eventing","@cbas","@backup"]},
 {admin_pass,"*****"},
 {engines,[{membase,[{engine,"/opt/couchbase/lib/memcached/ep.so"},
                     {static_config_string,"failpartialwarmup=false"}]},
           {memcached,[{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
                       {static_config_string,"vb0=true"}]}]},
 {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
 {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
 {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
 {log_prefix,"memcached.log"},
 {log_generations,20},
 {log_cyclesize,10485760},
 {log_rotation_period,39003}]
[ns_server:debug,2025-05-15T18:47:22.860Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',isasl} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]},
 {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]
[ns_server:debug,2025-05-15T18:47:22.860Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',is_enterprise} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|true]
[ns_server:debug,2025-05-15T18:47:22.860Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',indexer_stmaint_port} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|9105]
[ns_server:debug,2025-05-15T18:47:22.860Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',indexer_stinit_port} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|9103]
[ns_server:debug,2025-05-15T18:47:22.860Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',indexer_stcatchup_port} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|9104]
[ns_server:debug,2025-05-15T18:47:22.860Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',indexer_scan_port} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|9101]
[ns_server:debug,2025-05-15T18:47:22.860Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',indexer_https_port} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|19102]
[ns_server:debug,2025-05-15T18:47:22.860Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',indexer_http_port} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|9102]
[ns_server:debug,2025-05-15T18:47:22.860Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',indexer_admin_port} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|9100]
[ns_server:debug,2025-05-15T18:47:22.861Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',index_dir} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2025-05-15T18:47:22.861Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',fts_ssl_port} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|18094]
[ns_server:debug,2025-05-15T18:47:22.861Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',fts_http_port} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|8094]
[ns_server:debug,2025-05-15T18:47:22.861Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',fts_grpc_ssl_port} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|19130]
[ns_server:debug,2025-05-15T18:47:22.861Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',fts_grpc_port} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|9130]
[ns_server:debug,2025-05-15T18:47:22.861Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',eventing_https_port} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|18096]
[ns_server:debug,2025-05-15T18:47:22.861Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',eventing_http_port} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|8096]
[ns_server:debug,2025-05-15T18:47:22.861Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',eventing_debug_port} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|9140]
[ns_server:debug,2025-05-15T18:47:22.861Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',event_log} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]},
 {filename,"/opt/couchbase/var/lib/couchbase/event_log"}]
[ns_server:debug,2025-05-15T18:47:22.861Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',database_dir} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2025-05-15T18:47:22.861Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',config_version} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|{7,6}]
[ns_server:debug,2025-05-15T18:47:22.861Z,ns_1@db2.lan:<0.2159.0>:dist_manager:wait_for_node:288]Observed node 'couchdb_ns_1@cb.local' to come up
[ns_server:debug,2025-05-15T18:47:22.861Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',compaction_daemon} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]},
 {check_interval,30},
 {min_db_file_size,131072},
 {min_view_file_size,20971520}]
[ns_server:debug,2025-05-15T18:47:22.861Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_ssl_port} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|18095]
[ns_server:debug,2025-05-15T18:47:22.861Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_result_port} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|9117]
[ns_server:debug,2025-05-15T18:47:22.862Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_replication_port} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|9120]
[ns_server:debug,2025-05-15T18:47:22.862Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_parent_port} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|9122]
[ns_server:debug,2025-05-15T18:47:22.862Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_metadata_port} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|9121]
[ns_server:debug,2025-05-15T18:47:22.862Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_metadata_callback_port} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|9119]
[ns_server:debug,2025-05-15T18:47:22.862Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_messaging_port} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|9118]
[ns_server:debug,2025-05-15T18:47:22.862Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_http_port} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|8095]
[ns_server:debug,2025-05-15T18:47:22.862Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_debug_port} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|-1]
[ns_server:debug,2025-05-15T18:47:22.862Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_data_port} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|9116]
[ns_server:debug,2025-05-15T18:47:22.862Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_console_port} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|9114]
[ns_server:debug,2025-05-15T18:47:22.862Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_cluster_port} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|9115]
[ns_server:debug,2025-05-15T18:47:22.862Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_cc_http_port} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|9111]
[ns_server:debug,2025-05-15T18:47:22.862Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_cc_cluster_port} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|9112]
[ns_server:debug,2025-05-15T18:47:22.862Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_cc_client_port} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|9113]
[ns_server:debug,2025-05-15T18:47:22.862Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_admin_port} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|9110]
[ns_server:debug,2025-05-15T18:47:22.862Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',capi_port} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|8092]
[ns_server:debug,2025-05-15T18:47:22.862Z,ns_1@db2.lan:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:restart_regenerate_client_cert_timer:1447]Time left before client cert regeneration: 70588800000
[ns_server:debug,2025-05-15T18:47:22.862Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',backup_https_port} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|18097]
[ns_server:debug,2025-05-15T18:47:22.862Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',backup_http_port} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|8097]
[ns_server:debug,2025-05-15T18:47:22.862Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',backup_grpc_port} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|9124]
[ns_server:debug,2025-05-15T18:47:22.862Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',audit} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}]
[ns_server:debug,2025-05-15T18:47:22.863Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
quorum_nodes ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554042}}]},
 'ns_1@db2.lan']
[ns_server:debug,2025-05-15T18:47:22.863Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',address_family} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|inet]
[ns_server:debug,2025-05-15T18:47:22.863Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',node_encryption} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|false]
[ns_server:debug,2025-05-15T18:47:22.863Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',erl_external_listeners} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]},
 {inet,false}]
[ns_server:debug,2025-05-15T18:47:22.863Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',n2n_client_cert_auth} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]}|false]
[ns_server:debug,2025-05-15T18:47:22.863Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_dirs} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]},
 "/opt/couchbase/var/lib/couchbase/data"]
[ns_server:debug,2025-05-15T18:47:22.863Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',eventing_dir} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2025-05-15T18:47:22.863Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',prometheus_auth_info} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{3,63914554042}}]}|
 {"@prometheus",
  {auth,
   [{<<"hash">>,
     {[{<<"hashes">>,
        {sanitized,<<"HmfAF2EJsiMXLpFCMXkwI34vHpKDSuQ9OLt/WXmflzM=">>}},
       {<<"algorithm">>,<<"argon2id">>},
       {<<"salt">>,<<"uKWlA91cJAr0Ue2ihPum/w==">>},
       {<<"parallelism">>,1},
       {<<"time">>,3},
       {<<"memory">>,524288}]}},
    {<<"scram-sha-512">>,
     {[{<<"salt">>,
        <<"B9qHYrF6B8PUh2HylXGlWHK8C5mZNOug98FyWD9UiLqElSnKTzdCIZzgFR8u69cjhSjtbD3VEM/dcZbWXNTBGw==">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"Jr1d3/BDp9K+TGL8YrZCBCGqzGbFloqcMPVzFi39KC0=">>}}]}},
    {<<"scram-sha-256">>,
     {[{<<"salt">>,<<"1nOjbJ6vT1sVuqxQCaa4EQHYdQk5w+2NYTeK1TUhKmY=">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"x86lsx8NdTsLHhvU9aNAyIeE61P+Yt6GyVOHrh2MFQo=">>}}]}},
    {<<"scram-sha-1">>,
     {[{<<"salt">>,<<"2rk59nwO1XOIg7dCVQ5eTg7p0y8=">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"4dXFSggAV9gFKuhCJ2pKs13gZZUKMc03Uwl7Sc+puO0=">>}}]}}]}}]
[ns_server:debug,2025-05-15T18:47:22.863Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',node_cert} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{3,63914554042}}]},
 {subject,<<"CN=Couchbase Server Node (db2.lan)">>},
 {not_after,63985747642},
 {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
 {load_timestamp,63914554042},
 {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQ"...>>},
 {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDOjCCAiKgAwIBAgIIGD/HyAMALgMwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjJaFw0y\nNzA4MTcxODQ3MjJaMCoxKDAmBgNVBAMTH0NvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nZGIyLmxhbikwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC63r7EIhIk\nOyMwE9mw7bzVJfstC9FfdZvk0bP8vxZkuhNt/bVcND8p00ihMP8FCbfsa5d8VgWC\nDoao/+R"...>>},
 {pkey_passphrase_settings,[]},
 {certs_epoch,0},
 {type,generated},
 {hostname,"db2.lan"}]
[ns_server:debug,2025-05-15T18:47:22.863Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',client_cert} ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{3,63914554042}}]},
 {subject,<<"CN=Couchbase Internal Client (116550952)">>},
 {not_after,63985747642},
 {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
 {load_timestamp,63914554042},
 {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQ"...>>},
 {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDWjCCAkKgAwIBAgIIGD/HyA7ClpEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjJaFw0y\nNzA4MTcxODQ3MjJaMDAxLjAsBgNVBAMTJUNvdWNoYmFzZSBJbnRlcm5hbCBDbGll\nbnQgKDExNjU1MDk1MikwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDg\nAaBAyJnrm54fDG0xlsEQH9E51WTLpkwtleiz980c498XKRPsgRWnjY920KTUIH2/\ndQcryFy"...>>},
 {pkey_passphrase_settings,[]},
 {certs_epoch,0},
 {type,generated},
 {name,"@internal"}]
[ns_server:debug,2025-05-15T18:47:22.863Z,ns_1@db2.lan:ns_config_rep<0.554.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 23 us
[ns_server:debug,2025-05-15T18:47:22.864Z,ns_1@db2.lan:<0.2159.0>:dist_manager:complete_rename:411]Node 'ns_1@cb.local' has been renamed to 'ns_1@db2.lan'.
[ns_server:debug,2025-05-15T18:47:22.864Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Ensure config is going to change listeners. Will be stopped: [], will be started: [], will be restarted: []
[ns_server:debug,2025-05-15T18:47:22.864Z,ns_1@db2.lan:<0.588.0>:restartable:loop:65]Restarting child <0.589.0>
  MFA: {ns_doctor_sup,start_link,[]}
  Shutdown policy: infinity
  Caller: {<0.2159.0>,#Ref<0.3735524962.3650355206.242581>}
[ns_server:debug,2025-05-15T18:47:22.864Z,ns_1@db2.lan:<0.594.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.593.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:22.864Z,ns_1@db2.lan:<0.588.0>:restartable:shutdown_child:114]Successfully terminated process <0.589.0>
[error_logger:info,2025-05-15T18:47:22.864Z,ns_1@db2.lan:ns_doctor_sup<0.2214.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_doctor_sup}
    started: [{pid,<0.2215.0>},
              {id,ns_doctor_events},
              {mfargs,{gen_event,start_link,[{local,ns_doctor_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:22.865Z,ns_1@db2.lan:ns_doctor_sup<0.2214.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_doctor_sup}
    started: [{pid,<0.2216.0>},
              {id,ns_doctor},
              {mfargs,{ns_doctor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:22.865Z,ns_1@db2.lan:<0.588.0>:restartable:start_child:92]Started child process <0.2214.0>
  MFA: {ns_doctor_sup,start_link,[]}
[ns_server:debug,2025-05-15T18:47:22.865Z,ns_1@db2.lan:<0.758.0>:restartable:loop:65]Restarting child <0.761.0>
  MFA: {leader_services_sup,start_link,[]}
  Shutdown policy: infinity
  Caller: {<0.2159.0>,#Ref<0.3735524962.3650355205.242961>}
[ns_server:info,2025-05-15T18:47:22.865Z,ns_1@db2.lan:mb_master<0.774.0>:mb_master:terminate:247]Synchronously shutting down child mb_master_sup
[ns_server:debug,2025-05-15T18:47:22.865Z,ns_1@db2.lan:<0.898.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.897.0>} exited with reason shutdown
[ns_server:info,2025-05-15T18:47:22.865Z,ns_1@db2.lan:leader_registry<0.772.0>:leader_registry:handle_down:286]Process <0.897.0> registered as 'license_reporting' terminated.
[ns_server:debug,2025-05-15T18:47:22.865Z,ns_1@db2.lan:<0.896.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.895.0>} exited with reason shutdown
[ns_server:info,2025-05-15T18:47:22.865Z,ns_1@db2.lan:leader_registry<0.772.0>:leader_registry:handle_down:286]Process <0.893.0> registered as 'tombstone_purger' terminated.
[ns_server:debug,2025-05-15T18:47:22.865Z,ns_1@db2.lan:<0.883.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.881.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:22.865Z,ns_1@db2.lan:<0.882.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {compat_mode_events,<0.881.0>} exited with reason shutdown
[ns_server:info,2025-05-15T18:47:22.865Z,ns_1@db2.lan:leader_registry<0.772.0>:leader_registry:handle_down:286]Process <0.881.0> registered as 'auto_failover' terminated.
[ns_server:info,2025-05-15T18:47:22.865Z,ns_1@db2.lan:leader_registry<0.772.0>:leader_registry:handle_down:286]Process <0.879.0> registered as 'ns_orchestrator' terminated.
[ns_server:debug,2025-05-15T18:47:22.865Z,ns_1@db2.lan:<0.2205.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.2194.0>} exited with reason shutdown
[ns_server:info,2025-05-15T18:47:22.865Z,ns_1@db2.lan:leader_registry<0.772.0>:leader_registry:handle_down:286]Process <0.878.0> registered as 'auto_rebalance' terminated.
[ns_server:info,2025-05-15T18:47:22.865Z,ns_1@db2.lan:leader_registry<0.772.0>:leader_registry:handle_down:286]Process <0.877.0> registered as 'auto_reprovision' terminated.
[ns_server:debug,2025-05-15T18:47:22.865Z,ns_1@db2.lan:leader_activities<0.764.0>:leader_activities:handle_internal_process_down:440]Process {quorum_nodes_manager,<0.2194.0>} terminated with reason shutdown
[ns_server:info,2025-05-15T18:47:22.866Z,ns_1@db2.lan:leader_registry<0.772.0>:leader_registry:handle_down:286]Process <0.814.0> registered as 'chronicle_master' terminated.
[ns_server:debug,2025-05-15T18:47:22.865Z,ns_1@db2.lan:<0.815.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {'kv-events',<0.814.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:22.866Z,ns_1@db2.lan:leader_activities<0.764.0>:leader_activities:handle_internal_process_down:440]Process {acquirer,<0.793.0>} terminated with reason shutdown
[ns_server:debug,2025-05-15T18:47:22.866Z,ns_1@db2.lan:<0.794.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_node_disco_events,<0.793.0>} exited with reason shutdown
[ns_server:info,2025-05-15T18:47:22.866Z,ns_1@db2.lan:leader_registry<0.772.0>:leader_registry:handle_down:286]Process <0.811.0> registered as 'ns_tick' terminated.
[ns_server:debug,2025-05-15T18:47:22.866Z,ns_1@db2.lan:leader_registry<0.772.0>:leader_registry:handle_new_leader:275]New leader is undefined. Invalidating name cache.
[ns_server:debug,2025-05-15T18:47:22.866Z,ns_1@db2.lan:<0.775.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.774.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:22.866Z,ns_1@db2.lan:<0.773.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {leader_events,<0.772.0>} exited with reason shutdown
[ns_server:warn,2025-05-15T18:47:22.866Z,ns_1@db2.lan:leader_lease_agent<0.767.0>:leader_lease_agent:handle_terminate:302]Terminating with reason shutdown when we own an active lease:
{lease,
    {lease_holder,<<"445e053598a21dcd845301efeef2f950">>,'ns_1@cb.local'},
    -576460710129722772,-576460695129722772,
    {timer,#Ref<0.3735524962.3650355202.242247>,
        {lease_expired,
            {lease_holder,<<"445e053598a21dcd845301efeef2f950">>,
                'ns_1@cb.local'}}},
    active}
Persisting updated lease.
[ns_server:debug,2025-05-15T18:47:22.868Z,ns_1@db2.lan:leader_activities<0.764.0>:leader_activities:handle_internal_process_down:440]Process {agent,<0.767.0>} terminated with reason shutdown
[ns_server:debug,2025-05-15T18:47:22.868Z,ns_1@db2.lan:<0.758.0>:restartable:shutdown_child:114]Successfully terminated process <0.761.0>
[error_logger:info,2025-05-15T18:47:22.869Z,ns_1@db2.lan:leader_leases_sup<0.2226.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_leases_sup}
    started: [{pid,<0.2227.0>},
              {id,leader_activities},
              {mfargs,{leader_activities,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,10000},
              {child_type,worker}]

[ns_server:warn,2025-05-15T18:47:22.869Z,ns_1@db2.lan:leader_lease_agent<0.2228.0>:leader_lease_agent:maybe_recover_persisted_lease:393]Found persisted lease [{node,'ns_1@cb.local'},
                       {uuid,<<"445e053598a21dcd845301efeef2f950">>},
                       {time_left,14472},
                       {status,active}]
[error_logger:info,2025-05-15T18:47:22.869Z,ns_1@db2.lan:leader_leases_sup<0.2226.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_leases_sup}
    started: [{pid,<0.2228.0>},
              {id,leader_lease_agent},
              {mfargs,{leader_lease_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:22.870Z,ns_1@db2.lan:leader_services_sup<0.2223.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_services_sup}
    started: [{pid,<0.2226.0>},
              {id,leader_leases_sup},
              {mfargs,{leader_leases_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:22.870Z,ns_1@db2.lan:leader_registry_sup<0.2229.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_registry_sup}
    started: [{pid,<0.2230.0>},
              {id,leader_registry},
              {mfargs,{leader_registry,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:22.870Z,ns_1@db2.lan:leader_registry_sup<0.2229.0>:mb_master:check_master_takeover_needed:183]Sending master node question to the following nodes: []
[ns_server:debug,2025-05-15T18:47:22.870Z,ns_1@db2.lan:leader_registry_sup<0.2229.0>:mb_master:check_master_takeover_needed:187]Got replies: []
[ns_server:debug,2025-05-15T18:47:22.870Z,ns_1@db2.lan:leader_registry_sup<0.2229.0>:mb_master:check_master_takeover_needed:193]Was unable to discover master, not going to force mastership takeover
[ns_server:debug,2025-05-15T18:47:22.870Z,ns_1@db2.lan:mb_master<0.2232.0>:mb_master:init:86]Heartbeat interval is 2000
[user:info,2025-05-15T18:47:22.870Z,ns_1@db2.lan:mb_master<0.2232.0>:mb_master:init:91]I'm the only node, so I'm the master.
[ns_server:info,2025-05-15T18:47:22.870Z,ns_1@db2.lan:ns_log<0.506.0>:ns_log:is_duplicate_log:156]suppressing duplicate log mb_master:0([<<"I'm the only node, so I'm the master.">>]) because it's been seen 1 times in the past 32.633545 secs (last seen 32.633545 secs ago
[ns_server:debug,2025-05-15T18:47:22.870Z,ns_1@db2.lan:leader_registry<0.2230.0>:leader_registry:handle_new_leader:275]New leader is 'ns_1@db2.lan'. Invalidating name cache.
[error_logger:info,2025-05-15T18:47:22.870Z,ns_1@db2.lan:mb_master_sup<0.2234.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.2235.0>},
              {id,leader_lease_acquirer},
              {mfargs,{leader_lease_acquirer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,10000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:22.870Z,ns_1@db2.lan:leader_quorum_nodes_manager<0.2239.0>:leader_quorum_nodes_manager:pull_config:99]Attempting to pull config from nodes:
[]
[error_logger:info,2025-05-15T18:47:22.870Z,ns_1@db2.lan:mb_master_sup<0.2234.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.2239.0>},
              {id,leader_quorum_nodes_manager},
              {mfargs,{leader_quorum_nodes_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:22.870Z,ns_1@db2.lan:leader_quorum_nodes_manager<0.2239.0>:leader_quorum_nodes_manager:pull_config:104]Pulled config successfully.
[ns_server:warn,2025-05-15T18:47:22.870Z,ns_1@db2.lan:<0.2240.0>:leader_lease_acquire_worker:handle_lease_already_acquired:226]Failed to acquire lease from 'ns_1@db2.lan' because its already taken by {'ns_1@cb.local',
                                                                          <<"445e053598a21dcd845301efeef2f950">>} (valid for 14470ms)
[ns_server:info,2025-05-15T18:47:22.871Z,ns_1@db2.lan:mb_master_sup<0.2234.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,ns_tick},
                                         ns_tick,[],[]]): started as <0.2245.0> on 'ns_1@db2.lan'

[error_logger:info,2025-05-15T18:47:22.871Z,ns_1@db2.lan:mb_master_sup<0.2234.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.2245.0>},
              {id,ns_tick},
              {mfargs,{ns_tick,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,10},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:22.871Z,ns_1@db2.lan:<0.2247.0>:chronicle_master:do_init:115]Starting with SelfRef = #Ref<0.3735524962.3650355207.241528>
[ns_server:info,2025-05-15T18:47:22.871Z,ns_1@db2.lan:mb_master_sup<0.2234.0>:misc:start_singleton:913]start_singleton(gen_server2, start_link, [{via,leader_registry,
                                           chronicle_master},
                                          chronicle_master,[],[]]): started as <0.2247.0> on 'ns_1@db2.lan'

[error_logger:info,2025-05-15T18:47:22.871Z,ns_1@db2.lan:mb_master_sup<0.2234.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.2247.0>},
              {id,chronicle_master},
              {mfargs,{chronicle_master,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:22.871Z,ns_1@db2.lan:ns_orchestrator_sup<0.2249.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.2250.0>},
              {id,compat_mode_events},
              {mfargs,{gen_event,start_link,[{local,compat_mode_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:22.872Z,ns_1@db2.lan:ns_orchestrator_sup<0.2249.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.2251.0>},
              {id,compat_mode_manager},
              {mfargs,{compat_mode_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:22.872Z,ns_1@db2.lan:ns_orchestrator_child_sup<0.2252.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.2253.0>},
              {id,ns_janitor_server},
              {mfargs,{ns_janitor_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:47:22.872Z,ns_1@db2.lan:ns_orchestrator_child_sup<0.2252.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          auto_reprovision},
                                         auto_reprovision,[],[]]): started as <0.2254.0> on 'ns_1@db2.lan'

[error_logger:info,2025-05-15T18:47:22.872Z,ns_1@db2.lan:ns_orchestrator_child_sup<0.2252.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.2254.0>},
              {id,auto_reprovision},
              {mfargs,{auto_reprovision,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:47:22.872Z,ns_1@db2.lan:ns_orchestrator_child_sup<0.2252.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,auto_rebalance},
                                         auto_rebalance,[],[]]): started as <0.2256.0> on 'ns_1@db2.lan'

[error_logger:info,2025-05-15T18:47:22.872Z,ns_1@db2.lan:ns_orchestrator_child_sup<0.2252.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.2256.0>},
              {id,auto_rebalance},
              {mfargs,{auto_rebalance,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:47:22.872Z,ns_1@db2.lan:ns_orchestrator_child_sup<0.2252.0>:misc:start_singleton:913]start_singleton(gen_statem, start_link, [{via,leader_registry,ns_orchestrator},
                                         ns_orchestrator,[],[]]): started as <0.2257.0> on 'ns_1@db2.lan'

[error_logger:info,2025-05-15T18:47:22.872Z,ns_1@db2.lan:ns_orchestrator_child_sup<0.2252.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.2257.0>},
              {id,ns_orchestrator},
              {mfargs,{ns_orchestrator,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:22.873Z,ns_1@db2.lan:ns_orchestrator_sup<0.2249.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.2252.0>},
              {id,ns_orchestrator_child_sup},
              {mfargs,{ns_orchestrator_child_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:47:22.873Z,ns_1@db2.lan:<0.2259.0>:auto_failover:init:223]init auto_failover.
[user:info,2025-05-15T18:47:22.873Z,ns_1@db2.lan:<0.2259.0>:auto_failover:enable_auto_failover:460]Enabled auto-failover with timeout 120 and max count 1
[ns_server:debug,2025-05-15T18:47:22.873Z,ns_1@db2.lan:<0.2259.0>:auto_failover:update_and_save_auto_failover_state:479]No change in timeout 120
[ns_server:info,2025-05-15T18:47:22.873Z,ns_1@db2.lan:ns_log<0.506.0>:ns_log:is_duplicate_log:156]suppressing duplicate log auto_failover:0([<<"Enabled auto-failover with timeout 120 and max count 1">>]) because it's been seen 1 times in the past 32.40165 secs (last seen 32.40165 secs ago
[ns_server:debug,2025-05-15T18:47:22.873Z,ns_1@db2.lan:<0.2259.0>:auto_failover:update_and_save_auto_failover_state:487]No change in max count 1
[ns_server:debug,2025-05-15T18:47:22.873Z,ns_1@db2.lan:<0.2259.0>:auto_failover:init_logic_state:249]Using auto-failover logic state {state,[],
                                    [{service_state,kv,nil,false},
                                     {service_state,n1ql,nil,false},
                                     {service_state,index,nil,false},
                                     {service_state,fts,nil,false},
                                     {service_state,cbas,nil,false},
                                     {service_state,eventing,nil,false},
                                     {service_state,backup,nil,false}],
                                    118}
[ns_server:info,2025-05-15T18:47:22.873Z,ns_1@db2.lan:ns_orchestrator_sup<0.2249.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,auto_failover},
                                         auto_failover,[],[]]): started as <0.2259.0> on 'ns_1@db2.lan'

[error_logger:info,2025-05-15T18:47:22.873Z,ns_1@db2.lan:ns_orchestrator_sup<0.2249.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.2259.0>},
              {id,auto_failover},
              {mfargs,{auto_failover,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:22.873Z,ns_1@db2.lan:mb_master_sup<0.2234.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.2249.0>},
              {id,ns_orchestrator_sup},
              {mfargs,{ns_orchestrator_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:info,2025-05-15T18:47:22.873Z,ns_1@db2.lan:mb_master_sup<0.2234.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          tombstone_purger},
                                         tombstone_purger,[],[]]): started as <0.2265.0> on 'ns_1@db2.lan'

[error_logger:info,2025-05-15T18:47:22.873Z,ns_1@db2.lan:mb_master_sup<0.2234.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.2265.0>},
              {id,tombstone_purger},
              {mfargs,{tombstone_purger,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:22.873Z,ns_1@db2.lan:mb_master_sup<0.2234.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.2267.0>},
              {id,global_tasks},
              {mfargs,{global_tasks,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:22.874Z,ns_1@db2.lan:guardrail_enforcer<0.2269.0>:guardrail_enforcer:maybe_notify_services:137]Changed Statuses: #{}
[error_logger:info,2025-05-15T18:47:22.874Z,ns_1@db2.lan:mb_master_sup<0.2234.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.2269.0>},
              {id,guardrail_enforcer},
              {mfargs,{guardrail_enforcer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:22.874Z,ns_1@db2.lan:<0.2271.0>:license_reporting:init:60]Starting license_reporting server
[ns_server:info,2025-05-15T18:47:22.874Z,ns_1@db2.lan:mb_master_sup<0.2234.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          license_reporting},
                                         license_reporting,[],[]]): started as <0.2271.0> on 'ns_1@db2.lan'

[error_logger:info,2025-05-15T18:47:22.874Z,ns_1@db2.lan:mb_master_sup<0.2234.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.2271.0>},
              {id,license_reporting},
              {mfargs,{license_reporting,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:22.874Z,ns_1@db2.lan:leader_registry_sup<0.2229.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_registry_sup}
    started: [{pid,<0.2232.0>},
              {id,mb_master},
              {mfargs,{mb_master,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:22.874Z,ns_1@db2.lan:leader_services_sup<0.2223.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_services_sup}
    started: [{pid,<0.2229.0>},
              {id,leader_registry_sup},
              {mfargs,{leader_registry_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:47:22.874Z,ns_1@db2.lan:<0.758.0>:restartable:start_child:92]Started child process <0.2223.0>
  MFA: {leader_services_sup,start_link,[]}
[ns_server:debug,2025-05-15T18:47:22.875Z,ns_1@db2.lan:ns_node_disco<0.545.0>:ns_node_disco:handle_info:160]Node renaming transaction ended. MRef = #Ref<0.3735524962.3650355207.241365>
[ns_server:debug,2025-05-15T18:47:22.875Z,ns_1@db2.lan:remote_monitors<0.297.0>:remote_monitors:handle_info:86]Node renaming transaction ended. MRef = #Ref<0.3735524962.3650355207.241372>
[ns_server:debug,2025-05-15T18:47:22.875Z,ns_1@db2.lan:ns_cookie_manager<0.238.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[ns_server:debug,2025-05-15T18:47:22.875Z,ns_1@db2.lan:<0.707.0>:terse_cluster_info_uploader:handle_info:64]Got DOWN with reason: unpaused from memcached port server: <16971.139.0>. Shutting down
[ns_server:debug,2025-05-15T18:47:22.875Z,ns_1@db2.lan:memcached_config_mgr<0.704.0>:memcached_config_mgr:handle_info:198]Got DOWN with reason: unpaused from memcached port server: <16971.139.0>. Shutting down
[ns_server:debug,2025-05-15T18:47:22.875Z,ns_1@db2.lan:wait_link_to_couchdb_node<0.472.0>:ns_server_nodes_sup:wait_link_to_couchdb_node_loop:202]Link to couchdb node was unpaused.
[ns_server:debug,2025-05-15T18:47:22.875Z,ns_1@db2.lan:wait_link_to_couchdb_node<0.472.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:158]Waiting for ns_couchdb node to start
[ns_server:debug,2025-05-15T18:47:22.875Z,ns_1@db2.lan:ns_node_disco_events<0.544.0>:ns_config_rep:handle_node_disco_event:513]Detected new nodes (['ns_1@db2.lan']).  Moving config around.
[ns_server:debug,2025-05-15T18:47:22.875Z,ns_1@db2.lan:<0.806.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.704.0>} exited with reason {shutdown,
                                                                                {memcached_port_server_down,
                                                                                 <16971.139.0>,
                                                                                 unpaused}}
[ns_server:debug,2025-05-15T18:47:22.875Z,ns_1@db2.lan:<0.708.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {bucket_info_cache_invalidations,<0.707.0>} exited with reason {shutdown,
                                                                                               {memcached_port_server_down,
                                                                                                <16971.139.0>,
                                                                                                unpaused}}
[ns_server:debug,2025-05-15T18:47:22.875Z,ns_1@db2.lan:<0.2275.0>:ns_node_disco:do_nodes_wanted_updated_fun:210]ns_node_disco: nodes_wanted updated: ['ns_1@db2.lan'], with cookie: {sanitized,
                                                                     <<"laexClf6j/iiXLRHc+IgLQ5C2ku89YxpvAez8UPpQjA=">>}
[ns_server:info,2025-05-15T18:47:22.875Z,ns_1@db2.lan:ns_node_disco_events<0.544.0>:ns_node_disco_log:handle_event:40]ns_node_disco_log: nodes changed: ['ns_1@db2.lan']
[error_logger:error,2025-05-15T18:47:22.875Z,ns_1@db2.lan:<0.703.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {<0.703.0>,suppress_max_restart_intensity}
    errorContext: child_terminated
    reason: {shutdown,{memcached_port_server_down,<16971.139.0>,unpaused}}
    offender: [{pid,<0.704.0>},
               {id,memcached_config_mgr},
               {mfargs,{memcached_config_mgr,start_link,[]}},
               {restart_type,permanent},
               {significant,false},
               {shutdown,1000},
               {child_type,worker}]

[error_logger:error,2025-05-15T18:47:22.875Z,ns_1@db2.lan:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {local,ns_server_sup}
    errorContext: child_terminated
    reason: {shutdown,{memcached_port_server_down,<16971.139.0>,unpaused}}
    offender: [{pid,<0.707.0>},
               {id,terse_cluster_info_uploader},
               {mfargs,{terse_cluster_info_uploader,start_link,[]}},
               {restart_type,permanent},
               {significant,false},
               {shutdown,1000},
               {child_type,worker}]

[cluster:debug,2025-05-15T18:47:22.875Z,ns_1@db2.lan:ns_cluster<0.273.0>:ns_cluster:maybe_rename:751]Renamed node from 'ns_1@cb.local' to 'ns_1@db2.lan'.
[ns_server:debug,2025-05-15T18:47:22.875Z,ns_1@db2.lan:memcached_config_mgr<0.2277.0>:memcached_config_mgr:memcached_port_pid:154]waiting for completion of initial ns_ports_setup round
[cluster:info,2025-05-15T18:47:22.875Z,ns_1@db2.lan:ns_cluster<0.273.0>:ns_cluster:do_change_address:720]Renamed node. New name is 'ns_1@db2.lan'.
[ns_server:debug,2025-05-15T18:47:22.875Z,ns_1@db2.lan:<0.2275.0>:ns_node_disco:do_nodes_wanted_updated_fun:213]ns_node_disco: nodes_wanted pong: ['ns_1@db2.lan'], with cookie: {sanitized,
                                                                  <<"laexClf6j/iiXLRHc+IgLQ5C2ku89YxpvAez8UPpQjA=">>}
[error_logger:info,2025-05-15T18:47:22.875Z,ns_1@db2.lan:<0.703.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.703.0>,suppress_max_restart_intensity}
    started: [{pid,<0.2277.0>},
              {id,memcached_config_mgr},
              {mfargs,{memcached_config_mgr,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:22.875Z,ns_1@db2.lan:<0.2278.0>:memcached_config_mgr:memcached_port_pid:154]waiting for completion of initial ns_ports_setup round
[error_logger:info,2025-05-15T18:47:22.875Z,ns_1@db2.lan:ns_server_sup<0.496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2278.0>},
              {id,terse_cluster_info_uploader},
              {mfargs,{terse_cluster_info_uploader,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:22.875Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
otp ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{2,63914554042}}]},
 {cookie,{sanitized,<<"ORNEWLx0XMugmBkJLCvC9wwYSBqJj1X9W2089cGBvPY=">>}}]
[user:info,2025-05-15T18:47:22.875Z,ns_1@db2.lan:ns_cookie_manager<0.238.0>:ns_cookie_manager:do_cookie_init:78]Initial otp cookie generated: {sanitized,
                                  <<"ORNEWLx0XMugmBkJLCvC9wwYSBqJj1X9W2089cGBvPY=">>}
[ns_server:debug,2025-05-15T18:47:22.875Z,ns_1@db2.lan:ns_cookie_manager<0.238.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[cluster:debug,2025-05-15T18:47:22.875Z,ns_1@db2.lan:ns_cluster<0.273.0>:ns_cluster:handle_call:426]engage_cluster(..) -> {ok,ok}
[ns_server:debug,2025-05-15T18:47:22.875Z,ns_1@db2.lan:<0.2282.0>:ns_node_disco:do_nodes_wanted_updated_fun:210]ns_node_disco: nodes_wanted updated: ['ns_1@db2.lan'], with cookie: {sanitized,
                                                                     <<"ORNEWLx0XMugmBkJLCvC9wwYSBqJj1X9W2089cGBvPY=">>}
[ns_server:debug,2025-05-15T18:47:22.876Z,ns_1@db2.lan:ns_config_rep<0.554.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([otp]..)
[ns_server:debug,2025-05-15T18:47:22.876Z,ns_1@db2.lan:<0.2282.0>:ns_node_disco:do_nodes_wanted_updated_fun:213]ns_node_disco: nodes_wanted pong: ['ns_1@db2.lan'], with cookie: {sanitized,
                                                                  <<"ORNEWLx0XMugmBkJLCvC9wwYSBqJj1X9W2089cGBvPY=">>}
[ns_server:debug,2025-05-15T18:47:22.882Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Accepted new connection from <0.2176.0> DistCtrl #Port<0.85>: {con,
                                                                        #Ref<0.3735524962.3650355204.243099>,
                                                                        inet_tcp_dist,
                                                                        undefined,
                                                                        undefined}
[ns_server:debug,2025-05-15T18:47:22.882Z,ns_1@db2.lan:net_kernel<0.2175.0>:cb_dist:info_msg:1098]cb_dist: Accepting connection from <0.2176.0> using module inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:22.882Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3735524962.3650355204.243099>,
                                  inet_tcp_dist,<0.2289.0>,
                                  #Ref<0.3735524962.3650355204.243102>}
[ns_server:debug,2025-05-15T18:47:22.883Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3735524962.3650355204.243099>,
                               inet_tcp_dist,<0.2289.0>,
                               #Ref<0.3735524962.3650355204.243102>}
[error_logger:info,2025-05-15T18:47:22.883Z,ns_1@db2.lan:net_kernel<0.2175.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.2289.0>,shutdown}}
[ns_server:debug,2025-05-15T18:47:22.899Z,ns_1@db2.lan:ns_ports_setup<0.677.0>:ns_ports_setup:children_loop_continue:98]Remote monitor <16971.133.0> was unpaused after node name change. Restart loop.
[ns_server:debug,2025-05-15T18:47:22.899Z,ns_1@db2.lan:ns_ports_setup<0.677.0>:ns_ports_manager:set_dynamic_children:48]Setting children [memcached,saslauthd_port,goxdcr]
[ns_server:debug,2025-05-15T18:47:22.900Z,ns_1@db2.lan:ns_ports_setup<0.677.0>:ns_ports_setup:set_children:65]Monitor ns_child_ports_sup <16971.133.0>
[ns_server:debug,2025-05-15T18:47:22.900Z,ns_1@db2.lan:memcached_config_mgr<0.2277.0>:memcached_config_mgr:memcached_port_pid:156]ns_ports_setup seems to be ready
[ns_server:debug,2025-05-15T18:47:22.900Z,ns_1@db2.lan:<0.2278.0>:memcached_config_mgr:memcached_port_pid:156]ns_ports_setup seems to be ready
[ns_server:debug,2025-05-15T18:47:22.901Z,ns_1@db2.lan:<0.2278.0>:memcached_config_mgr:find_port_pid_loop:164]Found memcached port <16971.139.0>
[ns_server:debug,2025-05-15T18:47:22.901Z,ns_1@db2.lan:memcached_config_mgr<0.2277.0>:memcached_config_mgr:find_port_pid_loop:164]Found memcached port <16971.139.0>
[ns_server:debug,2025-05-15T18:47:22.901Z,ns_1@db2.lan:<0.2278.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":27,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true,\"hostname\":\"db2.lan\",\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[ns_server:debug,2025-05-15T18:47:22.902Z,ns_1@db2.lan:memcached_config_mgr<0.2277.0>:memcached_config_mgr:do_read_current_memcached_config:371]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2025-05-15T18:47:22.902Z,ns_1@db2.lan:memcached_config_mgr<0.2277.0>:memcached_config_mgr:init:100]found memcached port to be already active
[ns_server:info,2025-05-15T18:47:22.903Z,ns_1@db2.lan:memcached_config_mgr<0.2277.0>:memcached_config_mgr:push_tls_config:233]Pushing TLS config to memcached:
{[{<<"private key">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem">>},
  {<<"certificate chain">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem">>},
  {<<"CA file">>,<<"/opt/couchbase/var/lib/couchbase/config/certs/ca.pem">>},
  {<<"minimum version">>,<<"TLS 1.2">>},
  {<<"cipher list">>,
   {[{<<"TLS 1.2">>,<<"HIGH">>},
     {<<"TLS 1.3">>,
      <<"TLS_AES_256_GCM_SHA384:TLS_AES_128_GCM_SHA256:TLS_CHACHA20_POLY1305_SHA256">>}]}},
  {<<"cipher order">>,true},
  {<<"client cert auth">>,<<"disabled">>}]}
[ns_server:info,2025-05-15T18:47:22.906Z,ns_1@db2.lan:memcached_config_mgr<0.2277.0>:memcached_config_mgr:push_tls_config:237]Successfully pushed TLS config to memcached
[ns_server:debug,2025-05-15T18:47:22.937Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Accepted new connection from <0.2176.0> DistCtrl #Port<0.86>: {con,
                                                                        #Ref<0.3735524962.3650355204.243106>,
                                                                        inet_tcp_dist,
                                                                        undefined,
                                                                        undefined}
[ns_server:debug,2025-05-15T18:47:22.937Z,ns_1@db2.lan:net_kernel<0.2175.0>:cb_dist:info_msg:1098]cb_dist: Accepting connection from <0.2176.0> using module inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:22.937Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3735524962.3650355204.243106>,
                                  inet_tcp_dist,<0.2299.0>,
                                  #Ref<0.3735524962.3650355208.241941>}
[error_logger:error,2025-05-15T18:47:22.938Z,ns_1@db2.lan:<0.2299.0>:ale_error_logger_handler:do_log:101]
=========================ERROR REPORT=========================
** Connection attempt from node 'ns_1@172.19.0.4' rejected. Invalid challenge reply. **

[ns_server:debug,2025-05-15T18:47:22.939Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3735524962.3650355204.243106>,
                               inet_tcp_dist,<0.2299.0>,
                               #Ref<0.3735524962.3650355208.241941>}
[error_logger:info,2025-05-15T18:47:22.939Z,ns_1@db2.lan:net_kernel<0.2175.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.2299.0>,{recv_challenge_reply_failed,bad_cookie}}}
[error_logger:info,2025-05-15T18:47:22.939Z,ns_1@db2.lan:net_kernel<0.2175.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'ns_1@172.19.0.4'}}
[ns_server:debug,2025-05-15T18:47:22.959Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Accepted new connection from <0.2176.0> DistCtrl #Port<0.87>: {con,
                                                                        #Ref<0.3735524962.3650355204.243109>,
                                                                        inet_tcp_dist,
                                                                        undefined,
                                                                        undefined}
[ns_server:debug,2025-05-15T18:47:22.959Z,ns_1@db2.lan:net_kernel<0.2175.0>:cb_dist:info_msg:1098]cb_dist: Accepting connection from <0.2176.0> using module inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:22.959Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3735524962.3650355204.243109>,
                                  inet_tcp_dist,<0.2301.0>,
                                  #Ref<0.3735524962.3650355204.243112>}
[error_logger:error,2025-05-15T18:47:22.959Z,ns_1@db2.lan:<0.2301.0>:ale_error_logger_handler:do_log:101]
=========================ERROR REPORT=========================
** Connection attempt from node 'ns_1@172.19.0.4' rejected. Invalid challenge reply. **

[ns_server:debug,2025-05-15T18:47:22.959Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3735524962.3650355204.243109>,
                               inet_tcp_dist,<0.2301.0>,
                               #Ref<0.3735524962.3650355204.243112>}
[error_logger:info,2025-05-15T18:47:22.959Z,ns_1@db2.lan:net_kernel<0.2175.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.2301.0>,{recv_challenge_reply_failed,bad_cookie}}}
[error_logger:info,2025-05-15T18:47:22.959Z,ns_1@db2.lan:net_kernel<0.2175.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'ns_1@172.19.0.4'}}
[cluster:debug,2025-05-15T18:47:22.996Z,ns_1@db2.lan:ns_cluster<0.273.0>:ns_cluster:handle_call:431]handling complete_join([{<<"targetNode">>,<<"ns_1@db2.lan">>},
                        {<<"requestedServices">>,
                         [<<"index">>,<<"kv">>,<<"n1ql">>]},
                        {<<"chronicleInfo">>,
                         <<"g3QAAAAEZAAPY29tbWl0dGVkX3NlcW5vYRFkAA5jb21wYXRfdmVyc2lvbmEAZAAGY29uZmlnaAVkAAlsb2dfZW50cnltAAAAIDVhYWIwM2RiZWNhZDA2YjUwZmZiNDc0ZGE1OWEwMGM5aAJhBGQAD25zXzFAMTcyLjE5LjAuNGERaApkAAZjb25maWdoA20AAAAgMWRkNTk5MTQ2M2Q1MTllMjM3MjlkZGEyYzgwYzM3NmJhAGEBYQBtAAAAIDQ5ZDU3NjJlMjg3Nzg2YzMxMjc5MDU1MTBmN2E0MTE1dAAAAAJkAA9uc18xQDE3Mi4xOS4wLjR0AAAAAmQAAmlkbQAAACAxZGQ1OTkxNDYzZDUxOWUyMzcyOWRkYTJjODBjMzc2YmQABHJvbGVkAAV2b3RlcmQADG5zXzFAZGIyLmxhbnQAAAACZAACaWRtAAAAIDlmNjg5NGI1MWZmYTQ5N2NkZmM4Yjc5MmQ5ZTY3MjZiZAAEcm9sZWQAB3JlcGxpY2FkAAl1bmRlZmluZWR0AAAAAmQAFGNocm9uaWNsZV9jb25maWdfcnNtaANkAApyc21fY29uZmlnZAAUY2hyb25pY2xlX2NvbmZpZ19yc21qZAACa3ZoA2QACnJzbV9jb25maWdkAAxjaHJvbmljbGVfa3ZqdAAAAABkAAl1bmRlZmluZWRsAAAAAWgCbQAAACA1YWFiMDNkYmVjYWQwNmI1MGZmYjQ3NGRhNTlhMDBjOWEAamQACmhpc3RvcnlfaWRtAAAAIDVhYWIwM2RiZWNhZDA2YjUwZmZiNDc0ZGE1OWEwMGM5">>},
                        {<<"availableStorage">>,
                         {[{<<"hdd">>,
                            [{[{<<"path">>,<<"/">>},
                               {<<"sizeKBytes">>,1056557396},
                               {<<"usagePercent">>,2}]},
                             {[{<<"path">>,<<"/dev">>},
                               {<<"sizeKBytes">>,65536},
                               {<<"usagePercent">>,0}]},
                             {[{<<"path">>,<<"/dev/shm">>},
                               {<<"sizeKBytes">>,65536},
                               {<<"usagePercent">>,0}]},
                             {[{<<"path">>,<<"/etc/resolv.conf">>},
                               {<<"sizeKBytes">>,1056557396},
                               {<<"usagePercent">>,2}]},
                             {[{<<"path">>,<<"/etc/hostname">>},
                               {<<"sizeKBytes">>,1056557396},
                               {<<"usagePercent">>,2}]},
                             {[{<<"path">>,<<"/etc/hosts">>},
                               {<<"sizeKBytes">>,1056557396},
                               {<<"usagePercent">>,2}]},
                             {[{<<"path">>,<<"/opt/couchbase/var">>},
                               {<<"sizeKBytes">>,482797652},
                               {<<"usagePercent">>,92}]},
                             {[{<<"path">>,<<"/proc/kcore">>},
                               {<<"sizeKBytes">>,65536},
                               {<<"usagePercent">>,0}]},
                             {[{<<"path">>,<<"/proc/keys">>},
                               {<<"sizeKBytes">>,65536},
                               {<<"usagePercent">>,0}]},
                             {[{<<"path">>,<<"/proc/timer_list">>},
                               {<<"sizeKBytes">>,65536},
                               {<<"usagePercent">>,0}]},
                             {[{<<"path">>,<<"/proc/scsi">>},
                               {<<"sizeKBytes">>,4012680},
                               {<<"usagePercent">>,0}]},
                             {[{<<"path">>,<<"/sys/firmware">>},
                               {<<"sizeKBytes">>,4012680},
                               {<<"usagePercent">>,0}]}]}]}},
                        {<<"storageTotals">>,
                         {[{<<"ram">>,
                            {[{<<"total">>,8217968640},
                              {<<"quotaTotal">>,3221225472},
                              {<<"quotaUsed">>,0},
                              {<<"used">>,4251119616},
                              {<<"usedByData">>,0},
                              {<<"quotaUsedPerNode">>,0},
                              {<<"quotaTotalPerNode">>,3221225472}]}},
                           {<<"hdd">>,
                            {[{<<"total">>,494384795648},
                              {<<"quotaTotal">>,494384795648},
                              {<<"used">>,454834011996},
                              {<<"usedByData">>,0},
                              {<<"free">>,39550783652}]}}]}},
                        {<<"storage">>,
                         {[{<<"ssd">>,[]},
                           {<<"hdd">>,
                            [{[{<<"path">>,
                                <<"/opt/couchbase/var/lib/couchbase/data">>},
                               {<<"index_path">>,
                                <<"/opt/couchbase/var/lib/couchbase/data">>},
                               {<<"cbas_dirs">>,
                                [<<"/opt/couchbase/var/lib/couchbase/data">>]},
                               {<<"eventing_path">>,
                                <<"/opt/couchbase/var/lib/couchbase/data">>},
                               {<<"java_home">>,<<>>},
                               {<<"quotaMb">>,<<"none">>},
                               {<<"state">>,<<"ok">>}]}]}]}},
                        {<<"clusterMembership">>,<<"active">>},
                        {<<"recoveryType">>,<<"none">>},
                        {<<"status">>,<<"healthy">>},
                        {<<"otpNode">>,<<"ns_1@172.19.0.4">>},
                        {<<"thisNode">>,true},
                        {<<"hostname">>,<<"172.19.0.4:8091">>},
                        {<<"nodeUUID">>,
                         <<"28569ac00b9c1d7c50e39741027d428c">>},
                        {<<"clusterCompatibility">>,458758},
                        {<<"version">>,<<"7.6.2-3721-enterprise">>},
                        {<<"os">>,<<"aarch64-unknown-linux-gnu">>},
                        {<<"cpuCount">>,10},
                        {<<"ports">>,
                         {[{<<"direct">>,11210},
                           {<<"httpsMgmt">>,18091},
                           {<<"httpsCAPI">>,18092},
                           {<<"distTCP">>,21100},
                           {<<"distTLS">>,21150}]}},
                        {<<"services">>,[<<"index">>,<<"kv">>,<<"n1ql">>]},
                        {<<"nodeEncryption">>,false},
                        {<<"nodeEncryptionClientCertVerification">>,false},
                        {<<"addressFamilyOnly">>,false},
                        {<<"configuredHostname">>,<<"172.19.0.4:8091">>},
                        {<<"addressFamily">>,<<"inet">>},
                        {<<"externalListeners">>,
                         [{[{<<"afamily">>,<<"inet">>},
                            {<<"nodeEncryption">>,false}]}]},
                        {<<"serverGroup">>,<<"Group 1">>},
                        {<<"otpCookie">>,
                         {sanitized,<<"biSYTaQUFDVndTs/b+IcDmD2L0woktv9naRdv3GCK6A=">>}},
                        {<<"couchApiBase">>,<<"http://172.19.0.4:8092/">>},
                        {<<"couchApiBaseHTTPS">>,
                         <<"https://172.19.0.4:18092/">>},
                        {<<"nodeHash">>,38045879},
                        {<<"systemStats">>,
                         {[{<<"cpu_utilization_rate">>,3.89883035089008},
                           {<<"cpu_stolen_rate">>,0},
                           {<<"swap_total">>,1073737728},
                           {<<"swap_used">>,3956736},
                           {<<"mem_total">>,8217968640},
                           {<<"mem_free">>,4805804032},
                           {<<"mem_limit">>,8217968640},
                           {<<"cpu_cores_available">>,10},
                           {<<"allocstall">>,17}]}},
                        {<<"interestingStats">>,{[]}},
                        {<<"uptime">>,<<"41">>},
                        {<<"memoryTotal">>,8217968640},
                        {<<"memoryFree">>,4805804032},
                        {<<"mcdMemoryReserved">>,6269},
                        {<<"mcdMemoryAllocated">>,6269},
                        {<<"memoryQuota">>,3072},
                        {<<"queryMemoryQuota">>,0},
                        {<<"indexMemoryQuota">>,512},
                        {<<"ftsMemoryQuota">>,512},
                        {<<"cbasMemoryQuota">>,1024},
                        {<<"eventingMemoryQuota">>,256}])
[user:info,2025-05-15T18:47:22.997Z,ns_1@db2.lan:ns_cluster<0.273.0>:ns_cluster:perform_actual_join:1590]Node 'ns_1@db2.lan' is joining cluster via node 'ns_1@172.19.0.4'.
[ns_server:debug,2025-05-15T18:47:22.997Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
i_am_a_dead_man ->
[{'_vclock',[{<<"b0a71fbd3e8ac51d55b35452995f45cf">>,{1,63914554042}}]}|true]
[ns_server:debug,2025-05-15T18:47:22.998Z,ns_1@db2.lan:ns_config_rep<0.554.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([i_am_a_dead_man]..)
[ns_server:debug,2025-05-15T18:47:23.050Z,ns_1@db2.lan:ns_ports_setup<0.677.0>:ns_ports_setup:children_loop_continue:86]Send shutdown to all go ports
[ns_server:debug,2025-05-15T18:47:23.050Z,ns_1@db2.lan:ns_ports_setup<0.677.0>:ns_ports_manager:set_dynamic_children:48]Setting children [memcached,saslauthd_port]
[ns_server:debug,2025-05-15T18:47:23.052Z,ns_1@db2.lan:<0.984.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.643.0>} exited with reason normal
[ns_server:debug,2025-05-15T18:47:23.052Z,ns_1@db2.lan:<0.982.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.642.0>} exited with reason normal
[ns_server:debug,2025-05-15T18:47:23.052Z,ns_1@db2.lan:json_rpc_connection-goxdcr-cbauth<0.940.0>:json_rpc_connection:handle_info:142]Socket closed
[ns_server:debug,2025-05-15T18:47:23.052Z,ns_1@db2.lan:<0.961.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.640.0>} exited with reason normal
[ns_server:debug,2025-05-15T18:47:23.052Z,ns_1@db2.lan:<0.956.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.636.0>} exited with reason normal
[ns_server:debug,2025-05-15T18:47:23.052Z,ns_1@db2.lan:<0.964.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.641.0>} exited with reason normal
[ns_server:info,2025-05-15T18:47:23.052Z,ns_1@db2.lan:menelaus_cbauth_worker-goxdcr-cbauth<0.948.0>:menelaus_cbauth_worker:handle_info:93]Observed json rpc process <0.940.0> died with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.052Z,ns_1@db2.lan:<0.944.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.940.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.052Z,ns_1@db2.lan:menelaus_cbauth<0.668.0>:menelaus_cbauth:handle_info:258]Observed worker process {worker,<0.948.0>,"goxdcr-cbauth",internal,
                                #Ref<0.3735524962.3650355205.242002>,
                                <0.940.0>} died with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.056Z,ns_1@db2.lan:<0.931.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.930.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.056Z,ns_1@db2.lan:<0.929.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.928.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.056Z,ns_1@db2.lan:<0.925.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.923.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.056Z,ns_1@db2.lan:<0.924.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.923.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.056Z,ns_1@db2.lan:<0.911.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.910.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.056Z,ns_1@db2.lan:<0.917.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.916.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.056Z,ns_1@db2.lan:<0.918.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.916.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.056Z,ns_1@db2.lan:<0.908.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.907.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.056Z,ns_1@db2.lan:<0.905.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {master_activity_events,<0.904.0>} exited with reason killed
[ns_server:debug,2025-05-15T18:47:23.057Z,ns_1@db2.lan:<0.900.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {leader_events,<0.899.0>} exited with reason shutdown
[ns_server:info,2025-05-15T18:47:23.057Z,ns_1@db2.lan:mb_master<0.2232.0>:mb_master:terminate:247]Synchronously shutting down child mb_master_sup
[ns_server:info,2025-05-15T18:47:23.057Z,ns_1@db2.lan:leader_registry<0.2230.0>:leader_registry:handle_down:286]Process <0.2271.0> registered as 'license_reporting' terminated.
[ns_server:debug,2025-05-15T18:47:23.057Z,ns_1@db2.lan:<0.2270.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.2269.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.057Z,ns_1@db2.lan:<0.2273.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.2271.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.057Z,ns_1@db2.lan:<0.2261.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.2259.0>} exited with reason shutdown
[ns_server:info,2025-05-15T18:47:23.057Z,ns_1@db2.lan:leader_registry<0.2230.0>:leader_registry:handle_down:286]Process <0.2265.0> registered as 'tombstone_purger' terminated.
[ns_server:debug,2025-05-15T18:47:23.057Z,ns_1@db2.lan:<0.2260.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {compat_mode_events,<0.2259.0>} exited with reason shutdown
[ns_server:info,2025-05-15T18:47:23.057Z,ns_1@db2.lan:leader_registry<0.2230.0>:leader_registry:handle_down:286]Process <0.2259.0> registered as 'auto_failover' terminated.
[ns_server:debug,2025-05-15T18:47:23.057Z,ns_1@db2.lan:<0.2248.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {'kv-events',<0.2247.0>} exited with reason shutdown
[ns_server:info,2025-05-15T18:47:23.057Z,ns_1@db2.lan:leader_registry<0.2230.0>:leader_registry:handle_down:286]Process <0.2257.0> registered as 'ns_orchestrator' terminated.
[ns_server:debug,2025-05-15T18:47:23.057Z,ns_1@db2.lan:leader_activities<0.2227.0>:leader_activities:handle_internal_process_down:440]Process {quorum_nodes_manager,<0.2239.0>} terminated with reason shutdown
[ns_server:info,2025-05-15T18:47:23.057Z,ns_1@db2.lan:leader_registry<0.2230.0>:leader_registry:handle_down:286]Process <0.2256.0> registered as 'auto_rebalance' terminated.
[ns_server:debug,2025-05-15T18:47:23.057Z,ns_1@db2.lan:<0.2246.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.2239.0>} exited with reason shutdown
[ns_server:info,2025-05-15T18:47:23.057Z,ns_1@db2.lan:leader_registry<0.2230.0>:leader_registry:handle_down:286]Process <0.2254.0> registered as 'auto_reprovision' terminated.
[ns_server:debug,2025-05-15T18:47:23.057Z,ns_1@db2.lan:leader_activities<0.2227.0>:leader_activities:handle_internal_process_down:440]Process {acquirer,<0.2235.0>} terminated with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.057Z,ns_1@db2.lan:<0.2236.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_node_disco_events,<0.2235.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.057Z,ns_1@db2.lan:leader_lease_agent<0.2228.0>:leader_lease_agent:handle_abolish_lease:246]Received abolish lease request from {lease_holder,
                                     <<"7aaf062c8f422075d53377c25e3946a4">>,
                                     'ns_1@db2.lan'} when lease is {lease,
                                                                    {lease_holder,
                                                                     <<"445e053598a21dcd845301efeef2f950">>,
                                                                     'ns_1@cb.local'},
                                                                    undefined,
                                                                    -576460695126967772,
                                                                    {timer,
                                                                     #Ref<0.3735524962.3650355207.241495>,
                                                                     {lease_expired,
                                                                      {lease_holder,
                                                                       <<"445e053598a21dcd845301efeef2f950">>,
                                                                       'ns_1@cb.local'}}},
                                                                    active}
[ns_server:info,2025-05-15T18:47:23.057Z,ns_1@db2.lan:leader_registry<0.2230.0>:leader_registry:handle_down:286]Process <0.2247.0> registered as 'chronicle_master' terminated.
[ns_server:debug,2025-05-15T18:47:23.057Z,ns_1@db2.lan:<0.2233.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.2232.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.057Z,ns_1@db2.lan:leader_lease_agent<0.2228.0>:leader_lease_agent:handle_abolish_lease:258]Ignoring stale abolish request
[ns_server:debug,2025-05-15T18:47:23.057Z,ns_1@db2.lan:<0.2231.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {leader_events,<0.2230.0>} exited with reason shutdown
[ns_server:warn,2025-05-15T18:47:23.057Z,ns_1@db2.lan:leader_lease_agent<0.2228.0>:leader_lease_agent:handle_terminate:302]Terminating with reason shutdown when we own an active lease:
{lease,
    {lease_holder,<<"445e053598a21dcd845301efeef2f950">>,'ns_1@cb.local'},
    undefined,-576460695126967772,
    {timer,#Ref<0.3735524962.3650355207.241495>,
        {lease_expired,
            {lease_holder,<<"445e053598a21dcd845301efeef2f950">>,
                'ns_1@cb.local'}}},
    active}
Persisting updated lease.
[ns_server:debug,2025-05-15T18:47:23.059Z,ns_1@db2.lan:leader_activities<0.2227.0>:leader_activities:handle_internal_process_down:440]Process {agent,<0.2228.0>} terminated with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.059Z,ns_1@db2.lan:<0.758.0>:restartable:shutdown_child:114]Successfully terminated process <0.2223.0>
[ns_server:debug,2025-05-15T18:47:23.059Z,ns_1@db2.lan:<0.756.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.755.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.059Z,ns_1@db2.lan:<0.749.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.748.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.059Z,ns_1@db2.lan:<0.744.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.743.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.059Z,ns_1@db2.lan:<0.738.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.737.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.059Z,ns_1@db2.lan:<0.734.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.733.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.059Z,ns_1@db2.lan:<0.742.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_node_disco_events,<0.740.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.059Z,ns_1@db2.lan:<0.735.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_node_disco_events,<0.733.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.059Z,ns_1@db2.lan:<0.741.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.740.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.059Z,ns_1@db2.lan:<0.739.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_node_disco_events,<0.737.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.060Z,ns_1@db2.lan:<0.726.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_tick_event,<0.725.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.060Z,ns_1@db2.lan:<0.724.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_tick_event,<0.723.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.060Z,ns_1@db2.lan:<0.722.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_tick_event,<0.721.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.060Z,ns_1@db2.lan:<0.720.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_tick_event,<0.719.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.060Z,ns_1@db2.lan:<0.934.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ale_stats_events,<0.932.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.060Z,ns_1@db2.lan:<0.2279.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {bucket_info_cache_invalidations,<0.2278.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.060Z,ns_1@db2.lan:<0.712.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.711.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.060Z,ns_1@db2.lan:<0.2292.0>:remote_monitors:handle_down:151]Caller of remote monitor <0.2278.0> died with shutdown. Exiting
[ns_server:debug,2025-05-15T18:47:23.060Z,ns_1@db2.lan:<0.2296.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.2277.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.060Z,ns_1@db2.lan:<0.2293.0>:remote_monitors:handle_down:151]Caller of remote monitor <0.2277.0> died with shutdown. Exiting
[ns_server:debug,2025-05-15T18:47:23.060Z,ns_1@db2.lan:<0.692.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.691.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.060Z,ns_1@db2.lan:<0.687.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.686.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.060Z,ns_1@db2.lan:<0.683.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.682.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.060Z,ns_1@db2.lan:<0.679.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {user_storage_events,<0.677.0>} exited with reason killed
[ns_server:debug,2025-05-15T18:47:23.060Z,ns_1@db2.lan:<0.2291.0>:remote_monitors:handle_down:151]Caller of remote monitor <0.677.0> died with killed. Exiting
[ns_server:debug,2025-05-15T18:47:23.060Z,ns_1@db2.lan:<0.678.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.677.0>} exited with reason killed
[ns_server:info,2025-05-15T18:47:23.060Z,ns_1@db2.lan:menelaus_cbauth<0.668.0>:menelaus_cbauth:terminate_external_connections:159]External connections to be terminated: []
[ns_server:debug,2025-05-15T18:47:23.060Z,ns_1@db2.lan:<0.670.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_node_disco_events,<0.668.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.060Z,ns_1@db2.lan:<0.673.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ssl_service_events,<0.668.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.060Z,ns_1@db2.lan:<0.672.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {user_storage_events,<0.668.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.060Z,ns_1@db2.lan:<0.669.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {json_rpc_events,<0.668.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.060Z,ns_1@db2.lan:<0.671.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.668.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.060Z,ns_1@db2.lan:<0.666.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.665.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.061Z,ns_1@db2.lan:<0.627.0>:restartable:shutdown_child:114]Successfully terminated process <0.628.0>
[ns_server:debug,2025-05-15T18:47:23.061Z,ns_1@db2.lan:<0.618.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.617.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.061Z,ns_1@db2.lan:<0.607.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.606.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.061Z,ns_1@db2.lan:<0.601.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.600.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.061Z,ns_1@db2.lan:<0.605.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.604.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.061Z,ns_1@db2.lan:<0.2217.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.2216.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.061Z,ns_1@db2.lan:<0.603.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.602.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.061Z,ns_1@db2.lan:<0.588.0>:restartable:shutdown_child:114]Successfully terminated process <0.2214.0>
[ns_server:debug,2025-05-15T18:47:23.061Z,ns_1@db2.lan:<0.586.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {buckets_events,<0.585.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.085Z,ns_1@db2.lan:chronicle_kv_log<0.504.0>:chronicle_kv_log:log:59]update (key: tasks, rev: {<<"132dcb91b709843f36fcbbab0f42c041">>,14})
[]
[ns_server:debug,2025-05-15T18:47:23.085Z,ns_1@db2.lan:<0.566.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.565.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.085Z,ns_1@db2.lan:<0.573.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.572.0>} exited with reason killed
[ns_server:debug,2025-05-15T18:47:23.085Z,ns_1@db2.lan:<0.557.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_node_disco_events,<0.554.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.085Z,ns_1@db2.lan:<0.556.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.554.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.085Z,ns_1@db2.lan:<0.578.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.575.0>} exited with reason killed
[ns_server:debug,2025-05-15T18:47:23.085Z,ns_1@db2.lan:<0.546.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.545.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.085Z,ns_1@db2.lan:<0.555.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events_local,<0.554.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.085Z,ns_1@db2.lan:<0.570.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.569.0>} exited with reason killed
[ns_server:debug,2025-05-15T18:47:23.085Z,ns_1@db2.lan:<0.535.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {user_storage_events,<0.533.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.085Z,ns_1@db2.lan:<0.534.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.533.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.085Z,ns_1@db2.lan:<0.532.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {user_storage_events,<0.530.0>} exited with reason shutdown
[ns_server:info,2025-05-15T18:47:23.085Z,ns_1@db2.lan:prometheus_cfg<0.517.0>:prometheus_cfg:terminate:609]Terminate: shutdown
[ns_server:debug,2025-05-15T18:47:23.085Z,ns_1@db2.lan:prometheus_cfg<0.517.0>:prometheus_cfg:terminate_prometheus:773]Terminating Prometheus gracefully
[ns_server:debug,2025-05-15T18:47:23.085Z,ns_1@db2.lan:<0.531.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.530.0>} exited with reason shutdown
[error_logger:error,2025-05-15T18:47:23.088Z,ns_1@db2.lan:bucket_info_cache_invalidations<0.576.0>:ale_error_logger_handler:do_log:101]
=========================CRASH REPORT=========================
  crasher:
    initial call: gen_event:init_it/6
    pid: <0.576.0>
    registered_name: bucket_info_cache_invalidations
    exception exit: killed
      in function  gen_event:terminate_server/4 (gen_event.erl, line 580)
    ancestors: [bucket_info_cache,ns_server_sup,ns_server_nodes_sup,
                  <0.290.0>,ns_server_cluster_sup,root_sup,<0.154.0>]
    message_queue_len: 0
    messages: []
    links: []
    dictionary: []
    trap_exit: true
    status: running
    heap_size: 376
    stack_size: 28
    reductions: 981
  neighbours:

[ns_server:debug,2025-05-15T18:47:23.089Z,ns_1@db2.lan:prometheus-goport<0.529.0>:goport:handle_eof:592]Stream 'stdout' closed
[ns_server:debug,2025-05-15T18:47:23.089Z,ns_1@db2.lan:prometheus-goport<0.529.0>:goport:handle_eof:592]Stream 'stderr' closed
[ns_server:info,2025-05-15T18:47:23.089Z,ns_1@db2.lan:prometheus-goport<0.529.0>:goport:handle_process_exit:573]Port exited with status 0.
[ns_server:info,2025-05-15T18:47:23.089Z,ns_1@db2.lan:<0.523.0>:ns_port_server:handle_info:149]Got {exit_status,0} from port prometheus. Exiting normally
[ns_server:debug,2025-05-15T18:47:23.089Z,ns_1@db2.lan:prometheus_cfg<0.517.0>:prometheus_cfg:terminate_prometheus:794]Prometheus port server stopped successfully
[ns_server:debug,2025-05-15T18:47:23.090Z,ns_1@db2.lan:<0.518.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.517.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.090Z,ns_1@db2.lan:<0.511.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.510.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.091Z,ns_1@db2.lan:<0.509.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.508.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.093Z,ns_1@db2.lan:<0.505.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_kv_event_manager,<0.504.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.093Z,ns_1@db2.lan:<0.2281.0>:remote_monitors:handle_down:151]Caller of remote monitor <0.472.0> died with shutdown. Exiting
[ns_server:debug,2025-05-15T18:47:23.093Z,ns_1@db2.lan:ns_couchdb_port<0.470.0>:ns_port_server:terminate:198]Shutting down port ns_couchdb
[ns_server:debug,2025-05-15T18:47:23.093Z,ns_1@db2.lan:ns_couchdb_port<0.470.0>:ns_port_server:port_shutdown:348]Shutdown command: "shutdown"
[error_logger:info,2025-05-15T18:47:23.100Z,ns_1@db2.lan:net_kernel<0.2175.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',#Port<0.84>,normal}}
[ns_server:debug,2025-05-15T18:47:23.100Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3735524962.3650355205.242898>,
                               inet_tcp_dist,<0.2193.0>,
                               #Ref<0.3735524962.3650355205.242904>}
[error_logger:info,2025-05-15T18:47:23.101Z,ns_1@db2.lan:net_kernel<0.2175.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.2193.0>,connection_closed}}
[error_logger:info,2025-05-15T18:47:23.101Z,ns_1@db2.lan:net_kernel<0.2175.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {2355630,#Ref<0.3735524962.3650486276.241707>}}}
[ns_server:debug,2025-05-15T18:47:23.101Z,ns_1@db2.lan:net_kernel<0.2175.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:23.101Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3735524962.3650355209.241387>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:47:23.101Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3735524962.3650355209.241387>,
                                  inet_tcp_dist,<0.2323.0>,
                                  #Ref<0.3735524962.3650355209.241390>}
[ns_server:info,2025-05-15T18:47:23.101Z,ns_1@db2.lan:ns_couchdb_port<0.470.0>:ns_port_server:handle_info:149]Got {exit_status,0} from port ns_couchdb. Exiting normally
[ns_server:debug,2025-05-15T18:47:23.102Z,ns_1@db2.lan:ns_couchdb_port<0.470.0>:ns_port_server:terminate:201]ns_couchdb has exited
[ns_server:info,2025-05-15T18:47:23.102Z,ns_1@db2.lan:ns_couchdb_port<0.470.0>:ns_port_server:log:226]ns_couchdb<0.470.0>: 218: got shutdown request. Exiting
ns_couchdb<0.470.0>: [os_mon] cpu supervisor port (cpu_sup): Erlang has closed
ns_couchdb<0.470.0>: [os_mon] memory supervisor port (memsup): Erlang has closed

[ns_server:debug,2025-05-15T18:47:23.102Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3735524962.3650355209.241387>,
                               inet_tcp_dist,<0.2323.0>,
                               #Ref<0.3735524962.3650355209.241390>}
[error_logger:info,2025-05-15T18:47:23.102Z,ns_1@db2.lan:net_kernel<0.2175.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.2323.0>,shutdown}}
[error_logger:info,2025-05-15T18:47:23.102Z,ns_1@db2.lan:net_kernel<0.2175.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:47:23.102Z,ns_1@db2.lan:<0.461.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.458.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.102Z,ns_1@db2.lan:<0.460.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {user_storage_events,<0.458.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.102Z,ns_1@db2.lan:<0.459.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.458.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.102Z,ns_1@db2.lan:<0.457.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.455.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.102Z,ns_1@db2.lan:<0.445.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.444.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.102Z,ns_1@db2.lan:<0.456.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {user_storage_events,<0.455.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.102Z,ns_1@db2.lan:<0.430.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.429.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.102Z,ns_1@db2.lan:<0.409.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.408.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.102Z,ns_1@db2.lan:<0.328.0>:restartable:shutdown_child:114]Successfully terminated process <0.2109.0>
[ns_server:debug,2025-05-15T18:47:23.102Z,ns_1@db2.lan:<0.309.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.308.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.102Z,ns_1@db2.lan:<0.290.0>:restartable:shutdown_child:114]Successfully terminated process <0.291.0>
[ns_server:debug,2025-05-15T18:47:23.102Z,ns_1@db2.lan:<0.295.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.292.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.102Z,ns_1@db2.lan:<0.945.0>:ns_pubsub:do_subscribe_link_continue:142]gen_event chronicle_compat_event_manager is shutting down. Propagating to the subscriber <0.941.0>.
[ns_server:debug,2025-05-15T18:47:23.102Z,ns_1@db2.lan:<0.296.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {'kv-events',<0.292.0>} exited with reason shutdown
[cluster:debug,2025-05-15T18:47:23.103Z,ns_1@db2.lan:ns_cluster<0.273.0>:ns_cluster:perform_actual_join:1600]ns_cluster: joining cluster. Child has exited.
[error_logger:error,2025-05-15T18:47:23.103Z,ns_1@db2.lan:<0.945.0>:ale_error_logger_handler:do_log:101]
=========================CRASH REPORT=========================
  crasher:
    initial call: ns_pubsub:do_subscribe_link/4
    pid: <0.945.0>
    registered_name: []
    exception exit: {gen_event_shutdown,chronicle_compat_event_manager}
      in function  ns_pubsub:do_subscribe_link_continue/3 (src/ns_pubsub.erl, line 168)
    ancestors: ['json_rpc_connection-saslauthd-saslauthd-port',
                  json_rpc_connection_sup,ns_server_cluster_sup,root_sup,
                  <0.154.0>]
    message_queue_len: 0
    messages: []
    links: [<0.941.0>]
    dictionary: []
    trap_exit: true
    status: running
    heap_size: 1598
    stack_size: 28
    reductions: 2418
  neighbours:
    neighbour:
      pid: <0.943.0>
      registered_name: []
      initial call: erlang:apply/2
      current_function: {prim_inet,recv0,3}
      ancestors: ['json_rpc_connection-saslauthd-saslauthd-port',
                  json_rpc_connection_sup,ns_server_cluster_sup,root_sup,
                  <0.154.0>]
      message_queue_len: 0
      links: [<0.941.0>]
      trap_exit: false
      status: waiting
      heap_size: 233
      stack_size: 11
      reductions: 145
      current_stacktrace: [{prim_inet,recv0,3,[]},
                  {json_rpc_connection,receiver_loop,3,
                      [{file,"src/json_rpc_connection.erl"},{line,195}]},
                  {proc_lib,init_p_do_apply,3,
                      [{file,"proc_lib.erl"},{line,240}]}]
    neighbour:
      pid: <0.941.0>
      registered_name: 'json_rpc_connection-saslauthd-saslauthd-port'
      initial call: json_rpc_connection:init/1
      current_function: {gen_server,loop,7}
      ancestors: [json_rpc_connection_sup,ns_server_cluster_sup,root_sup,
                  <0.154.0>]
      message_queue_len: 0
      links: [<0.943.0>,#Port<0.41>,<0.945.0>,<0.289.0>]
      trap_exit: false
      status: waiting
      heap_size: 4185
      stack_size: 11
      reductions: 9576
      current_stacktrace: [{gen_server,loop,7,[{file,"gen_server.erl"},{line,871}]},
                  {proc_lib,init_p_do_apply,3,
                            [{file,"proc_lib.erl"},{line,240}]}]
[error_logger:error,2025-05-15T18:47:23.104Z,ns_1@db2.lan:json_rpc_connection_sup<0.289.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {local,json_rpc_connection_sup}
    errorContext: child_terminated
    reason: {gen_event_shutdown,chronicle_compat_event_manager}
    offender: [{pid,<0.941.0>},
               {id,json_rpc_connection},
               {mfargs,{json_rpc_connection,start_link,undefined}},
               {restart_type,temporary},
               {significant,false},
               {shutdown,brutal_kill},
               {child_type,worker}]

[ns_server:info,2025-05-15T18:47:23.106Z,ns_1@db2.lan:ns_cluster<0.273.0>:menelaus_users:delete_storage_offline:175]User storage "/opt/couchbase/var/lib/couchbase/config/users.dets" was deleted
[ns_server:debug,2025-05-15T18:47:23.106Z,ns_1@db2.lan:ns_config<0.280.0>:ns_config:handle_call:920]Regenerated node UUID: <<"7a33d46fd1976e65466c5efc8b45ef2e">> 

[ns_server:debug,2025-05-15T18:47:23.106Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',uuid} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
 <<"7a33d46fd1976e65466c5efc8b45ef2e">>]
[ns_server:debug,2025-05-15T18:47:23.106Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',{project_intact,is_vulnerable}} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|false]
[ns_server:debug,2025-05-15T18:47:23.106Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',xdcr_rest_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9998]
[ns_server:debug,2025-05-15T18:47:23.106Z,ns_1@db2.lan:chronicle_local<0.239.0>:chronicle_local:handle_call:77]Wiping chronicle before prepare join.
[ns_server:debug,2025-05-15T18:47:23.106Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',ssl_rest_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|18091]
[ns_server:debug,2025-05-15T18:47:23.106Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',ssl_query_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|18093]
[chronicle:info,2025-05-15T18:47:23.106Z,ns_1@db2.lan:chronicle_agent<0.252.0>:chronicle_agent:handle_wipe:1261]Wipe requested.
[ns_server:debug,2025-05-15T18:47:23.106Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',ssl_capi_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|18092]
[ns_server:debug,2025-05-15T18:47:23.106Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',saslauthd_enabled} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|true]
[chronicle:info,2025-05-15T18:47:23.106Z,ns_1@db2.lan:chronicle_proposer<0.2190.0>:chronicle_proposer:handle_stop:1269]Proposer for term {4,'ns_1@db2.lan'} in history <<"132dcb91b709843f36fcbbab0f42c041">> is terminating.
[ns_server:debug,2025-05-15T18:47:23.106Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',rest} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
 {port,8091},
 {port_meta,global}]
[chronicle:debug,2025-05-15T18:47:23.106Z,ns_1@db2.lan:chronicle_agent<0.252.0>:chronicle_agent:handle_local_mark_committed:1997]Marked seqno 14 committed
[ns_server:debug,2025-05-15T18:47:23.107Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',query_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|8093]
[ns_server:debug,2025-05-15T18:47:23.107Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',prometheus_http_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9123]
[chronicle:info,2025-05-15T18:47:23.107Z,ns_1@db2.lan:chronicle_server<0.260.0>:chronicle_proposer:stop:136]Proposer <0.2190.0> stopped: {shutdown,stop}
[ns_server:debug,2025-05-15T18:47:23.107Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',projector_ssl_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9999]
[ns_server:debug,2025-05-15T18:47:23.107Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',projector_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9999]
[ns_server:debug,2025-05-15T18:47:23.107Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',port_servers} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}]
[chronicle:info,2025-05-15T18:47:23.107Z,ns_1@db2.lan:chronicle_agent<0.252.0>:chronicle_agent:handle_prepare_wipe_done:1295]All secondary processes have terminated.
[chronicle:info,2025-05-15T18:47:23.107Z,ns_1@db2.lan:chronicle_agent<0.252.0>:chronicle_agent:handle_prepare_wipe_done:1305]Wiping
[ns_server:debug,2025-05-15T18:47:23.107Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',ns_log} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
 {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]
[chronicle:debug,2025-05-15T18:47:23.107Z,ns_1@db2.lan:chronicle_storage_writer<0.253.0>:chronicle_storage:writer_loop:1489]Got exit from <0.252.0> with reason shutdown
[ns_server:debug,2025-05-15T18:47:23.107Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',memcached_prometheus} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|11280]
[ns_server:debug,2025-05-15T18:47:23.107Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',memcached_defaults} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
 {max_connections,65000},
 {system_connections,5000},
 {connection_idle_time,0},
 {verbosity,0},
 {privilege_debug,false},
 {breakpad_enabled,true},
 {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
 {dedupe_nmvb_maps,false},
 {je_malloc_conf,undefined},
 {tracing_enabled,true},
 {datatype_snappy,true},
 {num_reader_threads,<<"default">>},
 {num_writer_threads,<<"default">>},
 {num_auxio_threads,<<"default">>},
 {num_nonio_threads,<<"default">>},
 {num_storage_threads,<<"default">>},
 {tcp_keepalive_idle,360},
 {tcp_keepalive_interval,10},
 {tcp_keepalive_probes,3},
 {tcp_user_timeout,30},
 {always_collect_trace_info,true},
 {connection_limit_mode,<<"disconnect">>},
 {free_connection_pool_size,0},
 {max_client_connection_details,0}]
[ns_server:debug,2025-05-15T18:47:23.107Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',memcached_dedicated_ssl_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|11206]
[ns_server:debug,2025-05-15T18:47:23.107Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',memcached_config} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
 {[{interfaces,{memcached_config_mgr,get_interfaces,[]}},
   {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
   {connection_idle_time,connection_idle_time},
   {privilege_debug,privilege_debug},
   {breakpad,
    {[{enabled,breakpad_enabled},
      {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
   {deployment_model,{memcached_config_mgr,get_config_profile,[]}},
   {verbosity,verbosity},
   {audit_file,{"~s",[audit_file]}},
   {rbac_file,{"~s",[rbac_file]}},
   {dedupe_nmvb_maps,dedupe_nmvb_maps},
   {tracing_enabled,tracing_enabled},
   {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
   {xattr_enabled,true},
   {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
   {collections_enabled,true},
   {max_connections,max_connections},
   {system_connections,system_connections},
   {num_reader_threads,num_reader_threads},
   {num_writer_threads,num_writer_threads},
   {num_auxio_threads,num_auxio_threads},
   {num_nonio_threads,num_nonio_threads},
   {num_storage_threads,num_storage_threads},
   {logger,
    {[{filename,{"~s/~s",[log_path,log_prefix]}},{cyclesize,log_cyclesize}]}},
   {external_auth_service,{memcached_config_mgr,get_external_auth_service,[]}},
   {active_external_users_push_interval,
    {memcached_config_mgr,get_external_users_push_interval,[]}},
   {prometheus,{memcached_config_mgr,prometheus_cfg,[]}},
   {sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
   {ssl_sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
   {tcp_keepalive_idle,tcp_keepalive_idle},
   {tcp_keepalive_interval,tcp_keepalive_interval},
   {tcp_keepalive_probes,tcp_keepalive_probes},
   {tcp_user_timeout,tcp_user_timeout},
   {always_collect_trace_info,always_collect_trace_info},
   {connection_limit_mode,connection_limit_mode},
   {free_connection_pool_size,free_connection_pool_size},
   {max_client_connection_details,max_client_connection_details}]}]
[ns_server:debug,2025-05-15T18:47:23.108Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',memcached} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
 {port,11210},
 {dedicated_port,11209},
 {dedicated_ssl_port,11206},
 {ssl_port,11207},
 {admin_user,"@ns_server"},
 {other_users,["@cbq-engine","@projector","@goxdcr","@index","@fts",
               "@eventing","@cbas","@backup"]},
 {admin_pass,"*****"},
 {engines,[{membase,[{engine,"/opt/couchbase/lib/memcached/ep.so"},
                     {static_config_string,"failpartialwarmup=false"}]},
           {memcached,[{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
                       {static_config_string,"vb0=true"}]}]},
 {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
 {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
 {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
 {log_prefix,"memcached.log"},
 {log_generations,20},
 {log_cyclesize,10485760},
 {log_rotation_period,39003}]
[ns_server:debug,2025-05-15T18:47:23.108Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',isasl} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
 {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]
[ns_server:debug,2025-05-15T18:47:23.108Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',is_enterprise} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|true]
[ns_server:debug,2025-05-15T18:47:23.108Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',indexer_stmaint_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9105]
[ns_server:debug,2025-05-15T18:47:23.108Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',indexer_stinit_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9103]
[ns_server:debug,2025-05-15T18:47:23.108Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',indexer_stcatchup_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9104]
[ns_server:debug,2025-05-15T18:47:23.108Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',indexer_scan_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9101]
[ns_server:debug,2025-05-15T18:47:23.108Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',indexer_https_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|19102]
[ns_server:debug,2025-05-15T18:47:23.108Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',indexer_http_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9102]
[ns_server:debug,2025-05-15T18:47:23.108Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',indexer_admin_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9100]
[ns_server:debug,2025-05-15T18:47:23.108Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',index_dir} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2025-05-15T18:47:23.109Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',fts_ssl_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|18094]
[ns_server:debug,2025-05-15T18:47:23.109Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',fts_http_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|8094]
[ns_server:debug,2025-05-15T18:47:23.109Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',fts_grpc_ssl_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|19130]
[ns_server:debug,2025-05-15T18:47:23.109Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',fts_grpc_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9130]
[ns_server:debug,2025-05-15T18:47:23.109Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',eventing_https_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|18096]
[ns_server:debug,2025-05-15T18:47:23.109Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',eventing_http_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|8096]
[ns_server:debug,2025-05-15T18:47:23.109Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',eventing_debug_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9140]
[ns_server:debug,2025-05-15T18:47:23.109Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',event_log} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
 {filename,"/opt/couchbase/var/lib/couchbase/event_log"}]
[ns_server:debug,2025-05-15T18:47:23.109Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',database_dir} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2025-05-15T18:47:23.109Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',config_version} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|{7,6}]
[ns_server:debug,2025-05-15T18:47:23.109Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',compaction_daemon} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
 {check_interval,30},
 {min_db_file_size,131072},
 {min_view_file_size,20971520}]
[ns_server:debug,2025-05-15T18:47:23.109Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_ssl_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|18095]
[ns_server:debug,2025-05-15T18:47:23.109Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_result_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9117]
[ns_server:debug,2025-05-15T18:47:23.109Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_replication_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9120]
[ns_server:debug,2025-05-15T18:47:23.109Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_parent_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9122]
[ns_server:debug,2025-05-15T18:47:23.109Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_metadata_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9121]
[ns_server:debug,2025-05-15T18:47:23.109Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_metadata_callback_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9119]
[ns_server:debug,2025-05-15T18:47:23.109Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_messaging_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9118]
[ns_server:debug,2025-05-15T18:47:23.109Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_http_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|8095]
[ns_server:debug,2025-05-15T18:47:23.110Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_debug_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|-1]
[ns_server:debug,2025-05-15T18:47:23.110Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_data_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9116]
[ns_server:debug,2025-05-15T18:47:23.110Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_console_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9114]
[ns_server:debug,2025-05-15T18:47:23.110Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_cluster_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9115]
[ns_server:debug,2025-05-15T18:47:23.110Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_cc_http_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9111]
[ns_server:debug,2025-05-15T18:47:23.110Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_cc_cluster_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9112]
[ns_server:debug,2025-05-15T18:47:23.110Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_cc_client_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9113]
[ns_server:debug,2025-05-15T18:47:23.110Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_admin_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9110]
[ns_server:debug,2025-05-15T18:47:23.110Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',capi_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|8092]
[ns_server:debug,2025-05-15T18:47:23.110Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',backup_https_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|18097]
[ns_server:debug,2025-05-15T18:47:23.110Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',backup_http_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|8097]
[ns_server:debug,2025-05-15T18:47:23.110Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',backup_grpc_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9124]
[ns_server:debug,2025-05-15T18:47:23.110Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',audit} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}]
[ns_server:debug,2025-05-15T18:47:23.110Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
nodes_wanted ->
['ns_1@db2.lan','ns_1@172.19.0.4']
[ns_server:debug,2025-05-15T18:47:23.110Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',address_family} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|inet]
[ns_server:debug,2025-05-15T18:47:23.110Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',node_encryption} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|false]
[ns_server:debug,2025-05-15T18:47:23.110Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',erl_external_listeners} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
 {inet,false}]
[ns_server:debug,2025-05-15T18:47:23.110Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',n2n_client_cert_auth} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|false]
[ns_server:debug,2025-05-15T18:47:23.110Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_dirs} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
 "/opt/couchbase/var/lib/couchbase/data"]
[ns_server:debug,2025-05-15T18:47:23.110Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',eventing_dir} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2025-05-15T18:47:23.111Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',prometheus_auth_info} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
 {"@prometheus",
  {auth,
   [{<<"hash">>,
     {[{<<"hashes">>,
        {sanitized,<<"HmfAF2EJsiMXLpFCMXkwI34vHpKDSuQ9OLt/WXmflzM=">>}},
       {<<"algorithm">>,<<"argon2id">>},
       {<<"salt">>,<<"uKWlA91cJAr0Ue2ihPum/w==">>},
       {<<"parallelism">>,1},
       {<<"time">>,3},
       {<<"memory">>,524288}]}},
    {<<"scram-sha-512">>,
     {[{<<"salt">>,
        <<"B9qHYrF6B8PUh2HylXGlWHK8C5mZNOug98FyWD9UiLqElSnKTzdCIZzgFR8u69cjhSjtbD3VEM/dcZbWXNTBGw==">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"Jr1d3/BDp9K+TGL8YrZCBCGqzGbFloqcMPVzFi39KC0=">>}}]}},
    {<<"scram-sha-256">>,
     {[{<<"salt">>,<<"1nOjbJ6vT1sVuqxQCaa4EQHYdQk5w+2NYTeK1TUhKmY=">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"x86lsx8NdTsLHhvU9aNAyIeE61P+Yt6GyVOHrh2MFQo=">>}}]}},
    {<<"scram-sha-1">>,
     {[{<<"salt">>,<<"2rk59nwO1XOIg7dCVQ5eTg7p0y8=">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"4dXFSggAV9gFKuhCJ2pKs13gZZUKMc03Uwl7Sc+puO0=">>}}]}}]}}]
[ns_server:debug,2025-05-15T18:47:23.111Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',node_cert} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
 {subject,<<"CN=Couchbase Server Node (db2.lan)">>},
 {not_after,63985747642},
 {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
 {load_timestamp,63914554042},
 {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQ"...>>},
 {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDOjCCAiKgAwIBAgIIGD/HyAMALgMwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjJaFw0y\nNzA4MTcxODQ3MjJaMCoxKDAmBgNVBAMTH0NvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nZGIyLmxhbikwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC63r7EIhIk\nOyMwE9mw7bzVJfstC9FfdZvk0bP8vxZkuhNt/bVcND8p00ihMP8FCbfsa5d8VgWC\nDoao/+R"...>>},
 {pkey_passphrase_settings,[]},
 {certs_epoch,0},
 {type,generated},
 {hostname,"db2.lan"}]
[ns_server:debug,2025-05-15T18:47:23.111Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',client_cert} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
 {subject,<<"CN=Couchbase Internal Client (116550952)">>},
 {not_after,63985747642},
 {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
 {load_timestamp,63914554042},
 {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQ"...>>},
 {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDWjCCAkKgAwIBAgIIGD/HyA7ClpEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjJaFw0y\nNzA4MTcxODQ3MjJaMDAxLjAsBgNVBAMTJUNvdWNoYmFzZSBJbnRlcm5hbCBDbGll\nbnQgKDExNjU1MDk1MikwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDg\nAaBAyJnrm54fDG0xlsEQH9E51WTLpkwtleiz980c498XKRPsgRWnjY920KTUIH2/\ndQcryFy"...>>},
 {pkey_passphrase_settings,[]},
 {certs_epoch,0},
 {type,generated},
 {name,"@internal"}]
[ns_server:debug,2025-05-15T18:47:23.111Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
otp ->
[{cookie,{sanitized,<<"biSYTaQUFDVndTs/b+IcDmD2L0woktv9naRdv3GCK6A=">>}}]
[ns_server:debug,2025-05-15T18:47:23.111Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',uuid} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
 <<"7a33d46fd1976e65466c5efc8b45ef2e">>]
[ns_server:debug,2025-05-15T18:47:23.111Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
otp ->
[{cookie,{sanitized,<<"biSYTaQUFDVndTs/b+IcDmD2L0woktv9naRdv3GCK6A=">>}}]
[ns_server:debug,2025-05-15T18:47:23.111Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
nodes_wanted ->
['ns_1@db2.lan','ns_1@172.19.0.4']
[ns_server:debug,2025-05-15T18:47:23.111Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
cluster_compat_mode ->
undefined
[ns_server:debug,2025-05-15T18:47:23.111Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',membership} ->
inactiveAdded
[chronicle:info,2025-05-15T18:47:23.116Z,ns_1@db2.lan:chronicle_agent<0.252.0>:chronicle_agent:handle_prepare_wipe_done:1308]Wiped successfully
[ns_server:debug,2025-05-15T18:47:23.116Z,ns_1@db2.lan:chronicle_local<0.239.0>:chronicle_local:handle_call:84]Prepare join. Info: #{committed_seqno => 17,compat_version => 0,
                      config =>
                          {log_entry,<<"5aab03dbecad06b50ffb474da59a00c9">>,
                              {4,'ns_1@172.19.0.4'},
                              17,
                              {config,
                                  {<<"1dd5991463d519e23729dda2c80c376b">>,0,1},
                                  0,<<"49d5762e287786c3127905510f7a4115">>,
                                  #{'ns_1@172.19.0.4' =>
                                        #{id =>
                                              <<"1dd5991463d519e23729dda2c80c376b">>,
                                          role => voter},
                                    'ns_1@db2.lan' =>
                                        #{id =>
                                              <<"9f6894b51ffa497cdfc8b792d9e6726b">>,
                                          role => replica}},
                                  undefined,
                                  #{chronicle_config_rsm =>
                                        {rsm_config,chronicle_config_rsm,[]},
                                    kv => {rsm_config,chronicle_kv,[]}},
                                  #{},undefined,
                                  [{<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                    0}]}},
                      history_id => <<"5aab03dbecad06b50ffb474da59a00c9">>}
[chronicle:info,2025-05-15T18:47:23.119Z,ns_1@db2.lan:chronicle_agent<0.252.0>:chronicle_storage:open_logs:180]Reading log files [{0,
                    "/opt/couchbase/var/lib/couchbase/config/chronicle/logs/0.log"}]
[chronicle:info,2025-05-15T18:47:23.120Z,ns_1@db2.lan:chronicle_agent<0.252.0>:chronicle_storage:open_current_log:232]Error while opening log file "/opt/couchbase/var/lib/couchbase/config/chronicle/logs/0.log": no_header
[chronicle:info,2025-05-15T18:47:23.120Z,ns_1@db2.lan:chronicle_agent<0.252.0>:chronicle_storage:create_log:280]Creating log file /opt/couchbase/var/lib/couchbase/config/chronicle/logs/0.log (high seqno = 0)
[chronicle:info,2025-05-15T18:47:23.122Z,ns_1@db2.lan:chronicle_agent<0.252.0>:chronicle_agent:maybe_seed_storage:2444]Found empty storage. Seeding it with default metadata:
#{committed_seqno => 0,history_id => <<"no-history">>,peer => nonode@nohost,
  peer_id => <<>>,pending_branch => undefined,state => not_provisioned,
  term => {0,nonode@nohost}}
[ns_server:debug,2025-05-15T18:47:23.123Z,ns_1@db2.lan:ns_config<0.280.0>:ns_config:handle_call:895]Reload config
[ns_server:debug,2025-05-15T18:47:23.123Z,ns_1@db2.lan:ns_config<0.280.0>:ns_config:wait_saver:844]Done waiting for saver.
[error_logger:info,2025-05-15T18:47:23.123Z,ns_1@db2.lan:<0.255.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.255.0>,dynamic_supervisor}
    started: [{pid,<0.2330.0>},
              {id,chronicle_leader},
              {mfargs,{chronicle_leader,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:47:23.126Z,ns_1@db2.lan:ns_config<0.280.0>:ns_config:load_config:1113]Loading static config from "/opt/couchbase/etc/couchbase/config"
[ns_server:info,2025-05-15T18:47:23.127Z,ns_1@db2.lan:ns_config<0.280.0>:ns_config:load_config:1127]Loading dynamic config from "/opt/couchbase/var/lib/couchbase/config/config.dat"
[ns_server:debug,2025-05-15T18:47:23.129Z,ns_1@db2.lan:ns_config<0.280.0>:ns_config:load_config:1135]Here's full dynamic config we loaded:
[[{{node,'ns_1@db2.lan',membership},inactiveAdded},
  {cluster_compat_mode,undefined},
  {nodes_wanted,['ns_1@db2.lan','ns_1@172.19.0.4']},
  {otp,
   [{cookie,{sanitized,<<"biSYTaQUFDVndTs/b+IcDmD2L0woktv9naRdv3GCK6A=">>}}]},
  {{node,'ns_1@db2.lan',uuid},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    <<"7a33d46fd1976e65466c5efc8b45ef2e">>]},
  {{node,'ns_1@db2.lan',client_cert},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
    {subject,<<"CN=Couchbase Internal Client (116550952)">>},
    {not_after,63985747642},
    {verified_with,
     <<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
    {load_timestamp,63914554042},
    {ca,
     <<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQCuy0dFqLPrJ6xY21RJ5ttVPR78ssxxXSe9JdTj/IsUczkwyxfZfwK\npBszlGYuHO0Utnwq1K4ruMPYFYfpLacJSfsb3KWEmZwAoNuRpjvE24AsH3tvj7cB\nB5v49HJ0axvNAFm75GWDyUr7T7UwcTJaFIFtfy+J0Umt2TjFNY0DPpt1WEzkKFKx\nOdBs+7cNc35ZG4rwVu/EJ14OXhQMXkpbUpYJxS8jzkzSI+1Eb67kH4j5QhnR9H+w\nw2S9AgMBAAGjQjBAMA4GA1UdDwEB/wQEAwIBhjAPBgNVHRMBAf8EBTADAQH/MB0G\nA1UdDgQWBBS8ASq2Mkh0UsZe4fIvzs4mCSfTmjANBgkqhkiG9w0BAQsFAAOCAQEA\nB3q1mzdJc3VY17JDCaRPKY/Veni3oZ6zGJ9r9LnWKE7IO9AxBAKYGMELHMf9httQ\nmRZrXiGQi/tYXLlFFk7IcIID6iOOZLLagDhCQJPi52dGhBB4a2EnBvGF9OJI4zW9\nJw04HNR5cF1cxWIrRdGJAX/kbs/M9jz0STlXziVNwiAYeGIxkjq/fwKk6weai8iI\nwQVu6GsbtSBL8f4FRCk2vK6a1MD54BQDasRShdi3k/DJAEtY92muu13M0jc48mCM\ndEVeXxM8rIrG6LavziTwtO+jkFyWjlRyNbSr6il+neSWJ7ZspTo+ZYFy55CAh5ws\nwk6Ljo2wlQgfGflJbKcxAw==\n-----END CERTIFICATE-----\n">>},
    {pem,
     <<"-----BEGIN CERTIFICATE-----\nMIIDWjCCAkKgAwIBAgIIGD/HyA7ClpEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjJaFw0y\nNzA4MTcxODQ3MjJaMDAxLjAsBgNVBAMTJUNvdWNoYmFzZSBJbnRlcm5hbCBDbGll\nbnQgKDExNjU1MDk1MikwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDg\nAaBAyJnrm54fDG0xlsEQH9E51WTLpkwtleiz980c498XKRPsgRWnjY920KTUIH2/\ndQcryFyvGTggbCNUUsAqs6qWzDX6iZW6uhcXUh9ujNASr1VNBMLPY60ejukP9Jbq\nxlTfAYvoidlCAZ43nBN17IbZjObU9wEL5AJVX+DLfxX9K0uTI37qWIX6lODZMnek\nj8dGQcmpjyB0ktQaFPLCfzIyXDoA0XE4zsxXW+qX4Xji5cwFwgUgJ3E1IYamZJZv\nF3WkWl7ai8xuos3gTqe41d4rvGemY5Pxao60ClgbgFPaimvOFT7Is2h4TzVQI8f7\nonIen4hIAiwE18Iz4WRJAgMBAAGjgYMwgYAwDgYDVR0PAQH/BAQDAgWgMBMGA1Ud\nJQQMMAoGCCsGAQUFBwMCMAwGA1UdEwEB/wQCMAAwHwYDVR0jBBgwFoAUvAEqtjJI\ndFLGXuHyL87OJgkn05owKgYDVR0RBCMwIYEfaW50ZXJuYWxAaW50ZXJuYWwuY291\nY2hiYXNlLmNvbTANBgkqhkiG9w0BAQsFAAOCAQEAjrpupGzgLn2L0Zf3xDsp7paD\nNHbJDPJeJAeG3u4nAur7tcZqAQbUgzkIPe9IFvz1dfdS9CRfyUYLzjcGK7M6Nzyt\nhgAwmdTOjAxTtcjVstxSdyhSxLORZnyMQcFpzJH+weMhVe5khi/T3nDWyR9yqwW0\nw9iReXBJhh/QyE+EBLMYJ58gXG5qQOHiOVw0ePQmQtoQhkjEM2jWVJegD+boNmOK\nKoCXNSxWZB2dC1MOQxtvA5ggysGIVOYH1ASMyXyZszSCRjKfGDzTYyJSfnBACijg\nWXt8NyZIaucN/aXR9H4CzrC077tJz9hihz0Ld4uhaIbVddLOY13TGq/V/0H6cg==\n-----END CERTIFICATE-----\n">>},
    {pkey_passphrase_settings,[]},
    {certs_epoch,0},
    {type,generated},
    {name,"@internal"}]},
  {{node,'ns_1@db2.lan',node_cert},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
    {subject,<<"CN=Couchbase Server Node (db2.lan)">>},
    {not_after,63985747642},
    {verified_with,
     <<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
    {load_timestamp,63914554042},
    {ca,
     <<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQCuy0dFqLPrJ6xY21RJ5ttVPR78ssxxXSe9JdTj/IsUczkwyxfZfwK\npBszlGYuHO0Utnwq1K4ruMPYFYfpLacJSfsb3KWEmZwAoNuRpjvE24AsH3tvj7cB\nB5v49HJ0axvNAFm75GWDyUr7T7UwcTJaFIFtfy+J0Umt2TjFNY0DPpt1WEzkKFKx\nOdBs+7cNc35ZG4rwVu/EJ14OXhQMXkpbUpYJxS8jzkzSI+1Eb67kH4j5QhnR9H+w\nw2S9AgMBAAGjQjBAMA4GA1UdDwEB/wQEAwIBhjAPBgNVHRMBAf8EBTADAQH/MB0G\nA1UdDgQWBBS8ASq2Mkh0UsZe4fIvzs4mCSfTmjANBgkqhkiG9w0BAQsFAAOCAQEA\nB3q1mzdJc3VY17JDCaRPKY/Veni3oZ6zGJ9r9LnWKE7IO9AxBAKYGMELHMf9httQ\nmRZrXiGQi/tYXLlFFk7IcIID6iOOZLLagDhCQJPi52dGhBB4a2EnBvGF9OJI4zW9\nJw04HNR5cF1cxWIrRdGJAX/kbs/M9jz0STlXziVNwiAYeGIxkjq/fwKk6weai8iI\nwQVu6GsbtSBL8f4FRCk2vK6a1MD54BQDasRShdi3k/DJAEtY92muu13M0jc48mCM\ndEVeXxM8rIrG6LavziTwtO+jkFyWjlRyNbSr6il+neSWJ7ZspTo+ZYFy55CAh5ws\nwk6Ljo2wlQgfGflJbKcxAw==\n-----END CERTIFICATE-----\n">>},
    {pem,
     <<"-----BEGIN CERTIFICATE-----\nMIIDOjCCAiKgAwIBAgIIGD/HyAMALgMwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjJaFw0y\nNzA4MTcxODQ3MjJaMCoxKDAmBgNVBAMTH0NvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nZGIyLmxhbikwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC63r7EIhIk\nOyMwE9mw7bzVJfstC9FfdZvk0bP8vxZkuhNt/bVcND8p00ihMP8FCbfsa5d8VgWC\nDoao/+R+8KpeXDZeXWTzOfGPz6ySJpDMUbPsj/pymSkYBvwsl0iBlPrBYDVlHe6C\nIFfKfnfHOsS9itHFANxppJ+spLeJP7Na1p/rOGFELp9Y0pYnDvAccES6DCwgzMKr\n4lk/1l1L4jJkyHzK0hQ5UutHe7jGBhOxRoIyTCPO5wf6miZJNlKxA/1zwCJuLaft\ny4N6DDBMIs9R/dS02i1IrhKLDvC/7OnK/BkXi4HKLKY/wE3YNBmgMNXEwxUMeqZk\n2IUILCZn4W9fAgMBAAGjajBoMA4GA1UdDwEB/wQEAwIFoDATBgNVHSUEDDAKBggr\nBgEFBQcDATAMBgNVHRMBAf8EAjAAMB8GA1UdIwQYMBaAFLwBKrYySHRSxl7h8i/O\nziYJJ9OaMBIGA1UdEQQLMAmCB2RiMi5sYW4wDQYJKoZIhvcNAQELBQADggEBADkD\n1qqpKjJY4cyeoPzdD5mW7I12+dWN/oW0yUQ4heVH9TZzgThchsucMQvFg3vpZ9j5\nSbxsH9h23M26Cv8PxOjpu8lmlYAtENDDGnagAiwDJuMtSeBkKw36xzLrB4jj8UZ+\n8k+5CLVUzncPt9J9o7Lgm+wgavlprqSMJLUGfAc+O0qGzN+mQk1UeM2a8++Ty97Q\neks+wPVQCPbprWQU7k7F8mSepoAWHglkhfkBjBXgHGvGMIMGDv68l7yfNBJ9yRoJ\nO5cAQgoNiHZDzZ9clPCCGeE+JQnxNQCMOIxq7xhnTrYgBeU9hWu0hBcrHR6ZwO+m\n6MNtblxGbkNee9HO7LY=\n-----END CERTIFICATE-----\n">>},
    {pkey_passphrase_settings,[]},
    {certs_epoch,0},
    {type,generated},
    {hostname,"db2.lan"}]},
  {{node,'ns_1@db2.lan',prometheus_auth_info},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    {"@prometheus",
     {auth,
      [{<<"hash">>,
        {[{<<"hashes">>,
           {sanitized,<<"HmfAF2EJsiMXLpFCMXkwI34vHpKDSuQ9OLt/WXmflzM=">>}},
          {<<"algorithm">>,<<"argon2id">>},
          {<<"salt">>,<<"uKWlA91cJAr0Ue2ihPum/w==">>},
          {<<"parallelism">>,1},
          {<<"time">>,3},
          {<<"memory">>,524288}]}},
       {<<"scram-sha-512">>,
        {[{<<"salt">>,
           <<"B9qHYrF6B8PUh2HylXGlWHK8C5mZNOug98FyWD9UiLqElSnKTzdCIZzgFR8u69cjhSjtbD3VEM/dcZbWXNTBGw==">>},
          {<<"iterations">>,15000},
          {<<"hashes">>,
           {sanitized,<<"Jr1d3/BDp9K+TGL8YrZCBCGqzGbFloqcMPVzFi39KC0=">>}}]}},
       {<<"scram-sha-256">>,
        {[{<<"salt">>,<<"1nOjbJ6vT1sVuqxQCaa4EQHYdQk5w+2NYTeK1TUhKmY=">>},
          {<<"iterations">>,15000},
          {<<"hashes">>,
           {sanitized,<<"x86lsx8NdTsLHhvU9aNAyIeE61P+Yt6GyVOHrh2MFQo=">>}}]}},
       {<<"scram-sha-1">>,
        {[{<<"salt">>,<<"2rk59nwO1XOIg7dCVQ5eTg7p0y8=">>},
          {<<"iterations">>,15000},
          {<<"hashes">>,
           {sanitized,
            <<"4dXFSggAV9gFKuhCJ2pKs13gZZUKMc03Uwl7Sc+puO0=">>}}]}}]}}]},
  {{node,'ns_1@db2.lan',eventing_dir},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
    47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,
    105,98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
  {{node,'ns_1@db2.lan',cbas_dirs},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
    "/opt/couchbase/var/lib/couchbase/data"]},
  {{node,'ns_1@db2.lan',n2n_client_cert_auth},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    false]},
  {{node,'ns_1@db2.lan',erl_external_listeners},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
    {inet,false}]},
  {{node,'ns_1@db2.lan',node_encryption},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    false]},
  {{node,'ns_1@db2.lan',address_family},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    inet]},
  {{node,'ns_1@db2.lan',audit},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}]},
  {{node,'ns_1@db2.lan',backup_grpc_port},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    9124]},
  {{node,'ns_1@db2.lan',backup_http_port},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    8097]},
  {{node,'ns_1@db2.lan',backup_https_port},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    18097]},
  {{node,'ns_1@db2.lan',capi_port},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    8092]},
  {{node,'ns_1@db2.lan',cbas_admin_port},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    9110]},
  {{node,'ns_1@db2.lan',cbas_cc_client_port},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    9113]},
  {{node,'ns_1@db2.lan',cbas_cc_cluster_port},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    9112]},
  {{node,'ns_1@db2.lan',cbas_cc_http_port},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    9111]},
  {{node,'ns_1@db2.lan',cbas_cluster_port},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    9115]},
  {{node,'ns_1@db2.lan',cbas_console_port},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    9114]},
  {{node,'ns_1@db2.lan',cbas_data_port},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    9116]},
  {{node,'ns_1@db2.lan',cbas_debug_port},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    -1]},
  {{node,'ns_1@db2.lan',cbas_http_port},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    8095]},
  {{node,'ns_1@db2.lan',cbas_messaging_port},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    9118]},
  {{node,'ns_1@db2.lan',cbas_metadata_callback_port},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    9119]},
  {{node,'ns_1@db2.lan',cbas_metadata_port},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    9121]},
  {{node,'ns_1@db2.lan',cbas_parent_port},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    9122]},
  {{node,'ns_1@db2.lan',cbas_replication_port},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    9120]},
  {{node,'ns_1@db2.lan',cbas_result_port},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    9117]},
  {{node,'ns_1@db2.lan',cbas_ssl_port},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    18095]},
  {{node,'ns_1@db2.lan',compaction_daemon},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
    {check_interval,30},
    {min_db_file_size,131072},
    {min_view_file_size,20971520}]},
  {{node,'ns_1@db2.lan',config_version},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    {7,6}]},
  {{node,'ns_1@db2.lan',database_dir},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
    47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,
    105,98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
  {{node,'ns_1@db2.lan',event_log},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
    {filename,"/opt/couchbase/var/lib/couchbase/event_log"}]},
  {{node,'ns_1@db2.lan',eventing_debug_port},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    9140]},
  {{node,'ns_1@db2.lan',eventing_http_port},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    8096]},
  {{node,'ns_1@db2.lan',eventing_https_port},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    18096]},
  {{node,'ns_1@db2.lan',fts_grpc_port},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    9130]},
  {{node,'ns_1@db2.lan',fts_grpc_ssl_port},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    19130]},
  {{node,'ns_1@db2.lan',fts_http_port},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    8094]},
  {{node,'ns_1@db2.lan',fts_ssl_port},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    18094]},
  {{node,'ns_1@db2.lan',index_dir},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
    47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,
    105,98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
  {{node,'ns_1@db2.lan',indexer_admin_port},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    9100]},
  {{node,'ns_1@db2.lan',indexer_http_port},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    9102]},
  {{node,'ns_1@db2.lan',indexer_https_port},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    19102]},
  {{node,'ns_1@db2.lan',indexer_scan_port},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    9101]},
  {{node,'ns_1@db2.lan',indexer_stcatchup_port},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    9104]},
  {{node,'ns_1@db2.lan',indexer_stinit_port},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    9103]},
  {{node,'ns_1@db2.lan',indexer_stmaint_port},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    9105]},
  {{node,'ns_1@db2.lan',is_enterprise},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    true]},
  {{node,'ns_1@db2.lan',isasl},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
    {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
  {{node,'ns_1@db2.lan',memcached},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
    {port,11210},
    {dedicated_port,11209},
    {dedicated_ssl_port,11206},
    {ssl_port,11207},
    {admin_user,"@ns_server"},
    {other_users,
     ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
      "@cbas","@backup"]},
    {admin_pass,"*****"},
    {engines,
     [{membase,
       [{engine,"/opt/couchbase/lib/memcached/ep.so"},
        {static_config_string,"failpartialwarmup=false"}]},
      {memcached,
       [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
        {static_config_string,"vb0=true"}]}]},
    {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
    {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
    {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
    {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
    {log_prefix,"memcached.log"},
    {log_generations,20},
    {log_cyclesize,10485760},
    {log_rotation_period,39003}]},
  {{node,'ns_1@db2.lan',memcached_config},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    {[{interfaces,{memcached_config_mgr,get_interfaces,[]}},
      {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
      {connection_idle_time,connection_idle_time},
      {privilege_debug,privilege_debug},
      {breakpad,
       {[{enabled,breakpad_enabled},
         {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
      {deployment_model,{memcached_config_mgr,get_config_profile,[]}},
      {verbosity,verbosity},
      {audit_file,{"~s",[audit_file]}},
      {rbac_file,{"~s",[rbac_file]}},
      {dedupe_nmvb_maps,dedupe_nmvb_maps},
      {tracing_enabled,tracing_enabled},
      {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
      {xattr_enabled,true},
      {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
      {collections_enabled,true},
      {max_connections,max_connections},
      {system_connections,system_connections},
      {num_reader_threads,num_reader_threads},
      {num_writer_threads,num_writer_threads},
      {num_auxio_threads,num_auxio_threads},
      {num_nonio_threads,num_nonio_threads},
      {num_storage_threads,num_storage_threads},
      {logger,
       {[{filename,{"~s/~s",[log_path,log_prefix]}},
         {cyclesize,log_cyclesize}]}},
      {external_auth_service,
       {memcached_config_mgr,get_external_auth_service,[]}},
      {active_external_users_push_interval,
       {memcached_config_mgr,get_external_users_push_interval,[]}},
      {prometheus,{memcached_config_mgr,prometheus_cfg,[]}},
      {sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
      {ssl_sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
      {tcp_keepalive_idle,tcp_keepalive_idle},
      {tcp_keepalive_interval,tcp_keepalive_interval},
      {tcp_keepalive_probes,tcp_keepalive_probes},
      {tcp_user_timeout,tcp_user_timeout},
      {always_collect_trace_info,always_collect_trace_info},
      {connection_limit_mode,connection_limit_mode},
      {free_connection_pool_size,free_connection_pool_size},
      {max_client_connection_details,max_client_connection_details}]}]},
  {{node,'ns_1@db2.lan',memcached_dedicated_ssl_port},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    11206]},
  {{node,'ns_1@db2.lan',memcached_defaults},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
    {max_connections,65000},
    {system_connections,5000},
    {connection_idle_time,0},
    {verbosity,0},
    {privilege_debug,false},
    {breakpad_enabled,true},
    {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
    {dedupe_nmvb_maps,false},
    {je_malloc_conf,undefined},
    {tracing_enabled,true},
    {datatype_snappy,true},
    {num_reader_threads,<<"default">>},
    {num_writer_threads,<<"default">>},
    {num_auxio_threads,<<"default">>},
    {num_nonio_threads,<<"default">>},
    {num_storage_threads,<<"default">>},
    {tcp_keepalive_idle,360},
    {tcp_keepalive_interval,10},
    {tcp_keepalive_probes,3},
    {tcp_user_timeout,30},
    {always_collect_trace_info,true},
    {connection_limit_mode,<<"disconnect">>},
    {free_connection_pool_size,0},
    {max_client_connection_details,0}]},
  {{node,'ns_1@db2.lan',memcached_prometheus},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    11280]},
  {{node,'ns_1@db2.lan',ns_log},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
    {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
  {{node,'ns_1@db2.lan',port_servers},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}]},
  {{node,'ns_1@db2.lan',projector_port},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    9999]},
  {{node,'ns_1@db2.lan',projector_ssl_port},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    9999]},
  {{node,'ns_1@db2.lan',prometheus_http_port},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    9123]},
  {{node,'ns_1@db2.lan',query_port},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    8093]},
  {{node,'ns_1@db2.lan',rest},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
    {port,8091},
    {port_meta,global}]},
  {{node,'ns_1@db2.lan',saslauthd_enabled},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    true]},
  {{node,'ns_1@db2.lan',ssl_capi_port},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    18092]},
  {{node,'ns_1@db2.lan',ssl_query_port},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    18093]},
  {{node,'ns_1@db2.lan',ssl_rest_port},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    18091]},
  {{node,'ns_1@db2.lan',xdcr_rest_port},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    9998]},
  {{node,'ns_1@db2.lan',{project_intact,is_vulnerable}},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
    false]},
  {{local_changes_count,<<"7a33d46fd1976e65466c5efc8b45ef2e">>},
   [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{5,63914554043}}]}]}]]
[ns_server:info,2025-05-15T18:47:23.134Z,ns_1@db2.lan:ns_config<0.280.0>:ns_config:load_config:1157]Here's full dynamic config we loaded + static & default config:
[{resource_management,
  [{bucket,
    [{resident_ratio,
      [{enabled,false},{couchstore_minimum,1},{magma_minimum,0.2}]},
     {data_size,[{enabled,false},{couchstore_maximum,2},{magma_maximum,16}]}]},
   {index,[]},
   {cores_per_bucket,[{enabled,false},{minimum,0.4}]},
   {disk_usage,[{enabled,false},{maximum,96}]},
   {collections_per_quota,[{enabled,false},{maximum,1}]}]},
 {auto_failover_cfg,
  [{enabled,true},
   {timeout,120},
   {count,0},
   {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
   {failover_server_group,false},
   {max_count,1},
   {failed_over_server_groups,[]},
   {can_abort_rebalance,true}]},
 {retry_rebalance,[{enabled,false},{after_time_period,300},{max_attempts,1}]},
 {rest,[{port,8091}]},
 {password_policy,[{min_length,6},{must_present,[]}]},
 {health_monitor_refresh_interval,[]},
 {service_orchestrator_weight,
  [{kv,10000},
   {index,1000},
   {fts,1000},
   {cbas,1000},
   {n1ql,100},
   {eventing,100},
   {backup,10}]},
 {log_redaction_default_cfg,[{redact_level,none}]},
 {replication,[{enabled,true}]},
 {alert_limits,
  [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
 {email_alerts,
  [{recipients,["root@localhost"]},
   {sender,"couchbase@localhost"},
   {enabled,false},
   {email_server,
    [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
   {alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     indexer_low_resident_percentage,ep_clock_cas_drift_threshold_exceeded,
     communication_issue,time_out_of_sync,disk_usage_analyzer_stuck,
     cert_expires_soon,cert_expired,memory_threshold,history_size_warning,
     stuck_rebalance,memcached_connections]},
   {pop_up_alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     indexer_low_resident_percentage,ep_clock_cas_drift_threshold_exceeded,
     communication_issue,time_out_of_sync,disk_usage_analyzer_stuck,
     cert_expires_soon,cert_expired,memory_threshold,history_size_warning,
     stuck_rebalance,memcached_connections]}]},
 {secure_headers,[]},
 {buckets,[{configs,[]}]},
 {cbas_memory_quota,1549},
 {fts_memory_quota,512},
 {memory_quota,3406},
 {memcached,[]},
 {audit,
  [{auditd_enabled,false},
   {rotate_interval,86400},
   {rotate_size,20971520},
   {prune_age,0},
   {disabled,[]},
   {enabled,[]},
   {disabled_users,[]},
   {sync,[]},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
 {remote_clusters,[]},
 {scramsha_fallback_salt,<<55,187,62,55,248,32,6,50,18,123,218,116>>},
 {client_cert_auth,[{state,"disable"},{prefixes,[]}]},
 {rest_creds,null},
 {{metakv,<<"/analytics/settings/config">>},
  <<"{\"analytics.settings.num_replicas\":0}">>},
 {{metakv,<<"/query/settings/config">>},
  <<"{\"cleanupclientattempts\":true,\"cleanuplostattempts\":true,\"cleanupwindow\":\"60s\",\"completed-limit\":4000,\"completed-threshold\":1000,\"loglevel\":\"info\",\"max-parallelism\":1,\"memory-quota\":0,\"n1ql-feat-ctrl\":76,\"numatrs\":1024,\"pipeline-batch\":16,\"pipeline-cap\":512,\"prepared-limit\":16384,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"query.settings.tmp_space_size\":5120,\"scan-cap\":512,\"timeout\":0,\"txtimeout\":\"0ms\",\"use-cbo\":true}">>},
 {{metakv,<<"/eventing/settings/config">>},<<"{\"ram_quota\":256}">>},
 {{metakv,<<"/indexing/settings/config">>},
  <<"{\"indexer.settings.compaction.abort_exceed_interval\":false,\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.enable_page_bloom_filter\":false,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.log_level\":\"info\",\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.num_replica\":0,\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.storage_mode\":\"\"}">>},
 {{couchdb,max_parallel_replica_indexers},2},
 {{couchdb,max_parallel_indexers},4},
 {quorum_nodes,['ns_1@db2.lan']},
 {set_view_update_daemon,
  [{update_interval,5000},
   {update_min_changes,5000},
   {replica_update_min_changes,5000}]},
 {max_bucket_count,30},
 {index_aware_rebalance_disabled,false},
 {{local_changes_count,<<"7a33d46fd1976e65466c5efc8b45ef2e">>},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{5,63914554043}}]}]},
 {{node,'ns_1@db2.lan',{project_intact,is_vulnerable}},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   false]},
 {{node,'ns_1@db2.lan',xdcr_rest_port},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   9998]},
 {{node,'ns_1@db2.lan',ssl_rest_port},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   18091]},
 {{node,'ns_1@db2.lan',ssl_query_port},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   18093]},
 {{node,'ns_1@db2.lan',ssl_capi_port},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   18092]},
 {{node,'ns_1@db2.lan',saslauthd_enabled},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   true]},
 {{node,'ns_1@db2.lan',rest},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
   {port,8091},
   {port_meta,global}]},
 {{node,'ns_1@db2.lan',query_port},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   8093]},
 {{node,'ns_1@db2.lan',prometheus_http_port},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   9123]},
 {{node,'ns_1@db2.lan',projector_ssl_port},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   9999]},
 {{node,'ns_1@db2.lan',projector_port},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   9999]},
 {{node,'ns_1@db2.lan',port_servers},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}]},
 {{node,'ns_1@db2.lan',ns_log},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
 {{node,'ns_1@db2.lan',memcached_prometheus},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   11280]},
 {{node,'ns_1@db2.lan',memcached_defaults},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
   {max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {je_malloc_conf,undefined},
   {tracing_enabled,true},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>},
   {num_auxio_threads,<<"default">>},
   {num_nonio_threads,<<"default">>},
   {num_storage_threads,<<"default">>},
   {tcp_keepalive_idle,360},
   {tcp_keepalive_interval,10},
   {tcp_keepalive_probes,3},
   {tcp_user_timeout,30},
   {always_collect_trace_info,true},
   {connection_limit_mode,<<"disconnect">>},
   {free_connection_pool_size,0},
   {max_client_connection_details,0}]},
 {{node,'ns_1@db2.lan',memcached_dedicated_ssl_port},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   11206]},
 {{node,'ns_1@db2.lan',memcached_config},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   {[{interfaces,{memcached_config_mgr,get_interfaces,[]}},
     {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
     {connection_idle_time,connection_idle_time},
     {privilege_debug,privilege_debug},
     {breakpad,
      {[{enabled,breakpad_enabled},
        {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
     {deployment_model,{memcached_config_mgr,get_config_profile,[]}},
     {verbosity,verbosity},
     {audit_file,{"~s",[audit_file]}},
     {rbac_file,{"~s",[rbac_file]}},
     {dedupe_nmvb_maps,dedupe_nmvb_maps},
     {tracing_enabled,tracing_enabled},
     {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
     {xattr_enabled,true},
     {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
     {collections_enabled,true},
     {max_connections,max_connections},
     {system_connections,system_connections},
     {num_reader_threads,num_reader_threads},
     {num_writer_threads,num_writer_threads},
     {num_auxio_threads,num_auxio_threads},
     {num_nonio_threads,num_nonio_threads},
     {num_storage_threads,num_storage_threads},
     {logger,
      {[{filename,{"~s/~s",[log_path,log_prefix]}},
        {cyclesize,log_cyclesize}]}},
     {external_auth_service,
      {memcached_config_mgr,get_external_auth_service,[]}},
     {active_external_users_push_interval,
      {memcached_config_mgr,get_external_users_push_interval,[]}},
     {prometheus,{memcached_config_mgr,prometheus_cfg,[]}},
     {sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
     {ssl_sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
     {tcp_keepalive_idle,tcp_keepalive_idle},
     {tcp_keepalive_interval,tcp_keepalive_interval},
     {tcp_keepalive_probes,tcp_keepalive_probes},
     {tcp_user_timeout,tcp_user_timeout},
     {always_collect_trace_info,always_collect_trace_info},
     {connection_limit_mode,connection_limit_mode},
     {free_connection_pool_size,free_connection_pool_size},
     {max_client_connection_details,max_client_connection_details}]}]},
 {{node,'ns_1@db2.lan',memcached},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
   {port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,11206},
   {ssl_port,11207},
   {admin_user,"@ns_server"},
   {other_users,
    ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing","@cbas",
     "@backup"]},
   {admin_pass,"*****"},
   {engines,
    [{membase,
      [{engine,"/opt/couchbase/lib/memcached/ep.so"},
       {static_config_string,"failpartialwarmup=false"}]},
     {memcached,
      [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
       {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_rotation_period,39003}]},
 {{node,'ns_1@db2.lan',isasl},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
   {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
 {{node,'ns_1@db2.lan',is_enterprise},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   true]},
 {{node,'ns_1@db2.lan',indexer_stmaint_port},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   9105]},
 {{node,'ns_1@db2.lan',indexer_stinit_port},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   9103]},
 {{node,'ns_1@db2.lan',indexer_stcatchup_port},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   9104]},
 {{node,'ns_1@db2.lan',indexer_scan_port},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   9101]},
 {{node,'ns_1@db2.lan',indexer_https_port},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   19102]},
 {{node,'ns_1@db2.lan',indexer_http_port},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   9102]},
 {{node,'ns_1@db2.lan',indexer_admin_port},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   9100]},
 {{node,'ns_1@db2.lan',index_dir},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
   47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
   98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
 {{node,'ns_1@db2.lan',fts_ssl_port},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   18094]},
 {{node,'ns_1@db2.lan',fts_http_port},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   8094]},
 {{node,'ns_1@db2.lan',fts_grpc_ssl_port},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   19130]},
 {{node,'ns_1@db2.lan',fts_grpc_port},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   9130]},
 {{node,'ns_1@db2.lan',eventing_https_port},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   18096]},
 {{node,'ns_1@db2.lan',eventing_http_port},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   8096]},
 {{node,'ns_1@db2.lan',eventing_debug_port},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   9140]},
 {{node,'ns_1@db2.lan',event_log},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/event_log"}]},
 {{node,'ns_1@db2.lan',database_dir},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
   47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
   98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
 {{node,'ns_1@db2.lan',config_version},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   {7,6}]},
 {{node,'ns_1@db2.lan',compaction_daemon},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
   {check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}]},
 {{node,'ns_1@db2.lan',cbas_ssl_port},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   18095]},
 {{node,'ns_1@db2.lan',cbas_result_port},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   9117]},
 {{node,'ns_1@db2.lan',cbas_replication_port},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   9120]},
 {{node,'ns_1@db2.lan',cbas_parent_port},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   9122]},
 {{node,'ns_1@db2.lan',cbas_metadata_port},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   9121]},
 {{node,'ns_1@db2.lan',cbas_metadata_callback_port},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   9119]},
 {{node,'ns_1@db2.lan',cbas_messaging_port},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   9118]},
 {{node,'ns_1@db2.lan',cbas_http_port},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   8095]},
 {{node,'ns_1@db2.lan',cbas_debug_port},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|-1]},
 {{node,'ns_1@db2.lan',cbas_data_port},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   9116]},
 {{node,'ns_1@db2.lan',cbas_console_port},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   9114]},
 {{node,'ns_1@db2.lan',cbas_cluster_port},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   9115]},
 {{node,'ns_1@db2.lan',cbas_cc_http_port},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   9111]},
 {{node,'ns_1@db2.lan',cbas_cc_cluster_port},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   9112]},
 {{node,'ns_1@db2.lan',cbas_cc_client_port},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   9113]},
 {{node,'ns_1@db2.lan',cbas_admin_port},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   9110]},
 {{node,'ns_1@db2.lan',capi_port},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   8092]},
 {{node,'ns_1@db2.lan',backup_https_port},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   18097]},
 {{node,'ns_1@db2.lan',backup_http_port},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   8097]},
 {{node,'ns_1@db2.lan',backup_grpc_port},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   9124]},
 {{node,'ns_1@db2.lan',audit},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}]},
 {{node,'ns_1@db2.lan',address_family},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   inet]},
 {{node,'ns_1@db2.lan',node_encryption},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   false]},
 {{node,'ns_1@db2.lan',erl_external_listeners},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
   {inet,false}]},
 {{node,'ns_1@db2.lan',n2n_client_cert_auth},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   false]},
 {{node,'ns_1@db2.lan',cbas_dirs},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
   "/opt/couchbase/var/lib/couchbase/data"]},
 {{node,'ns_1@db2.lan',eventing_dir},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
   47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
   98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
 {{node,'ns_1@db2.lan',prometheus_auth_info},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   {"@prometheus",
    {auth,
     [{<<"hash">>,
       {[{<<"hashes">>,
          {sanitized,<<"HmfAF2EJsiMXLpFCMXkwI34vHpKDSuQ9OLt/WXmflzM=">>}},
         {<<"algorithm">>,<<"argon2id">>},
         {<<"salt">>,<<"uKWlA91cJAr0Ue2ihPum/w==">>},
         {<<"parallelism">>,1},
         {<<"time">>,3},
         {<<"memory">>,524288}]}},
      {<<"scram-sha-512">>,
       {[{<<"salt">>,
          <<"B9qHYrF6B8PUh2HylXGlWHK8C5mZNOug98FyWD9UiLqElSnKTzdCIZzgFR8u69cjhSjtbD3VEM/dcZbWXNTBGw==">>},
         {<<"iterations">>,15000},
         {<<"hashes">>,
          {sanitized,<<"Jr1d3/BDp9K+TGL8YrZCBCGqzGbFloqcMPVzFi39KC0=">>}}]}},
      {<<"scram-sha-256">>,
       {[{<<"salt">>,<<"1nOjbJ6vT1sVuqxQCaa4EQHYdQk5w+2NYTeK1TUhKmY=">>},
         {<<"iterations">>,15000},
         {<<"hashes">>,
          {sanitized,<<"x86lsx8NdTsLHhvU9aNAyIeE61P+Yt6GyVOHrh2MFQo=">>}}]}},
      {<<"scram-sha-1">>,
       {[{<<"salt">>,<<"2rk59nwO1XOIg7dCVQ5eTg7p0y8=">>},
         {<<"iterations">>,15000},
         {<<"hashes">>,
          {sanitized,
           <<"4dXFSggAV9gFKuhCJ2pKs13gZZUKMc03Uwl7Sc+puO0=">>}}]}}]}}]},
 {{node,'ns_1@db2.lan',node_cert},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
   {subject,<<"CN=Couchbase Server Node (db2.lan)">>},
   {not_after,63985747642},
   {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
   {load_timestamp,63914554042},
   {ca,
    <<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQCuy0dFqLPrJ6xY21RJ5ttVPR78ssxxXSe9JdTj/IsUczkwyxfZfwK\npBszlGYuHO0Utnwq1K4ruMPYFYfpLacJSfsb3KWEmZwAoNuRpjvE24AsH3tvj7cB\nB5v49HJ0axvNAFm75GWDyUr7T7UwcTJaFIFtfy+J0Umt2TjFNY0DPpt1WEzkKFKx\nOdBs+7cNc35ZG4rwVu/EJ14OXhQMXkpbUpYJxS8jzkzSI+1Eb67kH4j5QhnR9H+w\nw2S9AgMBAAGjQjBAMA4GA1UdDwEB/wQEAwIBhjAPBgNVHRMBAf8EBTADAQH/MB0G\nA1UdDgQWBBS8ASq2Mkh0UsZe4fIvzs4mCSfTmjANBgkqhkiG9w0BAQsFAAOCAQEA\nB3q1mzdJc3VY17JDCaRPKY/Veni3oZ6zGJ9r9LnWKE7IO9AxBAKYGMELHMf9httQ\nmRZrXiGQi/tYXLlFFk7IcIID6iOOZLLagDhCQJPi52dGhBB4a2EnBvGF9OJI4zW9\nJw04HNR5cF1cxWIrRdGJAX/kbs/M9jz0STlXziVNwiAYeGIxkjq/fwKk6weai8iI\nwQVu6GsbtSBL8f4FRCk2vK6a1MD54BQDasRShdi3k/DJAEtY92muu13M0jc48mCM\ndEVeXxM8rIrG6LavziTwtO+jkFyWjlRyNbSr6il+neSWJ7ZspTo+ZYFy55CAh5ws\nwk6Ljo2wlQgfGflJbKcxAw==\n-----END CERTIFICATE-----\n">>},
   {pem,
    <<"-----BEGIN CERTIFICATE-----\nMIIDOjCCAiKgAwIBAgIIGD/HyAMALgMwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjJaFw0y\nNzA4MTcxODQ3MjJaMCoxKDAmBgNVBAMTH0NvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nZGIyLmxhbikwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC63r7EIhIk\nOyMwE9mw7bzVJfstC9FfdZvk0bP8vxZkuhNt/bVcND8p00ihMP8FCbfsa5d8VgWC\nDoao/+R+8KpeXDZeXWTzOfGPz6ySJpDMUbPsj/pymSkYBvwsl0iBlPrBYDVlHe6C\nIFfKfnfHOsS9itHFANxppJ+spLeJP7Na1p/rOGFELp9Y0pYnDvAccES6DCwgzMKr\n4lk/1l1L4jJkyHzK0hQ5UutHe7jGBhOxRoIyTCPO5wf6miZJNlKxA/1zwCJuLaft\ny4N6DDBMIs9R/dS02i1IrhKLDvC/7OnK/BkXi4HKLKY/wE3YNBmgMNXEwxUMeqZk\n2IUILCZn4W9fAgMBAAGjajBoMA4GA1UdDwEB/wQEAwIFoDATBgNVHSUEDDAKBggr\nBgEFBQcDATAMBgNVHRMBAf8EAjAAMB8GA1UdIwQYMBaAFLwBKrYySHRSxl7h8i/O\nziYJJ9OaMBIGA1UdEQQLMAmCB2RiMi5sYW4wDQYJKoZIhvcNAQELBQADggEBADkD\n1qqpKjJY4cyeoPzdD5mW7I12+dWN/oW0yUQ4heVH9TZzgThchsucMQvFg3vpZ9j5\nSbxsH9h23M26Cv8PxOjpu8lmlYAtENDDGnagAiwDJuMtSeBkKw36xzLrB4jj8UZ+\n8k+5CLVUzncPt9J9o7Lgm+wgavlprqSMJLUGfAc+O0qGzN+mQk1UeM2a8++Ty97Q\neks+wPVQCPbprWQU7k7F8mSepoAWHglkhfkBjBXgHGvGMIMGDv68l7yfNBJ9yRoJ\nO5cAQgoNiHZDzZ9clPCCGeE+JQnxNQCMOIxq7xhnTrYgBeU9hWu0hBcrHR6ZwO+m\n6MNtblxGbkNee9HO7LY=\n-----END CERTIFICATE-----\n">>},
   {pkey_passphrase_settings,[]},
   {certs_epoch,0},
   {type,generated},
   {hostname,"db2.lan"}]},
 {{node,'ns_1@db2.lan',client_cert},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
   {subject,<<"CN=Couchbase Internal Client (116550952)">>},
   {not_after,63985747642},
   {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
   {load_timestamp,63914554042},
   {ca,
    <<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQCuy0dFqLPrJ6xY21RJ5ttVPR78ssxxXSe9JdTj/IsUczkwyxfZfwK\npBszlGYuHO0Utnwq1K4ruMPYFYfpLacJSfsb3KWEmZwAoNuRpjvE24AsH3tvj7cB\nB5v49HJ0axvNAFm75GWDyUr7T7UwcTJaFIFtfy+J0Umt2TjFNY0DPpt1WEzkKFKx\nOdBs+7cNc35ZG4rwVu/EJ14OXhQMXkpbUpYJxS8jzkzSI+1Eb67kH4j5QhnR9H+w\nw2S9AgMBAAGjQjBAMA4GA1UdDwEB/wQEAwIBhjAPBgNVHRMBAf8EBTADAQH/MB0G\nA1UdDgQWBBS8ASq2Mkh0UsZe4fIvzs4mCSfTmjANBgkqhkiG9w0BAQsFAAOCAQEA\nB3q1mzdJc3VY17JDCaRPKY/Veni3oZ6zGJ9r9LnWKE7IO9AxBAKYGMELHMf9httQ\nmRZrXiGQi/tYXLlFFk7IcIID6iOOZLLagDhCQJPi52dGhBB4a2EnBvGF9OJI4zW9\nJw04HNR5cF1cxWIrRdGJAX/kbs/M9jz0STlXziVNwiAYeGIxkjq/fwKk6weai8iI\nwQVu6GsbtSBL8f4FRCk2vK6a1MD54BQDasRShdi3k/DJAEtY92muu13M0jc48mCM\ndEVeXxM8rIrG6LavziTwtO+jkFyWjlRyNbSr6il+neSWJ7ZspTo+ZYFy55CAh5ws\nwk6Ljo2wlQgfGflJbKcxAw==\n-----END CERTIFICATE-----\n">>},
   {pem,
    <<"-----BEGIN CERTIFICATE-----\nMIIDWjCCAkKgAwIBAgIIGD/HyA7ClpEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjJaFw0y\nNzA4MTcxODQ3MjJaMDAxLjAsBgNVBAMTJUNvdWNoYmFzZSBJbnRlcm5hbCBDbGll\nbnQgKDExNjU1MDk1MikwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDg\nAaBAyJnrm54fDG0xlsEQH9E51WTLpkwtleiz980c498XKRPsgRWnjY920KTUIH2/\ndQcryFyvGTggbCNUUsAqs6qWzDX6iZW6uhcXUh9ujNASr1VNBMLPY60ejukP9Jbq\nxlTfAYvoidlCAZ43nBN17IbZjObU9wEL5AJVX+DLfxX9K0uTI37qWIX6lODZMnek\nj8dGQcmpjyB0ktQaFPLCfzIyXDoA0XE4zsxXW+qX4Xji5cwFwgUgJ3E1IYamZJZv\nF3WkWl7ai8xuos3gTqe41d4rvGemY5Pxao60ClgbgFPaimvOFT7Is2h4TzVQI8f7\nonIen4hIAiwE18Iz4WRJAgMBAAGjgYMwgYAwDgYDVR0PAQH/BAQDAgWgMBMGA1Ud\nJQQMMAoGCCsGAQUFBwMCMAwGA1UdEwEB/wQCMAAwHwYDVR0jBBgwFoAUvAEqtjJI\ndFLGXuHyL87OJgkn05owKgYDVR0RBCMwIYEfaW50ZXJuYWxAaW50ZXJuYWwuY291\nY2hiYXNlLmNvbTANBgkqhkiG9w0BAQsFAAOCAQEAjrpupGzgLn2L0Zf3xDsp7paD\nNHbJDPJeJAeG3u4nAur7tcZqAQbUgzkIPe9IFvz1dfdS9CRfyUYLzjcGK7M6Nzyt\nhgAwmdTOjAxTtcjVstxSdyhSxLORZnyMQcFpzJH+weMhVe5khi/T3nDWyR9yqwW0\nw9iReXBJhh/QyE+EBLMYJ58gXG5qQOHiOVw0ePQmQtoQhkjEM2jWVJegD+boNmOK\nKoCXNSxWZB2dC1MOQxtvA5ggysGIVOYH1ASMyXyZszSCRjKfGDzTYyJSfnBACijg\nWXt8NyZIaucN/aXR9H4CzrC077tJz9hihz0Ld4uhaIbVddLOY13TGq/V/0H6cg==\n-----END CERTIFICATE-----\n">>},
   {pkey_passphrase_settings,[]},
   {certs_epoch,0},
   {type,generated},
   {name,"@internal"}]},
 {{node,'ns_1@db2.lan',uuid},
  [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
   <<"7a33d46fd1976e65466c5efc8b45ef2e">>]},
 {otp,
  [{cookie,{sanitized,<<"biSYTaQUFDVndTs/b+IcDmD2L0woktv9naRdv3GCK6A=">>}}]},
 {nodes_wanted,['ns_1@db2.lan','ns_1@172.19.0.4']},
 {cluster_compat_mode,undefined},
 {{node,'ns_1@db2.lan',membership},inactiveAdded}]
[ns_server:debug,2025-05-15T18:47:23.138Z,ns_1@db2.lan:tombstone_keeper<0.277.0>:tombstone_keeper:handle_call:52]Refresh skipped due to chronicle being in state joining_cluster
[cluster:debug,2025-05-15T18:47:23.140Z,ns_1@db2.lan:ns_cluster<0.273.0>:ns_cluster:perform_actual_join:1613]pre-join cleaned config is:
{config,
 {full,"/opt/couchbase/etc/couchbase/config",undefined,ns_config_default},
 [[],
  [{{node,'ns_1@db2.lan',config_version},{7,6}},
   {directory,"/opt/couchbase/var/lib/couchbase/config"},
   {{node,'ns_1@db2.lan',is_enterprise},true},
   {{node,'ns_1@db2.lan',saslauthd_enabled},true},
   {index_aware_rebalance_disabled,false},
   {max_bucket_count,30},
   {set_view_update_daemon,
    [{update_interval,5000},
     {update_min_changes,5000},
     {replica_update_min_changes,5000}]},
   {{node,'ns_1@db2.lan',compaction_daemon},
    [{check_interval,30},
     {min_db_file_size,131072},
     {min_view_file_size,20971520}]},
   {nodes_wanted,[]},
   {quorum_nodes,['ns_1@db2.lan']},
   {{couchdb,max_parallel_indexers},4},
   {{couchdb,max_parallel_replica_indexers},2},
   {{metakv,<<"/indexing/settings/config">>},
    <<"{\"indexer.settings.compaction.abort_exceed_interval\":false,\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.enable_page_bloom_filter\":false,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.log_level\":\"info\",\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.num_replica\":0,\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.storage_mode\":\"\"}">>},
   {{metakv,<<"/eventing/settings/config">>},<<"{\"ram_quota\":256}">>},
   {{metakv,<<"/query/settings/config">>},
    <<"{\"cleanupclientattempts\":true,\"cleanuplostattempts\":true,\"cleanupwindow\":\"60s\",\"completed-limit\":4000,\"completed-threshold\":1000,\"loglevel\":\"info\",\"max-parallelism\":1,\"memory-quota\":0,\"n1ql-feat-ctrl\":76,\"numatrs\":1024,\"pipeline-batch\":16,\"pipeline-cap\":512,\"prepared-limit\":16384,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"query.settings.tmp_space_size\":5120,\"scan-cap\":512,\"timeout\":0,\"txtimeout\":\"0ms\",\"use-cbo\":true}">>},
   {{metakv,<<"/analytics/settings/config">>},
    <<"{\"analytics.settings.num_replicas\":0}">>},
   {rest_creds,null},
   {client_cert_auth,[{state,"disable"},{prefixes,[]}]},
   {scramsha_fallback_salt,<<55,187,62,55,248,32,6,50,18,123,218,116>>},
   {remote_clusters,[]},
   {{node,'ns_1@db2.lan',isasl},
    [{path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
   {audit,
    [{auditd_enabled,false},
     {rotate_interval,86400},
     {rotate_size,20971520},
     {prune_age,0},
     {disabled,[]},
     {enabled,[]},
     {disabled_users,[]},
     {sync,[]},
     {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
   {{node,'ns_1@db2.lan',audit},[]},
   {memcached,[]},
   {{node,'ns_1@db2.lan',memcached_defaults},
    [{max_connections,65000},
     {system_connections,5000},
     {connection_idle_time,0},
     {verbosity,0},
     {privilege_debug,false},
     {breakpad_enabled,true},
     {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
     {dedupe_nmvb_maps,false},
     {je_malloc_conf,undefined},
     {tracing_enabled,true},
     {datatype_snappy,true},
     {num_reader_threads,<<"default">>},
     {num_writer_threads,<<"default">>},
     {num_auxio_threads,<<"default">>},
     {num_nonio_threads,<<"default">>},
     {num_storage_threads,<<"default">>},
     {tcp_keepalive_idle,360},
     {tcp_keepalive_interval,10},
     {tcp_keepalive_probes,3},
     {tcp_user_timeout,30},
     {always_collect_trace_info,true},
     {connection_limit_mode,<<"disconnect">>},
     {free_connection_pool_size,0},
     {max_client_connection_details,0}]},
   {{node,'ns_1@db2.lan',memcached},
    [{port,11210},
     {dedicated_port,11209},
     {dedicated_ssl_port,11206},
     {ssl_port,11207},
     {admin_user,"@ns_server"},
     {other_users,
      ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
       "@cbas","@backup"]},
     {admin_pass,"*****"},
     {engines,
      [{membase,
        [{engine,"/opt/couchbase/lib/memcached/ep.so"},
         {static_config_string,"failpartialwarmup=false"}]},
       {memcached,
        [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
         {static_config_string,"vb0=true"}]}]},
     {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
     {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
     {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
     {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
     {log_prefix,"memcached.log"},
     {log_generations,20},
     {log_cyclesize,10485760},
     {log_rotation_period,39003}]},
   {{node,'ns_1@db2.lan',memcached_config},
    {[{interfaces,{memcached_config_mgr,get_interfaces,[]}},
      {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
      {connection_idle_time,connection_idle_time},
      {privilege_debug,privilege_debug},
      {breakpad,
       {[{enabled,breakpad_enabled},
         {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
      {deployment_model,{memcached_config_mgr,get_config_profile,[]}},
      {verbosity,verbosity},
      {audit_file,{"~s",[audit_file]}},
      {rbac_file,{"~s",[rbac_file]}},
      {dedupe_nmvb_maps,dedupe_nmvb_maps},
      {tracing_enabled,tracing_enabled},
      {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
      {xattr_enabled,true},
      {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
      {collections_enabled,true},
      {max_connections,max_connections},
      {system_connections,system_connections},
      {num_reader_threads,num_reader_threads},
      {num_writer_threads,num_writer_threads},
      {num_auxio_threads,num_auxio_threads},
      {num_nonio_threads,num_nonio_threads},
      {num_storage_threads,num_storage_threads},
      {logger,
       {[{filename,{"~s/~s",[log_path,log_prefix]}},
         {cyclesize,log_cyclesize}]}},
      {external_auth_service,
       {memcached_config_mgr,get_external_auth_service,[]}},
      {active_external_users_push_interval,
       {memcached_config_mgr,get_external_users_push_interval,[]}},
      {prometheus,{memcached_config_mgr,prometheus_cfg,[]}},
      {sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
      {ssl_sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
      {tcp_keepalive_idle,tcp_keepalive_idle},
      {tcp_keepalive_interval,tcp_keepalive_interval},
      {tcp_keepalive_probes,tcp_keepalive_probes},
      {tcp_user_timeout,tcp_user_timeout},
      {always_collect_trace_info,always_collect_trace_info},
      {connection_limit_mode,connection_limit_mode},
      {free_connection_pool_size,free_connection_pool_size},
      {max_client_connection_details,max_client_connection_details}]}},
   {memory_quota,3406},
   {fts_memory_quota,512},
   {cbas_memory_quota,1549},
   {buckets,[{configs,[]}]},
   {secure_headers,[]},
   {{node,'ns_1@db2.lan',port_servers},[]},
   {{node,'ns_1@db2.lan',ns_log},
    [{filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
   {{node,'ns_1@db2.lan',event_log},
    [{filename,"/opt/couchbase/var/lib/couchbase/event_log"}]},
   {email_alerts,
    [{recipients,["root@localhost"]},
     {sender,"couchbase@localhost"},
     {enabled,false},
     {email_server,
      [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
     {alerts,
      [auto_failover_node,auto_failover_maximum_reached,
       auto_failover_other_nodes_down,auto_failover_cluster_too_small,
       auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
       ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
       indexer_low_resident_percentage,ep_clock_cas_drift_threshold_exceeded,
       communication_issue,time_out_of_sync,disk_usage_analyzer_stuck,
       cert_expires_soon,cert_expired,memory_threshold,history_size_warning,
       stuck_rebalance,memcached_connections]},
     {pop_up_alerts,
      [auto_failover_node,auto_failover_maximum_reached,
       auto_failover_other_nodes_down,auto_failover_cluster_too_small,
       auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
       ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
       indexer_low_resident_percentage,ep_clock_cas_drift_threshold_exceeded,
       communication_issue,time_out_of_sync,disk_usage_analyzer_stuck,
       cert_expires_soon,cert_expired,memory_threshold,history_size_warning,
       stuck_rebalance,memcached_connections]}]},
   {alert_limits,
    [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
   {replication,[{enabled,true}]},
   {log_redaction_default_cfg,[{redact_level,none}]},
   {service_orchestrator_weight,
    [{kv,10000},
     {index,1000},
     {fts,1000},
     {cbas,1000},
     {n1ql,100},
     {eventing,100},
     {backup,10}]},
   {health_monitor_refresh_interval,[]},
   {password_policy,[{min_length,6},{must_present,[]}]},
   {rest,[{port,8091}]},
   {{node,'ns_1@db2.lan',rest},[{port,8091},{port_meta,global}]},
   {{node,'ns_1@db2.lan',ssl_rest_port},18091},
   {{node,'ns_1@db2.lan',xdcr_rest_port},9998},
   {{node,'ns_1@db2.lan',memcached_dedicated_ssl_port},11206},
   {{node,'ns_1@db2.lan',memcached_prometheus},11280},
   {{node,'ns_1@db2.lan',projector_port},9999},
   {{node,'ns_1@db2.lan',projector_ssl_port},9999},
   {{node,'ns_1@db2.lan',query_port},8093},
   {{node,'ns_1@db2.lan',ssl_query_port},18093},
   {{node,'ns_1@db2.lan',indexer_admin_port},9100},
   {{node,'ns_1@db2.lan',indexer_scan_port},9101},
   {{node,'ns_1@db2.lan',indexer_http_port},9102},
   {{node,'ns_1@db2.lan',indexer_stinit_port},9103},
   {{node,'ns_1@db2.lan',indexer_stcatchup_port},9104},
   {{node,'ns_1@db2.lan',indexer_stmaint_port},9105},
   {{node,'ns_1@db2.lan',indexer_https_port},19102},
   {{node,'ns_1@db2.lan',fts_http_port},8094},
   {{node,'ns_1@db2.lan',fts_ssl_port},18094},
   {{node,'ns_1@db2.lan',fts_grpc_port},9130},
   {{node,'ns_1@db2.lan',fts_grpc_ssl_port},19130},
   {{node,'ns_1@db2.lan',eventing_http_port},8096},
   {{node,'ns_1@db2.lan',eventing_debug_port},9140},
   {{node,'ns_1@db2.lan',eventing_https_port},18096},
   {{node,'ns_1@db2.lan',cbas_http_port},8095},
   {{node,'ns_1@db2.lan',cbas_ssl_port},18095},
   {{node,'ns_1@db2.lan',cbas_admin_port},9110},
   {{node,'ns_1@db2.lan',cbas_cc_http_port},9111},
   {{node,'ns_1@db2.lan',cbas_cc_cluster_port},9112},
   {{node,'ns_1@db2.lan',cbas_cc_client_port},9113},
   {{node,'ns_1@db2.lan',cbas_console_port},9114},
   {{node,'ns_1@db2.lan',cbas_cluster_port},9115},
   {{node,'ns_1@db2.lan',cbas_data_port},9116},
   {{node,'ns_1@db2.lan',cbas_result_port},9117},
   {{node,'ns_1@db2.lan',cbas_messaging_port},9118},
   {{node,'ns_1@db2.lan',cbas_metadata_callback_port},9119},
   {{node,'ns_1@db2.lan',cbas_replication_port},9120},
   {{node,'ns_1@db2.lan',cbas_metadata_port},9121},
   {{node,'ns_1@db2.lan',cbas_parent_port},9122},
   {{node,'ns_1@db2.lan',cbas_debug_port},-1},
   {{node,'ns_1@db2.lan',prometheus_http_port},9123},
   {{node,'ns_1@db2.lan',backup_http_port},8097},
   {{node,'ns_1@db2.lan',backup_https_port},18097},
   {{node,'ns_1@db2.lan',backup_grpc_port},9124},
   {{node,'ns_1@db2.lan',capi_port},8092},
   {{node,'ns_1@db2.lan',ssl_capi_port},18092},
   {{node,'ns_1@db2.lan',{project_intact,is_vulnerable}},false},
   {retry_rebalance,
    [{enabled,false},{after_time_period,300},{max_attempts,1}]},
   {auto_failover_cfg,
    [{enabled,true},
     {timeout,120},
     {count,0},
     {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
     {failover_server_group,false},
     {max_count,1},
     {failed_over_server_groups,[]},
     {can_abort_rebalance,true}]},
   {{node,'ns_1@db2.lan',database_dir},
    "/opt/couchbase/var/lib/couchbase/data"},
   {{node,'ns_1@db2.lan',index_dir},"/opt/couchbase/var/lib/couchbase/data"},
   {resource_management,
    [{bucket,
      [{resident_ratio,
        [{enabled,false},{couchstore_minimum,1},{magma_minimum,0.2}]},
       {data_size,
        [{enabled,false},{couchstore_maximum,2},{magma_maximum,16}]}]},
     {index,[]},
     {cores_per_bucket,[{enabled,false},{minimum,0.4}]},
     {disk_usage,[{enabled,false},{maximum,96}]},
     {collections_per_quota,[{enabled,false},{maximum,1}]}]}]],
 [[{{node,'ns_1@db2.lan',{project_intact,is_vulnerable}},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     false]},
   {{node,'ns_1@db2.lan',xdcr_rest_port},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     9998]},
   {{node,'ns_1@db2.lan',uuid},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     <<"7a33d46fd1976e65466c5efc8b45ef2e">>]},
   {{node,'ns_1@db2.lan',ssl_rest_port},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     18091]},
   {{node,'ns_1@db2.lan',ssl_query_port},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     18093]},
   {{node,'ns_1@db2.lan',ssl_capi_port},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     18092]},
   {{node,'ns_1@db2.lan',saslauthd_enabled},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     true]},
   {{node,'ns_1@db2.lan',rest},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
     {port,8091},
     {port_meta,global}]},
   {{node,'ns_1@db2.lan',query_port},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     8093]},
   {{node,'ns_1@db2.lan',prometheus_http_port},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     9123]},
   {{node,'ns_1@db2.lan',prometheus_auth_info},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     {"@prometheus",
      {auth,
       [{<<"hash">>,
         {[{<<"hashes">>,
            {sanitized,<<"HmfAF2EJsiMXLpFCMXkwI34vHpKDSuQ9OLt/WXmflzM=">>}},
           {<<"algorithm">>,<<"argon2id">>},
           {<<"salt">>,<<"uKWlA91cJAr0Ue2ihPum/w==">>},
           {<<"parallelism">>,1},
           {<<"time">>,3},
           {<<"memory">>,524288}]}},
        {<<"scram-sha-512">>,
         {[{<<"salt">>,
            <<"B9qHYrF6B8PUh2HylXGlWHK8C5mZNOug98FyWD9UiLqElSnKTzdCIZzgFR8u69cjhSjtbD3VEM/dcZbWXNTBGw==">>},
           {<<"iterations">>,15000},
           {<<"hashes">>,
            {sanitized,<<"Jr1d3/BDp9K+TGL8YrZCBCGqzGbFloqcMPVzFi39KC0=">>}}]}},
        {<<"scram-sha-256">>,
         {[{<<"salt">>,<<"1nOjbJ6vT1sVuqxQCaa4EQHYdQk5w+2NYTeK1TUhKmY=">>},
           {<<"iterations">>,15000},
           {<<"hashes">>,
            {sanitized,<<"x86lsx8NdTsLHhvU9aNAyIeE61P+Yt6GyVOHrh2MFQo=">>}}]}},
        {<<"scram-sha-1">>,
         {[{<<"salt">>,<<"2rk59nwO1XOIg7dCVQ5eTg7p0y8=">>},
           {<<"iterations">>,15000},
           {<<"hashes">>,
            {sanitized,
             <<"4dXFSggAV9gFKuhCJ2pKs13gZZUKMc03Uwl7Sc+puO0=">>}}]}}]}}]},
   {{node,'ns_1@db2.lan',projector_ssl_port},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     9999]},
   {{node,'ns_1@db2.lan',projector_port},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     9999]},
   {{node,'ns_1@db2.lan',port_servers},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}]},
   {{node,'ns_1@db2.lan',ns_log},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
     {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
   {{node,'ns_1@db2.lan',node_encryption},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     false]},
   {{node,'ns_1@db2.lan',node_cert},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
     {subject,<<"CN=Couchbase Server Node (db2.lan)">>},
     {not_after,63985747642},
     {verified_with,
      <<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
     {load_timestamp,63914554042},
     {ca,
      <<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQCuy0dFqLPrJ6xY21RJ5ttVPR78ssxxXSe9JdTj/IsUczkwyxfZfwK\npBszlGYuHO0Utnwq1K4ruMPYFYfpLacJSfsb3KWEmZwAoNuRpjvE24AsH3tvj7cB\nB5v49HJ0axvNAFm75GWDyUr7T7UwcTJaFIFtfy+J0Umt2TjFNY0DPpt1WEzkKFKx\nOdBs+7cNc35ZG4rwVu/EJ14OXhQMXkpbUpYJxS8jzkzSI+1Eb67kH4j5QhnR9H+w\nw2S9AgMBAAGjQjBAMA4GA1UdDwEB/wQEAwIBhjAPBgNVHRMBAf8EBTADAQH/MB0G\nA1UdDgQWBBS8ASq2Mkh0UsZe4fIvzs4mCSfTmjANBgkqhkiG9w0BAQsFAAOCAQEA\nB3q1mzdJc3VY17JDCaRPKY/Veni3oZ6zGJ9r9LnWKE7IO9AxBAKYGMELHMf9httQ\nmRZrXiGQi/tYXLlFFk7IcIID6iOOZLLagDhCQJPi52dGhBB4a2EnBvGF9OJI4zW9\nJw04HNR5cF1cxWIrRdGJAX/kbs/M9jz0STlXziVNwiAYeGIxkjq/fwKk6weai8iI\nwQVu6GsbtSBL8f4FRCk2vK6a1MD54BQDasRShdi3k/DJAEtY92muu13M0jc48mCM\ndEVeXxM8rIrG6LavziTwtO+jkFyWjlRyNbSr6il+neSWJ7ZspTo+ZYFy55CAh5ws\nwk6Ljo2wlQgfGflJbKcxAw==\n-----END CERTIFICATE-----\n">>},
     {pem,
      <<"-----BEGIN CERTIFICATE-----\nMIIDOjCCAiKgAwIBAgIIGD/HyAMALgMwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjJaFw0y\nNzA4MTcxODQ3MjJaMCoxKDAmBgNVBAMTH0NvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nZGIyLmxhbikwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC63r7EIhIk\nOyMwE9mw7bzVJfstC9FfdZvk0bP8vxZkuhNt/bVcND8p00ihMP8FCbfsa5d8VgWC\nDoao/+R+8KpeXDZeXWTzOfGPz6ySJpDMUbPsj/pymSkYBvwsl0iBlPrBYDVlHe6C\nIFfKfnfHOsS9itHFANxppJ+spLeJP7Na1p/rOGFELp9Y0pYnDvAccES6DCwgzMKr\n4lk/1l1L4jJkyHzK0hQ5UutHe7jGBhOxRoIyTCPO5wf6miZJNlKxA/1zwCJuLaft\ny4N6DDBMIs9R/dS02i1IrhKLDvC/7OnK/BkXi4HKLKY/wE3YNBmgMNXEwxUMeqZk\n2IUILCZn4W9fAgMBAAGjajBoMA4GA1UdDwEB/wQEAwIFoDATBgNVHSUEDDAKBggr\nBgEFBQcDATAMBgNVHRMBAf8EAjAAMB8GA1UdIwQYMBaAFLwBKrYySHRSxl7h8i/O\nziYJJ9OaMBIGA1UdEQQLMAmCB2RiMi5sYW4wDQYJKoZIhvcNAQELBQADggEBADkD\n1qqpKjJY4cyeoPzdD5mW7I12+dWN/oW0yUQ4heVH9TZzgThchsucMQvFg3vpZ9j5\nSbxsH9h23M26Cv8PxOjpu8lmlYAtENDDGnagAiwDJuMtSeBkKw36xzLrB4jj8UZ+\n8k+5CLVUzncPt9J9o7Lgm+wgavlprqSMJLUGfAc+O0qGzN+mQk1UeM2a8++Ty97Q\neks+wPVQCPbprWQU7k7F8mSepoAWHglkhfkBjBXgHGvGMIMGDv68l7yfNBJ9yRoJ\nO5cAQgoNiHZDzZ9clPCCGeE+JQnxNQCMOIxq7xhnTrYgBeU9hWu0hBcrHR6ZwO+m\n6MNtblxGbkNee9HO7LY=\n-----END CERTIFICATE-----\n">>},
     {pkey_passphrase_settings,[]},
     {certs_epoch,0},
     {type,generated},
     {hostname,"db2.lan"}]},
   {{node,'ns_1@db2.lan',n2n_client_cert_auth},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     false]},
   {{node,'ns_1@db2.lan',memcached_prometheus},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     11280]},
   {{node,'ns_1@db2.lan',memcached_defaults},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
     {max_connections,65000},
     {system_connections,5000},
     {connection_idle_time,0},
     {verbosity,0},
     {privilege_debug,false},
     {breakpad_enabled,true},
     {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
     {dedupe_nmvb_maps,false},
     {je_malloc_conf,undefined},
     {tracing_enabled,true},
     {datatype_snappy,true},
     {num_reader_threads,<<"default">>},
     {num_writer_threads,<<"default">>},
     {num_auxio_threads,<<"default">>},
     {num_nonio_threads,<<"default">>},
     {num_storage_threads,<<"default">>},
     {tcp_keepalive_idle,360},
     {tcp_keepalive_interval,10},
     {tcp_keepalive_probes,3},
     {tcp_user_timeout,30},
     {always_collect_trace_info,true},
     {connection_limit_mode,<<"disconnect">>},
     {free_connection_pool_size,0},
     {max_client_connection_details,0}]},
   {{node,'ns_1@db2.lan',memcached_dedicated_ssl_port},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     11206]},
   {{node,'ns_1@db2.lan',memcached_config},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     {[{interfaces,{memcached_config_mgr,get_interfaces,[]}},
       {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
       {connection_idle_time,connection_idle_time},
       {privilege_debug,privilege_debug},
       {breakpad,
        {[{enabled,breakpad_enabled},
          {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
       {deployment_model,{memcached_config_mgr,get_config_profile,[]}},
       {verbosity,verbosity},
       {audit_file,{"~s",[audit_file]}},
       {rbac_file,{"~s",[rbac_file]}},
       {dedupe_nmvb_maps,dedupe_nmvb_maps},
       {tracing_enabled,tracing_enabled},
       {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
       {xattr_enabled,true},
       {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
       {collections_enabled,true},
       {max_connections,max_connections},
       {system_connections,system_connections},
       {num_reader_threads,num_reader_threads},
       {num_writer_threads,num_writer_threads},
       {num_auxio_threads,num_auxio_threads},
       {num_nonio_threads,num_nonio_threads},
       {num_storage_threads,num_storage_threads},
       {logger,
        {[{filename,{"~s/~s",[log_path,log_prefix]}},
          {cyclesize,log_cyclesize}]}},
       {external_auth_service,
        {memcached_config_mgr,get_external_auth_service,[]}},
       {active_external_users_push_interval,
        {memcached_config_mgr,get_external_users_push_interval,[]}},
       {prometheus,{memcached_config_mgr,prometheus_cfg,[]}},
       {sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
       {ssl_sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
       {tcp_keepalive_idle,tcp_keepalive_idle},
       {tcp_keepalive_interval,tcp_keepalive_interval},
       {tcp_keepalive_probes,tcp_keepalive_probes},
       {tcp_user_timeout,tcp_user_timeout},
       {always_collect_trace_info,always_collect_trace_info},
       {connection_limit_mode,connection_limit_mode},
       {free_connection_pool_size,free_connection_pool_size},
       {max_client_connection_details,max_client_connection_details}]}]},
   {{node,'ns_1@db2.lan',memcached},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
     {port,11210},
     {dedicated_port,11209},
     {dedicated_ssl_port,11206},
     {ssl_port,11207},
     {admin_user,"@ns_server"},
     {other_users,
      ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
       "@cbas","@backup"]},
     {admin_pass,"*****"},
     {engines,
      [{membase,
        [{engine,"/opt/couchbase/lib/memcached/ep.so"},
         {static_config_string,"failpartialwarmup=false"}]},
       {memcached,
        [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
         {static_config_string,"vb0=true"}]}]},
     {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
     {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
     {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
     {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
     {log_prefix,"memcached.log"},
     {log_generations,20},
     {log_cyclesize,10485760},
     {log_rotation_period,39003}]},
   {{node,'ns_1@db2.lan',membership},inactiveAdded},
   {{node,'ns_1@db2.lan',isasl},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
     {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
   {{node,'ns_1@db2.lan',is_enterprise},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     true]},
   {{node,'ns_1@db2.lan',indexer_stmaint_port},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     9105]},
   {{node,'ns_1@db2.lan',indexer_stinit_port},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     9103]},
   {{node,'ns_1@db2.lan',indexer_stcatchup_port},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     9104]},
   {{node,'ns_1@db2.lan',indexer_scan_port},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     9101]},
   {{node,'ns_1@db2.lan',indexer_https_port},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     19102]},
   {{node,'ns_1@db2.lan',indexer_http_port},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     9102]},
   {{node,'ns_1@db2.lan',indexer_admin_port},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     9100]},
   {{node,'ns_1@db2.lan',index_dir},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
     47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,
     105,98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
   {{node,'ns_1@db2.lan',fts_ssl_port},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     18094]},
   {{node,'ns_1@db2.lan',fts_http_port},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     8094]},
   {{node,'ns_1@db2.lan',fts_grpc_ssl_port},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     19130]},
   {{node,'ns_1@db2.lan',fts_grpc_port},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     9130]},
   {{node,'ns_1@db2.lan',eventing_https_port},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     18096]},
   {{node,'ns_1@db2.lan',eventing_http_port},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     8096]},
   {{node,'ns_1@db2.lan',eventing_dir},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
     47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,
     105,98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
   {{node,'ns_1@db2.lan',eventing_debug_port},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     9140]},
   {{node,'ns_1@db2.lan',event_log},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
     {filename,"/opt/couchbase/var/lib/couchbase/event_log"}]},
   {{node,'ns_1@db2.lan',erl_external_listeners},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
     {inet,false}]},
   {{node,'ns_1@db2.lan',database_dir},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
     47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,
     105,98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
   {{node,'ns_1@db2.lan',config_version},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     {7,6}]},
   {{node,'ns_1@db2.lan',compaction_daemon},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
     {check_interval,30},
     {min_db_file_size,131072},
     {min_view_file_size,20971520}]},
   {{node,'ns_1@db2.lan',client_cert},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
     {subject,<<"CN=Couchbase Internal Client (116550952)">>},
     {not_after,63985747642},
     {verified_with,
      <<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
     {load_timestamp,63914554042},
     {ca,
      <<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQCuy0dFqLPrJ6xY21RJ5ttVPR78ssxxXSe9JdTj/IsUczkwyxfZfwK\npBszlGYuHO0Utnwq1K4ruMPYFYfpLacJSfsb3KWEmZwAoNuRpjvE24AsH3tvj7cB\nB5v49HJ0axvNAFm75GWDyUr7T7UwcTJaFIFtfy+J0Umt2TjFNY0DPpt1WEzkKFKx\nOdBs+7cNc35ZG4rwVu/EJ14OXhQMXkpbUpYJxS8jzkzSI+1Eb67kH4j5QhnR9H+w\nw2S9AgMBAAGjQjBAMA4GA1UdDwEB/wQEAwIBhjAPBgNVHRMBAf8EBTADAQH/MB0G\nA1UdDgQWBBS8ASq2Mkh0UsZe4fIvzs4mCSfTmjANBgkqhkiG9w0BAQsFAAOCAQEA\nB3q1mzdJc3VY17JDCaRPKY/Veni3oZ6zGJ9r9LnWKE7IO9AxBAKYGMELHMf9httQ\nmRZrXiGQi/tYXLlFFk7IcIID6iOOZLLagDhCQJPi52dGhBB4a2EnBvGF9OJI4zW9\nJw04HNR5cF1cxWIrRdGJAX/kbs/M9jz0STlXziVNwiAYeGIxkjq/fwKk6weai8iI\nwQVu6GsbtSBL8f4FRCk2vK6a1MD54BQDasRShdi3k/DJAEtY92muu13M0jc48mCM\ndEVeXxM8rIrG6LavziTwtO+jkFyWjlRyNbSr6il+neSWJ7ZspTo+ZYFy55CAh5ws\nwk6Ljo2wlQgfGflJbKcxAw==\n-----END CERTIFICATE-----\n">>},
     {pem,
      <<"-----BEGIN CERTIFICATE-----\nMIIDWjCCAkKgAwIBAgIIGD/HyA7ClpEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjJaFw0y\nNzA4MTcxODQ3MjJaMDAxLjAsBgNVBAMTJUNvdWNoYmFzZSBJbnRlcm5hbCBDbGll\nbnQgKDExNjU1MDk1MikwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDg\nAaBAyJnrm54fDG0xlsEQH9E51WTLpkwtleiz980c498XKRPsgRWnjY920KTUIH2/\ndQcryFyvGTggbCNUUsAqs6qWzDX6iZW6uhcXUh9ujNASr1VNBMLPY60ejukP9Jbq\nxlTfAYvoidlCAZ43nBN17IbZjObU9wEL5AJVX+DLfxX9K0uTI37qWIX6lODZMnek\nj8dGQcmpjyB0ktQaFPLCfzIyXDoA0XE4zsxXW+qX4Xji5cwFwgUgJ3E1IYamZJZv\nF3WkWl7ai8xuos3gTqe41d4rvGemY5Pxao60ClgbgFPaimvOFT7Is2h4TzVQI8f7\nonIen4hIAiwE18Iz4WRJAgMBAAGjgYMwgYAwDgYDVR0PAQH/BAQDAgWgMBMGA1Ud\nJQQMMAoGCCsGAQUFBwMCMAwGA1UdEwEB/wQCMAAwHwYDVR0jBBgwFoAUvAEqtjJI\ndFLGXuHyL87OJgkn05owKgYDVR0RBCMwIYEfaW50ZXJuYWxAaW50ZXJuYWwuY291\nY2hiYXNlLmNvbTANBgkqhkiG9w0BAQsFAAOCAQEAjrpupGzgLn2L0Zf3xDsp7paD\nNHbJDPJeJAeG3u4nAur7tcZqAQbUgzkIPe9IFvz1dfdS9CRfyUYLzjcGK7M6Nzyt\nhgAwmdTOjAxTtcjVstxSdyhSxLORZnyMQcFpzJH+weMhVe5khi/T3nDWyR9yqwW0\nw9iReXBJhh/QyE+EBLMYJ58gXG5qQOHiOVw0ePQmQtoQhkjEM2jWVJegD+boNmOK\nKoCXNSxWZB2dC1MOQxtvA5ggysGIVOYH1ASMyXyZszSCRjKfGDzTYyJSfnBACijg\nWXt8NyZIaucN/aXR9H4CzrC077tJz9hihz0Ld4uhaIbVddLOY13TGq/V/0H6cg==\n-----END CERTIFICATE-----\n">>},
     {pkey_passphrase_settings,[]},
     {certs_epoch,0},
     {type,generated},
     {name,"@internal"}]},
   {{node,'ns_1@db2.lan',cbas_ssl_port},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     18095]},
   {{node,'ns_1@db2.lan',cbas_result_port},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     9117]},
   {{node,'ns_1@db2.lan',cbas_replication_port},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     9120]},
   {{node,'ns_1@db2.lan',cbas_parent_port},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     9122]},
   {{node,'ns_1@db2.lan',cbas_metadata_port},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     9121]},
   {{node,'ns_1@db2.lan',cbas_metadata_callback_port},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     9119]},
   {{node,'ns_1@db2.lan',cbas_messaging_port},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     9118]},
   {{node,'ns_1@db2.lan',cbas_http_port},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     8095]},
   {{node,'ns_1@db2.lan',cbas_dirs},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
     "/opt/couchbase/var/lib/couchbase/data"]},
   {{node,'ns_1@db2.lan',cbas_debug_port},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     -1]},
   {{node,'ns_1@db2.lan',cbas_data_port},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     9116]},
   {{node,'ns_1@db2.lan',cbas_console_port},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     9114]},
   {{node,'ns_1@db2.lan',cbas_cluster_port},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     9115]},
   {{node,'ns_1@db2.lan',cbas_cc_http_port},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     9111]},
   {{node,'ns_1@db2.lan',cbas_cc_cluster_port},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     9112]},
   {{node,'ns_1@db2.lan',cbas_cc_client_port},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     9113]},
   {{node,'ns_1@db2.lan',cbas_admin_port},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     9110]},
   {{node,'ns_1@db2.lan',capi_port},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     8092]},
   {{node,'ns_1@db2.lan',backup_https_port},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     18097]},
   {{node,'ns_1@db2.lan',backup_http_port},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     8097]},
   {{node,'ns_1@db2.lan',backup_grpc_port},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     9124]},
   {{node,'ns_1@db2.lan',audit},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}]},
   {{node,'ns_1@db2.lan',address_family},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     inet]},
   {{metakv,<<"/query/settings/config">>},
    <<"{\"cleanupclientattempts\":true,\"cleanuplostattempts\":true,\"cleanupwindow\":\"60s\",\"completed-limit\":4000,\"completed-threshold\":1000,\"loglevel\":\"info\",\"max-parallelism\":1,\"memory-quota\":0,\"n1ql-feat-ctrl\":76,\"numatrs\":1024,\"pipeline-batch\":16,\"pipeline-cap\":512,\"prepared-limit\":16384,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"query.settings.tmp_space_size\":5120,\"scan-cap\":512,\"timeout\":0,\"txtimeout\":\"0ms\",\"use-cbo\":true}">>},
   {{metakv,<<"/indexing/settings/config">>},
    <<"{\"indexer.settings.compaction.abort_exceed_interval\":false,\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.enable_page_bloom_filter\":false,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.log_level\":\"info\",\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.num_replica\":0,\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.storage_mode\":\"\"}">>},
   {{metakv,<<"/eventing/settings/config">>},<<"{\"ram_quota\":256}">>},
   {{metakv,<<"/analytics/settings/config">>},
    <<"{\"analytics.settings.num_replicas\":0}">>},
   {{local_changes_count,<<"7a33d46fd1976e65466c5efc8b45ef2e">>},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{5,63914554043}}]}]},
   {{couchdb,max_parallel_replica_indexers},2},
   {{couchdb,max_parallel_indexers},4},
   {set_view_update_daemon,
    [{update_interval,5000},
     {update_min_changes,5000},
     {replica_update_min_changes,5000}]},
   {service_orchestrator_weight,
    [{kv,10000},
     {index,1000},
     {fts,1000},
     {cbas,1000},
     {n1ql,100},
     {eventing,100},
     {backup,10}]},
   {secure_headers,[]},
   {scramsha_fallback_salt,<<55,187,62,55,248,32,6,50,18,123,218,116>>},
   {retry_rebalance,
    [{enabled,false},{after_time_period,300},{max_attempts,1}]},
   {rest_creds,null},
   {rest,[{port,8091}]},
   {resource_management,
    [{bucket,
      [{resident_ratio,
        [{enabled,false},{couchstore_minimum,1},{magma_minimum,0.2}]},
       {data_size,
        [{enabled,false},{couchstore_maximum,2},{magma_maximum,16}]}]},
     {index,[]},
     {cores_per_bucket,[{enabled,false},{minimum,0.4}]},
     {disk_usage,[{enabled,false},{maximum,96}]},
     {collections_per_quota,[{enabled,false},{maximum,1}]}]},
   {replication,[{enabled,true}]},
   {remote_clusters,[]},
   {quorum_nodes,['ns_1@db2.lan']},
   {password_policy,[{min_length,6},{must_present,[]}]},
   {otp,
    [{cookie,{sanitized,<<"biSYTaQUFDVndTs/b+IcDmD2L0woktv9naRdv3GCK6A=">>}}]},
   {nodes_wanted,['ns_1@db2.lan','ns_1@172.19.0.4']},
   {memory_quota,3406},
   {memcached,[]},
   {max_bucket_count,30},
   {log_redaction_default_cfg,[{redact_level,none}]},
   {index_aware_rebalance_disabled,false},
   {health_monitor_refresh_interval,[]},
   {fts_memory_quota,512},
   {email_alerts,
    [{recipients,["root@localhost"]},
     {sender,"couchbase@localhost"},
     {enabled,false},
     {email_server,
      [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
     {alerts,
      [auto_failover_node,auto_failover_maximum_reached,
       auto_failover_other_nodes_down,auto_failover_cluster_too_small,
       auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
       ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
       indexer_low_resident_percentage,ep_clock_cas_drift_threshold_exceeded,
       communication_issue,time_out_of_sync,disk_usage_analyzer_stuck,
       cert_expires_soon,cert_expired,memory_threshold,history_size_warning,
       stuck_rebalance,memcached_connections]},
     {pop_up_alerts,
      [auto_failover_node,auto_failover_maximum_reached,
       auto_failover_other_nodes_down,auto_failover_cluster_too_small,
       auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
       ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
       indexer_low_resident_percentage,ep_clock_cas_drift_threshold_exceeded,
       communication_issue,time_out_of_sync,disk_usage_analyzer_stuck,
       cert_expires_soon,cert_expired,memory_threshold,history_size_warning,
       stuck_rebalance,memcached_connections]}]},
   {cluster_compat_mode,undefined},
   {client_cert_auth,[{state,"disable"},{prefixes,[]}]},
   {cbas_memory_quota,1549},
   {buckets,[{configs,[]}]},
   {auto_failover_cfg,
    [{enabled,true},
     {timeout,120},
     {count,0},
     {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
     {failover_server_group,false},
     {max_count,1},
     {failed_over_server_groups,[]},
     {can_abort_rebalance,true}]},
   {audit,
    [{auditd_enabled,false},
     {rotate_interval,86400},
     {rotate_size,20971520},
     {prune_age,0},
     {disabled,[]},
     {enabled,[]},
     {disabled_users,[]},
     {sync,[]},
     {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
   {alert_limits,
    [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]}]],
 ns_config_default,
 {ns_config_default,encrypt_and_save,[]},
 <0.2332.0>,false,<<"7a33d46fd1976e65466c5efc8b45ef2e">>,
 #Fun<ns_config.18.54949477>}
[ns_server:debug,2025-05-15T18:47:23.148Z,ns_1@db2.lan:ns_cookie_manager<0.238.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[user:info,2025-05-15T18:47:23.148Z,ns_1@db2.lan:ns_cookie_manager<0.238.0>:ns_cookie_manager:do_cookie_sync:122]Node 'ns_1@db2.lan' synchronized otp cookie {sanitized,
                                             <<"biSYTaQUFDVndTs/b+IcDmD2L0woktv9naRdv3GCK6A=">>} from cluster
[ns_server:debug,2025-05-15T18:47:23.148Z,ns_1@db2.lan:ns_cluster<0.273.0>:ns_cluster:perform_actual_join:1622]Trying to connect to node 'ns_1@172.19.0.4'...
[error_logger:info,2025-05-15T18:47:23.148Z,ns_1@db2.lan:net_kernel<0.2175.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{connect,normal,'ns_1@172.19.0.4'}}
[ns_server:debug,2025-05-15T18:47:23.148Z,ns_1@db2.lan:net_kernel<0.2175.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'ns_1@172.19.0.4' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:23.148Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3735524962.3650355201.245934>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:47:23.148Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3735524962.3650355201.245934>,
                                  inet_tcp_dist,<0.2333.0>,
                                  #Ref<0.3735524962.3650355201.245937>}
[cluster:debug,2025-05-15T18:47:23.150Z,ns_1@db2.lan:ns_cluster<0.273.0>:ns_cluster:perform_actual_join:1632]Connection from 'ns_1@db2.lan' to 'ns_1@172.19.0.4':  true
[ns_server:debug,2025-05-15T18:47:23.150Z,ns_1@db2.lan:chronicle_local<0.239.0>:chronicle_local:handle_call:89]Joining cluster. Info: #{committed_seqno => 17,compat_version => 0,
                         config =>
                          {log_entry,<<"5aab03dbecad06b50ffb474da59a00c9">>,
                           {4,'ns_1@172.19.0.4'},
                           17,
                           {config,
                            {<<"1dd5991463d519e23729dda2c80c376b">>,0,1},
                            0,<<"49d5762e287786c3127905510f7a4115">>,
                            #{'ns_1@172.19.0.4' =>
                               #{id => <<"1dd5991463d519e23729dda2c80c376b">>,
                                 role => voter},
                              'ns_1@db2.lan' =>
                               #{id => <<"9f6894b51ffa497cdfc8b792d9e6726b">>,
                                 role => replica}},
                            undefined,
                            #{chronicle_config_rsm =>
                               {rsm_config,chronicle_config_rsm,[]},
                              kv => {rsm_config,chronicle_kv,[]}},
                            #{},undefined,
                            [{<<"5aab03dbecad06b50ffb474da59a00c9">>,0}]}},
                         history_id => <<"5aab03dbecad06b50ffb474da59a00c9">>}
[chronicle:info,2025-05-15T18:47:23.151Z,ns_1@db2.lan:chronicle_leader<0.2330.0>:chronicle_leader:handle_provisioned:475]System became provisioned.
[error_logger:info,2025-05-15T18:47:23.152Z,ns_1@db2.lan:chronicle_secondary_restartable_sup<0.2336.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_secondary_restartable_sup}
    started: [{pid,<0.2337.0>},
              {id,chronicle_status},
              {mfargs,{chronicle_status,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:23.152Z,ns_1@db2.lan:chronicle_secondary_restartable_sup<0.2336.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_secondary_restartable_sup}
    started: [{pid,<0.2338.0>},
              {id,chronicle_failover},
              {mfargs,{chronicle_failover,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:23.152Z,ns_1@db2.lan:<0.255.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.255.0>,dynamic_supervisor}
    started: [{pid,<0.2336.0>},
              {id,chronicle_secondary_restartable_sup},
              {mfargs,{chronicle_secondary_restartable_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:23.152Z,ns_1@db2.lan:<0.255.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.255.0>,dynamic_supervisor}
    started: [{pid,<0.2339.0>},
              {id,chronicle_server},
              {mfargs,{chronicle_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[chronicle:info,2025-05-15T18:47:23.153Z,ns_1@db2.lan:chronicle_config_rsm<0.2343.0>:chronicle_rsm:get_incarnation:1594]No incarnation file found at '/opt/couchbase/var/lib/couchbase/config/chronicle/rsms/chronicle_config_rsm/incarnation'
[chronicle:debug,2025-05-15T18:47:23.154Z,ns_1@db2.lan:chronicle_server<0.2339.0>:chronicle_server:handle_register_rsm:361]Registering RSM chronicle_config_rsm with pid <0.2343.0>
[error_logger:info,2025-05-15T18:47:23.154Z,ns_1@db2.lan:<0.2342.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2342.0>,chronicle_single_rsm_sup}
    started: [{pid,<0.2343.0>},
              {id,chronicle_config_rsm},
              {mfargs,{chronicle_rsm,start_link,
                                     [chronicle_config_rsm,
                                      <<"9f6894b51ffa497cdfc8b792d9e6726b">>,
                                      chronicle_config_rsm,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:23.154Z,ns_1@db2.lan:<0.2341.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2341.0>,dynamic_supervisor}
    started: [{pid,<0.2342.0>},
              {id,chronicle_config_rsm},
              {mfargs,
                  {chronicle_single_rsm_sup,start_link,
                      [chronicle_config_rsm,
                       <<"9f6894b51ffa497cdfc8b792d9e6726b">>,
                       chronicle_config_rsm,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:23.154Z,ns_1@db2.lan:<0.2346.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2346.0>,chronicle_single_rsm_sup}
    started: [{pid,<0.2347.0>},
              {id,'kv-events'},
              {mfargs,{gen_event,start_link,[{local,'kv-events'}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[chronicle:info,2025-05-15T18:47:23.155Z,ns_1@db2.lan:kv<0.2348.0>:chronicle_rsm:get_incarnation:1594]No incarnation file found at '/opt/couchbase/var/lib/couchbase/config/chronicle/rsms/kv/incarnation'
[chronicle:debug,2025-05-15T18:47:23.156Z,ns_1@db2.lan:chronicle_server<0.2339.0>:chronicle_server:handle_register_rsm:361]Registering RSM kv with pid <0.2348.0>
[error_logger:info,2025-05-15T18:47:23.156Z,ns_1@db2.lan:<0.2346.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2346.0>,chronicle_single_rsm_sup}
    started: [{pid,<0.2348.0>},
              {id,kv},
              {mfargs,{chronicle_rsm,start_link,
                                     [kv,
                                      <<"9f6894b51ffa497cdfc8b792d9e6726b">>,
                                      chronicle_kv,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:23.156Z,ns_1@db2.lan:<0.2341.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2341.0>,dynamic_supervisor}
    started: [{pid,<0.2346.0>},
              {id,kv},
              {mfargs,
                  {chronicle_single_rsm_sup,start_link,
                      [kv,<<"9f6894b51ffa497cdfc8b792d9e6726b">>,chronicle_kv,
                       []]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:23.156Z,ns_1@db2.lan:<0.255.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.255.0>,dynamic_supervisor}
    started: [{pid,<0.2340.0>},
              {id,chronicle_rsm_sup},
              {mfargs,{chronicle_rsm_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:47:23.156Z,ns_1@db2.lan:tombstone_keeper<0.277.0>:tombstone_keeper:handle_call:49]Refreshed with timestamps []
[ns_server:info,2025-05-15T18:47:23.156Z,ns_1@db2.lan:ns_cluster<0.273.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@172.19.0.4'
[ns_server:debug,2025-05-15T18:47:23.158Z,ns_1@db2.lan:ns_cluster<0.273.0>:ns_config:log_conflict:1423]Conflicting configuration changes to field nodes_wanted:
[] and
['ns_1@db2.lan','ns_1@172.19.0.4'], choosing the former.
[ns_server:debug,2025-05-15T18:47:23.158Z,ns_1@db2.lan:ns_cluster<0.273.0>:ns_config:log_conflict:1423]Conflicting configuration changes to field scramsha_fallback_salt:
<<55,187,62,55,248,32,6,50,18,123,218,116>> and
<<199,70,35,121,194,74,234,136,104,77,98,154>>, choosing the former.
[ns_server:debug,2025-05-15T18:47:23.159Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
nodes_wanted ->
[]
[ns_server:debug,2025-05-15T18:47:23.161Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
audit_decriptors ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554009}}]},
 {8243,
  [{name,<<"mutate document">>},
   {description,<<"Document was mutated via the REST API">>},
   {enabled,true},
   {module,ns_server}]},
 {8255,
  [{name,<<"read document">>},
   {description,<<"Document was read via the REST API">>},
   {enabled,false},
   {module,ns_server}]},
 {8257,
  [{name,<<"alert email sent">>},
   {description,<<"An alert email was successfully sent">>},
   {enabled,true},
   {module,ns_server}]},
 {8265,
  [{name,<<"RBAC information retrieved">>},
   {description,<<"RBAC information was retrieved">>},
   {enabled,true},
   {module,ns_server}]},
 {16384,
  [{name,<<"remote cluster ref creation">>},
   {description,<<"created remote cluster ref">>},
   {enabled,true},
   {module,xdcr}]},
 {16385,
  [{name,<<"remote cluster ref update">>},
   {description,<<"updated remote cluster ref">>},
   {enabled,true},
   {module,xdcr}]},
 {16386,
  [{name,<<"remote cluster ref deletion">>},
   {description,<<"deleted remote cluster ref">>},
   {enabled,true},
   {module,xdcr}]},
 {16387,
  [{name,<<"replication creation">>},
   {description,<<"created replication">>},
   {enabled,true},
   {module,xdcr}]},
 {16388,
  [{name,<<"replication pause">>},
   {description,<<"paused replication">>},
   {enabled,true},
   {module,xdcr}]},
 {16389,
  [{name,<<"replication resume">>},
   {description,<<"resumed replication">>},
   {enabled,true},
   {module,xdcr}]},
 {16390,
  [{name,<<"replication cancellation">>},
   {description,<<"canceled replication">>},
   {enabled,true},
   {module,xdcr}]},
 {16391,
  [{name,<<"default replication settings update">>},
   {description,<<"updated default replication settings">>},
   {enabled,true},
   {module,xdcr}]},
 {16392,
  [{name,<<"individual replication settings update">>},
   {description,<<"updated individual replication settings">>},
   {enabled,true},
   {module,xdcr}]},
 {16393,
  [{name,<<"bucket settings update">>},
   {description,<<"updated bucket settings">>},
   {enabled,true},
   {module,xdcr}]},
 {16394,
  [{name,<<"authorization failure while adding remote cluster ref">>},
   {description,<<"failed to add remote cluster ref because of authorization failure">>},
   {enabled,true},
   {module,xdcr}]},
 {16395,
  [{name,<<"authorization failure while updating remote cluster ref">>},
   {description,<<"failed to update remote cluster ref because of authorization failure">>},
   {enabled,true},
   {module,xdcr}]},
 {16396,
  [{name,<<"access denied">>},
   {description,<<"access denied">>},
   {enabled,true},
   {module,xdcr}]},
 {20480,
  [{name,<<"opened DCP connection">>},
   {description,<<"opened DCP connection">>},
   {enabled,true},
   {module,memcached}]},
 {20482,
  [{name,<<"external memcached bucket flush">>},
   {description,<<"External user flushed the content of a memcached bucket">>},
   {enabled,true},
   {module,memcached}]},
 {20483,
  [{name,<<"invalid packet">>},
   {description,<<"Rejected an invalid packet">>},
   {enabled,true},
   {module,memcached}]},
 {20485,
  [{name,<<"authentication succeeded">>},
   {description,<<"Authentication to the cluster succeeded">>},
   {enabled,false},
   {module,memcached}]},
 {20488,
  [{name,<<"document read">>},
   {description,<<"Document was read">>},
   {enabled,false},
   {module,memcached}]},
 {20489,
  [{name,<<"document locked">>},
   {description,<<"Document was locked">>},
   {enabled,false},
   {module,memcached}]},
 {20490,
  [{name,<<"document modify">>},
   {description,<<"Document was modified">>},
   {enabled,false},
   {module,memcached}]},
 {20491,
  [{name,<<"document delete">>},
   {description,<<"Document was deleted">>},
   {enabled,false},
   {module,memcached}]},
 {20492,
  [{name,<<"select bucket">>},
   {description,<<"The specified bucket was selected">>},
   {enabled,true},
   {module,memcached}]},
 {20493,
  [{name,<<"session terminated">>},
   {description,<<"Session to the cluster has terminated">>},
   {enabled,false},
   {module,memcached}]},
 {20494,
  [{name,<<"tenant rate limited">>},
   {description,<<"The given tenant was rate limited">>},
   {enabled,true},
   {module,memcached}]},
 {24576,
  [{name,<<"Delete index">>},
   {description,<<"FTS index was deleted">>},
   {enabled,true},
   {module,fts}]},
 {24577,
  [{name,<<"Create/Update index">>},
   {description,<<"FTS index was created/Updated">>},
   {enabled,true},
   {module,fts}]},
 {24579,
  [{name,<<"Control index">>},
   {description,<<"FTS index control command was issued">>},
   {enabled,true},
   {module,fts}]},
 {24582,
  [{name,<<"GC run">>},
   {description,<<"GC run was triggered">>},
   {enabled,true},
   {module,fts}]},
 {24583,
  [{name,<<"CPU profile">>},
   {description,<<"CPU profiling was started">>},
   {enabled,true},
   {module,fts}]},
 {24584,
  [{name,<<"Memory profile">>},
   {description,<<"Memory profiling was started">>},
   {enabled,true},
   {module,fts}]},
 {28672,
  [{name,<<"SELECT statement">>},
   {description,<<"A N1QL SELECT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28673,
  [{name,<<"EXPLAIN statement">>},
   {description,<<"A N1QL EXPLAIN statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28674,
  [{name,<<"PREPARE statement">>},
   {description,<<"A N1QL PREPARE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28675,
  [{name,<<"INFER statement">>},
   {description,<<"A N1QL INFER statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28676,
  [{name,<<"INSERT statement">>},
   {description,<<"A N1QL INSERT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28677,
  [{name,<<"UPSERT statement">>},
   {description,<<"A N1QL UPSERT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28678,
  [{name,<<"DELETE statement">>},
   {description,<<"A N1QL DELETE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28679,
  [{name,<<"UPDATE statement">>},
   {description,<<"A N1QL UPDATE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28680,
  [{name,<<"MERGE statement">>},
   {description,<<"A N1QL MERGE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28681,
  [{name,<<"CREATE INDEX statement">>},
   {description,<<"A N1QL CREATE INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28682,
  [{name,<<"DROP INDEX statement">>},
   {description,<<"A N1QL DROP INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28683,
  [{name,<<"ALTER INDEX statement">>},
   {description,<<"A N1QL ALTER INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28684,
  [{name,<<"BUILD INDEX statement">>},
   {description,<<"A N1QL BUILD INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28685,
  [{name,<<"GRANT ROLE statement">>},
   {description,<<"A N1QL GRANT ROLE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28686,
  [{name,<<"REVOKE ROLE statement">>},
   {description,<<"A N1QL REVOKE ROLE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28687,
  [{name,<<"UNRECOGNIZED statement">>},
   {description,<<"An unrecognized statement was received by the N1QL query engine">>},
   {enabled,false},
   {module,n1ql}]},
 {28688,
  [{name,<<"CREATE PRIMARY INDEX statement">>},
   {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28689,
  [{name,<<"/admin/stats API request">>},
   {description,<<"An HTTP request was made to the API at /admin/stats.">>},
   {enabled,false},
   {module,n1ql}]},
 {28690,
  [{name,<<"/admin/vitals API request">>},
   {description,<<"An HTTP request was made to the API at /admin/vitals.">>},
   {enabled,false},
   {module,n1ql}]},
 {28691,
  [{name,<<"/admin/prepareds API request">>},
   {description,<<"An HTTP request was made to the API at /admin/prepareds.">>},
   {enabled,false},
   {module,n1ql}]},
 {28692,
  [{name,<<"/admin/active_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/active_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28693,
  [{name,<<"/admin/indexes/prepareds API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
   {enabled,false},
   {module,n1ql}]},
 {28694,
  [{name,<<"/admin/indexes/active_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28695,
  [{name,<<"/admin/indexes/completed_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28697,
  [{name,<<"/admin/ping API request">>},
   {description,<<"An HTTP request was made to the API at /admin/ping.">>},
   {enabled,false},
   {module,n1ql}]},
 {28698,
  [{name,<<"/admin/config API request">>},
   {description,<<"An HTTP request was made to the API at /admin/config.">>},
   {enabled,false},
   {module,n1ql}]},
 {28699,
  [{name,<<"/admin/ssl_cert API request">>},
   {description,<<"An HTTP request was made to the API at /admin/ssl_cert.">>},
   {enabled,false},
   {module,n1ql}]},
 {28700,
  [{name,<<"/admin/settings API request">>},
   {description,<<"An HTTP request was made to the API at /admin/settings.">>},
   {enabled,false},
   {module,n1ql}]},
 {28701,
  [{name,<<"/admin/clusters API request">>},
   {description,<<"An HTTP request was made to the API at /admin/clusters.">>},
   {enabled,false},
   {module,n1ql}]},
 {28702,
  [{name,<<"/admin/completed_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/completed_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28704,
  [{name,<<"/admin/functions API request">>},
   {description,<<"An HTTP request was made to the API at /admin/functions.">>},
   {enabled,false},
   {module,n1ql}]},
 {28705,
  [{name,<<"/admin/indexes/functions API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/functions.">>},
   {enabled,false},
   {module,n1ql}]},
 {28706,
  [{name,<<"CREATE FUNCTION statement">>},
   {description,<<"A N1QL CREATE FUNCTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28707,
  [{name,<<"DROP FUNCTION statement">>},
   {description,<<"A N1QL DROP FUNCTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28708,
  [{name,<<"EXECUTE FUNCTION statement">>},
   {description,<<"A N1QL EXECUTE FUNCTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28709,
  [{name,<<"/admin/tasks API request">>},
   {description,<<"An HTTP request was made to the API at /admin/tasks.">>},
   {enabled,false},
   {module,n1ql}]},
 {28710,
  [{name,<<"/admin/indexes/tasks API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/tasks.">>},
   {enabled,false},
   {module,n1ql}]},
 {28711,
  [{name,<<"/admin/dictionary_cache API request">>},
   {description,<<"An HTTP request was made to the API at /admin/dictionary_cache.">>},
   {enabled,false},
   {module,n1ql}]},
 {28712,
  [{name,<<"/admin/indexes/dictionary_cache API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/dictionary_cache.">>},
   {enabled,false},
   {module,n1ql}]},
 {28713,
  [{name,<<"CREATE SCOPE statement">>},
   {description,<<"A N1QL CREATE SCOPE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28714,
  [{name,<<"DROP SCOPE statement">>},
   {description,<<"A N1QL DROP SCOPE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28715,
  [{name,<<"CREATE COLLECTION statement">>},
   {description,<<"A N1QL CREATE COLLECTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28716,
  [{name,<<"DROP COLLECTION statement">>},
   {description,<<"A N1QL DROP COLLECTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28717,
  [{name,<<"FLUSH COLLECTION statement">>},
   {description,<<"A N1QL FLUSH COLLECTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28718,
  [{name,<<"UPDATE STATISTICS statement">>},
   {description,<<"A N1QL UPDATE STATISTICS statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28719,
  [{name,<<"ADVISE statement">>},
   {description,<<"A N1QL ADVISE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28720,
  [{name,<<"START TRANSACTION statement">>},
   {description,<<"A N1QL START TRANSACTION statement was execu"...>>},
   {enabled,false},
   {module,n1ql}]},
 {28721,
  [{name,<<"COMMIT TRANSACTION statement">>},
   {description,<<"A N1QL COMMIT TRANSACTION statement was "...>>},
   {enabled,false},
   {module,n1ql}]},
 {28722,
  [{name,<<"ROLLBACK TRANSACTION statement">>},
   {description,<<"A N1QL ROLLBACK TRANSACTION statemen"...>>},
   {enabled,false},
   {module,n1ql}]},
 {28723,
  [{name,<<"ROLLBACK TRANSACTION TO SAVEPOINT st"...>>},
   {description,<<"A N1QL ROLLBACK TRANSACTION TO S"...>>},
   {enabled,false},
   {module,n1ql}]},
 {28724,
  [{name,<<"SET TRANSACTION ISOLATION statem"...>>},
   {description,<<"A N1QL SET TRANSACTION ISOLA"...>>},
   {enabled,false},
   {module,n1ql}]},
 {28725,
  [{name,<<"SAVEPOINT statement">>},
   {description,<<"A N1QL SAVEPOINT stateme"...>>},
   {enabled,false},
   {module,n1ql}]},
 {28726,
  [{name,<<"/admin/transactions API "...>>},
   {description,<<"An HTTP request was "...>>},
   {enabled,false},
   {module,n1ql}]},
 {28727,
  [{name,<<"/admin/indexes/trans"...>>},
   {description,<<"An HTTP request "...>>},
   {enabled,false},
   {module,n1ql}]},
 {28728,
  [{name,<<"N1QL backup / re"...>>},
   {description,<<"An HTTP requ"...>>},
   {enabled,false},
   {module,n1ql}]},
 {28729,
  [{name,<<"/admin/shutd"...>>},
   {description,<<"An HTTP "...>>},
   {enabled,false},
   {module,n1ql}]},
 {28730,
  [{name,<<"/admin/g"...>>},
   {description,<<"An H"...>>},
   {enabled,false},
   {module,...}]},
 {28731,[{name,<<"/adm"...>>},{description,<<...>>},{enabled,...},{...}]},
 {28732,[{name,<<...>>},{description,...},{...}|...]},
 {28733,[{name,...},{...}|...]},
 {28734,[{...}|...]},
 {28735,[...]},
 {28736,...},
 {...}|...]
[ns_server:debug,2025-05-15T18:47:23.161Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
auto_failover_cfg ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{3,63914554009}}]},
 {enabled,true},
 {timeout,120},
 {count,0},
 {max_count,1},
 {disable_max_count,false},
 {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
 {failover_server_group,false},
 {failed_over_server_groups,[]},
 {can_abort_rebalance,true},
 {failover_preserve_durability_majority,false}]
[ns_server:debug,2025-05-15T18:47:23.161Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
cbas_memory_quota ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554041}}]}|1024]
[ns_server:debug,2025-05-15T18:47:23.161Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
cluster_compat_version ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{3,63914554009}}]},7,6]
[ns_server:debug,2025-05-15T18:47:23.161Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
memory_quota ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554041}}]}|3072]
[ns_server:debug,2025-05-15T18:47:23.161Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
otp ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554009}}]},
 {cookie,{sanitized,<<"biSYTaQUFDVndTs/b+IcDmD2L0woktv9naRdv3GCK6A=">>}}]
[ns_server:debug,2025-05-15T18:47:23.161Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
quorum_nodes ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554042}}]},
 'ns_1@172.19.0.4']
[ns_server:debug,2025-05-15T18:47:23.161Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
rbac_upgrade ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554009}}]}|
 '_deleted']
[ns_server:debug,2025-05-15T18:47:23.162Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
resource_management ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554009}}]},
 {bucket,[{resident_ratio,[{enabled,false},
                           {couchstore_minimum,1},
                           {magma_minimum,0.2}]},
          {data_size,[{enabled,false},
                      {couchstore_maximum,2},
                      {magma_maximum,16}]}]},
 {index,[]},
 {cores_per_bucket,[{enabled,false},{minimum,0.4}]},
 {disk_usage,[{enabled,false},{maximum,96}]},
 {collections_per_quota,[{enabled,false},{maximum,1}]}]
[ns_server:debug,2025-05-15T18:47:23.162Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
rest_creds ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554041}}]}|
 {"<ud>jaba_admin</ud>",
  {auth,
   [{<<"hash">>,
     {[{<<"hashes">>,
        {sanitized,<<"3I14lVC6WTo2HkE1Ma2OkSoP1R5UwTP501YfMTKM7ak=">>}},
       {<<"algorithm">>,<<"argon2id">>},
       {<<"salt">>,<<"5CFoS4MqJh4VclPVzeJxBw==">>},
       {<<"parallelism">>,1},
       {<<"time">>,3},
       {<<"memory">>,524288}]}},
    {<<"scram-sha-512">>,
     {[{<<"salt">>,
        <<"pulx/Jw0I8WxLIxMcCD8CqAxiVNfHNaHWpBI6wjnArfbB+WyxXwBuk5SnOGA7W5LgTZZWGevGaY3ih1tGH4jeg==">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"+8aFWL6Fwfs8HKehUbj7ZkLi5LW9/xRhbkbg/BwGA9w=">>}}]}},
    {<<"scram-sha-256">>,
     {[{<<"salt">>,<<"6mOH6zwaMRxGOfTsFTCuTxfMVctxE7w9qyktmzrJtVg=">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"t9IO+WS5kijXIacgFn1gB5R0bN61IAKLCL/JWibrJSA=">>}}]}},
    {<<"scram-sha-1">>,
     {[{<<"salt">>,<<"pEmVjw1zOFSsjZc/8lZKrEstBkA=">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"tJt6Wglz80zHvML1mQcTdrXmQggip4HdkH+YfamhrEc=">>}}]}}]}}]
[ns_server:debug,2025-05-15T18:47:23.162Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
settings ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554041}}]},
 {stats,[{send_stats,true}]}]
[ns_server:debug,2025-05-15T18:47:23.162Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
uuid ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554041}}]}|
 <<"61933a8028482692b0278a91060e0d90">>]
[ns_server:debug,2025-05-15T18:47:23.162Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/analytics/settings/config">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554009}}]}|
 <<"{\"analytics.settings.blob_storage_bucket\":\"\",\"analytics.settings.blob_storage_prefix\":\"\",\"analytics.settings.blob_storage_region\":\"\",\"analytics.settings.blob_storage_scheme\":\"\",\"analytics.settings.num_replicas\":0}">>]
[ns_server:debug,2025-05-15T18:47:23.162Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554041}}]}|
 <<"{\"indexer.settings.compaction.abort_exceed_interval\":false,\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.enable_page_bloom_filter\":false,\"indexer.settings.enable_"...>>]
[ns_server:debug,2025-05-15T18:47:23.162Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/query/functions_cache/counter">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554042}}]}|
 <<"[172.19.0.4:8091]0">>]
[ns_server:debug,2025-05-15T18:47:23.162Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/query/sequences_cache/revision">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554009}}]}|
 <<"0">>]
[ns_server:debug,2025-05-15T18:47:23.162Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/query/settings/config">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554009}}]}|
 <<"{\"cleanupclientattempts\":true,\"cleanuplostattempts\":true,\"cleanupwindow\":\"60s\",\"completed-limit\":4000,\"completed-max-plan-size\":262144,\"completed-threshold\":1000,\"loglevel\":\"info\",\"max-parallelism\":1,\"memory-quota\":0,\"n1ql-feat-ctrl\":76,\"node-quota\":0,\"node-quota-val-percent\":67,\"num-cpus\":0,\"numatrs\":1024,\"pipeline-batch\":16,\"pipeline-cap\":512,\"prepared-limit\":16384,\"query.settings.cu"...>>]
[ns_server:debug,2025-05-15T18:47:23.162Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',address_family} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|inet]
[ns_server:debug,2025-05-15T18:47:23.162Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',audit} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}]
[ns_server:debug,2025-05-15T18:47:23.162Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',backup_grpc_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9124]
[ns_server:debug,2025-05-15T18:47:23.162Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',backup_http_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|8097]
[ns_server:debug,2025-05-15T18:47:23.162Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',backup_https_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|18097]
[ns_server:debug,2025-05-15T18:47:23.162Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',capi_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|8092]
[ns_server:debug,2025-05-15T18:47:23.162Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_admin_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9110]
[ns_server:debug,2025-05-15T18:47:23.163Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_cc_client_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9113]
[ns_server:debug,2025-05-15T18:47:23.163Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_cc_cluster_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9112]
[ns_server:debug,2025-05-15T18:47:23.163Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_cc_http_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9111]
[ns_server:debug,2025-05-15T18:47:23.163Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_cluster_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9115]
[ns_server:debug,2025-05-15T18:47:23.163Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_console_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9114]
[ns_server:debug,2025-05-15T18:47:23.163Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_data_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9116]
[ns_server:debug,2025-05-15T18:47:23.163Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_debug_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|-1]
[ns_server:debug,2025-05-15T18:47:23.163Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_dirs} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
 "/opt/couchbase/var/lib/couchbase/data"]
[ns_server:debug,2025-05-15T18:47:23.163Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_http_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|8095]
[ns_server:debug,2025-05-15T18:47:23.163Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_messaging_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9118]
[ns_server:debug,2025-05-15T18:47:23.163Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_metadata_callback_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9119]
[ns_server:debug,2025-05-15T18:47:23.163Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_metadata_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9121]
[ns_server:debug,2025-05-15T18:47:23.163Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_parent_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9122]
[ns_server:debug,2025-05-15T18:47:23.163Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_replication_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9120]
[ns_server:debug,2025-05-15T18:47:23.163Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_result_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9117]
[ns_server:debug,2025-05-15T18:47:23.163Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_ssl_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|18095]
[ns_server:debug,2025-05-15T18:47:23.163Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',client_cert} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
 {subject,<<"CN=Couchbase Internal Client (70858842)">>},
 {not_after,63985747606},
 {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
 {load_timestamp,63914554006},
 {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQ"...>>},
 {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDWTCCAkGgAwIBAgIIGD/Hv7XHejcwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ2NDZaFw0y\nNzA4MTcxODQ2NDZaMC8xLTArBgNVBAMTJENvdWNoYmFzZSBJbnRlcm5hbCBDbGll\nbnQgKDcwODU4ODQyKTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMSK\nvXvBcS/ChHK2Kb84PvDZyFlVdyCnMwkD8aug5L/C9zj5626M2/KO3H+iam8QErOa\nUj7TQ/A"...>>},
 {pkey_passphrase_settings,[]},
 {certs_epoch,0},
 {type,generated},
 {name,"@internal"}]
[ns_server:debug,2025-05-15T18:47:23.163Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',compaction_daemon} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
 {check_interval,30},
 {min_db_file_size,131072},
 {min_view_file_size,20971520}]
[ns_server:debug,2025-05-15T18:47:23.163Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',config_version} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|{7,6}]
[ns_server:debug,2025-05-15T18:47:23.163Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',database_dir} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2025-05-15T18:47:23.163Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',erl_external_listeners} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
 {inet,false}]
[ns_server:debug,2025-05-15T18:47:23.164Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',event_log} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
 {filename,"/opt/couchbase/var/lib/couchbase/event_log"}]
[ns_server:debug,2025-05-15T18:47:23.164Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',eventing_debug_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9140]
[ns_server:debug,2025-05-15T18:47:23.164Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',eventing_dir} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2025-05-15T18:47:23.164Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',eventing_http_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|8096]
[ns_server:debug,2025-05-15T18:47:23.164Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',eventing_https_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|18096]
[ns_server:debug,2025-05-15T18:47:23.164Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',fts_grpc_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9130]
[ns_server:debug,2025-05-15T18:47:23.164Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',fts_grpc_ssl_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|19130]
[ns_server:debug,2025-05-15T18:47:23.164Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',fts_http_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|8094]
[ns_server:debug,2025-05-15T18:47:23.164Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',fts_ssl_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|18094]
[ns_server:debug,2025-05-15T18:47:23.164Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',index_dir} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2025-05-15T18:47:23.164Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',indexer_admin_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9100]
[ns_server:debug,2025-05-15T18:47:23.164Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',indexer_http_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9102]
[ns_server:debug,2025-05-15T18:47:23.164Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',indexer_https_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|19102]
[ns_server:debug,2025-05-15T18:47:23.164Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',indexer_scan_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9101]
[ns_server:debug,2025-05-15T18:47:23.164Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',indexer_stcatchup_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9104]
[ns_server:debug,2025-05-15T18:47:23.164Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',indexer_stinit_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9103]
[ns_server:debug,2025-05-15T18:47:23.164Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',indexer_stmaint_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9105]
[ns_server:debug,2025-05-15T18:47:23.164Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',is_enterprise} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|true]
[ns_server:debug,2025-05-15T18:47:23.164Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',isasl} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
 {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]
[ns_server:debug,2025-05-15T18:47:23.165Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',memcached} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
 {port,11210},
 {dedicated_port,11209},
 {dedicated_ssl_port,11206},
 {ssl_port,11207},
 {admin_user,"@ns_server"},
 {other_users,["@cbq-engine","@projector","@goxdcr","@index","@fts",
               "@eventing","@cbas","@backup"]},
 {admin_pass,"*****"},
 {engines,[{membase,[{engine,"/opt/couchbase/lib/memcached/ep.so"},
                     {static_config_string,"failpartialwarmup=false"}]},
           {memcached,[{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
                       {static_config_string,"vb0=true"}]}]},
 {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
 {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
 {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
 {log_prefix,"memcached.log"},
 {log_generations,20},
 {log_cyclesize,10485760},
 {log_rotation_period,39003}]
[ns_server:debug,2025-05-15T18:47:23.166Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',memcached_config} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
 {[{interfaces,{memcached_config_mgr,get_interfaces,[]}},
   {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
   {connection_idle_time,connection_idle_time},
   {privilege_debug,privilege_debug},
   {breakpad,
    {[{enabled,breakpad_enabled},
      {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
   {deployment_model,{memcached_config_mgr,get_config_profile,[]}},
   {verbosity,verbosity},
   {audit_file,{"~s",[audit_file]}},
   {rbac_file,{"~s",[rbac_file]}},
   {dedupe_nmvb_maps,dedupe_nmvb_maps},
   {tracing_enabled,tracing_enabled},
   {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
   {xattr_enabled,true},
   {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
   {collections_enabled,true},
   {max_connections,max_connections},
   {system_connections,system_connections},
   {num_reader_threads,num_reader_threads},
   {num_writer_threads,num_writer_threads},
   {num_auxio_threads,num_auxio_threads},
   {num_nonio_threads,num_nonio_threads},
   {num_storage_threads,num_storage_threads},
   {logger,
    {[{filename,{"~s/~s",[log_path,log_prefix]}},{cyclesize,log_cyclesize}]}},
   {external_auth_service,{memcached_config_mgr,get_external_auth_service,[]}},
   {active_external_users_push_interval,
    {memcached_config_mgr,get_external_users_push_interval,[]}},
   {prometheus,{memcached_config_mgr,prometheus_cfg,[]}},
   {sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
   {ssl_sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
   {tcp_keepalive_idle,tcp_keepalive_idle},
   {tcp_keepalive_interval,tcp_keepalive_interval},
   {tcp_keepalive_probes,tcp_keepalive_probes},
   {tcp_user_timeout,tcp_user_timeout},
   {always_collect_trace_info,always_collect_trace_info},
   {connection_limit_mode,connection_limit_mode},
   {free_connection_pool_size,free_connection_pool_size},
   {max_client_connection_details,max_client_connection_details}]}]
[ns_server:debug,2025-05-15T18:47:23.166Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',memcached_dedicated_ssl_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|11206]
[ns_server:debug,2025-05-15T18:47:23.166Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',memcached_defaults} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
 {max_connections,65000},
 {system_connections,5000},
 {connection_idle_time,0},
 {verbosity,0},
 {privilege_debug,false},
 {breakpad_enabled,true},
 {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
 {dedupe_nmvb_maps,false},
 {je_malloc_conf,undefined},
 {tracing_enabled,true},
 {datatype_snappy,true},
 {num_reader_threads,<<"default">>},
 {num_writer_threads,<<"default">>},
 {num_auxio_threads,<<"default">>},
 {num_nonio_threads,<<"default">>},
 {num_storage_threads,<<"default">>},
 {tcp_keepalive_idle,360},
 {tcp_keepalive_interval,10},
 {tcp_keepalive_probes,3},
 {tcp_user_timeout,30},
 {always_collect_trace_info,true},
 {connection_limit_mode,<<"disconnect">>},
 {free_connection_pool_size,0},
 {max_client_connection_details,0}]
[ns_server:debug,2025-05-15T18:47:23.166Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',memcached_prometheus} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|11280]
[ns_server:debug,2025-05-15T18:47:23.166Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',n2n_client_cert_auth} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|false]
[ns_server:debug,2025-05-15T18:47:23.166Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',node_cert} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{3,63914554042}}]},
 {subject,<<"CN=Couchbase Server Node (172.19.0.4)">>},
 {not_after,63985747642},
 {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
 {load_timestamp,63914554042},
 {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQ"...>>},
 {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDOjCCAiKgAwIBAgIIGD/Hx/dYFPowDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjJaFw0y\nNzA4MTcxODQ3MjJaMC0xKzApBgNVBAMTIkNvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nMTcyLjE5LjAuNCkwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQD6Fik5\nZTKKG0O216KNVrKyjS2SrdB4IermP3HNcmWnhkrwz0Ppm9expYhmviu+eAmFSeOK\n3LRZtjU"...>>},
 {pkey_passphrase_settings,[]},
 {certs_epoch,0},
 {type,generated},
 {hostname,"172.19.0.4"}]
[ns_server:debug,2025-05-15T18:47:23.166Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',node_encryption} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|false]
[ns_server:debug,2025-05-15T18:47:23.166Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',ns_log} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
 {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]
[ns_server:debug,2025-05-15T18:47:23.166Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',port_servers} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}]
[ns_server:debug,2025-05-15T18:47:23.166Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',projector_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9999]
[ns_server:debug,2025-05-15T18:47:23.166Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',projector_ssl_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9999]
[ns_server:debug,2025-05-15T18:47:23.166Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',prometheus_auth_info} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{3,63914554042}}]}|
 {"@prometheus",
  {auth,
   [{<<"hash">>,
     {[{<<"hashes">>,
        {sanitized,<<"ocCjmLXobw413zJaq2v5hbk6F9JaoNKMHAEhwOXAF/k=">>}},
       {<<"algorithm">>,<<"argon2id">>},
       {<<"salt">>,<<"FlBs6+3flsH1DlD+dLEwjg==">>},
       {<<"parallelism">>,1},
       {<<"time">>,3},
       {<<"memory">>,524288}]}},
    {<<"scram-sha-512">>,
     {[{<<"salt">>,
        <<"F/Zqupd2IseMgxcuAf9fe4gpX0DBCFm1/fWg3iuaAuQdnTMBGYBMeqXKzW13cDp+Q9KGShsd3l23qMAKTVwiVA==">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"fNgWati8g2faSgrPjZUuj7XSR1xhSOtSh9nzY5vi8O4=">>}}]}},
    {<<"scram-sha-256">>,
     {[{<<"salt">>,<<"OKVHe2p1Uu/nuWHnr4hDmY1YmTEi+LTh2QSQmLrl6JU=">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"wJZdBxOuuYIylriwbbRV9JdvCNKKT1kZtZR+FOCJUQg=">>}}]}},
    {<<"scram-sha-1">>,
     {[{<<"salt">>,<<"FmlE6xOIeEZ8k+MADCxX6iijEW8=">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"gfRT+N7lHhRndbdyfaO+d/dOfRbaCQl3VgGgatuBZck=">>}}]}}]}}]
[ns_server:debug,2025-05-15T18:47:23.166Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',prometheus_http_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9123]
[ns_server:debug,2025-05-15T18:47:23.167Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',query_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|8093]
[ns_server:debug,2025-05-15T18:47:23.167Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',rest} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
 {port,8091},
 {port_meta,global}]
[ns_server:debug,2025-05-15T18:47:23.167Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',saslauthd_enabled} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|true]
[ns_server:debug,2025-05-15T18:47:23.167Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',ssl_capi_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|18092]
[ns_server:debug,2025-05-15T18:47:23.167Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',ssl_query_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|18093]
[ns_server:debug,2025-05-15T18:47:23.167Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',ssl_rest_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|18091]
[ns_server:debug,2025-05-15T18:47:23.167Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',uuid} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
 <<"28569ac00b9c1d7c50e39741027d428c">>]
[ns_server:debug,2025-05-15T18:47:23.167Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',xdcr_rest_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9998]
[ns_server:debug,2025-05-15T18:47:23.167Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',{project_intact,is_vulnerable}} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|false]
[cluster:debug,2025-05-15T18:47:23.167Z,ns_1@db2.lan:ns_cluster<0.273.0>:ns_cluster:perform_actual_join:1641]pre-join merged config is:
{config,
 {full,"/opt/couchbase/etc/couchbase/config",undefined,ns_config_default},
 [[],
  [{{node,'ns_1@db2.lan',config_version},{7,6}},
   {directory,"/opt/couchbase/var/lib/couchbase/config"},
   {{node,'ns_1@db2.lan',is_enterprise},true},
   {{node,'ns_1@db2.lan',saslauthd_enabled},true},
   {index_aware_rebalance_disabled,false},
   {max_bucket_count,30},
   {set_view_update_daemon,
    [{update_interval,5000},
     {update_min_changes,5000},
     {replica_update_min_changes,5000}]},
   {{node,'ns_1@db2.lan',compaction_daemon},
    [{check_interval,30},
     {min_db_file_size,131072},
     {min_view_file_size,20971520}]},
   {nodes_wanted,[]},
   {quorum_nodes,['ns_1@db2.lan']},
   {{couchdb,max_parallel_indexers},4},
   {{couchdb,max_parallel_replica_indexers},2},
   {{metakv,<<"/indexing/settings/config">>},
    <<"{\"indexer.settings.compaction.abort_exceed_interval\":false,\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.enable_page_bloom_filter\":false,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.log_level\":\"info\",\"indexer.settin"...>>},
   {{metakv,<<"/eventing/settings/config">>},<<"{\"ram_quota\":256}">>},
   {{metakv,<<"/query/settings/config">>},
    <<"{\"cleanupclientattempts\":true,\"cleanuplostattempts\":true,\"cleanupwindow\":\"60s\",\"completed-limit\":4000,\"completed-threshold\":1000,\"loglevel\":\"info\",\"max-parallelism\":1,\"memory-quota\":0,\"n1ql-feat-ctrl\":76,\"numatrs\":1024,\"pipeline-batch\":16,\"pipeline-cap\":512,\"prepared-limit\":16384,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"query.settin"...>>},
   {{metakv,<<"/analytics/settings/config">>},
    <<"{\"analytics.settings.num_replicas\":0}">>},
   {rest_creds,null},
   {client_cert_auth,[{state,"disable"},{prefixes,[]}]},
   {scramsha_fallback_salt,<<55,187,62,55,248,32,6,50,18,123,218,116>>},
   {remote_clusters,[]},
   {{node,'ns_1@db2.lan',isasl},
    [{path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
   {audit,
    [{auditd_enabled,false},
     {rotate_interval,86400},
     {rotate_size,20971520},
     {prune_age,0},
     {disabled,[]},
     {enabled,[]},
     {disabled_users,[]},
     {sync,[]},
     {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
   {{node,'ns_1@db2.lan',audit},[]},
   {memcached,[]},
   {{node,'ns_1@db2.lan',memcached_defaults},
    [{max_connections,65000},
     {system_connections,5000},
     {connection_idle_time,0},
     {verbosity,0},
     {privilege_debug,false},
     {breakpad_enabled,true},
     {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
     {dedupe_nmvb_maps,false},
     {je_malloc_conf,undefined},
     {tracing_enabled,true},
     {datatype_snappy,true},
     {num_reader_threads,<<"default">>},
     {num_writer_threads,<<"default">>},
     {num_auxio_threads,<<"default">>},
     {num_nonio_threads,<<"default">>},
     {num_storage_threads,<<"default">>},
     {tcp_keepalive_idle,360},
     {tcp_keepalive_interval,10},
     {tcp_keepalive_probes,3},
     {tcp_user_timeout,30},
     {always_collect_trace_info,true},
     {connection_limit_mode,<<"disconnect">>},
     {free_connection_pool_size,0},
     {max_client_connection_details,0}]},
   {{node,'ns_1@db2.lan',memcached},
    [{port,11210},
     {dedicated_port,11209},
     {dedicated_ssl_port,11206},
     {ssl_port,11207},
     {admin_user,"@ns_server"},
     {other_users,
      ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
       "@cbas","@backup"]},
     {admin_pass,"*****"},
     {engines,
      [{membase,
        [{engine,"/opt/couchbase/lib/memcached/ep.so"},
         {static_config_string,"failpartialwarmup=false"}]},
       {memcached,
        [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
         {static_config_string,"vb0=true"}]}]},
     {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
     {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
     {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
     {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
     {log_prefix,"memcached.log"},
     {log_generations,20},
     {log_cyclesize,10485760},
     {log_rotation_period,39003}]},
   {{node,'ns_1@db2.lan',memcached_config},
    {[{interfaces,{memcached_config_mgr,get_interfaces,[]}},
      {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
      {connection_idle_time,connection_idle_time},
      {privilege_debug,privilege_debug},
      {breakpad,
       {[{enabled,breakpad_enabled},
         {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
      {deployment_model,{memcached_config_mgr,get_config_profile,[]}},
      {verbosity,verbosity},
      {audit_file,{"~s",[audit_file]}},
      {rbac_file,{"~s",[rbac_file]}},
      {dedupe_nmvb_maps,dedupe_nmvb_maps},
      {tracing_enabled,tracing_enabled},
      {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
      {xattr_enabled,true},
      {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
      {collections_enabled,true},
      {max_connections,max_connections},
      {system_connections,system_connections},
      {num_reader_threads,num_reader_threads},
      {num_writer_threads,num_writer_threads},
      {num_auxio_threads,num_auxio_threads},
      {num_nonio_threads,num_nonio_threads},
      {num_storage_threads,num_storage_threads},
      {logger,
       {[{filename,{"~s/~s",[log_path,log_prefix]}},
         {cyclesize,log_cyclesize}]}},
      {external_auth_service,
       {memcached_config_mgr,get_external_auth_service,[]}},
      {active_external_users_push_interval,
       {memcached_config_mgr,get_external_users_push_interval,[]}},
      {prometheus,{memcached_config_mgr,prometheus_cfg,[]}},
      {sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
      {ssl_sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
      {tcp_keepalive_idle,tcp_keepalive_idle},
      {tcp_keepalive_interval,tcp_keepalive_interval},
      {tcp_keepalive_probes,tcp_keepalive_probes},
      {tcp_user_timeout,tcp_user_timeout},
      {always_collect_trace_info,always_collect_trace_info},
      {connection_limit_mode,connection_limit_mode},
      {free_connection_pool_size,free_connection_pool_size},
      {max_client_connection_details,max_client_connection_details}]}},
   {memory_quota,3406},
   {fts_memory_quota,512},
   {cbas_memory_quota,1549},
   {buckets,[{configs,[]}]},
   {secure_headers,[]},
   {{node,'ns_1@db2.lan',port_servers},[]},
   {{node,'ns_1@db2.lan',ns_log},
    [{filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
   {{node,'ns_1@db2.lan',event_log},
    [{filename,"/opt/couchbase/var/lib/couchbase/event_log"}]},
   {email_alerts,
    [{recipients,["root@localhost"]},
     {sender,"couchbase@localhost"},
     {enabled,false},
     {email_server,
      [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
     {alerts,
      [auto_failover_node,auto_failover_maximum_reached,
       auto_failover_other_nodes_down,auto_failover_cluster_too_small,
       auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
       ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
       indexer_low_resident_percentage,ep_clock_cas_drift_threshold_exceeded,
       communication_issue,time_out_of_sync,disk_usage_analyzer_stuck,
       cert_expires_soon,cert_expired,memory_threshold,history_size_warning,
       stuck_rebalance,memcached_connections]},
     {pop_up_alerts,
      [auto_failover_node,auto_failover_maximum_reached,
       auto_failover_other_nodes_down,auto_failover_cluster_too_small,
       auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
       ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
       indexer_low_resident_percentage,ep_clock_cas_drift_threshold_exceeded,
       communication_issue,time_out_of_sync,disk_usage_analyzer_stuck,
       cert_expires_soon,cert_expired,memory_threshold,history_size_warning,
       stuck_rebalance,memcached_connections]}]},
   {alert_limits,
    [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
   {replication,[{enabled,true}]},
   {log_redaction_default_cfg,[{redact_level,none}]},
   {service_orchestrator_weight,
    [{kv,10000},
     {index,1000},
     {fts,1000},
     {cbas,1000},
     {n1ql,100},
     {eventing,100},
     {backup,10}]},
   {health_monitor_refresh_interval,[]},
   {password_policy,[{min_length,6},{must_present,[]}]},
   {rest,[{port,8091}]},
   {{node,'ns_1@db2.lan',rest},[{port,8091},{port_meta,global}]},
   {{node,'ns_1@db2.lan',ssl_rest_port},18091},
   {{node,'ns_1@db2.lan',xdcr_rest_port},9998},
   {{node,'ns_1@db2.lan',memcached_dedicated_ssl_port},11206},
   {{node,'ns_1@db2.lan',memcached_prometheus},11280},
   {{node,'ns_1@db2.lan',projector_port},9999},
   {{node,'ns_1@db2.lan',projector_ssl_port},9999},
   {{node,'ns_1@db2.lan',query_port},8093},
   {{node,'ns_1@db2.lan',ssl_query_port},18093},
   {{node,'ns_1@db2.lan',indexer_admin_port},9100},
   {{node,'ns_1@db2.lan',indexer_scan_port},9101},
   {{node,'ns_1@db2.lan',indexer_http_port},9102},
   {{node,'ns_1@db2.lan',indexer_stinit_port},9103},
   {{node,'ns_1@db2.lan',indexer_stcatchup_port},9104},
   {{node,'ns_1@db2.lan',indexer_stmaint_port},9105},
   {{node,'ns_1@db2.lan',indexer_https_port},19102},
   {{node,'ns_1@db2.lan',fts_http_port},8094},
   {{node,'ns_1@db2.lan',fts_ssl_port},18094},
   {{node,'ns_1@db2.lan',fts_grpc_port},9130},
   {{node,'ns_1@db2.lan',fts_grpc_ssl_port},19130},
   {{node,'ns_1@db2.lan',eventing_http_port},8096},
   {{node,'ns_1@db2.lan',eventing_debug_port},9140},
   {{node,'ns_1@db2.lan',eventing_https_port},18096},
   {{node,'ns_1@db2.lan',cbas_http_port},8095},
   {{node,'ns_1@db2.lan',cbas_ssl_port},18095},
   {{node,'ns_1@db2.lan',cbas_admin_port},9110},
   {{node,'ns_1@db2.lan',cbas_cc_http_port},9111},
   {{node,'ns_1@db2.lan',cbas_cc_cluster_port},9112},
   {{node,'ns_1@db2.lan',cbas_cc_client_port},9113},
   {{node,'ns_1@db2.lan',cbas_console_port},9114},
   {{node,'ns_1@db2.lan',cbas_cluster_port},9115},
   {{node,'ns_1@db2.lan',cbas_data_port},9116},
   {{node,'ns_1@db2.lan',cbas_result_port},9117},
   {{node,'ns_1@db2.lan',cbas_messaging_port},9118},
   {{node,'ns_1@db2.lan',cbas_metadata_callback_port},9119},
   {{node,'ns_1@db2.lan',cbas_replication_port},9120},
   {{node,'ns_1@db2.lan',cbas_metadata_port},9121},
   {{node,'ns_1@db2.lan',cbas_parent_port},9122},
   {{node,'ns_1@db2.lan',cbas_debug_port},-1},
   {{node,'ns_1@db2.lan',prometheus_http_port},9123},
   {{node,'ns_1@db2.lan',backup_http_port},8097},
   {{node,'ns_1@db2.lan',backup_https_port},18097},
   {{node,'ns_1@db2.lan',backup_grpc_port},9124},
   {{node,'ns_1@db2.lan',capi_port},8092},
   {{node,'ns_1@db2.lan',ssl_capi_port},18092},
   {{node,'ns_1@db2.lan',{project_intact,is_vulnerable}},false},
   {retry_rebalance,
    [{enabled,false},{after_time_period,300},{max_attempts,1}]},
   {auto_failover_cfg,
    [{enabled,true},
     {timeout,120},
     {count,0},
     {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
     {failover_server_group,false},
     {max_count,1},
     {failed_over_server_groups,[]},
     {can_abort_rebalance,true}]},
   {{node,'ns_1@db2.lan',database_dir},
    "/opt/couchbase/var/lib/couchbase/data"},
   {{node,'ns_1@db2.lan',index_dir},"/opt/couchbase/var/lib/couchbase/data"},
   {resource_management,
    [{bucket,
      [{resident_ratio,
        [{enabled,false},{couchstore_minimum,1},{magma_minimum,0.2}]},
       {data_size,
        [{enabled,false},{couchstore_maximum,2},{magma_maximum,16}]}]},
     {index,[]},
     {cores_per_bucket,[{enabled,false},{minimum,0.4}]},
     {disk_usage,[{enabled,false},{maximum,96}]},
     {collections_per_quota,[{enabled,false},{maximum,1}]}]}]],
 [[{alert_limits,
    [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
   {audit,
    [{auditd_enabled,false},
     {rotate_interval,86400},
     {rotate_size,20971520},
     {prune_age,0},
     {disabled,[]},
     {enabled,[]},
     {disabled_users,[]},
     {sync,[]},
     {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
   {audit_decriptors,
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554009}}]},
     {8243,
      [{name,<<"mutate document">>},
       {description,<<"Document was mutated via the REST API">>},
       {enabled,true},
       {module,ns_server}]},
     {8255,
      [{name,<<"read document">>},
       {description,<<"Document was read via the REST API">>},
       {enabled,false},
       {module,ns_server}]},
     {8257,
      [{name,<<"alert email sent">>},
       {description,<<"An alert email was successfully sent">>},
       {enabled,true},
       {module,ns_server}]},
     {8265,
      [{name,<<"RBAC information retrieved">>},
       {description,<<"RBAC information was retrieved">>},
       {enabled,true},
       {module,ns_server}]},
     {16384,
      [{name,<<"remote cluster ref creation">>},
       {description,<<"created remote cluster ref">>},
       {enabled,true},
       {module,xdcr}]},
     {16385,
      [{name,<<"remote cluster ref update">>},
       {description,<<"updated remote cluster ref">>},
       {enabled,true},
       {module,xdcr}]},
     {16386,
      [{name,<<"remote cluster ref deletion">>},
       {description,<<"deleted remote cluster ref">>},
       {enabled,true},
       {module,xdcr}]},
     {16387,
      [{name,<<"replication creation">>},
       {description,<<"created replication">>},
       {enabled,true},
       {module,xdcr}]},
     {16388,
      [{name,<<"replication pause">>},
       {description,<<"paused replication">>},
       {enabled,true},
       {module,xdcr}]},
     {16389,
      [{name,<<"replication resume">>},
       {description,<<"resumed replication">>},
       {enabled,true},
       {module,xdcr}]},
     {16390,
      [{name,<<"replication cancellation">>},
       {description,<<"canceled replication">>},
       {enabled,true},
       {module,xdcr}]},
     {16391,
      [{name,<<"default replication settings update">>},
       {description,<<"updated default replication settings">>},
       {enabled,true},
       {module,xdcr}]},
     {16392,
      [{name,<<"individual replication settings update">>},
       {description,<<"updated individual replication settings">>},
       {enabled,true},
       {module,xdcr}]},
     {16393,
      [{name,<<"bucket settings update">>},
       {description,<<"updated bucket settings">>},
       {enabled,true},
       {module,xdcr}]},
     {16394,
      [{name,<<"authorization failure while adding remote cluster ref">>},
       {description,
        <<"failed to add remote cluster ref because of authorization failure">>},
       {enabled,true},
       {module,xdcr}]},
     {16395,
      [{name,<<"authorization failure while updating remote cluster ref">>},
       {description,
        <<"failed to update remote cluster ref because of authorization failure">>},
       {enabled,true},
       {module,xdcr}]},
     {16396,
      [{name,<<"access denied">>},
       {description,<<"access denied">>},
       {enabled,true},
       {module,xdcr}]},
     {20480,
      [{name,<<"opened DCP connection">>},
       {description,<<"opened DCP connection">>},
       {enabled,true},
       {module,memcached}]},
     {20482,
      [{name,<<"external memcached bucket flush">>},
       {description,
        <<"External user flushed the content of a memcached bucket">>},
       {enabled,true},
       {module,memcached}]},
     {20483,
      [{name,<<"invalid packet">>},
       {description,<<"Rejected an invalid packet">>},
       {enabled,true},
       {module,memcached}]},
     {20485,
      [{name,<<"authentication succeeded">>},
       {description,<<"Authentication to the cluster succeeded">>},
       {enabled,false},
       {module,memcached}]},
     {20488,
      [{name,<<"document read">>},
       {description,<<"Document was read">>},
       {enabled,false},
       {module,memcached}]},
     {20489,
      [{name,<<"document locked">>},
       {description,<<"Document was locked">>},
       {enabled,false},
       {module,memcached}]},
     {20490,
      [{name,<<"document modify">>},
       {description,<<"Document was modified">>},
       {enabled,false},
       {module,memcached}]},
     {20491,
      [{name,<<"document delete">>},
       {description,<<"Document was deleted">>},
       {enabled,false},
       {module,memcached}]},
     {20492,
      [{name,<<"select bucket">>},
       {description,<<"The specified bucket was selected">>},
       {enabled,true},
       {module,memcached}]},
     {20493,
      [{name,<<"session terminated">>},
       {description,<<"Session to the cluster has terminated">>},
       {enabled,false},
       {module,memcached}]},
     {20494,
      [{name,<<"tenant rate limited">>},
       {description,<<"The given tenant was rate limited">>},
       {enabled,true},
       {module,memcached}]},
     {24576,
      [{name,<<"Delete index">>},
       {description,<<"FTS index was deleted">>},
       {enabled,true},
       {module,fts}]},
     {24577,
      [{name,<<"Create/Update index">>},
       {description,<<"FTS index was created/Updated">>},
       {enabled,true},
       {module,fts}]},
     {24579,
      [{name,<<"Control index">>},
       {description,<<"FTS index control command was issued">>},
       {enabled,true},
       {module,fts}]},
     {24582,
      [{name,<<"GC run">>},
       {description,<<"GC run was triggered">>},
       {enabled,true},
       {module,fts}]},
     {24583,
      [{name,<<"CPU profile">>},
       {description,<<"CPU profiling was started">>},
       {enabled,true},
       {module,fts}]},
     {24584,
      [{name,<<"Memory profile">>},
       {description,<<"Memory profiling was started">>},
       {enabled,true},
       {module,fts}]},
     {28672,
      [{name,<<"SELECT statement">>},
       {description,<<"A N1QL SELECT statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28673,
      [{name,<<"EXPLAIN statement">>},
       {description,<<"A N1QL EXPLAIN statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28674,
      [{name,<<"PREPARE statement">>},
       {description,<<"A N1QL PREPARE statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28675,
      [{name,<<"INFER statement">>},
       {description,<<"A N1QL INFER statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28676,
      [{name,<<"INSERT statement">>},
       {description,<<"A N1QL INSERT statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28677,
      [{name,<<"UPSERT statement">>},
       {description,<<"A N1QL UPSERT statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28678,
      [{name,<<"DELETE statement">>},
       {description,<<"A N1QL DELETE statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28679,
      [{name,<<"UPDATE statement">>},
       {description,<<"A N1QL UPDATE statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28680,
      [{name,<<"MERGE statement">>},
       {description,<<"A N1QL MERGE statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28681,
      [{name,<<"CREATE INDEX statement">>},
       {description,<<"A N1QL CREATE INDEX statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28682,
      [{name,<<"DROP INDEX statement">>},
       {description,<<"A N1QL DROP INDEX statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28683,
      [{name,<<"ALTER INDEX statement">>},
       {description,<<"A N1QL ALTER INDEX statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28684,
      [{name,<<"BUILD INDEX statement">>},
       {description,<<"A N1QL BUILD INDEX statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28685,
      [{name,<<"GRANT ROLE statement">>},
       {description,<<"A N1QL GRANT ROLE statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28686,
      [{name,<<"REVOKE ROLE statement">>},
       {description,<<"A N1QL REVOKE ROLE statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28687,
      [{name,<<"UNRECOGNIZED statement">>},
       {description,
        <<"An unrecognized statement was received by the N1QL query engine">>},
       {enabled,false},
       {module,n1ql}]},
     {28688,
      [{name,<<"CREATE PRIMARY INDEX statement">>},
       {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28689,
      [{name,<<"/admin/stats API request">>},
       {description,
        <<"An HTTP request was made to the API at /admin/stats.">>},
       {enabled,false},
       {module,n1ql}]},
     {28690,
      [{name,<<"/admin/vitals API request">>},
       {description,
        <<"An HTTP request was made to the API at /admin/vitals.">>},
       {enabled,false},
       {module,n1ql}]},
     {28691,
      [{name,<<"/admin/prepareds API request">>},
       {description,
        <<"An HTTP request was made to the API at /admin/prepareds.">>},
       {enabled,false},
       {module,n1ql}]},
     {28692,
      [{name,<<"/admin/active_requests API request">>},
       {description,
        <<"An HTTP request was made to the API at /admin/active_requests.">>},
       {enabled,false},
       {module,n1ql}]},
     {28693,
      [{name,<<"/admin/indexes/prepareds API request">>},
       {description,
        <<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
       {enabled,false},
       {module,n1ql}]},
     {28694,
      [{name,<<"/admin/indexes/active_requests API request">>},
       {description,
        <<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
       {enabled,false},
       {module,n1ql}]},
     {28695,
      [{name,<<"/admin/indexes/completed_requests API request">>},
       {description,
        <<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
       {enabled,false},
       {module,n1ql}]},
     {28697,
      [{name,<<"/admin/ping API request">>},
       {description,<<"An HTTP request was made to the API at /admin/ping.">>},
       {enabled,false},
       {module,n1ql}]},
     {28698,
      [{name,<<"/admin/config API request">>},
       {description,
        <<"An HTTP request was made to the API at /admin/config.">>},
       {enabled,false},
       {module,n1ql}]},
     {28699,
      [{name,<<"/admin/ssl_cert API request">>},
       {description,
        <<"An HTTP request was made to the API at /admin/ssl_cert.">>},
       {enabled,false},
       {module,n1ql}]},
     {28700,
      [{name,<<"/admin/settings API request">>},
       {description,
        <<"An HTTP request was made to the API at /admin/settings.">>},
       {enabled,false},
       {module,n1ql}]},
     {28701,
      [{name,<<"/admin/clusters API request">>},
       {description,
        <<"An HTTP request was made to the API at /admin/clusters.">>},
       {enabled,false},
       {module,n1ql}]},
     {28702,
      [{name,<<"/admin/completed_requests API request">>},
       {description,
        <<"An HTTP request was made to the API at /admin/completed_requests.">>},
       {enabled,false},
       {module,n1ql}]},
     {28704,
      [{name,<<"/admin/functions API request">>},
       {description,
        <<"An HTTP request was made to the API at /admin/functions.">>},
       {enabled,false},
       {module,n1ql}]},
     {28705,
      [{name,<<"/admin/indexes/functions API request">>},
       {description,
        <<"An HTTP request was made to the API at /admin/indexes/functions.">>},
       {enabled,false},
       {module,n1ql}]},
     {28706,
      [{name,<<"CREATE FUNCTION statement">>},
       {description,<<"A N1QL CREATE FUNCTION statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28707,
      [{name,<<"DROP FUNCTION statement">>},
       {description,<<"A N1QL DROP FUNCTION statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28708,
      [{name,<<"EXECUTE FUNCTION statement">>},
       {description,<<"A N1QL EXECUTE FUNCTION statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28709,
      [{name,<<"/admin/tasks API request">>},
       {description,
        <<"An HTTP request was made to the API at /admin/tasks.">>},
       {enabled,false},
       {module,n1ql}]},
     {28710,
      [{name,<<"/admin/indexes/tasks API request">>},
       {description,
        <<"An HTTP request was made to the API at /admin/indexes/tasks.">>},
       {enabled,false},
       {module,n1ql}]},
     {28711,
      [{name,<<"/admin/dictionary_cache API request">>},
       {description,
        <<"An HTTP request was made to the API at /admin/dictionary_cache.">>},
       {enabled,false},
       {module,n1ql}]},
     {28712,
      [{name,<<"/admin/indexes/dictionary_cache API request">>},
       {description,
        <<"An HTTP request was made to the API at /admin/indexes/dictionary_cache.">>},
       {enabled,false},
       {module,n1ql}]},
     {28713,
      [{name,<<"CREATE SCOPE statement">>},
       {description,<<"A N1QL CREATE SCOPE statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28714,
      [{name,<<"DROP SCOPE statement">>},
       {description,<<"A N1QL DROP SCOPE statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28715,
      [{name,<<"CREATE COLLECTION statement">>},
       {description,<<"A N1QL CREATE COLLECTION statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28716,
      [{name,<<"DROP COLLECTION statement">>},
       {description,<<"A N1QL DROP COLLECTION statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28717,
      [{name,<<"FLUSH COLLECTION statement">>},
       {description,<<"A N1QL FLUSH COLLECTION statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28718,
      [{name,<<"UPDATE STATISTICS statement">>},
       {description,<<"A N1QL UPDATE STATISTICS statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28719,
      [{name,<<"ADVISE statement">>},
       {description,<<"A N1QL ADVISE statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28720,
      [{name,<<"START TRANSACTION statement">>},
       {description,<<"A N1QL START TRANSACTION statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28721,
      [{name,<<"COMMIT TRANSACTION statement">>},
       {description,<<"A N1QL COMMIT TRANSACTION statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28722,
      [{name,<<"ROLLBACK TRANSACTION statement">>},
       {description,<<"A N1QL ROLLBACK TRANSACTION statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28723,
      [{name,<<"ROLLBACK TRANSACTION TO SAVEPOINT statement">>},
       {description,
        <<"A N1QL ROLLBACK TRANSACTION TO SAVEPOINT statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28724,
      [{name,<<"SET TRANSACTION ISOLATION statement">>},
       {description,
        <<"A N1QL SET TRANSACTION ISOLATION statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28725,
      [{name,<<"SAVEPOINT statement">>},
       {description,<<"A N1QL SAVEPOINT statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28726,
      [{name,<<"/admin/transactions API request">>},
       {description,
        <<"An HTTP request was made to the API at /admin/transactions.">>},
       {enabled,false},
       {module,n1ql}]},
     {28727,
      [{name,<<"/admin/indexes/transactions API request">>},
       {description,
        <<"An HTTP request was made to the API at /admin/indexes/transactions.">>},
       {enabled,false},
       {module,n1ql}]},
     {28728,
      [{name,<<"N1QL backup / restore API request">>},
       {description,
        <<"An HTTP request was made to archive or restore N1QL metadata">>},
       {enabled,false},
       {module,n1ql}]},
     {28729,
      [{name,<<"/admin/shutdown API request">>},
       {description,
        <<"An HTTP request was made to initiate graceful shutdown">>},
       {enabled,false},
       {module,n1ql}]},
     {28730,
      [{name,<<"/admin/gc API request">>},
       {description,<<"An HTTP request was made to run garbage collection">>},
       {enabled,false},
       {module,n1ql}]},
     {28731,
      [{name,<<"/admin/ffdc API request">>},
       {description,<<"An HTTP request was made to run an FFDC collection">>},
       {enabled,false},
       {module,n1ql}]},
     {28732,
      [{name,<<"/admin/log/ API request">>},
       {description,<<"An HTTP request was made to access diagnostic logs">>},
       {enabled,false},
       {module,n1ql}]},
     {28733,
      [{name,<<"/admin/sequences_cache API request">>},
       {description,<<"An HTTP request was made to access sequences">>},
       {enabled,false},
       {module,n1ql}]},
     {28734,
      [{name,<<"CREATE SEQUENCE statement">>},
       {description,<<"A N1QL CREATE SEQUENCE statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28735,
      [{name,<<"ALTER SEQUENCE statement">>},
       {description,<<"A N1QL ALTER SEQUENCE statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28736,
      [{name,<<"DROP SEQUENCE statement">>},
       {description,<<"A N1QL DROP SEQUENCE statement was executed">>},
       {enabled,false},
       {module,n1ql}]},
     {28737,
      [{name,<<"Migration abort">>},
       {description,<<"Migration was aborted">>},
       {enabled,false},
       {module,n1ql}]},
     {32768,
      [{name,<<"Create Function">>},
       {description,
        <<"Request to create or update eventing function definition">>},
       {enabled,true},
       {module,eventing}]},
     {32769,
      [{name,<<"Delete Function">>},
       {description,<<"Request to delete eventing function definition">>},
       {enabled,true},
       {module,eventing}]},
     {32770,
      [{name,<<"Fetch Functions">>},
       {description,<<"Request to fetch eventing function definition">>},
       {enabled,false},
       {module,eventing}]},
     {32771,
      [{name,<<"List Deployed">>},
       {description,<<"Request to fetch eventing deployed functions list">>},
       {enabled,false},
       {module,eventing}]},
     {32772,
      [{name,<<"Fetch Drafts">>},
       {description,
        <<"Request to fetch eventing function draft definitions">>},
       {enabled,false},
       {module,eventing}]},
     {32773,
      [{name,<<"Delete Drafts">>},
       {description,
        <<"Request to delete eventing function draft definitions">>},
       {enabled,true},
       {module,eventing}]},
     {32774,
      [{name,<<"Save Draft">>},
       {description,<<"Request to save a draft definition">>},
       {enabled,true},
       {module,eventing}]},
     {32775,
      [{name,<<"Start Debug">>},
       {description,<<"Request to start eventing function debugger">>},
       {enabled,true},
       {module,eventing}]},
     {32776,
      [{name,<<"Stop Debug">>},
       {description,<<"Request to stop eventing function debugger">>},
       {enabled,true},
       {module,eventing}]},
     {32777,
      [{name,<<"Start Tracing">>},
       {description,<<"Request to start tracing eventing function e"...>>},
       {enabled,true},
       {module,eventing}]},
     {32778,
      [{name,<<"Stop Tracing">>},
       {description,<<"Request to stop tracing eventing functio"...>>},
       {enabled,true},
       {module,eventing}]},
     {32779,
      [{name,<<"Set Settings">>},
       {description,<<"Request to save settings for an even"...>>},
       {enabled,true},
       {module,eventing}]},
     {32780,
      [{name,<<"Fetch Config">>},
       {description,<<"Request to fetch eventing config">>},
       {enabled,false},
       {module,eventing}]},
     {32781,
      [{name,<<"Save Config">>},
       {description,<<"Request to save eventing con"...>>},
       {enabled,true},
       {module,eventing}]},
     {32783,
      [{name,<<"Get Settings">>},
       {description,<<"Request to fetch eventin"...>>},
       {enabled,false},
       {module,eventing}]},
     {32784,
      [{name,<<"Import Functions">>},
       {description,<<"Request to import on"...>>},
       {enabled,true},
       {module,eventing}]},
     {32785,
      [{name,<<"Export Functions">>},
       {description,<<"Request to expor"...>>},
       {enabled,false},
       {module,eventing}]},
     {32786,
      [{name,<<"List Running">>},
       {description,<<"Request to f"...>>},
       {enabled,false},
       {module,eventing}]},
     {32789,
      [{name,<<"Deploy Funct"...>>},
       {description,<<"Request "...>>},
       {enabled,true},
       {module,eventing}]},
     {32790,
      [{name,<<"Undeploy"...>>},
       {description,<<"Requ"...>>},
       {enabled,true},
       {module,...}]},
     {32791,[{name,<<"Paus"...>>},{description,<<...>>},{enabled,...},{...}]},
     {32792,[{name,<<...>>},{description,...},{...}|...]},
     {32793,[{name,...},{...}|...]},
     {32794,[{...}|...]},
     {32795,[...]},
     {32796,...},
     {...}|...]},
   {auto_failover_cfg,
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{3,63914554009}}]},
     {enabled,true},
     {timeout,120},
     {count,0},
     {max_count,1},
     {disable_max_count,false},
     {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
     {failover_server_group,false},
     {failed_over_server_groups,[]},
     {can_abort_rebalance,true},
     {failover_preserve_durability_majority,false}]},
   {buckets,[{configs,[]}]},
   {cbas_memory_quota,
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554041}}]}|
     1024]},
   {client_cert_auth,[{state,"disable"},{prefixes,[]}]},
   {cluster_compat_mode,undefined},
   {cluster_compat_version,
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{3,63914554009}}]},
     7,6]},
   {email_alerts,
    [{recipients,["root@localhost"]},
     {sender,"couchbase@localhost"},
     {enabled,false},
     {email_server,
      [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
     {alerts,
      [auto_failover_node,auto_failover_maximum_reached,
       auto_failover_other_nodes_down,auto_failover_cluster_too_small,
       auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
       ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
       indexer_low_resident_percentage,ep_clock_cas_drift_threshold_exceeded,
       communication_issue,time_out_of_sync,disk_usage_analyzer_stuck,
       cert_expires_soon,cert_expired,memory_threshold,history_size_warning,
       stuck_rebalance,memcached_connections]},
     {pop_up_alerts,
      [auto_failover_node,auto_failover_maximum_reached,
       auto_failover_other_nodes_down,auto_failover_cluster_too_small,
       auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
       ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
       indexer_low_resident_percentage,ep_clock_cas_drift_threshold_exceeded,
       communication_issue,time_out_of_sync,disk_usage_analyzer_stuck,
       cert_expires_soon,cert_expired,memory_threshold,history_size_warning,
       stuck_rebalance,memcached_connections]}]},
   {fts_memory_quota,512},
   {health_monitor_refresh_interval,[]},
   {index_aware_rebalance_disabled,false},
   {log_redaction_default_cfg,[{redact_level,none}]},
   {max_bucket_count,30},
   {memcached,[]},
   {memory_quota,
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554041}}]}|
     3072]},
   {nodes_wanted,[]},
   {otp,
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554009}}]},
     {cookie,{sanitized,<<"biSYTaQUFDVndTs/b+IcDmD2L0woktv9naRdv3GCK6A=">>}}]},
   {password_policy,[{min_length,6},{must_present,[]}]},
   {quorum_nodes,
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554042}}]},
     'ns_1@172.19.0.4']},
   {rbac_upgrade,
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554009}}]}|
     '_deleted']},
   {remote_clusters,[]},
   {replication,[{enabled,true}]},
   {resource_management,
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554009}}]},
     {bucket,
      [{resident_ratio,
        [{enabled,false},{couchstore_minimum,1},{magma_minimum,0.2}]},
       {data_size,
        [{enabled,false},{couchstore_maximum,2},{magma_maximum,16}]}]},
     {index,[]},
     {cores_per_bucket,[{enabled,false},{minimum,0.4}]},
     {disk_usage,[{enabled,false},{maximum,96}]},
     {collections_per_quota,[{enabled,false},{maximum,1}]}]},
   {rest,[{port,8091}]},
   {rest_creds,
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554041}}]}|
     {"<ud>jaba_admin</ud>",
      {auth,
       [{<<"hash">>,
         {[{<<"hashes">>,
            {sanitized,<<"3I14lVC6WTo2HkE1Ma2OkSoP1R5UwTP501YfMTKM7ak=">>}},
           {<<"algorithm">>,<<"argon2id">>},
           {<<"salt">>,<<"5CFoS4MqJh4VclPVzeJxBw==">>},
           {<<"parallelism">>,1},
           {<<"time">>,3},
           {<<"memory">>,524288}]}},
        {<<"scram-sha-512">>,
         {[{<<"salt">>,
            <<"pulx/Jw0I8WxLIxMcCD8CqAxiVNfHNaHWpBI6wjnArfbB+WyxXwBuk5SnOGA7W5LgTZZWGevGaY3ih1tGH4jeg==">>},
           {<<"iterations">>,15000},
           {<<"hashes">>,
            {sanitized,<<"+8aFWL6Fwfs8HKehUbj7ZkLi5LW9/xRhbkbg/BwGA9w=">>}}]}},
        {<<"scram-sha-256">>,
         {[{<<"salt">>,<<"6mOH6zwaMRxGOfTsFTCuTxfMVctxE7w9qyktmzrJtVg=">>},
           {<<"iterations">>,15000},
           {<<"hashes">>,
            {sanitized,<<"t9IO+WS5kijXIacgFn1gB5R0bN61IAKLCL/JWibrJSA=">>}}]}},
        {<<"scram-sha-1">>,
         {[{<<"salt">>,<<"pEmVjw1zOFSsjZc/8lZKrEstBkA=">>},
           {<<"iterations">>,15000},
           {<<"hashes">>,
            {sanitized,
             <<"tJt6Wglz80zHvML1mQcTdrXmQggip4HdkH+YfamhrEc=">>}}]}}]}}]},
   {retry_rebalance,
    [{enabled,false},{after_time_period,300},{max_attempts,1}]},
   {scramsha_fallback_salt,<<55,187,62,55,248,32,6,50,18,123,218,116>>},
   {secure_headers,[]},
   {service_orchestrator_weight,
    [{kv,10000},
     {index,1000},
     {fts,1000},
     {cbas,1000},
     {n1ql,100},
     {eventing,100},
     {backup,10}]},
   {set_view_update_daemon,
    [{update_interval,5000},
     {update_min_changes,5000},
     {replica_update_min_changes,5000}]},
   {settings,
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554041}}]},
     {stats,[{send_stats,true}]}]},
   {uuid,
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554041}}]}|
     <<"61933a8028482692b0278a91060e0d90">>]},
   {{couchdb,max_parallel_indexers},4},
   {{couchdb,max_parallel_replica_indexers},2},
   {{local_changes_count,<<"28569ac00b9c1d7c50e39741027d428c">>},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{21,63914554042}}]}]},
   {{local_changes_count,<<"7a33d46fd1976e65466c5efc8b45ef2e">>},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{6,63914554043}}]}]},
   {{metakv,<<"/analytics/settings/config">>},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554009}}]}|
     <<"{\"analytics.settings.blob_storage_bucket\":\"\",\"analytics.settings.blob_storage_prefix\":\"\",\"analytics.settings.blob_storage_region\":\"\",\"analytics.settings.blob_storage_scheme\":\"\",\"analytics.settings.num_replicas\":0}">>]},
   {{metakv,<<"/eventing/settings/config">>},<<"{\"ram_quota\":256}">>},
   {{metakv,<<"/indexing/settings/config">>},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554041}}]}|
     <<"{\"indexer.settings.compaction.abort_exceed_interval\":false,\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.enable_page_b"...>>]},
   {{metakv,<<"/query/functions_cache/counter">>},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554042}}]}|
     <<"[172.19.0.4:8091]0">>]},
   {{metakv,<<"/query/sequences_cache/revision">>},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554009}}]}|
     <<"0">>]},
   {{metakv,<<"/query/settings/config">>},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554009}}]}|
     <<"{\"cleanupclientattempts\":true,\"cleanuplostattempts\":true,\"cleanupwindow\":\"60s\",\"completed-limit\":4000,\"completed-max-plan-size\":262144,\"completed-threshold\":1000,\"loglevel\":\"info\",\"max-parallelism\":1,\"memory-quota\":0,\"n1ql-feat-ctrl\":76,\"node-quota\":0,\"node-quota-val-percent\":67,\"num-cpus\":0,\"numatrs\":1024,\"pipeline-batch\":16,\"pip"...>>]},
   {{node,'ns_1@172.19.0.4',address_family},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     inet]},
   {{node,'ns_1@172.19.0.4',audit},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}]},
   {{node,'ns_1@172.19.0.4',backup_grpc_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9124]},
   {{node,'ns_1@172.19.0.4',backup_http_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     8097]},
   {{node,'ns_1@172.19.0.4',backup_https_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     18097]},
   {{node,'ns_1@172.19.0.4',capi_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     8092]},
   {{node,'ns_1@172.19.0.4',cbas_admin_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9110]},
   {{node,'ns_1@172.19.0.4',cbas_cc_client_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9113]},
   {{node,'ns_1@172.19.0.4',cbas_cc_cluster_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9112]},
   {{node,'ns_1@172.19.0.4',cbas_cc_http_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9111]},
   {{node,'ns_1@172.19.0.4',cbas_cluster_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9115]},
   {{node,'ns_1@172.19.0.4',cbas_console_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9114]},
   {{node,'ns_1@172.19.0.4',cbas_data_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9116]},
   {{node,'ns_1@172.19.0.4',cbas_debug_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     -1]},
   {{node,'ns_1@172.19.0.4',cbas_dirs},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
     "/opt/couchbase/var/lib/couchbase/data"]},
   {{node,'ns_1@172.19.0.4',cbas_http_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     8095]},
   {{node,'ns_1@172.19.0.4',cbas_messaging_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9118]},
   {{node,'ns_1@172.19.0.4',cbas_metadata_callback_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9119]},
   {{node,'ns_1@172.19.0.4',cbas_metadata_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9121]},
   {{node,'ns_1@172.19.0.4',cbas_parent_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9122]},
   {{node,'ns_1@172.19.0.4',cbas_replication_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9120]},
   {{node,'ns_1@172.19.0.4',cbas_result_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9117]},
   {{node,'ns_1@172.19.0.4',cbas_ssl_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     18095]},
   {{node,'ns_1@172.19.0.4',client_cert},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
     {subject,<<"CN=Couchbase Internal Client (70858842)">>},
     {not_after,63985747606},
     {verified_with,
      <<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
     {load_timestamp,63914554006},
     {ca,
      <<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZX"...>>},
     {pem,
      <<"-----BEGIN CERTIFICATE-----\nMIIDWTCCAkGgAwIBAgIIGD/Hv7XHejcwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ2NDZaFw0y\nNzA4MTcxODQ2NDZaMC8xLTArBgNVBAMTJENvdWNoYmFzZSBJbn"...>>},
     {pkey_passphrase_settings,[]},
     {certs_epoch,0},
     {type,generated},
     {name,"@internal"}]},
   {{node,'ns_1@172.19.0.4',compaction_daemon},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
     {check_interval,30},
     {min_db_file_size,131072},
     {min_view_file_size,20971520}]},
   {{node,'ns_1@172.19.0.4',config_version},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     {7,6}]},
   {{node,'ns_1@172.19.0.4',database_dir},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
     47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,
     105,98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
   {{node,'ns_1@172.19.0.4',erl_external_listeners},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
     {inet,false}]},
   {{node,'ns_1@172.19.0.4',event_log},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
     {filename,"/opt/couchbase/var/lib/couchbase/event_log"}]},
   {{node,'ns_1@172.19.0.4',eventing_debug_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9140]},
   {{node,'ns_1@172.19.0.4',eventing_dir},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
     47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,
     105,98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
   {{node,'ns_1@172.19.0.4',eventing_http_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     8096]},
   {{node,'ns_1@172.19.0.4',eventing_https_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     18096]},
   {{node,'ns_1@172.19.0.4',fts_grpc_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9130]},
   {{node,'ns_1@172.19.0.4',fts_grpc_ssl_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     19130]},
   {{node,'ns_1@172.19.0.4',fts_http_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     8094]},
   {{node,'ns_1@172.19.0.4',fts_ssl_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     18094]},
   {{node,'ns_1@172.19.0.4',index_dir},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
     47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,
     105,98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
   {{node,'ns_1@172.19.0.4',indexer_admin_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9100]},
   {{node,'ns_1@172.19.0.4',indexer_http_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9102]},
   {{node,'ns_1@172.19.0.4',indexer_https_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     19102]},
   {{node,'ns_1@172.19.0.4',indexer_scan_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9101]},
   {{node,'ns_1@172.19.0.4',indexer_stcatchup_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9104]},
   {{node,'ns_1@172.19.0.4',indexer_stinit_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9103]},
   {{node,'ns_1@172.19.0.4',indexer_stmaint_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9105]},
   {{node,'ns_1@172.19.0.4',is_enterprise},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     true]},
   {{node,'ns_1@172.19.0.4',isasl},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
     {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
   {{node,'ns_1@172.19.0.4',memcached},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
     {port,11210},
     {dedicated_port,11209},
     {dedicated_ssl_port,11206},
     {ssl_port,11207},
     {admin_user,"@ns_server"},
     {other_users,
      ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
       "@cbas","@backup"]},
     {admin_pass,"*****"},
     {engines,
      [{membase,
        [{engine,"/opt/couchbase/lib/memcached/ep.so"},
         {static_config_string,"failpartialwarmup=false"}]},
       {memcached,
        [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
         {static_config_string,"vb0=true"}]}]},
     {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
     {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
     {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
     {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
     {log_prefix,"memcached.log"},
     {log_generations,20},
     {log_cyclesize,10485760},
     {log_rotation_period,39003}]},
   {{node,'ns_1@172.19.0.4',memcached_config},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     {[{interfaces,{memcached_config_mgr,get_interfaces,[]}},
       {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
       {connection_idle_time,connection_idle_time},
       {privilege_debug,privilege_debug},
       {breakpad,
        {[{enabled,breakpad_enabled},
          {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
       {deployment_model,{memcached_config_mgr,get_config_profile,[]}},
       {verbosity,verbosity},
       {audit_file,{"~s",[audit_file]}},
       {rbac_file,{"~s",[rbac_file]}},
       {dedupe_nmvb_maps,dedupe_nmvb_maps},
       {tracing_enabled,tracing_enabled},
       {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
       {xattr_enabled,true},
       {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
       {collections_enabled,true},
       {max_connections,max_connections},
       {system_connections,system_connections},
       {num_reader_threads,num_reader_threads},
       {num_writer_threads,num_writer_threads},
       {num_auxio_threads,num_auxio_threads},
       {num_nonio_threads,num_nonio_threads},
       {num_storage_threads,num_storage_threads},
       {logger,
        {[{filename,{"~s/~s",[log_path,log_prefix]}},
          {cyclesize,log_cyclesize}]}},
       {external_auth_service,
        {memcached_config_mgr,get_external_auth_service,[]}},
       {active_external_users_push_interval,
        {memcached_config_mgr,get_external_users_push_interval,[]}},
       {prometheus,{memcached_config_mgr,prometheus_cfg,[]}},
       {sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
       {ssl_sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
       {tcp_keepalive_idle,tcp_keepalive_idle},
       {tcp_keepalive_interval,tcp_keepalive_interval},
       {tcp_keepalive_probes,tcp_keepalive_probes},
       {tcp_user_timeout,...},
       {...}|...]}]},
   {{node,'ns_1@172.19.0.4',memcached_dedicated_ssl_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     11206]},
   {{node,'ns_1@172.19.0.4',memcached_defaults},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
     {max_connections,65000},
     {system_connections,5000},
     {connection_idle_time,0},
     {verbosity,0},
     {privilege_debug,false},
     {breakpad_enabled,true},
     {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
     {dedupe_nmvb_maps,false},
     {je_malloc_conf,undefined},
     {tracing_enabled,true},
     {datatype_snappy,true},
     {num_reader_threads,<<"default">>},
     {num_writer_threads,<<"default">>},
     {num_auxio_threads,<<"default">>},
     {num_nonio_threads,<<"default">>},
     {num_storage_threads,<<"default">>},
     {tcp_keepalive_idle,360},
     {tcp_keepalive_interval,10},
     {tcp_keepalive_probes,3},
     {tcp_user_timeout,30},
     {always_collect_trace_info,true},
     {connection_limit_mode,<<"disconnect">>},
     {free_connection_pool_size,0},
     {max_client_connection_details,0}]},
   {{node,'ns_1@172.19.0.4',memcached_prometheus},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     11280]},
   {{node,'ns_1@172.19.0.4',n2n_client_cert_auth},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     false]},
   {{node,'ns_1@172.19.0.4',node_cert},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{3,63914554042}}]},
     {subject,<<"CN=Couchbase Server Node (172.19.0.4)">>},
     {not_after,63985747642},
     {verified_with,
      <<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
     {load_timestamp,63914554042},
     {ca,
      <<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE"...>>},
     {pem,
      <<"-----BEGIN CERTIFICATE-----\nMIIDOjCCAiKgAwIBAgIIGD/Hx/dYFPowDQYJKoZIhvcNAQELBQAwJDEiMCAG"...>>},
     {pkey_passphrase_settings,[]},
     {certs_epoch,0},
     {type,generated},
     {hostname,"172.19.0.4"}]},
   {{node,'ns_1@172.19.0.4',node_encryption},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     false]},
   {{node,'ns_1@172.19.0.4',ns_log},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
     {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
   {{node,'ns_1@172.19.0.4',port_servers},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}]},
   {{node,'ns_1@172.19.0.4',projector_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9999]},
   {{node,'ns_1@172.19.0.4',projector_ssl_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9999]},
   {{node,'ns_1@172.19.0.4',prometheus_auth_info},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{3,63914554042}}]}|
     {"@prometheus",
      {auth,
       [{<<"hash">>,
         {[{<<"hashes">>,
            {sanitized,<<"ocCjmLXobw413zJaq2v5hbk6F9JaoNKMHAEhwOXA"...>>}},
           {<<"algorithm">>,<<"argon2id">>},
           {<<"salt">>,<<"FlBs6+3flsH1DlD+dLEwjg==">>},
           {<<"parallelism">>,1},
           {<<"time">>,3},
           {<<"memory">>,524288}]}},
        {<<"scram-sha-512">>,
         {[{<<"salt">>,<<"F/Zqupd2IseMgxcuAf9fe4gpX0DBCFm1/fWg3iuaAuQd"...>>},
           {<<"iterations">>,15000},
           {<<"hashes">>,{sanitized,<<"fNgWati8g2faSgrPjZUuj7XSR1xh"...>>}}]}},
        {<<"scram-sha-256">>,
         {[{<<"salt">>,<<"OKVHe2p1Uu/nuWHnr4hDmY1YmTEi+LTh2QSQmLrl"...>>},
           {<<"iterations">>,15000},
           {<<"hashes">>,{sanitized,<<"wJZdBxOuuYIylriwbbRV9Jdv"...>>}}]}},
        {<<"scram-sha-1">>,
         {[{<<"salt">>,<<"FmlE6xOIeEZ8k+MADCxX6iijEW8=">>},
           {<<"iterations">>,15000},
           {<<"hashes">>,{sanitized,<<"gfRT+N7lHhRndbdyfaO+"...>>}}]}}]}}]},
   {{node,'ns_1@172.19.0.4',prometheus_http_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9123]},
   {{node,'ns_1@172.19.0.4',query_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     8093]},
   {{node,'ns_1@172.19.0.4',rest},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
     {port,8091},
     {port_meta,global}]},
   {{node,'ns_1@172.19.0.4',saslauthd_enabled},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     true]},
   {{node,'ns_1@172.19.0.4',ssl_capi_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     18092]},
   {{node,'ns_1@172.19.0.4',ssl_query_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     18093]},
   {{node,'ns_1@172.19.0.4',ssl_rest_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     18091]},
   {{node,'ns_1@172.19.0.4',uuid},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     <<"28569ac00b9c1d7c50e39741027d428c">>]},
   {{node,'ns_1@172.19.0.4',xdcr_rest_port},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     9998]},
   {{node,'ns_1@172.19.0.4',{project_intact,is_vulnerable}},
    [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
     false]},
   {{node,'ns_1@db2.lan',address_family},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
     inet]},
   {{node,'ns_1@db2.lan',audit},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}]},
   {{node,'ns_1@db2.lan',backup_grpc_port},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45"...>>,{1,63914554043}}]}|
     9124]},
   {{node,'ns_1@db2.lan',backup_http_port},
    [{'_vclock',[{<<"7a33d46fd1976e65466c5efc"...>>,{1,63914554043}}]}|8097]},
   {{node,'ns_1@db2.lan',backup_https_port},
    [{'_vclock',[{<<"7a33d46fd1976e65466c"...>>,{1,63914554043}}]}|18097]},
   {{node,'ns_1@db2.lan',capi_port},
    [{'_vclock',[{<<"7a33d46fd1976e65"...>>,{1,63914554043}}]}|8092]},
   {{node,'ns_1@db2.lan',cbas_admin_port},
    [{'_vclock',[{<<"7a33d46fd197"...>>,{1,63914554043}}]}|9110]},
   {{node,'ns_1@db2.lan',cbas_cc_client_port},
    [{'_vclock',[{<<"7a33d46f"...>>,{1,...}}]}|9113]},
   {{node,'ns_1@db2.lan',cbas_cc_cluster_port},
    [{'_vclock',[{<<"7a33"...>>,{...}}]}|9112]},
   {{node,'ns_1@db2.lan',cbas_cc_http_port},
    [{'_vclock',[{<<...>>,...}]}|9111]},
   {{node,'ns_1@db2.lan',cbas_cluster_port},[{'_vclock',[{...}]}|9115]},
   {{node,'ns_1@db2.lan',cbas_console_port},[{'_vclock',[...]}|9114]},
   {{node,'ns_1@db2.lan',cbas_data_port},[{'_vclock',...}|9116]},
   {{node,'ns_1@db2.lan',...},[{...}|...]},
   {{node,...},[...]},
   {{...},...},
   {...}|...]],
 ns_config_default,
 {ns_config_default,encrypt_and_save,[]},
 <0.2351.0>,false,<<"7a33d46fd1976e65466c5efc8b45ef2e">>,
 #Fun<ns_config.18.54949477>}
[ns_server:info,2025-05-15T18:47:23.189Z,ns_1@db2.lan:ns_cluster<0.273.0>:ns_ssl_services_setup:update_cert_epoch:1334]Updated generated node_cert epoch: 0
[ns_server:debug,2025-05-15T18:47:23.189Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',node_cert} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{2,63914554043}}]},
 {certs_epoch,0},
 {subject,<<"CN=Couchbase Server Node (db2.lan)">>},
 {not_after,63985747642},
 {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
 {load_timestamp,63914554042},
 {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hX"...>>},
 {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDOjCCAiKgAwIBAgIIGD/HyAMALgMwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjJaFw0y\nNzA4MTcxODQ3MjJaMCoxKDAmBgNVBAMTH0NvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nZGIyLmxhbikwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC63r7EIhIk\nOyMwE9mw7bzVJfstC9FfdZvk0bP8vxZkuhNt/bVcND8p00ihMP8FCbfsa5d8VgWC\nDoa"...>>},
 {pkey_passphrase_settings,[]},
 {type,generated},
 {hostname,"db2.lan"}]
[ns_server:info,2025-05-15T18:47:23.190Z,ns_1@db2.lan:ns_cluster<0.273.0>:ns_ssl_services_setup:update_cert_epoch:1334]Updated generated client_cert epoch: 0
[ns_server:debug,2025-05-15T18:47:23.190Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',client_cert} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{2,63914554043}}]},
 {certs_epoch,0},
 {subject,<<"CN=Couchbase Internal Client (116550952)">>},
 {not_after,63985747642},
 {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
 {load_timestamp,63914554042},
 {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hX"...>>},
 {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDWjCCAkKgAwIBAgIIGD/HyA7ClpEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjJaFw0y\nNzA4MTcxODQ3MjJaMDAxLjAsBgNVBAMTJUNvdWNoYmFzZSBJbnRlcm5hbCBDbGll\nbnQgKDExNjU1MDk1MikwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDg\nAaBAyJnrm54fDG0xlsEQH9E51WTLpkwtleiz980c498XKRPsgRWnjY920KTUIH2/\ndQc"...>>},
 {pkey_passphrase_settings,[]},
 {type,generated},
 {name,"@internal"}]
[cluster:debug,2025-05-15T18:47:23.190Z,ns_1@db2.lan:ns_cluster<0.273.0>:ns_cluster:perform_actual_join:1651]Join succeded, starting ns_server_cluster back
[error_logger:info,2025-05-15T18:47:23.190Z,ns_1@db2.lan:ns_server_nodes_sup<0.2356.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.2357.0>},
              {id,chronicle_compat_events},
              {mfargs,{chronicle_compat_events,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:23.190Z,ns_1@db2.lan:ns_server_nodes_sup<0.2356.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.2362.0>},
              {id,remote_monitors},
              {mfargs,{remote_monitors,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:23.190Z,ns_1@db2.lan:menelaus_barrier<0.2363.0>:one_shot_barrier:barrier_body:52]Barrier menelaus_barrier has started
[error_logger:info,2025-05-15T18:47:23.190Z,ns_1@db2.lan:ns_server_nodes_sup<0.2356.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.2363.0>},
              {id,menelaus_barrier},
              {mfargs,{menelaus_sup,barrier_start_link,[]}},
              {restart_type,temporary},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:23.191Z,ns_1@db2.lan:<0.2366.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,
                                    rest_lhttpc_pool} with delay of 1000
[error_logger:info,2025-05-15T18:47:23.191Z,ns_1@db2.lan:<0.2367.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2367.0>,suppress_max_restart_intensity}
    started: [{pid,<0.2368.0>},
              {id,rest_lhttpc_pool},
              {mfargs,{lhttpc_manager,start_link,
                                      [[{name,rest_lhttpc_pool},
                                        {connection_timeout,120000},
                                        {pool_size,20}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:23.191Z,ns_1@db2.lan:<0.2365.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2365.0>,suppress_max_restart_intensity}
    started: [{pid,<0.2366.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      rest_lhttpc_pool}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,rest_lhttpc_pool},
                       1000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 1,id => rest_lhttpc_pool,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [lhttpc_manager],
                          restart => permanent,shutdown => 1000,
                          start =>
                              {lhttpc_manager,start_link,
                                  [[{name,rest_lhttpc_pool},
                                    {connection_timeout,120000},
                                    {pool_size,20}]]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:23.191Z,ns_1@db2.lan:rest_lhttpc_pool_sup<0.2364.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,rest_lhttpc_pool_sup}
    started: [{pid,<0.2365.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,rest_lhttpc_pool}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 1,id => rest_lhttpc_pool,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [lhttpc_manager],
                         restart => permanent,shutdown => 1000,
                         start =>
                             {lhttpc_manager,start_link,
                                 [[{name,rest_lhttpc_pool},
                                   {connection_timeout,120000},
                                   {pool_size,20}]]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:23.191Z,ns_1@db2.lan:ns_server_nodes_sup<0.2356.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.2364.0>},
              {id,rest_lhttpc_pool_sup},
              {mfargs,{rest_lhttpc_pool_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:47:23.191Z,ns_1@db2.lan:memcached_refresh<0.2369.0>:memcached_refresh:init:45]Starting during memcached lifetime. Try to refresh all files.
[ns_server:debug,2025-05-15T18:47:23.192Z,ns_1@db2.lan:memcached_refresh<0.2369.0>:memcached_refresh:update_refresh_state:137]Refresh of [isasl,rbac] skipped. Retry in 1000 ms.
[error_logger:info,2025-05-15T18:47:23.192Z,ns_1@db2.lan:ns_server_nodes_sup<0.2356.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.2369.0>},
              {id,memcached_refresh},
              {mfargs,{memcached_refresh,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:23.192Z,ns_1@db2.lan:ns_server_nodes_sup<0.2356.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.2370.0>},
              {id,ns_secrets},
              {mfargs,{ns_secrets,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:23.192Z,ns_1@db2.lan:ns_ssl_services_sup<0.2371.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_ssl_services_sup}
    started: [{pid,<0.2372.0>},
              {id,ssl_service_events},
              {mfargs,{gen_event,start_link,[{local,ssl_service_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:23.193Z,ns_1@db2.lan:ns_ssl_services_setup<0.2373.0>:ns_ssl_services_setup:maybe_store_ca_certs:832]Considering to store CA certs
[ns_server:debug,2025-05-15T18:47:23.194Z,ns_1@db2.lan:ns_ssl_services_setup<0.2373.0>:ns_ssl_services_setup:maybe_store_ca_certs:847]Updating CA file with 1 certificates
[ns_server:info,2025-05-15T18:47:23.198Z,ns_1@db2.lan:ns_ssl_services_setup<0.2373.0>:ns_ssl_services_setup:maybe_store_ca_certs:850]CA file updated: 1 cert(s) written
[ns_server:debug,2025-05-15T18:47:23.200Z,ns_1@db2.lan:ns_ssl_services_setup<0.2373.0>:ns_ssl_services_setup:restart_regenerate_client_cert_timer:1447]Time left before client cert regeneration: 70588799000
[error_logger:info,2025-05-15T18:47:23.200Z,ns_1@db2.lan:ns_ssl_services_sup<0.2371.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_ssl_services_sup}
    started: [{pid,<0.2373.0>},
              {id,ns_ssl_services_setup},
              {mfargs,{ns_ssl_services_setup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:47:23.201Z,ns_1@db2.lan:ns_ssl_services_setup<0.2373.0>:ns_ssl_services_setup:validate_pkey:1032]Private key (node_cert) passphrase validation suceeded
[ns_server:info,2025-05-15T18:47:23.201Z,ns_1@db2.lan:ns_ssl_services_setup<0.2373.0>:ns_ssl_services_setup:validate_pkey:1032]Private key (client_cert) passphrase validation suceeded
[ns_server:debug,2025-05-15T18:47:23.201Z,ns_1@db2.lan:ns_ssl_services_setup<0.2373.0>:ns_ssl_services_setup:notify_services:1146]Going to notify following services: [cb_dist_tls,capi_ssl_service,memcached,
                                     server_cert_event,client_cert_event]
[ns_server:debug,2025-05-15T18:47:23.201Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Restarting tls distribution protocols (if any)
[ns_server:info,2025-05-15T18:47:23.201Z,ns_1@db2.lan:<0.2392.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service client_cert_event
[ns_server:warn,2025-05-15T18:47:23.201Z,ns_1@db2.lan:<0.2390.0>:ns_ssl_services_setup:notify_service:1184]Failed to notify service memcached: {error,no_proccess}
[ns_server:info,2025-05-15T18:47:23.201Z,ns_1@db2.lan:<0.2391.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service server_cert_event
[error_logger:info,2025-05-15T18:47:23.201Z,ns_1@db2.lan:net_kernel<0.2175.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {2355631,#Ref<0.3735524962.3650486276.241707>}}}
[ns_server:info,2025-05-15T18:47:23.202Z,ns_1@db2.lan:<0.2380.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2025-05-15T18:47:23.202Z,ns_1@db2.lan:<0.2380.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2025-05-15T18:47:23.202Z,ns_1@db2.lan:<0.2380.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2025-05-15T18:47:23.202Z,ns_1@db2.lan:<0.2380.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:debug,2025-05-15T18:47:23.202Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Ensure config is going to change listeners. Will be stopped: [], will be started: [], will be restarted: []
[ns_server:info,2025-05-15T18:47:23.202Z,ns_1@db2.lan:<0.2388.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service cb_dist_tls
[ns_server:debug,2025-05-15T18:47:23.202Z,ns_1@db2.lan:net_kernel<0.2175.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:info,2025-05-15T18:47:23.203Z,ns_1@db2.lan:<0.2380.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:debug,2025-05-15T18:47:23.203Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3735524962.3650355207.241856>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:47:23.203Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3735524962.3650355207.241856>,
                                  inet_tcp_dist,<0.2394.0>,
                                  #Ref<0.3735524962.3650355204.243366>}
[error_logger:info,2025-05-15T18:47:23.203Z,ns_1@db2.lan:net_kernel<0.2175.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.2394.0>,shutdown}}
[ns_server:debug,2025-05-15T18:47:23.203Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3735524962.3650355207.241856>,
                               inet_tcp_dist,<0.2394.0>,
                               #Ref<0.3735524962.3650355204.243366>}
[error_logger:info,2025-05-15T18:47:23.203Z,ns_1@db2.lan:net_kernel<0.2175.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:47:23.203Z,ns_1@db2.lan:<0.2389.0>:ns_couchdb_api:rpc_couchdb_node:152]RPC to couchdb node failed for restart_capi_ssl_service with {badrpc,nodedown}
Stack: [{ns_couchdb_api,rpc_couchdb_node,4,
                        [{file,"src/ns_couchdb_api.erl"},{line,150}]},
        {ns_ssl_services_setup,do_notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1203}]},
        {ns_ssl_services_setup,notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1179}]},
        {async,'-async_init/4-fun-1-',3,[{file,"src/async.erl"},{line,199}]}]
[ns_server:warn,2025-05-15T18:47:23.203Z,ns_1@db2.lan:<0.2389.0>:ns_ssl_services_setup:notify_service:1184]Failed to notify service capi_ssl_service: {'EXIT',{error,{badrpc,nodedown}}}
[ns_server:info,2025-05-15T18:47:23.204Z,ns_1@db2.lan:ns_ssl_services_setup<0.2373.0>:ns_ssl_services_setup:notify_services:1162]Succesfully notified services [client_cert_event,server_cert_event,
                               cb_dist_tls]
[ns_server:info,2025-05-15T18:47:23.204Z,ns_1@db2.lan:<0.2380.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"0.0.0.0"},
 [{keyfile,"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem"},
  {certfile,"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem"},
  {versions,['tlsv1.2','tlsv1.3']},
  {cacerts,[<<48,130,3,12,48,130,1,244,160,3,2,1,2,2,8,24,63,199,191,156,197,
              82,17,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,48,
              32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,101,
              114,118,101,114,32,99,54,52,100,57,49,52,101,48,30,23,13,49,51,
              48,49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,51,
              53,57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,117,
              99,104,98,97,115,101,32,83,101,114,118,101,114,32,99,54,52,100,
              57,49,52,101,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,1,
              5,0,3,130,1,15,0,48,130,1,10,2,130,1,1,0,189,71,7,138,162,116,
              202,173,172,80,169,207,183,229,131,28,103,231,118,113,23,184,
              163,118,132,50,140,197,100,9,14,13,222,41,184,111,89,132,2,102,
              81,20,185,124,217,165,68,88,93,96,211,184,188,36,17,35,176,167,
              109,82,1,80,201,124,250,21,203,22,244,2,187,45,29,22,162,207,
              172,158,177,99,109,81,39,155,109,84,244,123,242,203,49,197,116,
              158,244,151,83,143,242,44,81,204,228,195,44,95,101,252,10,164,
              27,51,148,102,46,28,237,20,182,124,42,212,174,43,184,195,216,
              21,135,233,45,167,9,73,251,27,220,165,132,153,156,0,160,219,
              145,166,59,196,219,128,44,31,123,111,143,183,1,7,155,248,244,
              114,116,107,27,205,0,89,187,228,101,131,201,74,251,79,181,48,
              113,50,90,20,129,109,127,47,137,209,73,173,217,56,197,53,141,3,
              62,155,117,88,76,228,40,82,177,57,208,108,251,183,13,115,126,
              89,27,138,240,86,239,196,39,94,14,94,20,12,94,74,91,82,150,9,
              197,47,35,206,76,210,35,237,68,111,174,228,31,136,249,66,25,
              209,244,127,176,195,100,189,2,3,1,0,1,163,66,48,64,48,14,6,3,
              85,29,15,1,1,255,4,4,3,2,1,134,48,15,6,3,85,29,19,1,1,255,4,5,
              48,3,1,1,255,48,29,6,3,85,29,14,4,22,4,20,188,1,42,182,50,72,
              116,82,198,94,225,242,47,206,206,38,9,39,211,154,48,13,6,9,42,
              134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,7,122,181,155,55,73,
              115,117,88,215,178,67,9,164,79,41,143,213,122,120,183,161,158,
              179,24,159,107,244,185,214,40,78,200,59,208,49,4,2,152,24,193,
              11,28,199,253,134,219,80,153,22,107,94,33,144,139,251,88,92,
              185,69,22,78,200,112,130,3,234,35,142,100,178,218,128,56,66,64,
              147,226,231,103,70,132,16,120,107,97,39,6,241,133,244,226,72,
              227,53,189,39,13,56,28,212,121,112,93,92,197,98,43,69,209,137,
              1,127,228,110,207,204,246,60,244,73,57,87,206,37,77,194,32,24,
              120,98,49,146,58,191,127,2,164,235,7,154,139,200,136,193,5,110,
              232,107,27,181,32,75,241,254,5,68,41,54,188,174,154,212,192,
              249,224,20,3,106,196,82,133,216,183,147,240,201,0,75,88,247,
              105,174,187,93,204,210,55,56,242,96,140,116,69,94,95,19,60,172,
              138,198,232,182,175,206,36,240,180,239,163,144,92,150,142,84,
              114,53,180,171,234,41,126,157,228,150,39,182,108,165,58,62,101,
              129,114,231,144,128,135,156,44,194,78,139,142,141,176,149,8,31,
              25,249,73,108,167,49,3>>]},
  {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,
        10,118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,
        158,232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,
        66,211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,
        250,145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,
        104,159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,
        246,169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,
        110,167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,
        118,190,67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,
        74,8,205,174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,
        221,95,184,110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,
        76,187,66,211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,
        69,254,147,103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,
        202,133,173,72,6,69,167,89,112,174,40,229,171,2,1,2>>},
  {ciphers,[#{cipher => aes_256_gcm,key_exchange => any,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm_8,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_ecdsa,
              mac => aead,prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_rsa,
              mac => aead,prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_128_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf},
            #{cipher => aes_128_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf}]},
  {honor_cipher_order,true},
  {secure_renegotiate,true},
  {client_renegotiation,false},
  {password,"********"}],
 {ssl,true},
 {port,18091}]
[error_logger:info,2025-05-15T18:47:23.205Z,ns_1@db2.lan:<0.2380.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2380.0>,menelaus_web}
    started: [{pid,<0.2393.0>},
              {id,menelaus_web_ipv4},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet},
                        {ssl,true},
                        {name,menelaus_web_ssl},
                        {ssl_opts_fun,#Fun<ns_ssl_services_setup.11.39330155>},
                        {port,18091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:47:23.205Z,ns_1@db2.lan:ns_ssl_services_setup<0.2373.0>:ns_ssl_services_setup:notify_services:1173]Failed to notify some services. Will retry in 5 sec, [{{error,no_proccess},
                                                       memcached},
                                                      {{'EXIT',
                                                        {error,
                                                         {badrpc,nodedown}}},
                                                       capi_ssl_service}]
[ns_server:info,2025-05-15T18:47:23.206Z,ns_1@db2.lan:<0.2380.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2025-05-15T18:47:23.207Z,ns_1@db2.lan:<0.2380.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2025-05-15T18:47:23.207Z,ns_1@db2.lan:<0.2380.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2025-05-15T18:47:23.207Z,ns_1@db2.lan:<0.2380.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2025-05-15T18:47:23.207Z,ns_1@db2.lan:<0.2380.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2025-05-15T18:47:23.208Z,ns_1@db2.lan:<0.2380.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"::"},
 [{keyfile,"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem"},
  {certfile,"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem"},
  {versions,['tlsv1.2','tlsv1.3']},
  {cacerts,[<<48,130,3,12,48,130,1,244,160,3,2,1,2,2,8,24,63,199,191,156,197,
              82,17,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,48,
              32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,101,
              114,118,101,114,32,99,54,52,100,57,49,52,101,48,30,23,13,49,51,
              48,49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,51,
              53,57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,117,
              99,104,98,97,115,101,32,83,101,114,118,101,114,32,99,54,52,100,
              57,49,52,101,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,1,
              5,0,3,130,1,15,0,48,130,1,10,2,130,1,1,0,189,71,7,138,162,116,
              202,173,172,80,169,207,183,229,131,28,103,231,118,113,23,184,
              163,118,132,50,140,197,100,9,14,13,222,41,184,111,89,132,2,102,
              81,20,185,124,217,165,68,88,93,96,211,184,188,36,17,35,176,167,
              109,82,1,80,201,124,250,21,203,22,244,2,187,45,29,22,162,207,
              172,158,177,99,109,81,39,155,109,84,244,123,242,203,49,197,116,
              158,244,151,83,143,242,44,81,204,228,195,44,95,101,252,10,164,
              27,51,148,102,46,28,237,20,182,124,42,212,174,43,184,195,216,
              21,135,233,45,167,9,73,251,27,220,165,132,153,156,0,160,219,
              145,166,59,196,219,128,44,31,123,111,143,183,1,7,155,248,244,
              114,116,107,27,205,0,89,187,228,101,131,201,74,251,79,181,48,
              113,50,90,20,129,109,127,47,137,209,73,173,217,56,197,53,141,3,
              62,155,117,88,76,228,40,82,177,57,208,108,251,183,13,115,126,
              89,27,138,240,86,239,196,39,94,14,94,20,12,94,74,91,82,150,9,
              197,47,35,206,76,210,35,237,68,111,174,228,31,136,249,66,25,
              209,244,127,176,195,100,189,2,3,1,0,1,163,66,48,64,48,14,6,3,
              85,29,15,1,1,255,4,4,3,2,1,134,48,15,6,3,85,29,19,1,1,255,4,5,
              48,3,1,1,255,48,29,6,3,85,29,14,4,22,4,20,188,1,42,182,50,72,
              116,82,198,94,225,242,47,206,206,38,9,39,211,154,48,13,6,9,42,
              134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,7,122,181,155,55,73,
              115,117,88,215,178,67,9,164,79,41,143,213,122,120,183,161,158,
              179,24,159,107,244,185,214,40,78,200,59,208,49,4,2,152,24,193,
              11,28,199,253,134,219,80,153,22,107,94,33,144,139,251,88,92,
              185,69,22,78,200,112,130,3,234,35,142,100,178,218,128,56,66,64,
              147,226,231,103,70,132,16,120,107,97,39,6,241,133,244,226,72,
              227,53,189,39,13,56,28,212,121,112,93,92,197,98,43,69,209,137,
              1,127,228,110,207,204,246,60,244,73,57,87,206,37,77,194,32,24,
              120,98,49,146,58,191,127,2,164,235,7,154,139,200,136,193,5,110,
              232,107,27,181,32,75,241,254,5,68,41,54,188,174,154,212,192,
              249,224,20,3,106,196,82,133,216,183,147,240,201,0,75,88,247,
              105,174,187,93,204,210,55,56,242,96,140,116,69,94,95,19,60,172,
              138,198,232,182,175,206,36,240,180,239,163,144,92,150,142,84,
              114,53,180,171,234,41,126,157,228,150,39,182,108,165,58,62,101,
              129,114,231,144,128,135,156,44,194,78,139,142,141,176,149,8,31,
              25,249,73,108,167,49,3>>]},
  {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,
        10,118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,
        158,232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,
        66,211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,
        250,145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,
        104,159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,
        246,169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,
        110,167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,
        118,190,67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,
        74,8,205,174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,
        221,95,184,110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,
        76,187,66,211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,
        69,254,147,103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,
        202,133,173,72,6,69,167,89,112,174,40,229,171,2,1,2>>},
  {ciphers,[#{cipher => aes_256_gcm,key_exchange => any,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm_8,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_ecdsa,
              mac => aead,prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_rsa,
              mac => aead,prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_128_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf},
            #{cipher => aes_128_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf}]},
  {honor_cipher_order,true},
  {secure_renegotiate,true},
  {client_renegotiation,false},
  {password,"********"}],
 {ssl,true},
 {port,18091}]
[error_logger:info,2025-05-15T18:47:23.209Z,ns_1@db2.lan:<0.2380.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2380.0>,menelaus_web}
    started: [{pid,<0.2414.0>},
              {id,menelaus_web_ipv6},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet6},
                        {ssl,true},
                        {name,menelaus_web_ssl},
                        {ssl_opts_fun,#Fun<ns_ssl_services_setup.11.39330155>},
                        {port,18091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:23.209Z,ns_1@db2.lan:<0.2379.0>:restartable:start_child:92]Started child process <0.2380.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
[error_logger:info,2025-05-15T18:47:23.209Z,ns_1@db2.lan:ns_ssl_services_sup<0.2371.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_ssl_services_sup}
    started: [{pid,<0.2379.0>},
              {id,ns_rest_ssl_service},
              {mfargs,
                  {restartable,start_link,
                      [{ns_ssl_services_setup,start_link_rest_service,[]},
                       1000]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:23.209Z,ns_1@db2.lan:ns_server_nodes_sup<0.2356.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.2371.0>},
              {id,ns_ssl_services_sup},
              {mfargs,{ns_ssl_services_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:23.209Z,ns_1@db2.lan:ns_server_nodes_sup<0.2356.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.2433.0>},
              {id,ldap_auth_cache},
              {mfargs,{ldap_auth_cache,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:23.209Z,ns_1@db2.lan:cb_saml<0.2435.0>:cb_saml:handle_info:316]Settings have changed or this is the first start
[ns_server:debug,2025-05-15T18:47:23.209Z,ns_1@db2.lan:cb_saml<0.2435.0>:cb_saml:restart_refresh_timer:586]Restarting refresh timer: 29705 ms
[error_logger:info,2025-05-15T18:47:23.209Z,ns_1@db2.lan:ns_server_nodes_sup<0.2356.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.2435.0>},
              {id,cb_saml},
              {mfargs,{cb_saml,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:23.210Z,ns_1@db2.lan:users_sup<0.2437.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_sup}
    started: [{pid,<0.2438.0>},
              {id,user_storage_events},
              {mfargs,{gen_event,start_link,[{local,user_storage_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:23.210Z,ns_1@db2.lan:users_replicator<0.2440.0>:replicated_storage:wait_for_startup:47]Start waiting for startup
[error_logger:info,2025-05-15T18:47:23.210Z,ns_1@db2.lan:users_storage_sup<0.2439.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_storage_sup}
    started: [{pid,<0.2440.0>},
              {id,users_replicator},
              {mfargs,{menelaus_users,start_replicator,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:23.210Z,ns_1@db2.lan:users_storage<0.2441.0>:replicated_storage:announce_startup:61]Announce my startup to <0.2440.0>
[ns_server:debug,2025-05-15T18:47:23.210Z,ns_1@db2.lan:users_storage<0.2441.0>:replicated_dets:open:177]Opening file "/opt/couchbase/var/lib/couchbase/config/users.dets"
[ns_server:debug,2025-05-15T18:47:23.210Z,ns_1@db2.lan:users_replicator<0.2440.0>:replicated_storage:wait_for_startup:50]Received replicated storage registration from <0.2441.0>
[error_logger:info,2025-05-15T18:47:23.210Z,ns_1@db2.lan:users_storage_sup<0.2439.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_storage_sup}
    started: [{pid,<0.2441.0>},
              {id,users_storage},
              {mfargs,{menelaus_users,start_storage,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:23.210Z,ns_1@db2.lan:users_sup<0.2437.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_sup}
    started: [{pid,<0.2439.0>},
              {id,users_storage_sup},
              {mfargs,{users_storage_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:47:23.210Z,ns_1@db2.lan:compiled_roles_cache<0.2444.0>:versioned_cache:init:41]Starting versioned cache compiled_roles_cache
[error_logger:info,2025-05-15T18:47:23.210Z,ns_1@db2.lan:users_sup<0.2437.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_sup}
    started: [{pid,<0.2444.0>},
              {id,compiled_roles_cache},
              {mfargs,{menelaus_roles,start_compiled_roles_cache,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:23.210Z,ns_1@db2.lan:users_sup<0.2437.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_sup}
    started: [{pid,<0.2447.0>},
              {id,roles_cache},
              {mfargs,{roles_cache,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:23.210Z,ns_1@db2.lan:ns_server_nodes_sup<0.2356.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.2437.0>},
              {id,users_sup},
              {mfargs,{users_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:47:23.212Z,ns_1@db2.lan:users_storage<0.2441.0>:replicated_dets:select_from_table:312][dets] Starting select with {users_storage,
                                [{{docv2,'_','_','_'},[],['$_']}],
                                100}
[ns_server:debug,2025-05-15T18:47:23.212Z,ns_1@db2.lan:users_storage<0.2441.0>:replicated_dets:init_after_ack:170]Loading 0 items, 307 words took 2ms
[ns_server:debug,2025-05-15T18:47:23.212Z,ns_1@db2.lan:users_replicator<0.2440.0>:doc_replicator:loop:79]Replicating all docs to new nodes: ['ns_1@172.19.0.4']
[ns_server:debug,2025-05-15T18:47:23.212Z,ns_1@db2.lan:users_replicator<0.2440.0>:replicated_dets:select_from_table:312][dets] Starting select with {users_storage,[{'_',[],['$_']}],500}
[ns_server:debug,2025-05-15T18:47:23.214Z,ns_1@db2.lan:<0.2454.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,
                                    start_couchdb_node} with delay of 5000
[error_logger:info,2025-05-15T18:47:23.216Z,ns_1@db2.lan:<0.2455.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2455.0>,suppress_max_restart_intensity}
    started: [{pid,<0.2456.0>},
              {id,start_couchdb_node},
              {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,86400000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:23.217Z,ns_1@db2.lan:<0.2453.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2453.0>,suppress_max_restart_intensity}
    started: [{pid,<0.2454.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      start_couchdb_node}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,start_couchdb_node},
                       5000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 5,id => start_couchdb_node,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [],restart => permanent,
                          shutdown => 86400000,
                          start => {ns_server_nodes_sup,start_couchdb_node,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:23.217Z,ns_1@db2.lan:ns_server_nodes_sup<0.2356.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.2453.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,start_couchdb_node}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 5,id => start_couchdb_node,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [],restart => permanent,
                         shutdown => 86400000,
                         start => {ns_server_nodes_sup,start_couchdb_node,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:47:23.217Z,ns_1@db2.lan:wait_link_to_couchdb_node<0.2457.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:158]Waiting for ns_couchdb node to start
[error_logger:info,2025-05-15T18:47:23.217Z,ns_1@db2.lan:net_kernel<0.2175.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {2355632,#Ref<0.3735524962.3650486276.241707>}}}
[ns_server:debug,2025-05-15T18:47:23.217Z,ns_1@db2.lan:net_kernel<0.2175.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:23.217Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3735524962.3650355203.242426>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:47:23.218Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3735524962.3650355203.242426>,
                                  inet_tcp_dist,<0.2459.0>,
                                  #Ref<0.3735524962.3650355208.242135>}
[ns_server:debug,2025-05-15T18:47:23.218Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3735524962.3650355203.242426>,
                               inet_tcp_dist,<0.2459.0>,
                               #Ref<0.3735524962.3650355208.242135>}
[error_logger:info,2025-05-15T18:47:23.218Z,ns_1@db2.lan:net_kernel<0.2175.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.2459.0>,shutdown}}
[error_logger:info,2025-05-15T18:47:23.218Z,ns_1@db2.lan:net_kernel<0.2175.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:47:23.218Z,ns_1@db2.lan:<0.2458.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-05-15T18:47:23.420Z,ns_1@db2.lan:net_kernel<0.2175.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {2355633,#Ref<0.3735524962.3650486276.241707>}}}
[ns_server:debug,2025-05-15T18:47:23.420Z,ns_1@db2.lan:net_kernel<0.2175.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:23.420Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3735524962.3650355202.242583>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:47:23.421Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3735524962.3650355202.242583>,
                                  inet_tcp_dist,<0.2461.0>,
                                  #Ref<0.3735524962.3650355202.242586>}
[ns_server:debug,2025-05-15T18:47:23.421Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3735524962.3650355202.242583>,
                               inet_tcp_dist,<0.2461.0>,
                               #Ref<0.3735524962.3650355202.242586>}
[error_logger:info,2025-05-15T18:47:23.421Z,ns_1@db2.lan:net_kernel<0.2175.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.2461.0>,shutdown}}
[error_logger:info,2025-05-15T18:47:23.421Z,ns_1@db2.lan:net_kernel<0.2175.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:47:23.421Z,ns_1@db2.lan:<0.2458.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-05-15T18:47:23.622Z,ns_1@db2.lan:net_kernel<0.2175.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {2355634,#Ref<0.3735524962.3650486276.241707>}}}
[ns_server:debug,2025-05-15T18:47:23.623Z,ns_1@db2.lan:net_kernel<0.2175.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:23.623Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3735524962.3650355202.242595>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:47:23.623Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3735524962.3650355202.242595>,
                                  inet_tcp_dist,<0.2463.0>,
                                  #Ref<0.3735524962.3650355202.242599>}
[ns_server:debug,2025-05-15T18:47:23.652Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3735524962.3650355202.242595>,
                               inet_tcp_dist,<0.2463.0>,
                               #Ref<0.3735524962.3650355202.242599>}
[error_logger:info,2025-05-15T18:47:23.652Z,ns_1@db2.lan:net_kernel<0.2175.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.2463.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2025-05-15T18:47:23.652Z,ns_1@db2.lan:net_kernel<0.2175.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:47:23.652Z,ns_1@db2.lan:<0.2458.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-05-15T18:47:23.854Z,ns_1@db2.lan:net_kernel<0.2175.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {2355635,#Ref<0.3735524962.3650486276.241707>}}}
[ns_server:debug,2025-05-15T18:47:23.854Z,ns_1@db2.lan:net_kernel<0.2175.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:23.854Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3735524962.3650355201.246036>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:47:23.854Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3735524962.3650355201.246036>,
                                  inet_tcp_dist,<0.2465.0>,
                                  #Ref<0.3735524962.3650355201.246039>}
[ns_server:info,2025-05-15T18:47:23.856Z,ns_1@db2.lan:ns_couchdb_port<0.2456.0>:ns_port_server:log:226]ns_couchdb<0.2456.0>: =ERROR REPORT==== 15-May-2025::18:47:23.652081 ===
ns_couchdb<0.2456.0>: ** Connection attempt to/from node 'ns_1@db2.lan' rejected. Cookie is not set. **
ns_couchdb<0.2456.0>: 

[ns_server:debug,2025-05-15T18:47:23.856Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3735524962.3650355201.246036>,
                               inet_tcp_dist,<0.2465.0>,
                               #Ref<0.3735524962.3650355201.246039>}
[error_logger:info,2025-05-15T18:47:23.856Z,ns_1@db2.lan:net_kernel<0.2175.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.2465.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2025-05-15T18:47:23.856Z,ns_1@db2.lan:net_kernel<0.2175.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:47:23.856Z,ns_1@db2.lan:<0.2458.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-05-15T18:47:24.059Z,ns_1@db2.lan:net_kernel<0.2175.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {2355636,#Ref<0.3735524962.3650486276.241707>}}}
[ns_server:debug,2025-05-15T18:47:24.059Z,ns_1@db2.lan:net_kernel<0.2175.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:24.059Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3735524962.3650355202.242603>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:47:24.059Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3735524962.3650355202.242603>,
                                  inet_tcp_dist,<0.2467.0>,
                                  #Ref<0.3735524962.3650355202.242606>}
[error_logger:info,2025-05-15T18:47:24.062Z,ns_1@db2.lan:net_kernel<0.2175.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.2467.0>,{recv_challenge_failed,{error,closed}}}}
[ns_server:debug,2025-05-15T18:47:24.062Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3735524962.3650355202.242603>,
                               inet_tcp_dist,<0.2467.0>,
                               #Ref<0.3735524962.3650355202.242606>}
[error_logger:info,2025-05-15T18:47:24.062Z,ns_1@db2.lan:net_kernel<0.2175.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:47:24.062Z,ns_1@db2.lan:<0.2458.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2025-05-15T18:47:24.193Z,ns_1@db2.lan:memcached_refresh<0.2369.0>:memcached_refresh:update_refresh_state:137]Refresh of [isasl,rbac] skipped. Retry in 1000 ms.
[error_logger:info,2025-05-15T18:47:24.264Z,ns_1@db2.lan:net_kernel<0.2175.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {2355637,#Ref<0.3735524962.3650486276.241707>}}}
[ns_server:debug,2025-05-15T18:47:24.265Z,ns_1@db2.lan:net_kernel<0.2175.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:24.265Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3735524962.3650355202.242611>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:47:24.265Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3735524962.3650355202.242611>,
                                  inet_tcp_dist,<0.2469.0>,
                                  #Ref<0.3735524962.3650355202.242613>}
[error_logger:info,2025-05-15T18:47:24.265Z,ns_1@db2.lan:net_kernel<0.2175.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.2469.0>,{recv_challenge_failed,{error,closed}}}}
[ns_server:debug,2025-05-15T18:47:24.265Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3735524962.3650355202.242611>,
                               inet_tcp_dist,<0.2469.0>,
                               #Ref<0.3735524962.3650355202.242613>}
[error_logger:info,2025-05-15T18:47:24.265Z,ns_1@db2.lan:net_kernel<0.2175.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:47:24.265Z,ns_1@db2.lan:<0.2458.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-05-15T18:47:24.470Z,ns_1@db2.lan:net_kernel<0.2175.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {2355638,#Ref<0.3735524962.3650486276.241707>}}}
[ns_server:debug,2025-05-15T18:47:24.470Z,ns_1@db2.lan:net_kernel<0.2175.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:24.470Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3735524962.3650355202.242619>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:47:24.470Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3735524962.3650355202.242619>,
                                  inet_tcp_dist,<0.2471.0>,
                                  #Ref<0.3735524962.3650355202.242622>}
[ns_server:debug,2025-05-15T18:47:24.471Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3735524962.3650355202.242619>,
                               inet_tcp_dist,<0.2471.0>,
                               #Ref<0.3735524962.3650355202.242622>}
[error_logger:info,2025-05-15T18:47:24.471Z,ns_1@db2.lan:net_kernel<0.2175.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.2471.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2025-05-15T18:47:24.471Z,ns_1@db2.lan:net_kernel<0.2175.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:47:24.471Z,ns_1@db2.lan:<0.2458.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-05-15T18:47:24.673Z,ns_1@db2.lan:net_kernel<0.2175.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {2355639,#Ref<0.3735524962.3650486276.241707>}}}
[ns_server:debug,2025-05-15T18:47:24.673Z,ns_1@db2.lan:net_kernel<0.2175.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:24.673Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3735524962.3650355203.242455>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:47:24.673Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3735524962.3650355203.242455>,
                                  inet_tcp_dist,<0.2473.0>,
                                  #Ref<0.3735524962.3650355203.242458>}
[ns_server:debug,2025-05-15T18:47:24.677Z,ns_1@db2.lan:<0.2458.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: false
[error_logger:info,2025-05-15T18:47:24.933Z,ns_1@db2.lan:ns_server_nodes_sup<0.2356.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.2457.0>},
              {id,wait_for_couchdb_node},
              {mfargs,{erlang,apply,
                              [#Fun<ns_server_nodes_sup.0.120652322>,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:24.933Z,ns_1@db2.lan:<0.2479.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,ns_disksup} with delay of 4000
[error_logger:info,2025-05-15T18:47:24.934Z,ns_1@db2.lan:<0.2480.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2480.0>,suppress_max_restart_intensity}
    started: [{pid,<0.2481.0>},
              {id,ns_disksup},
              {mfargs,{ns_disksup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:24.934Z,ns_1@db2.lan:<0.2478.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2478.0>,suppress_max_restart_intensity}
    started: [{pid,<0.2479.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      ns_disksup}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,ns_disksup},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => ns_disksup,inherited_max_r => 20,
                          inherited_max_t => 10,modules => [],
                          restart => permanent,shutdown => 1000,
                          start => {ns_disksup,start_link,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:24.934Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2478.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,ns_disksup}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => ns_disksup,inherited_max_r => 20,
                         inherited_max_t => 10,modules => [],
                         restart => permanent,shutdown => 1000,
                         start => {ns_disksup,start_link,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:24.934Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2482.0>},
              {id,diag_handler_worker},
              {mfargs,{work_queue,start_link,[diag_handler_worker]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:47:24.934Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:dir_size:start_link:33]Starting quick version of dir_size with program name: godu
[error_logger:info,2025-05-15T18:47:24.935Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2483.0>},
              {id,dir_size},
              {mfargs,{dir_size,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:24.935Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2484.0>},
              {id,request_tracker},
              {mfargs,{request_tracker,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:24.935Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2485.0>},
              {id,chronicle_kv_log},
              {mfargs,{chronicle_kv_log,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:warn,2025-05-15T18:47:24.936Z,ns_1@db2.lan:ns_log<0.2487.0>:gossip_replicator:read_logs:244]Couldn't load logs from "/opt/couchbase/var/lib/couchbase/ns_log" (perhaps it's first
                         startup): {error,enoent}
[error_logger:info,2025-05-15T18:47:24.937Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2487.0>},
              {id,ns_log},
              {mfargs,{ns_log,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:24.937Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2488.0>},
              {id,event_log_events},
              {mfargs,{gen_event,start_link,[{local,event_log_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:24.938Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2489.0>},
              {id,event_log_server},
              {mfargs,{event_log_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:24.941Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2491.0>},
              {id,initargs_updater},
              {mfargs,{initargs_updater,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:24.941Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2493.0>},
              {id,timer_lag_recorder},
              {mfargs,{timer_lag_recorder,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:24.941Z,ns_1@db2.lan:<0.2495.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,
                                    ns_babysitter_log_consumer} with delay of 4000
[error_logger:info,2025-05-15T18:47:24.941Z,ns_1@db2.lan:<0.2496.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2496.0>,suppress_max_restart_intensity}
    started: [{pid,<0.2497.0>},
              {id,ns_babysitter_log_consumer},
              {mfargs,{ns_log,start_link_babysitter_log_consumer,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:24.941Z,ns_1@db2.lan:<0.2494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2494.0>,suppress_max_restart_intensity}
    started: [{pid,<0.2495.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      ns_babysitter_log_consumer}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,
                           ns_babysitter_log_consumer},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => ns_babysitter_log_consumer,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [],restart => permanent,shutdown => 1000,
                          start =>
                              {ns_log,start_link_babysitter_log_consumer,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:24.942Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2494.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,
                      ns_babysitter_log_consumer}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => ns_babysitter_log_consumer,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [],restart => permanent,shutdown => 1000,
                         start =>
                             {ns_log,start_link_babysitter_log_consumer,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:info,2025-05-15T18:47:24.975Z,ns_1@db2.lan:ns_couchdb_port<0.2456.0>:ns_port_server:log:226]ns_couchdb<0.2456.0>: Apache CouchDB v4.5.1-330-g3e5b8f24 (LogLevel=info) is starting.
ns_couchdb<0.2456.0>: Apache CouchDB has started. Time to relax.
ns_couchdb<0.2456.0>: 399: Booted. Waiting for shutdown request
ns_couchdb<0.2456.0>: 399: Booted. Waiting for shutdown request
ns_couchdb<0.2456.0>: working as port

[ns_server:debug,2025-05-15T18:47:25.013Z,ns_1@db2.lan:<0.2502.0>:goport:handle_eof:592]Stream 'stderr' closed
[ns_server:debug,2025-05-15T18:47:25.013Z,ns_1@db2.lan:<0.2502.0>:goport:handle_eof:592]Stream 'stdout' closed
[ns_server:info,2025-05-15T18:47:25.013Z,ns_1@db2.lan:<0.2502.0>:goport:handle_process_exit:573]Port exited with status 0.
[ns_server:debug,2025-05-15T18:47:25.016Z,ns_1@db2.lan:prometheus_cfg<0.2498.0>:prometheus_cfg:ensure_prometheus_config:932]Updating prometheus config file: /opt/couchbase/var/lib/couchbase/config/prometheus.yml
[ns_server:debug,2025-05-15T18:47:25.017Z,ns_1@db2.lan:prometheus_cfg<0.2498.0>:prometheus_cfg:ensure_prometheus_config:932]Updating prometheus config file: /opt/couchbase/var/lib/couchbase/config/prometheus_rules.yml
[ns_server:debug,2025-05-15T18:47:25.030Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',prometheus_auth_info} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{2,63914554045}}]}|
 {"@prometheus",
  {auth,
   [{<<"hash">>,
     {[{<<"hashes">>,
        {sanitized,<<"E4iwPDExOnOjGXs0qP6la711Y5qTaST+1nzp3WWzVqw=">>}},
       {<<"algorithm">>,<<"argon2id">>},
       {<<"salt">>,<<"61T+47ADFbRDEeE8WMrAGw==">>},
       {<<"parallelism">>,1},
       {<<"time">>,3},
       {<<"memory">>,524288}]}},
    {<<"scram-sha-512">>,
     {[{<<"salt">>,
        <<"kGjfTLY3stMxG3p1BzztyO2J8qheD+hlb8euMQoyGg1cr+r9pUQyPsr32qJ3H9T4JpSJnBUEyXbjZ/tnoEGXMw==">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"KuADGEOAaLhVMSkwIKxrztvRUhXdeMaZA8c8MPeK2qw=">>}}]}},
    {<<"scram-sha-256">>,
     {[{<<"salt">>,<<"ga97SSKrWv6mIdojxSSVO+hBOALInVDadwYP0X8uqjU=">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"QaFsRlNDNHpIcZSkMUdzIYnBSJznlCdgo3U/gZzgIW8=">>}}]}},
    {<<"scram-sha-1">>,
     {[{<<"salt">>,<<"kN6RJArUekQYl2C69D1t3KNt2Zc=">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"JyPn7MPQQ8xQtBYdBohgEg3u+BTyMU7ioflt+xVGsak=">>}}]}}]}}]
[ns_server:debug,2025-05-15T18:47:25.031Z,ns_1@db2.lan:prometheus_cfg<0.2498.0>:prometheus_cfg:apply_config:724]Restarting Prometheus as the start specs have changed
[error_logger:info,2025-05-15T18:47:25.035Z,ns_1@db2.lan:ale_dynamic_sup<0.78.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ale_dynamic_sup}
    started: [{pid,<0.2505.0>},
              {id,'sink-prometheus'},
              {mfargs,
                  {ale_dynamic_sup,delay_death,
                      [{ale_disk_sink,start_link,
                           ['sink-prometheus',
                            "/opt/couchbase/var/lib/couchbase/logs/prometheus.log",
                            [{rotation,
                                 [{compress,true},
                                  {size,41943040},
                                  {num_files,10},
                                  {buffer_size_max,52428800}]}]]},
                       1000]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:25.094Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: {service_map,index}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                        19})
['ns_1@172.19.0.4']
[error_logger:info,2025-05-15T18:47:25.147Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2498.0>},
              {id,prometheus_cfg},
              {mfargs,{prometheus_cfg,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:25.147Z,ns_1@db2.lan:memcached_passwords<0.2511.0>:memcached_cfg:init:60]Init config writer for memcached_passwords, "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2025-05-15T18:47:25.148Z,ns_1@db2.lan:memcached_passwords<0.2511.0>:memcached_cfg:write_cfg:156]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2025-05-15T18:47:25.161Z,ns_1@db2.lan:memcached_passwords<0.2511.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,{auth,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2025-05-15T18:47:25.162Z,ns_1@db2.lan:memcached_refresh<0.2369.0>:memcached_refresh:handle_call:57]File rename from "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw" is requested
[ns_server:debug,2025-05-15T18:47:25.162Z,ns_1@db2.lan:memcached_passwords<0.2511.0>:memcached_cfg:rename_and_refresh:178]Successfully renamed "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2025-05-15T18:47:25.162Z,ns_1@db2.lan:memcached_refresh<0.2369.0>:memcached_refresh:handle_cast:67]Refresh of isasl requested
[error_logger:info,2025-05-15T18:47:25.162Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2511.0>},
              {id,memcached_passwords},
              {mfargs,{memcached_passwords,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:25.162Z,ns_1@db2.lan:memcached_permissions<0.2514.0>:memcached_cfg:init:60]Init config writer for memcached_permissions, "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2025-05-15T18:47:25.163Z,ns_1@db2.lan:memcached_permissions<0.2514.0>:memcached_cfg:write_cfg:156]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2025-05-15T18:47:25.163Z,ns_1@db2.lan:memcached_permissions<0.2514.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,{user,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2025-05-15T18:47:25.164Z,ns_1@db2.lan:memcached_refresh<0.2369.0>:memcached_refresh:handle_call:57]File rename from "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac" is requested
[ns_server:debug,2025-05-15T18:47:25.164Z,ns_1@db2.lan:memcached_permissions<0.2514.0>:memcached_cfg:rename_and_refresh:178]Successfully renamed "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2025-05-15T18:47:25.164Z,ns_1@db2.lan:memcached_refresh<0.2369.0>:memcached_refresh:handle_cast:67]Refresh of rbac requested
[error_logger:info,2025-05-15T18:47:25.164Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2514.0>},
              {id,memcached_permissions},
              {mfargs,{memcached_permissions,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.164Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2517.0>},
              {id,ns_email_alert},
              {mfargs,{ns_email_alert,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.164Z,ns_1@db2.lan:ns_node_disco_sup<0.2518.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_node_disco_sup}
    started: [{pid,<0.2519.0>},
              {id,ns_node_disco_events},
              {mfargs,{gen_event,start_link,[{local,ns_node_disco_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:25.164Z,ns_1@db2.lan:ns_node_disco<0.2520.0>:ns_node_disco:init:111]Initting ns_node_disco with ['ns_1@172.19.0.4']
[ns_server:debug,2025-05-15T18:47:25.165Z,ns_1@db2.lan:ns_cookie_manager<0.238.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[error_logger:info,2025-05-15T18:47:25.165Z,ns_1@db2.lan:ns_node_disco_sup<0.2518.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_node_disco_sup}
    started: [{pid,<0.2520.0>},
              {id,ns_node_disco},
              {mfargs,{ns_node_disco,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.165Z,ns_1@db2.lan:ns_node_disco_sup<0.2518.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_node_disco_sup}
    started: [{pid,<0.2523.0>},
              {id,ns_node_disco_log},
              {mfargs,{ns_node_disco_log,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:25.165Z,ns_1@db2.lan:<0.2522.0>:ns_node_disco:do_nodes_wanted_updated_fun:210]ns_node_disco: nodes_wanted updated: ['ns_1@172.19.0.4','ns_1@db2.lan'], with cookie: {sanitized,
                                                                                       <<"biSYTaQUFDVndTs/b+IcDmD2L0woktv9naRdv3GCK6A=">>}
[error_logger:info,2025-05-15T18:47:25.165Z,ns_1@db2.lan:ns_config_rep_sup<0.2524.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_rep_sup}
    started: [{pid,<0.2525.0>},
              {id,ns_config_rep_merger},
              {mfargs,{ns_config_rep,start_link_merger,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:25.165Z,ns_1@db2.lan:ns_config_rep<0.2526.0>:ns_config_rep:init:80]init pulling
[ns_server:info,2025-05-15T18:47:25.165Z,ns_1@db2.lan:ns_config_rep<0.2526.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@172.19.0.4'
[ns_server:debug,2025-05-15T18:47:25.165Z,ns_1@db2.lan:<0.2522.0>:ns_node_disco:do_nodes_wanted_updated_fun:213]ns_node_disco: nodes_wanted pong: ['ns_1@172.19.0.4','ns_1@db2.lan'], with cookie: {sanitized,
                                                                                    <<"biSYTaQUFDVndTs/b+IcDmD2L0woktv9naRdv3GCK6A=">>}
[ns_server:debug,2025-05-15T18:47:25.167Z,ns_1@db2.lan:ns_config_rep<0.2526.0>:ns_config:log_conflict:1423]Conflicting configuration changes to field scramsha_fallback_salt:
<<55,187,62,55,248,32,6,50,18,123,218,116>> and
<<199,70,35,121,194,74,234,136,104,77,98,154>>, choosing the former.
[ns_server:debug,2025-05-15T18:47:25.167Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/indexing/rebalance/RebalanceToken">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554045}}]}|
 <<"{\"MasterId\":\"28569ac00b9c1d7c50e39741027d428c\",\"RebalId\":\"35d452100b72fa9639f18b2cc28124e2\",\"Source\":0,\"Error\":\"\",\"MasterIP\":\"172.19.0.4\",\"Version\":0,\"RebalPhase\":0,\"ActiveRebalancer\":1}">>]
[ns_server:debug,2025-05-15T18:47:25.168Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{3,63914554045}}]}|
 <<"{\"indexer.plasma.backIndex.enablePageBloomFilter\":false,\"indexer.settings.compaction.abort_exceed_interval\":false,\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.en"...>>]
[ns_server:debug,2025-05-15T18:47:25.168Z,ns_1@db2.lan:ns_config_rep<0.2526.0>:ns_config_rep:init:82]init pushing
[error_logger:info,2025-05-15T18:47:25.170Z,ns_1@db2.lan:ns_config_rep_sup<0.2524.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_rep_sup}
    started: [{pid,<0.2526.0>},
              {id,ns_config_rep},
              {mfargs,{ns_config_rep,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.170Z,ns_1@db2.lan:ns_node_disco_sup<0.2518.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_node_disco_sup}
    started: [{pid,<0.2524.0>},
              {id,ns_config_rep_sup},
              {mfargs,{ns_config_rep_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:25.170Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2518.0>},
              {id,ns_node_disco_sup},
              {mfargs,{ns_node_disco_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:47:25.170Z,ns_1@db2.lan:tombstone_keeper<0.277.0>:tombstone_keeper:handle_call:49]Refreshed with timestamps []
[error_logger:info,2025-05-15T18:47:25.170Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2547.0>},
              {id,tombstone_agent},
              {mfargs,{tombstone_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.170Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2553.0>},
              {id,vbucket_map_mirror},
              {mfargs,{vbucket_map_mirror,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.170Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2555.0>},
              {id,capi_url_cache},
              {mfargs,{capi_url_cache,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.171Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2557.0>},
              {id,bucket_info_cache},
              {mfargs,{bucket_info_cache,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.171Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2560.0>},
              {id,ns_tick_event},
              {mfargs,{gen_event,start_link,[{local,ns_tick_event}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.171Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2561.0>},
              {id,buckets_events},
              {mfargs,{gen_event,start_link,[{local,buckets_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.171Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2562.0>},
              {id,ns_stats_event},
              {mfargs,{gen_event,start_link,[{local,ns_stats_event}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.171Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2563.0>},
              {id,samples_loader_tasks},
              {mfargs,{samples_loader_tasks,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.171Z,ns_1@db2.lan:ns_heart_sup<0.2564.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_heart_sup}
    started: [{pid,<0.2565.0>},
              {id,ns_heart},
              {mfargs,{ns_heart,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.171Z,ns_1@db2.lan:ns_heart_sup<0.2564.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_heart_sup}
    started: [{pid,<0.2567.0>},
              {id,ns_heart_slow_updater},
              {mfargs,{ns_heart,start_link_slow_updater,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.171Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2564.0>},
              {id,ns_heart_sup},
              {mfargs,{ns_heart_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:25.171Z,ns_1@db2.lan:ns_doctor_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_doctor_sup}
    started: [{pid,<0.2570.0>},
              {id,ns_doctor_events},
              {mfargs,{gen_event,start_link,[{local,ns_doctor_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.172Z,ns_1@db2.lan:ns_doctor_sup<0.2569.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_doctor_sup}
    started: [{pid,<0.2571.0>},
              {id,ns_doctor},
              {mfargs,{ns_doctor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:25.172Z,ns_1@db2.lan:<0.2568.0>:restartable:start_child:92]Started child process <0.2569.0>
  MFA: {ns_doctor_sup,start_link,[]}
[error_logger:info,2025-05-15T18:47:25.172Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2568.0>},
              {id,ns_doctor_sup},
              {mfargs,{restartable,start_link,
                                   [{ns_doctor_sup,start_link,[]},infinity]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:25.172Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2578.0>},
              {id,master_activity_events},
              {mfargs,{gen_event,start_link,[{local,master_activity_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.172Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2579.0>},
              {id,xdcr_ckpt_store},
              {mfargs,{simple_store,start_link,[xdcr_ckpt_data]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.173Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2581.0>},
              {id,metakv_worker},
              {mfargs,{work_queue,start_link,[metakv_worker]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:error,2025-05-15T18:47:25.173Z,ns_1@db2.lan:<0.2580.0>:prometheus:post_async:217]Prometheus http request failed:
URL: http://127.0.0.1:9123/api/v1/query
Body: query=%7Bname%3D~%60kv_curr_items%7Ckv_curr_items_tot%7Ckv_mem_used_bytes%7Ccouch_docs_actual_disk_size%7Ccouch_views_actual_disk_size%7Ckv_ep_db_data_size_bytes%7Ckv_ep_bg_fetched%60%7D+or+kv_vb_curr_items%7Bstate%3D%27replica%27%7D+or+kv_vb_num_non_resident%7Bstate%3D%27active%27%7D+or+label_replace%28sum+by+%28bucket%2C+name%29+%28irate%28kv_ops%7Bop%3D%60get%60%7D%5B1m%5D%29%29%2C+%60name%60%2C%60cmd_get%60%2C+%60%60%2C+%60%60%29+or+label_replace%28irate%28kv_ops%7Bop%3D%60get%60%2Cresult%3D%60hit%60%7D%5B1m%5D%29%2C%60name%60%2C%60get_hits%60%2C%60%60%2C%60%60%29+or+label_replace%28sum+by+%28bucket%29+%28irate%28kv_cmd_lookup%5B1m%5D%29+or+irate%28kv_ops%7Bop%3D~%60set%7Cincr%7Cdecr%7Cdelete%7Cdel_meta%7Cget_meta%7Cset_meta%7Cset_ret_meta%7Cdel_ret_meta%60%7D%5B1m%5D%29%29%2C+%60name%60%2C+%60ops%60%2C+%60%60%2C+%60%60%29+or+sum+by+%28bucket%2C+name%29+%28%7Bname%3D~%60index_data_size%7Cindex_disk_size%7Ccouch_spatial_data_size%7Ccouch_spatial_disk_size%7Ccouch_views_data_size%60%7D%29&timeout=5s
Reason: {failed_connect,[{to_address,{"127.0.0.1",9123}},
                         {inet,[inet],econnrefused}]}
[error_logger:info,2025-05-15T18:47:25.173Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2582.0>},
              {id,index_events},
              {mfargs,{gen_event,start_link,[{local,index_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.174Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2585.0>},
              {id,index_settings_manager},
              {mfargs,{index_settings_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.175Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2587.0>},
              {id,query_settings_manager},
              {mfargs,{query_settings_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.175Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2590.0>},
              {id,eventing_settings_manager},
              {mfargs,{eventing_settings_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.175Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2592.0>},
              {id,analytics_settings_manager},
              {mfargs,{analytics_settings_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:error,2025-05-15T18:47:25.175Z,ns_1@db2.lan:<0.2589.0>:prometheus:post_async:217]Prometheus http request failed:
URL: http://127.0.0.1:9123/api/v1/query
Body: query=%7Bcategory%3D%60system-processes%60%2Cname%3D~%60sysproc_mem_resident%7Csysproc_mem_size%7Csysproc_cpu_utilization%7Csysproc_major_faults_raw%60%7D&timeout=5s
Reason: {failed_connect,[{to_address,{"127.0.0.1",9123}},
                         {inet,[inet],econnrefused}]}
[error_logger:info,2025-05-15T18:47:25.176Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2598.0>},
              {id,audit_events},
              {mfargs,{gen_event,start_link,[{local,audit_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:25.176Z,ns_1@db2.lan:<0.2602.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,
                                    encryption_service} with delay of 1000
[ns_server:error,2025-05-15T18:47:25.176Z,ns_1@db2.lan:<0.2603.0>:prometheus:post_async:217]Prometheus http request failed:
URL: http://127.0.0.1:9123/api/v1/query
Body: query=%7Bcategory%3D%60system%60%2Cname%3D~%60sys_cpu_utilization_rate%7Csys_cpu_stolen_rate%7Csys_swap_total%7Csys_swap_used%7Csys_mem_total%7Csys_mem_free%7Csys_mem_limit%7Csys_cpu_cores_available%7Csys_allocstall%60%7D&timeout=5s
Reason: {failed_connect,[{to_address,{"127.0.0.1",9123}},
                         {inet,[inet],econnrefused}]}
[ns_server:debug,2025-05-15T18:47:25.177Z,ns_1@db2.lan:encryption_service<0.2605.0>:encryption_service:recover:199]Config marker /opt/couchbase/var/lib/couchbase/config/sm_load_config_marker doesn't exist
[error_logger:info,2025-05-15T18:47:25.177Z,ns_1@db2.lan:<0.2604.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2604.0>,suppress_max_restart_intensity}
    started: [{pid,<0.2605.0>},
              {id,encryption_service},
              {mfargs,{encryption_service,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.177Z,ns_1@db2.lan:<0.2601.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2601.0>,suppress_max_restart_intensity}
    started: [{pid,<0.2602.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      encryption_service}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,encryption_service},
                       1000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 1,id => encryption_service,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [encryption_service],
                          restart => permanent,shutdown => 5000,
                          start => {encryption_service,start_link,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:25.177Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2601.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,encryption_service}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 1,id => encryption_service,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [encryption_service],
                         restart => permanent,shutdown => 5000,
                         start => {encryption_service,start_link,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:25.177Z,ns_1@db2.lan:menelaus_sup<0.2608.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.2609.0>},
              {id,menelaus_ui_auth},
              {mfargs,{menelaus_ui_auth,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.177Z,ns_1@db2.lan:menelaus_sup<0.2608.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.2611.0>},
              {id,scram_sha},
              {mfargs,{scram_sha,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.178Z,ns_1@db2.lan:menelaus_sup<0.2608.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.2612.0>},
              {id,menelaus_local_auth},
              {mfargs,{menelaus_local_auth,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.179Z,ns_1@db2.lan:menelaus_sup<0.2608.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.2613.0>},
              {id,menelaus_web_cache},
              {mfargs,{menelaus_web_cache,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.179Z,ns_1@db2.lan:menelaus_sup<0.2608.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.2614.0>},
              {id,menelaus_stats_gatherer},
              {mfargs,{menelaus_stats_gatherer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.179Z,ns_1@db2.lan:menelaus_sup<0.2608.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.2615.0>},
              {id,json_rpc_events},
              {mfargs,{gen_event,start_link,[{local,json_rpc_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.180Z,ns_1@db2.lan:menelaus_web_sup<0.2616.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_web_sup}
    started: [{pid,<0.2617.0>},
              {id,menelaus_event},
              {mfargs,{menelaus_event,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:47:25.180Z,ns_1@db2.lan:<0.2619.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2025-05-15T18:47:25.180Z,ns_1@db2.lan:<0.2619.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2025-05-15T18:47:25.181Z,ns_1@db2.lan:<0.2619.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:debug,2025-05-15T18:47:25.181Z,ns_1@db2.lan:ns_heart<0.2565.0>:cluster_logs_collection_task:maybe_build_cluster_logs_task:40]Ignoring exception trying to read cluster_logs_collection_task_status table: error:badarg
[ns_server:info,2025-05-15T18:47:25.183Z,ns_1@db2.lan:<0.2619.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2025-05-15T18:47:25.183Z,ns_1@db2.lan:<0.2619.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2025-05-15T18:47:25.184Z,ns_1@db2.lan:<0.2619.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"0.0.0.0"},{port,8091}]
[error_logger:info,2025-05-15T18:47:25.184Z,ns_1@db2.lan:<0.2619.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2619.0>,menelaus_web}
    started: [{pid,<0.2620.0>},
              {id,menelaus_web_ipv4},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet},
                        {name,menelaus_web},
                        {port,8091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:47:25.184Z,ns_1@db2.lan:<0.2619.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2025-05-15T18:47:25.185Z,ns_1@db2.lan:<0.2619.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2025-05-15T18:47:25.185Z,ns_1@db2.lan:<0.2619.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2025-05-15T18:47:25.185Z,ns_1@db2.lan:<0.2619.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2025-05-15T18:47:25.185Z,ns_1@db2.lan:<0.2619.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2025-05-15T18:47:25.186Z,ns_1@db2.lan:<0.2619.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"::"},{port,8091}]
[error_logger:info,2025-05-15T18:47:25.186Z,ns_1@db2.lan:<0.2619.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2619.0>,menelaus_web}
    started: [{pid,<0.2638.0>},
              {id,menelaus_web_ipv6},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet6},
                        {name,menelaus_web},
                        {port,8091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:25.186Z,ns_1@db2.lan:<0.2618.0>:restartable:start_child:92]Started child process <0.2619.0>
  MFA: {menelaus_web,start_link,[]}
[error_logger:info,2025-05-15T18:47:25.186Z,ns_1@db2.lan:menelaus_web_sup<0.2616.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_web_sup}
    started: [{pid,<0.2618.0>},
              {id,menelaus_web},
              {mfargs,{restartable,start_link,
                                   [{menelaus_web,start_link,[]},infinity]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[user:info,2025-05-15T18:47:25.186Z,ns_1@db2.lan:menelaus_sup<0.2608.0>:menelaus_web_sup:start_link:38]Couchbase Server has started on web port 8091 on node 'ns_1@db2.lan'. Version: "7.6.2-3721-enterprise".
[error_logger:info,2025-05-15T18:47:25.186Z,ns_1@db2.lan:menelaus_sup<0.2608.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.2616.0>},
              {id,menelaus_web_sup},
              {mfargs,{menelaus_web_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:25.186Z,ns_1@db2.lan:menelaus_sup<0.2608.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.2658.0>},
              {id,menelaus_web_alerts_srv},
              {mfargs,{menelaus_web_alerts_srv,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.186Z,ns_1@db2.lan:menelaus_sup<0.2608.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.2661.0>},
              {id,guardrail_monitor},
              {mfargs,{guardrail_monitor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.187Z,ns_1@db2.lan:menelaus_sup<0.2608.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.2662.0>},
              {id,menelaus_cbauth},
              {mfargs,{menelaus_cbauth,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.187Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2608.0>},
              {id,menelaus},
              {mfargs,{menelaus_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:47:25.187Z,ns_1@db2.lan:<0.2669.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,ns_ports_setup} with delay of 4000
[error_logger:info,2025-05-15T18:47:25.187Z,ns_1@db2.lan:<0.2670.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2670.0>,suppress_max_restart_intensity}
    started: [{pid,<0.2671.0>},
              {id,ns_ports_setup},
              {mfargs,{ns_ports_setup,start,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.187Z,ns_1@db2.lan:<0.2668.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2668.0>,suppress_max_restart_intensity}
    started: [{pid,<0.2669.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      ns_ports_setup}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,ns_ports_setup},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => ns_ports_setup,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [],restart => permanent,
                          shutdown => brutal_kill,
                          start => {ns_ports_setup,start,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:25.187Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2668.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,ns_ports_setup}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => ns_ports_setup,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [],restart => permanent,
                         shutdown => brutal_kill,
                         start => {ns_ports_setup,start,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:25.187Z,ns_1@db2.lan:service_agent_sup<0.2674.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_agent_sup}
    started: [{pid,<0.2675.0>},
              {id,service_agent_children_sup},
              {mfargs,{supervisor,start_link,
                                  [{local,service_agent_children_sup},
                                   service_agent_sup,child]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:25.188Z,ns_1@db2.lan:service_agent_sup<0.2674.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_agent_sup}
    started: [{pid,<0.2676.0>},
              {id,service_agent_worker},
              {mfargs,{erlang,apply,[#Fun<service_agent_sup.0.125430937>,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.188Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2674.0>},
              {id,service_agent_sup},
              {mfargs,{service_agent_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:25.188Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2678.0>},
              {id,ns_memcached_sockets_pool},
              {mfargs,{ns_memcached_sockets_pool,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:25.188Z,ns_1@db2.lan:memcached_auth_server<0.2679.0>:memcached_auth_server:reconnect:239]Skipping creation of 'Auth provider' connection because external users are disabled
[error_logger:info,2025-05-15T18:47:25.188Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2679.0>},
              {id,memcached_auth_server},
              {mfargs,{memcached_auth_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:25.188Z,ns_1@db2.lan:<0.2682.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,ns_audit_cfg} with delay of 4000
[ns_server:debug,2025-05-15T18:47:25.188Z,ns_1@db2.lan:ns_audit_cfg<0.2684.0>:ns_audit_cfg:write_audit_json:238]Writing new content to "/opt/couchbase/var/lib/couchbase/config/audit.json", Params [{descriptors_path,
                                                                                      "/opt/couchbase/etc/security"},
                                                                                     {version,
                                                                                      2},
                                                                                     {uuid,
                                                                                      "33985209"},
                                                                                     {event_states,
                                                                                      {[]}},
                                                                                     {filtering_enabled,
                                                                                      true},
                                                                                     {disabled_userids,
                                                                                      []},
                                                                                     {auditd_enabled,
                                                                                      false},
                                                                                     {log_path,
                                                                                      "/opt/couchbase/var/lib/couchbase/logs"},
                                                                                     {prune_age,
                                                                                      0},
                                                                                     {rotate_interval,
                                                                                      86400},
                                                                                     {rotate_size,
                                                                                      20971520},
                                                                                     {sync,
                                                                                      []}]
[ns_server:debug,2025-05-15T18:47:25.189Z,ns_1@db2.lan:ns_ports_setup<0.2671.0>:ns_ports_manager:set_dynamic_children:48]Setting children [memcached,saslauthd_port,goxdcr]
[ns_server:debug,2025-05-15T18:47:25.192Z,ns_1@db2.lan:ns_audit_cfg<0.2684.0>:ns_audit_cfg:notify_memcached:152]Instruct memcached to reload audit config
[error_logger:info,2025-05-15T18:47:25.192Z,ns_1@db2.lan:<0.2683.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2683.0>,suppress_max_restart_intensity}
    started: [{pid,<0.2684.0>},
              {id,ns_audit_cfg},
              {mfargs,{ns_audit_cfg,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:error,2025-05-15T18:47:25.192Z,ns_1@db2.lan:<0.2696.0>:prometheus:post_async:217]Prometheus http request failed:
URL: http://127.0.0.1:9123/api/v1/query
Body: query=%7Bname%3D~%60kv_curr_items%7Ckv_curr_items_tot%7Ckv_mem_used_bytes%7Ccouch_docs_actual_disk_size%7Ccouch_views_actual_disk_size%7Ckv_ep_db_data_size_bytes%7Ckv_ep_bg_fetched%60%7D+or+kv_vb_curr_items%7Bstate%3D%27replica%27%7D+or+kv_vb_num_non_resident%7Bstate%3D%27active%27%7D+or+label_replace%28sum+by+%28bucket%2C+name%29+%28irate%28kv_ops%7Bop%3D%60get%60%7D%5B1m%5D%29%29%2C+%60name%60%2C%60cmd_get%60%2C+%60%60%2C+%60%60%29+or+label_replace%28irate%28kv_ops%7Bop%3D%60get%60%2Cresult%3D%60hit%60%7D%5B1m%5D%29%2C%60name%60%2C%60get_hits%60%2C%60%60%2C%60%60%29+or+label_replace%28sum+by+%28bucket%29+%28irate%28kv_cmd_lookup%5B1m%5D%29+or+irate%28kv_ops%7Bop%3D~%60set%7Cincr%7Cdecr%7Cdelete%7Cdel_meta%7Cget_meta%7Cset_meta%7Cset_ret_meta%7Cdel_ret_meta%60%7D%5B1m%5D%29%29%2C+%60name%60%2C+%60ops%60%2C+%60%60%2C+%60%60%29+or+sum+by+%28bucket%2C+name%29+%28%7Bname%3D~%60index_data_size%7Cindex_disk_size%7Ccouch_spatial_data_size%7Ccouch_spatial_disk_size%7Ccouch_views_data_size%60%7D%29&timeout=5s
Reason: {failed_connect,[{to_address,{"127.0.0.1",9123}},
                         {inet,[inet],econnrefused}]}
[error_logger:info,2025-05-15T18:47:25.192Z,ns_1@db2.lan:<0.2681.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2681.0>,suppress_max_restart_intensity}
    started: [{pid,<0.2682.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      ns_audit_cfg}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,ns_audit_cfg},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => ns_audit_cfg,inherited_max_r => 20,
                          inherited_max_t => 10,modules => [],
                          restart => permanent,shutdown => 1000,
                          start => {ns_audit_cfg,start_link,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:25.192Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2681.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,ns_audit_cfg}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => ns_audit_cfg,inherited_max_r => 20,
                         inherited_max_t => 10,modules => [],
                         restart => permanent,shutdown => 1000,
                         start => {ns_audit_cfg,start_link,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:47:25.192Z,ns_1@db2.lan:<0.2702.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,ns_audit} with delay of 4000
[ns_server:error,2025-05-15T18:47:25.193Z,ns_1@db2.lan:<0.2706.0>:prometheus:post_async:217]Prometheus http request failed:
URL: http://127.0.0.1:9123/api/v1/query
Body: query=%7Bcategory%3D%60system-processes%60%2Cname%3D~%60sysproc_mem_resident%7Csysproc_mem_size%7Csysproc_cpu_utilization%7Csysproc_major_faults_raw%60%7D&timeout=5s
Reason: {failed_connect,[{to_address,{"127.0.0.1",9123}},
                         {inet,[inet],econnrefused}]}
[error_logger:info,2025-05-15T18:47:25.193Z,ns_1@db2.lan:<0.2703.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2703.0>,suppress_max_restart_intensity}
    started: [{pid,<0.2704.0>},
              {id,ns_audit},
              {mfargs,{ns_audit,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.193Z,ns_1@db2.lan:<0.2701.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2701.0>,suppress_max_restart_intensity}
    started: [{pid,<0.2702.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      ns_audit}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,ns_audit},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => ns_audit,inherited_max_r => 20,
                          inherited_max_t => 10,modules => [],
                          restart => permanent,shutdown => 1000,
                          start => {ns_audit,start_link,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:25.193Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2701.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,ns_audit}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => ns_audit,inherited_max_r => 20,
                         inherited_max_t => 10,modules => [],
                         restart => permanent,shutdown => 1000,
                         start => {ns_audit,start_link,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:47:25.193Z,ns_1@db2.lan:<0.2711.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,
                                    memcached_config_mgr} with delay of 4000
[ns_server:debug,2025-05-15T18:47:25.193Z,ns_1@db2.lan:memcached_config_mgr<0.2713.0>:memcached_config_mgr:memcached_port_pid:154]waiting for completion of initial ns_ports_setup round
[ns_server:error,2025-05-15T18:47:25.193Z,ns_1@db2.lan:<0.2709.0>:prometheus:post_async:217]Prometheus http request failed:
URL: http://127.0.0.1:9123/api/v1/query
Body: query=%7Bcategory%3D%60system%60%2Cname%3D~%60sys_cpu_utilization_rate%7Csys_cpu_stolen_rate%7Csys_swap_total%7Csys_swap_used%7Csys_mem_total%7Csys_mem_free%7Csys_mem_limit%7Csys_cpu_cores_available%7Csys_allocstall%60%7D&timeout=5s
Reason: {failed_connect,[{to_address,{"127.0.0.1",9123}},
                         {inet,[inet],econnrefused}]}
[error_logger:info,2025-05-15T18:47:25.193Z,ns_1@db2.lan:<0.2712.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2712.0>,suppress_max_restart_intensity}
    started: [{pid,<0.2713.0>},
              {id,memcached_config_mgr},
              {mfargs,{memcached_config_mgr,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.193Z,ns_1@db2.lan:<0.2710.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2710.0>,suppress_max_restart_intensity}
    started: [{pid,<0.2711.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      memcached_config_mgr}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,memcached_config_mgr},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => memcached_config_mgr,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [],restart => permanent,shutdown => 1000,
                          start => {memcached_config_mgr,start_link,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:25.193Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2710.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,memcached_config_mgr}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => memcached_config_mgr,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [],restart => permanent,shutdown => 1000,
                         start => {memcached_config_mgr,start_link,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:info,2025-05-15T18:47:25.194Z,ns_1@db2.lan:<0.2715.0>:ns_memcached_log_rotator:init:36]Starting log rotator on "/opt/couchbase/var/lib/couchbase/logs"/"memcached.log"* with an initial period of 39003ms
[ns_server:debug,2025-05-15T18:47:25.194Z,ns_1@db2.lan:ns_heart_slow_status_updater<0.2567.0>:cluster_logs_collection_task:maybe_build_cluster_logs_task:40]Ignoring exception trying to read cluster_logs_collection_task_status table: error:badarg
[error_logger:info,2025-05-15T18:47:25.194Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2715.0>},
              {id,ns_memcached_log_rotator},
              {mfargs,{ns_memcached_log_rotator,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.195Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2716.0>},
              {id,testconditions_store},
              {mfargs,{simple_store,start_link,[testconditions]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:25.195Z,ns_1@db2.lan:<0.2717.0>:memcached_config_mgr:memcached_port_pid:154]waiting for completion of initial ns_ports_setup round
[error_logger:info,2025-05-15T18:47:25.195Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2717.0>},
              {id,terse_cluster_info_uploader},
              {mfargs,{terse_cluster_info_uploader,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.195Z,ns_1@db2.lan:ns_bucket_worker_sup<0.2719.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_bucket_worker_sup}
    started: [{pid,<0.2720.0>},
              {id,ns_bucket_sup},
              {mfargs,{ns_bucket_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:25.196Z,ns_1@db2.lan:ns_bucket_worker_sup<0.2719.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_bucket_worker_sup}
    started: [{pid,<0.2721.0>},
              {id,ns_bucket_worker},
              {mfargs,{ns_bucket_worker,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.196Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2719.0>},
              {id,ns_bucket_worker_sup},
              {mfargs,{ns_bucket_worker_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:47:25.196Z,ns_1@db2.lan:<0.2714.0>:memcached_refresh:do_refresh:153]Successfully connected to memcached, Trying to refresh [isasl,rbac]
[error_logger:info,2025-05-15T18:47:25.196Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2725.0>},
              {id,ns_server_stats},
              {mfargs,{ns_server_stats,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.196Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2728.0>},
              {id,{stats_reader,"@system"}},
              {mfargs,{stats_reader,start_link,["@system"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:25.197Z,ns_1@db2.lan:<0.2727.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ale_stats_events,<0.2725.0>} exited with reason {noproc,
                                                                                 {gen_statem,
                                                                                  call,
                                                                                  [mb_master,
                                                                                   master_node,
                                                                                   infinity]}}
[error_logger:info,2025-05-15T18:47:25.197Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2732.0>},
              {id,{stats_reader,"@system-processes"}},
              {mfargs,{stats_reader,start_link,["@system-processes"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.197Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2735.0>},
              {id,{stats_reader,"@query"}},
              {mfargs,{stats_reader,start_link,["@query"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.197Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2737.0>},
              {id,{stats_reader,"@global"}},
              {mfargs,{stats_reader,start_link,["@global"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.197Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2739.0>},
              {id,goxdcr_status_keeper},
              {mfargs,{goxdcr_status_keeper,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.197Z,ns_1@db2.lan:services_stats_sup<0.2740.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,services_stats_sup}
    started: [{pid,<0.2741.0>},
              {id,service_stats_children_sup},
              {mfargs,{supervisor,start_link,
                                  [{local,service_stats_children_sup},
                                   services_stats_sup,child]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:25.197Z,ns_1@db2.lan:service_status_keeper_sup<0.2742.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_status_keeper_sup}
    started: [{pid,<0.2743.0>},
              {id,service_status_keeper_worker},
              {mfargs,{work_queue,start_link,[service_status_keeper_worker]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.197Z,ns_1@db2.lan:service_status_keeper_sup<0.2742.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_status_keeper_sup}
    started: [{pid,<0.2744.0>},
              {id,service_status_keeper_index},
              {mfargs,{service_index,start_keeper,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.197Z,ns_1@db2.lan:service_status_keeper_sup<0.2742.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_status_keeper_sup}
    started: [{pid,<0.2747.0>},
              {id,service_status_keeper_fts},
              {mfargs,{service_fts,start_keeper,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.198Z,ns_1@db2.lan:service_status_keeper_sup<0.2742.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_status_keeper_sup}
    started: [{pid,<0.2751.0>},
              {id,service_status_keeper_eventing},
              {mfargs,{service_eventing,start_keeper,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.198Z,ns_1@db2.lan:services_stats_sup<0.2740.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,services_stats_sup}
    started: [{pid,<0.2742.0>},
              {id,service_status_keeper_sup},
              {mfargs,{service_status_keeper_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:25.198Z,ns_1@db2.lan:services_stats_sup<0.2740.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,services_stats_sup}
    started: [{pid,<0.2755.0>},
              {id,service_stats_worker},
              {mfargs,{erlang,apply,[#Fun<services_stats_sup.0.35188242>,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.198Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2740.0>},
              {id,services_stats_sup},
              {mfargs,{services_stats_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:47:25.199Z,ns_1@db2.lan:<0.2760.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,
                                    compaction_daemon} with delay of 4000
[ns_server:debug,2025-05-15T18:47:25.199Z,ns_1@db2.lan:<0.2764.0>:new_concurrency_throttle:init:109]init concurrent throttle process, pid: <0.2764.0>, type: kv_throttle# of available token: 1
[ns_server:debug,2025-05-15T18:47:25.199Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2025-05-15T18:47:25.199Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[error_logger:info,2025-05-15T18:47:25.199Z,ns_1@db2.lan:<0.2761.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2761.0>,suppress_max_restart_intensity}
    started: [{pid,<0.2762.0>},
              {id,compaction_daemon},
              {mfargs,{compaction_daemon,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,86400000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.200Z,ns_1@db2.lan:<0.2759.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2759.0>,suppress_max_restart_intensity}
    started: [{pid,<0.2760.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      compaction_daemon}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,compaction_daemon},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => compaction_daemon,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [compaction_daemon],
                          restart => permanent,shutdown => 86400000,
                          start => {compaction_daemon,start_link,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:25.200Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2759.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,compaction_daemon}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => compaction_daemon,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [compaction_daemon],
                         restart => permanent,shutdown => 86400000,
                         start => {compaction_daemon,start_link,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:25.200Z,ns_1@db2.lan:cluster_logs_sup<0.2767.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,cluster_logs_sup}
    started: [{pid,<0.2768.0>},
              {id,ets_holder},
              {mfargs,{cluster_logs_collection_task,start_link_ets_holder,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:25.199Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_views. Rescheduling compaction.
[error_logger:info,2025-05-15T18:47:25.200Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2767.0>},
              {id,cluster_logs_sup},
              {mfargs,{cluster_logs_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:47:25.200Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:47:25.200Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_master. Rescheduling compaction.
[error_logger:info,2025-05-15T18:47:25.200Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2769.0>},
              {id,collections},
              {mfargs,{collections,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:25.200Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_master too soon. Next run will be in 3600s
[error_logger:info,2025-05-15T18:47:25.200Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2771.0>},
              {id,leader_events},
              {mfargs,{gen_event,start_link,[{local,leader_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.200Z,ns_1@db2.lan:leader_leases_sup<0.2774.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_leases_sup}
    started: [{pid,<0.2775.0>},
              {id,leader_activities},
              {mfargs,{leader_activities,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,10000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:25.201Z,ns_1@db2.lan:memcached_refresh<0.2369.0>:memcached_refresh:update_refresh_state:116]Refresh of [isasl,rbac] succeeded
[ns_server:warn,2025-05-15T18:47:25.203Z,ns_1@db2.lan:leader_lease_agent<0.2776.0>:leader_lease_agent:maybe_recover_persisted_lease:393]Found persisted lease [{node,'ns_1@cb.local'},
                       {uuid,<<"445e053598a21dcd845301efeef2f950">>},
                       {time_left,14284},
                       {status,active}]
[error_logger:info,2025-05-15T18:47:25.203Z,ns_1@db2.lan:leader_leases_sup<0.2774.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_leases_sup}
    started: [{pid,<0.2776.0>},
              {id,leader_lease_agent},
              {mfargs,{leader_lease_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.203Z,ns_1@db2.lan:leader_services_sup<0.2773.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_services_sup}
    started: [{pid,<0.2774.0>},
              {id,leader_leases_sup},
              {mfargs,{leader_leases_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:25.203Z,ns_1@db2.lan:leader_registry_sup<0.2780.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_registry_sup}
    started: [{pid,<0.2781.0>},
              {id,leader_registry},
              {mfargs,{leader_registry,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:25.203Z,ns_1@db2.lan:leader_registry_sup<0.2780.0>:mb_master:check_master_takeover_needed:183]Sending master node question to the following nodes: ['ns_1@172.19.0.4']
[ns_server:debug,2025-05-15T18:47:25.204Z,ns_1@db2.lan:leader_registry_sup<0.2780.0>:mb_master:check_master_takeover_needed:187]Got replies: ['ns_1@172.19.0.4']
[ns_server:debug,2025-05-15T18:47:25.204Z,ns_1@db2.lan:leader_registry_sup<0.2780.0>:mb_master:check_master_takeover_needed:203]Checking version of current master: 'ns_1@172.19.0.4'
[ns_server:debug,2025-05-15T18:47:25.204Z,ns_1@db2.lan:leader_registry_sup<0.2780.0>:mb_master:check_master_takeover_needed:219]Current master's compat version: [7,6,0] services [index,kv,n1ql]. This node's compat version: {[7,
                                                                                                 6,
                                                                                                 0],
                                                                                                release,
                                                                                                0} services [index,
                                                                                                             kv,
                                                                                                             n1ql]
[ns_server:debug,2025-05-15T18:47:25.204Z,ns_1@db2.lan:leader_registry_sup<0.2780.0>:mb_master:check_master_takeover_needed:233]Current master is strongly higher priority, not taking over
[ns_server:debug,2025-05-15T18:47:25.204Z,ns_1@db2.lan:mb_master<0.2783.0>:mb_master:init:86]Heartbeat interval is 2000
[ns_server:debug,2025-05-15T18:47:25.204Z,ns_1@db2.lan:mb_master<0.2783.0>:mb_master:init:102]Starting as candidate. Peers: ['ns_1@172.19.0.4','ns_1@db2.lan']
[error_logger:info,2025-05-15T18:47:25.204Z,ns_1@db2.lan:leader_registry_sup<0.2780.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_registry_sup}
    started: [{pid,<0.2783.0>},
              {id,mb_master},
              {mfargs,{mb_master,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:25.204Z,ns_1@db2.lan:leader_services_sup<0.2773.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_services_sup}
    started: [{pid,<0.2780.0>},
              {id,leader_registry_sup},
              {mfargs,{leader_registry_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:47:25.204Z,ns_1@db2.lan:<0.2772.0>:restartable:start_child:92]Started child process <0.2773.0>
  MFA: {leader_services_sup,start_link,[]}
[error_logger:info,2025-05-15T18:47:25.205Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2772.0>},
              {id,leader_services_sup},
              {mfargs,{restartable,start_link,
                                   [{leader_services_sup,start_link,[]},
                                    infinity]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:25.205Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2785.0>},
              {id,ns_tick_agent},
              {mfargs,{ns_tick_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.205Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2787.0>},
              {id,master_activity_events_ingress},
              {mfargs,{gen_event,start_link,
                                 [{local,master_activity_events_ingress}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.205Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2788.0>},
              {id,master_activity_events_timestamper},
              {mfargs,{master_activity_events,start_link_timestamper,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.205Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2789.0>},
              {id,master_activity_events_pids_watcher},
              {mfargs,{master_activity_events_pids_watcher,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.205Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2790.0>},
              {id,master_activity_events_keeper},
              {mfargs,{master_activity_events_keeper,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.205Z,ns_1@db2.lan:health_monitor_sup<0.2792.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.2793.0>},
              {id,ns_server_monitor},
              {mfargs,{ns_server_monitor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.205Z,ns_1@db2.lan:health_monitor_sup<0.2792.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.2797.0>},
              {id,service_monitor_children_sup},
              {mfargs,{supervisor,start_link,
                                  [{local,service_monitor_children_sup},
                                   health_monitor_sup,child]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:25.206Z,ns_1@db2.lan:health_monitor_sup<0.2792.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.2802.0>},
              {id,service_monitor_worker},
              {mfargs,{erlang,apply,[#Fun<health_monitor_sup.0.30770475>,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.206Z,ns_1@db2.lan:health_monitor_sup<0.2792.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.2804.0>},
              {id,node_monitor},
              {mfargs,{node_monitor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.206Z,ns_1@db2.lan:health_monitor_sup<0.2792.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.2811.0>},
              {id,node_status_analyzer},
              {mfargs,{node_status_analyzer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.206Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2792.0>},
              {id,health_monitor_sup},
              {mfargs,{health_monitor_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:25.206Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2814.0>},
              {id,rebalance_agent},
              {mfargs,{rebalance_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.206Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2815.0>},
              {id,kv_hibernation_agent},
              {mfargs,{kv_hibernation_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.208Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2816.0>},
              {id,ns_rebalance_report_manager},
              {mfargs,{ns_rebalance_report_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:25.208Z,ns_1@db2.lan:cb_creds_rotation<0.2819.0>:cb_creds_rotation:start_rotate_timer:214]Starting creds rotation timer (1800000ms)
[error_logger:info,2025-05-15T18:47:25.208Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2819.0>},
              {id,creds_rotation},
              {mfargs,{cb_creds_rotation,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:25.208Z,ns_1@db2.lan:ns_server_nodes_sup<0.2356.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.2477.0>},
              {id,ns_server_sup},
              {mfargs,{ns_server_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:47:25.208Z,ns_1@db2.lan:ns_server_nodes_sup<0.2356.0>:one_shot_barrier:notify:21]Notifying on barrier menelaus_barrier
[error_logger:error,2025-05-15T18:47:25.208Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {local,ns_server_sup}
    errorContext: child_terminated
    reason: {noproc,{gen_statem,call,[mb_master,master_node,infinity]}}
    offender: [{pid,<0.2725.0>},
               {id,ns_server_stats},
               {mfargs,{ns_server_stats,start_link,[]}},
               {restart_type,permanent},
               {significant,false},
               {shutdown,1000},
               {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:25.208Z,ns_1@db2.lan:menelaus_barrier<0.2363.0>:one_shot_barrier:barrier_body:56]Barrier menelaus_barrier got notification from <0.2356.0>
[ns_server:debug,2025-05-15T18:47:25.208Z,ns_1@db2.lan:ns_server_nodes_sup<0.2356.0>:one_shot_barrier:notify:26]Successfuly notified on barrier menelaus_barrier
[ns_server:debug,2025-05-15T18:47:25.208Z,ns_1@db2.lan:<0.2355.0>:restartable:start_child:92]Started child process <0.2356.0>
  MFA: {ns_server_nodes_sup,start_link,[]}
[error_logger:info,2025-05-15T18:47:25.208Z,ns_1@db2.lan:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.2355.0>},
              {id,ns_server_nodes_sup},
              {mfargs,{restartable,start_link,
                                   [{ns_server_nodes_sup,start_link,[]},
                                    infinity]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:25.208Z,ns_1@db2.lan:ns_server_sup<0.2477.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2821.0>},
              {id,ns_server_stats},
              {mfargs,{ns_server_stats,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[user:info,2025-05-15T18:47:25.209Z,ns_1@db2.lan:ns_cluster<0.273.0>:ns_cluster:perform_actual_join:1660]Node ns_1@db2.lan joined cluster
[cluster:debug,2025-05-15T18:47:25.210Z,ns_1@db2.lan:ns_cluster<0.273.0>:ns_cluster:handle_call:433]complete_join([{<<"targetNode">>,<<"ns_1@db2.lan">>},
               {<<"requestedServices">>,[<<"index">>,<<"kv">>,<<"n1ql">>]},
               {<<"chronicleInfo">>,
                <<"g3QAAAAEZAAPY29tbWl0dGVkX3NlcW5vYRFkAA5jb21wYXRfdmVyc2lvbmEAZAAGY29uZmlnaAVkAAlsb2dfZW50cnltAAAAIDVhYWIwM2RiZWNhZDA2YjUwZmZiNDc0ZGE1OWEwMGM5aAJhBGQAD25zXzFAMTcyLjE5LjAuNGERaApkAAZjb25maWdoA20AAAAgMWRkNTk5MTQ2M2Q1MTllMjM3MjlkZGEyYzgwYzM3NmJhAGEBYQBtAAAAIDQ5ZDU3NjJlMjg3Nzg2YzMxMjc5MDU1MTBmN2E0MTE1dAAAAAJkAA9uc18xQDE3Mi4xOS4wLjR0AAAAAmQAAmlkbQAAACAxZGQ1OTkxNDYzZDUxOWUyMzcyOWRkYTJjODBjMzc2YmQABHJvbGVkAAV2b3RlcmQADG5zXzFAZGIyLmxhbnQAAAACZAACaWRtAAAAIDlmNjg5NGI1MWZmYTQ5N2NkZmM4Yjc5MmQ5ZTY3MjZiZAAEcm9sZWQAB3JlcGxpY2FkAAl1bmRlZmluZWR0AAAAAmQAFGNocm9uaWNsZV9jb25maWdfcnNtaANkAApyc21fY29uZmlnZAAUY2hyb25pY2xlX2NvbmZpZ19yc21qZAACa3ZoA2QACnJzbV9jb25maWdkAAxjaHJvbmljbGVfa3ZqdAAAAABkAAl1bmRlZmluZWRsAAAAAWgCbQAAACA1YWFiMDNkYmVjYWQwNmI1MGZmYjQ3NGRhNTlhMDBjOWEAamQACmhpc3RvcnlfaWRtAAAAIDVhYWIwM2RiZWNhZDA2YjUwZmZiNDc0ZGE1OWEwMGM5">>},
               {<<"availableStorage">>,
                {[{<<"hdd">>,
                   [{[{<<"path">>,<<"/">>},
                      {<<"sizeKBytes">>,1056557396},
                      {<<"usagePercent">>,2}]},
                    {[{<<"path">>,<<"/dev">>},
                      {<<"sizeKBytes">>,65536},
                      {<<"usagePercent">>,0}]},
                    {[{<<"path">>,<<"/dev/shm">>},
                      {<<"sizeKBytes">>,65536},
                      {<<"usagePercent">>,0}]},
                    {[{<<"path">>,<<"/etc/resolv.conf">>},
                      {<<"sizeKBytes">>,1056557396},
                      {<<"usagePercent">>,2}]},
                    {[{<<"path">>,<<"/etc/hostname">>},
                      {<<"sizeKBytes">>,1056557396},
                      {<<"usagePercent">>,2}]},
                    {[{<<"path">>,<<"/etc/hosts">>},
                      {<<"sizeKBytes">>,1056557396},
                      {<<"usagePercent">>,2}]},
                    {[{<<"path">>,<<"/opt/couchbase/var">>},
                      {<<"sizeKBytes">>,482797652},
                      {<<"usagePercent">>,92}]},
                    {[{<<"path">>,<<"/proc/kcore">>},
                      {<<"sizeKBytes">>,65536},
                      {<<"usagePercent">>,0}]},
                    {[{<<"path">>,<<"/proc/keys">>},
                      {<<"sizeKBytes">>,65536},
                      {<<"usagePercent">>,0}]},
                    {[{<<"path">>,<<"/proc/timer_list">>},
                      {<<"sizeKBytes">>,65536},
                      {<<"usagePercent">>,0}]},
                    {[{<<"path">>,<<"/proc/scsi">>},
                      {<<"sizeKBytes">>,4012680},
                      {<<"usagePercent">>,0}]},
                    {[{<<"path">>,<<"/sys/firmware">>},
                      {<<"sizeKBytes">>,4012680},
                      {<<"usagePercent">>,0}]}]}]}},
               {<<"storageTotals">>,
                {[{<<"ram">>,
                   {[{<<"total">>,8217968640},
                     {<<"quotaTotal">>,3221225472},
                     {<<"quotaUsed">>,0},
                     {<<"used">>,4251119616},
                     {<<"usedByData">>,0},
                     {<<"quotaUsedPerNode">>,0},
                     {<<"quotaTotalPerNode">>,3221225472}]}},
                  {<<"hdd">>,
                   {[{<<"total">>,494384795648},
                     {<<"quotaTotal">>,494384795648},
                     {<<"used">>,454834011996},
                     {<<"usedByData">>,0},
                     {<<"free">>,39550783652}]}}]}},
               {<<"storage">>,
                {[{<<"ssd">>,[]},
                  {<<"hdd">>,
                   [{[{<<"path">>,<<"/opt/couchbase/var/lib/couchbase/data">>},
                      {<<"index_path">>,
                       <<"/opt/couchbase/var/lib/couchbase/data">>},
                      {<<"cbas_dirs">>,
                       [<<"/opt/couchbase/var/lib/couchbase/data">>]},
                      {<<"eventing_path">>,
                       <<"/opt/couchbase/var/lib/couchbase/data">>},
                      {<<"java_home">>,<<>>},
                      {<<"quotaMb">>,<<"none">>},
                      {<<"state">>,<<"ok">>}]}]}]}},
               {<<"clusterMembership">>,<<"active">>},
               {<<"recoveryType">>,<<"none">>},
               {<<"status">>,<<"healthy">>},
               {<<"otpNode">>,<<"ns_1@172.19.0.4">>},
               {<<"thisNode">>,true},
               {<<"hostname">>,<<"172.19.0.4:8091">>},
               {<<"nodeUUID">>,<<"28569ac00b9c1d7c50e39741027d428c">>},
               {<<"clusterCompatibility">>,458758},
               {<<"version">>,<<"7.6.2-3721-enterprise">>},
               {<<"os">>,<<"aarch64-unknown-linux-gnu">>},
               {<<"cpuCount">>,10},
               {<<"ports">>,
                {[{<<"direct">>,11210},
                  {<<"httpsMgmt">>,18091},
                  {<<"httpsCAPI">>,18092},
                  {<<"distTCP">>,21100},
                  {<<"distTLS">>,21150}]}},
               {<<"services">>,[<<"index">>,<<"kv">>,<<"n1ql">>]},
               {<<"nodeEncryption">>,false},
               {<<"nodeEncryptionClientCertVerification">>,false},
               {<<"addressFamilyOnly">>,false},
               {<<"configuredHostname">>,<<"172.19.0.4:8091">>},
               {<<"addressFamily">>,<<"inet">>},
               {<<"externalListeners">>,
                [{[{<<"afamily">>,<<"inet">>},{<<"nodeEncryption">>,false}]}]},
               {<<"serverGroup">>,<<"Group 1">>},
               {<<"otpCookie">>,
                {sanitized,<<"biSYTaQUFDVndTs/b+IcDmD2L0woktv9naRdv3GCK6A=">>}},
               {<<"couchApiBase">>,<<"http://172.19.0.4:8092/">>},
               {<<"couchApiBaseHTTPS">>,<<"https://172.19.0.4:18092/">>},
               {<<"nodeHash">>,38045879},
               {<<"systemStats">>,
                {[{<<"cpu_utilization_rate">>,3.89883035089008},
                  {<<"cpu_stolen_rate">>,0},
                  {<<"swap_total">>,1073737728},
                  {<<"swap_used">>,3956736},
                  {<<"mem_total">>,8217968640},
                  {<<"mem_free">>,4805804032},
                  {<<"mem_limit">>,8217968640},
                  {<<"cpu_cores_available">>,10},
                  {<<"allocstall">>,17}]}},
               {<<"interestingStats">>,{[]}},
               {<<"uptime">>,<<"41">>},
               {<<"memoryTotal">>,8217968640},
               {<<"memoryFree">>,4805804032},
               {<<"mcdMemoryReserved">>,6269},
               {<<"mcdMemoryAllocated">>,6269},
               {<<"memoryQuota">>,3072},
               {<<"queryMemoryQuota">>,0},
               {<<"indexMemoryQuota">>,512},
               {<<"ftsMemoryQuota">>,512},
               {<<"cbasMemoryQuota">>,1024},
               {<<"eventingMemoryQuota">>,256}]) -> {ok,ok}
[ns_server:info,2025-05-15T18:47:25.210Z,ns_1@db2.lan:ns_cluster<0.273.0>:ns_cluster:handle_info:514]Chronicle state is: provisioned
[ns_server:info,2025-05-15T18:47:25.210Z,ns_1@db2.lan:ns_cluster<0.273.0>:ns_cluster:handle_info:514]Chronicle state is: provisioned
[ns_server:warn,2025-05-15T18:47:25.285Z,ns_1@db2.lan:ns_tick_agent<0.2785.0>:ns_tick_agent:handle_tick:115]Ignoring tick from a non-master node 'ns_1@172.19.0.4'. Master: undefined
[ns_server:debug,2025-05-15T18:47:25.315Z,ns_1@db2.lan:ns_ports_setup<0.2671.0>:ns_ports_setup:set_children:65]Monitor ns_child_ports_sup <16971.133.0>
[ns_server:debug,2025-05-15T18:47:25.316Z,ns_1@db2.lan:<0.2717.0>:memcached_config_mgr:memcached_port_pid:156]ns_ports_setup seems to be ready
[ns_server:debug,2025-05-15T18:47:25.316Z,ns_1@db2.lan:memcached_config_mgr<0.2713.0>:memcached_config_mgr:memcached_port_pid:156]ns_ports_setup seems to be ready
[ns_server:debug,2025-05-15T18:47:25.316Z,ns_1@db2.lan:<0.2717.0>:memcached_config_mgr:find_port_pid_loop:164]Found memcached port <16971.139.0>
[ns_server:debug,2025-05-15T18:47:25.316Z,ns_1@db2.lan:memcached_config_mgr<0.2713.0>:memcached_config_mgr:find_port_pid_loop:164]Found memcached port <16971.139.0>
[ns_server:debug,2025-05-15T18:47:25.317Z,ns_1@db2.lan:<0.2717.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":53,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"indexAdmin\":9100,\"indexHttp\":9102,\"indexHttps\":19102,\"indexScan\":9101,\"indexStreamCatchup\":9104,\"indexStreamInit\":9103,\"indexStreamMaint\":9105,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"hostname\":\"172.19.0.4\",\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[ns_server:debug,2025-05-15T18:47:25.317Z,ns_1@db2.lan:memcached_config_mgr<0.2713.0>:memcached_config_mgr:do_read_current_memcached_config:371]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2025-05-15T18:47:25.317Z,ns_1@db2.lan:memcached_config_mgr<0.2713.0>:memcached_config_mgr:init:100]found memcached port to be already active
[ns_server:debug,2025-05-15T18:47:25.320Z,ns_1@db2.lan:memcached_config_mgr<0.2713.0>:memcached_config_mgr:apply_changed_memcached_config:262]New memcached config is hot-reloadable.
[ns_server:debug,2025-05-15T18:47:25.321Z,ns_1@db2.lan:memcached_config_mgr<0.2713.0>:memcached_config_mgr:do_read_current_memcached_config:371]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[user:info,2025-05-15T18:47:25.326Z,ns_1@db2.lan:memcached_config_mgr<0.2713.0>:memcached_config_mgr:hot_reload_config:331]Hot-reloaded memcached.json for config change of the following keys: [<<"scramsha_fallback_salt">>]
[ns_server:info,2025-05-15T18:47:25.327Z,ns_1@db2.lan:memcached_config_mgr<0.2713.0>:memcached_config_mgr:push_tls_config:233]Pushing TLS config to memcached:
{[{<<"private key">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem">>},
  {<<"certificate chain">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem">>},
  {<<"CA file">>,<<"/opt/couchbase/var/lib/couchbase/config/certs/ca.pem">>},
  {<<"minimum version">>,<<"TLS 1.2">>},
  {<<"cipher list">>,
   {[{<<"TLS 1.2">>,<<"HIGH">>},
     {<<"TLS 1.3">>,
      <<"TLS_AES_256_GCM_SHA384:TLS_AES_128_GCM_SHA256:TLS_CHACHA20_POLY1305_SHA256">>}]}},
  {<<"cipher order">>,true},
  {<<"client cert auth">>,<<"disabled">>}]}
[ns_server:info,2025-05-15T18:47:25.329Z,ns_1@db2.lan:memcached_config_mgr<0.2713.0>:memcached_config_mgr:push_tls_config:237]Successfully pushed TLS config to memcached
[ns_server:debug,2025-05-15T18:47:25.340Z,ns_1@db2.lan:compiled_roles_cache<0.2444.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@",admin}
[ns_server:debug,2025-05-15T18:47:25.340Z,ns_1@db2.lan:json_rpc_connection-goxdcr-cbauth<0.2848.0>:json_rpc_connection:init:71]Observed revrpc connection: label "goxdcr-cbauth", handling process <0.2848.0>
[ns_server:debug,2025-05-15T18:47:25.340Z,ns_1@db2.lan:menelaus_cbauth<0.2662.0>:menelaus_cbauth:handle_cast:203]Observed json rpc process {"goxdcr-cbauth",[{internal,true}],<0.2848.0>} started
[ns_server:debug,2025-05-15T18:47:25.344Z,ns_1@db2.lan:compiled_roles_cache<0.2444.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@goxdcr",admin}
[ns_server:debug,2025-05-15T18:47:25.828Z,ns_1@db2.lan:ns_cookie_manager<0.238.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[ns_server:debug,2025-05-15T18:47:25.828Z,ns_1@db2.lan:mb_master<0.2783.0>:mb_master:update_peers:543]List of peers has changed from ['ns_1@172.19.0.4','ns_1@db2.lan'] to ['ns_1@172.19.0.4',
                                                                      'ns_1@db2.lan',
                                                                      'ns_1@db3.lan']
[ns_server:debug,2025-05-15T18:47:25.828Z,ns_1@db2.lan:<0.2859.0>:ns_node_disco:do_nodes_wanted_updated_fun:210]ns_node_disco: nodes_wanted updated: ['ns_1@172.19.0.4','ns_1@db2.lan',
                                      'ns_1@db3.lan'], with cookie: {sanitized,
                                                                     <<"biSYTaQUFDVndTs/b+IcDmD2L0woktv9naRdv3GCK6A=">>}
[ns_server:debug,2025-05-15T18:47:25.829Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: unfinished_topology_operation, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                  22})
{regular,{add_replica,'ns_1@db3.lan',undefined,[index,kv,n1ql]},
         <<"45724915daa2a489c0a6816d20198585">>,
         #Ref<34787.2327127311.3650355203.217418>}
[ns_server:debug,2025-05-15T18:47:25.829Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: nodes_wanted, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,22})
['ns_1@172.19.0.4','ns_1@db2.lan','ns_1@db3.lan']
[ns_server:debug,2025-05-15T18:47:25.829Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@db3.lan',membership}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                     22})
inactiveAdded
[ns_server:debug,2025-05-15T18:47:25.829Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@db3.lan',services}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                   22})
[index,kv,n1ql]
[ns_server:debug,2025-05-15T18:47:25.829Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: server_groups, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,22})
[[{uuid,<<"0">>},
  {name,<<"Group 1">>},
  {nodes,['ns_1@172.19.0.4','ns_1@db2.lan','ns_1@db3.lan']}]]
[ns_server:debug,2025-05-15T18:47:25.829Z,ns_1@db2.lan:<0.2717.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":56,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"indexAdmin\":9100,\"indexHttp\":9102,\"indexHttps\":19102,\"indexScan\":9101,\"indexStreamCatchup\":9104,\"indexStreamInit\":9103,\"indexStreamMaint\":9105,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"hostname\":\"172.19.0.4\",\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[error_logger:info,2025-05-15T18:47:25.829Z,ns_1@db2.lan:net_kernel<0.2175.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'ns_1@db3.lan',
                          {5175339,#Ref<0.3735524962.3650486280.242433>}}}
[ns_server:debug,2025-05-15T18:47:25.833Z,ns_1@db2.lan:net_kernel<0.2175.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'ns_1@db3.lan' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:25.833Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3735524962.3650355207.242025>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:47:25.833Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3735524962.3650355207.242025>,
                                  inet_tcp_dist,<0.2862.0>,
                                  #Ref<0.3735524962.3650355206.243120>}
[ns_server:debug,2025-05-15T18:47:25.835Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3735524962.3650355207.242025>,
                               inet_tcp_dist,<0.2862.0>,
                               #Ref<0.3735524962.3650355206.243120>}
[error_logger:info,2025-05-15T18:47:25.835Z,ns_1@db2.lan:net_kernel<0.2175.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.2862.0>,{recv_challenge_ack_failed,{error,closed}}}}
[error_logger:info,2025-05-15T18:47:25.835Z,ns_1@db2.lan:net_kernel<0.2175.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'ns_1@db3.lan'}}
[error_logger:info,2025-05-15T18:47:25.835Z,ns_1@db2.lan:net_kernel<0.2175.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{disconnect,'ns_1@db3.lan'}}
[ns_server:debug,2025-05-15T18:47:25.835Z,ns_1@db2.lan:<0.2859.0>:ns_node_disco:do_nodes_wanted_updated_fun:213]ns_node_disco: nodes_wanted pong: ['ns_1@172.19.0.4','ns_1@db2.lan'], with cookie: {sanitized,
                                                                                    <<"biSYTaQUFDVndTs/b+IcDmD2L0woktv9naRdv3GCK6A=">>}
[ns_server:debug,2025-05-15T18:47:25.882Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:handle_info:42]delete (key: unfinished_topology_operation, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                  25})
[ns_server:debug,2025-05-15T18:47:25.909Z,ns_1@db2.lan:compiled_roles_cache<0.2444.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@ns_server",admin}
[ns_server:debug,2025-05-15T18:47:26.107Z,ns_1@db2.lan:json_rpc_connection-saslauthd-saslauthd-port<0.2868.0>:json_rpc_connection:init:71]Observed revrpc connection: label "saslauthd-saslauthd-port", handling process <0.2868.0>
[error_logger:info,2025-05-15T18:47:26.208Z,ns_1@db2.lan:net_kernel<0.2175.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'ns_1@db3.lan',
                          {5175340,#Ref<0.3735524962.3650486280.242433>}}}
[ns_server:debug,2025-05-15T18:47:26.209Z,ns_1@db2.lan:net_kernel<0.2175.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'ns_1@db3.lan' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:26.209Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3735524962.3650355206.243151>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:47:26.209Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3735524962.3650355206.243151>,
                                  inet_tcp_dist,<0.2884.0>,
                                  #Ref<0.3735524962.3650355206.243154>}
[ns_server:debug,2025-05-15T18:47:26.211Z,ns_1@db2.lan:<0.2443.0>:doc_replicator:nodeup_monitoring_loop:145]got nodeup event. Considering ddocs replication
[user:info,2025-05-15T18:47:26.211Z,ns_1@db2.lan:ns_node_disco<0.2520.0>:ns_node_disco:handle_info:163]Node 'ns_1@db2.lan' saw that node 'ns_1@db3.lan' came up. Tags: []
[ns_server:debug,2025-05-15T18:47:26.211Z,ns_1@db2.lan:users_replicator<0.2440.0>:doc_replicator:loop:79]Replicating all docs to new nodes: ['ns_1@db3.lan']
[ns_server:debug,2025-05-15T18:47:26.211Z,ns_1@db2.lan:ns_node_disco_events<0.2519.0>:ns_config_rep:handle_node_disco_event:513]Detected new nodes (['ns_1@db3.lan']).  Moving config around.
[ns_server:info,2025-05-15T18:47:26.211Z,ns_1@db2.lan:ns_node_disco_events<0.2519.0>:ns_node_disco_log:handle_event:40]ns_node_disco_log: nodes changed: ['ns_1@172.19.0.4','ns_1@db2.lan',
                                   'ns_1@db3.lan']
[ns_server:debug,2025-05-15T18:47:26.211Z,ns_1@db2.lan:users_replicator<0.2440.0>:replicated_dets:select_from_table:312][dets] Starting select with {users_storage,[{'_',[],['$_']}],500}
[ns_server:info,2025-05-15T18:47:26.285Z,ns_1@db2.lan:mb_master<0.2783.0>:mb_master:candidate:370]Changing master from undefined to 'ns_1@172.19.0.4'
[ns_server:debug,2025-05-15T18:47:26.285Z,ns_1@db2.lan:leader_registry<0.2781.0>:leader_registry:handle_new_leader:275]New leader is 'ns_1@172.19.0.4'. Invalidating name cache.
[ns_server:info,2025-05-15T18:47:27.824Z,ns_1@db2.lan:ns_ssl_services_setup<0.2373.0>:ns_ssl_services_setup:handle_info:764]cert_and_pkey changed
[ns_server:debug,2025-05-15T18:47:27.824Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',address_family} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|inet]
[ns_server:debug,2025-05-15T18:47:27.824Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',audit} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}]
[ns_server:debug,2025-05-15T18:47:27.824Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',backup_grpc_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9124]
[ns_server:debug,2025-05-15T18:47:27.824Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',backup_http_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|8097]
[ns_server:debug,2025-05-15T18:47:27.824Z,ns_1@db2.lan:<0.2717.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":68,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"indexAdmin\":9100,\"indexHttp\":9102,\"indexHttps\":19102,\"indexScan\":9101,\"indexStreamCatchup\":9104,\"indexStreamInit\":9103,\"indexStreamMaint\":9105,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"hostname\":\"172.19.0.4\",\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[ns_server:debug,2025-05-15T18:47:27.825Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',backup_https_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|18097]
[ns_server:debug,2025-05-15T18:47:27.825Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',capi_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|8092]
[ns_server:debug,2025-05-15T18:47:27.825Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_admin_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9110]
[ns_server:debug,2025-05-15T18:47:27.825Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_cc_client_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9113]
[ns_server:debug,2025-05-15T18:47:27.825Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_cc_cluster_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9112]
[ns_server:debug,2025-05-15T18:47:27.825Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_cc_http_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9111]
[ns_server:debug,2025-05-15T18:47:27.825Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_cluster_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9115]
[ns_server:debug,2025-05-15T18:47:27.825Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_console_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9114]
[ns_server:debug,2025-05-15T18:47:27.825Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_data_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9116]
[ns_server:debug,2025-05-15T18:47:27.826Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_debug_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|-1]
[ns_server:debug,2025-05-15T18:47:27.826Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_dirs} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
 "/opt/couchbase/var/lib/couchbase/data"]
[ns_server:debug,2025-05-15T18:47:27.826Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_http_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|8095]
[ns_server:debug,2025-05-15T18:47:27.826Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_messaging_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9118]
[ns_server:debug,2025-05-15T18:47:27.826Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_metadata_callback_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9119]
[ns_server:debug,2025-05-15T18:47:27.826Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_metadata_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9121]
[ns_server:debug,2025-05-15T18:47:27.826Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_parent_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9122]
[ns_server:debug,2025-05-15T18:47:27.826Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_replication_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9120]
[ns_server:debug,2025-05-15T18:47:27.826Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_result_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9117]
[ns_server:debug,2025-05-15T18:47:27.827Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_ssl_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|18095]
[ns_server:debug,2025-05-15T18:47:27.827Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',client_cert} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{2,63914554046}}]},
 {certs_epoch,0},
 {subject,<<"CN=Couchbase Internal Client (32660726)">>},
 {not_after,63985747645},
 {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
 {load_timestamp,63914554045},
 {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hX"...>>},
 {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDWTCCAkGgAwIBAgIIGD/HyMO1PBAwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjVaFw0y\nNzA4MTcxODQ3MjVaMC8xLTArBgNVBAMTJENvdWNoYmFzZSBJbnRlcm5hbCBDbGll\nbnQgKDMyNjYwNzI2KTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAKHe\n9VAt+X6MU19Fg7ok9H0Zxkmq5mDd0acYZWMO8WqPC7jBY2dopAKDJoF/AKgk8TLT\ncz5"...>>},
 {pkey_passphrase_settings,[]},
 {type,generated},
 {name,"@internal"}]
[ns_server:debug,2025-05-15T18:47:27.827Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',compaction_daemon} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
 {check_interval,30},
 {min_db_file_size,131072},
 {min_view_file_size,20971520}]
[ns_server:debug,2025-05-15T18:47:27.827Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',config_version} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|{7,6}]
[ns_server:debug,2025-05-15T18:47:27.827Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',database_dir} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2025-05-15T18:47:27.828Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',erl_external_listeners} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
 {inet,false}]
[ns_server:debug,2025-05-15T18:47:27.828Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',event_log} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
 {filename,"/opt/couchbase/var/lib/couchbase/event_log"}]
[ns_server:debug,2025-05-15T18:47:27.828Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',eventing_debug_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9140]
[ns_server:debug,2025-05-15T18:47:27.828Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',eventing_dir} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2025-05-15T18:47:27.828Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',eventing_http_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|8096]
[ns_server:debug,2025-05-15T18:47:27.828Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',eventing_https_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|18096]
[ns_server:debug,2025-05-15T18:47:27.828Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',fts_grpc_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9130]
[ns_server:debug,2025-05-15T18:47:27.828Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',fts_grpc_ssl_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|19130]
[ns_server:debug,2025-05-15T18:47:27.828Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',fts_http_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|8094]
[ns_server:debug,2025-05-15T18:47:27.828Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',fts_ssl_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|18094]
[ns_server:debug,2025-05-15T18:47:27.828Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',index_dir} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2025-05-15T18:47:27.828Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',indexer_admin_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9100]
[ns_server:debug,2025-05-15T18:47:27.828Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',indexer_http_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9102]
[ns_server:debug,2025-05-15T18:47:27.828Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',indexer_https_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|19102]
[ns_server:debug,2025-05-15T18:47:27.828Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',indexer_scan_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9101]
[ns_server:debug,2025-05-15T18:47:27.828Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',indexer_stcatchup_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9104]
[ns_server:debug,2025-05-15T18:47:27.828Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',indexer_stinit_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9103]
[ns_server:debug,2025-05-15T18:47:27.828Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',indexer_stmaint_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9105]
[ns_server:debug,2025-05-15T18:47:27.829Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',is_enterprise} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|true]
[ns_server:debug,2025-05-15T18:47:27.829Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',isasl} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
 {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]
[ns_server:debug,2025-05-15T18:47:27.829Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',membership} ->
inactiveAdded
[ns_server:debug,2025-05-15T18:47:27.829Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',memcached} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
 {port,11210},
 {dedicated_port,11209},
 {dedicated_ssl_port,11206},
 {ssl_port,11207},
 {admin_user,"@ns_server"},
 {other_users,["@cbq-engine","@projector","@goxdcr","@index","@fts",
               "@eventing","@cbas","@backup"]},
 {admin_pass,"*****"},
 {engines,[{membase,[{engine,"/opt/couchbase/lib/memcached/ep.so"},
                     {static_config_string,"failpartialwarmup=false"}]},
           {memcached,[{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
                       {static_config_string,"vb0=true"}]}]},
 {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
 {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
 {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
 {log_prefix,"memcached.log"},
 {log_generations,20},
 {log_cyclesize,10485760},
 {log_rotation_period,39003}]
[ns_server:debug,2025-05-15T18:47:27.829Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',memcached_config} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
 {[{interfaces,{memcached_config_mgr,get_interfaces,[]}},
   {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
   {connection_idle_time,connection_idle_time},
   {privilege_debug,privilege_debug},
   {breakpad,
    {[{enabled,breakpad_enabled},
      {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
   {deployment_model,{memcached_config_mgr,get_config_profile,[]}},
   {verbosity,verbosity},
   {audit_file,{"~s",[audit_file]}},
   {rbac_file,{"~s",[rbac_file]}},
   {dedupe_nmvb_maps,dedupe_nmvb_maps},
   {tracing_enabled,tracing_enabled},
   {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
   {xattr_enabled,true},
   {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
   {collections_enabled,true},
   {max_connections,max_connections},
   {system_connections,system_connections},
   {num_reader_threads,num_reader_threads},
   {num_writer_threads,num_writer_threads},
   {num_auxio_threads,num_auxio_threads},
   {num_nonio_threads,num_nonio_threads},
   {num_storage_threads,num_storage_threads},
   {logger,
    {[{filename,{"~s/~s",[log_path,log_prefix]}},{cyclesize,log_cyclesize}]}},
   {external_auth_service,{memcached_config_mgr,get_external_auth_service,[]}},
   {active_external_users_push_interval,
    {memcached_config_mgr,get_external_users_push_interval,[]}},
   {prometheus,{memcached_config_mgr,prometheus_cfg,[]}},
   {sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
   {ssl_sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
   {tcp_keepalive_idle,tcp_keepalive_idle},
   {tcp_keepalive_interval,tcp_keepalive_interval},
   {tcp_keepalive_probes,tcp_keepalive_probes},
   {tcp_user_timeout,tcp_user_timeout},
   {always_collect_trace_info,always_collect_trace_info},
   {connection_limit_mode,connection_limit_mode},
   {free_connection_pool_size,free_connection_pool_size},
   {max_client_connection_details,max_client_connection_details}]}]
[ns_server:debug,2025-05-15T18:47:27.829Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',memcached_dedicated_ssl_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|11206]
[ns_server:debug,2025-05-15T18:47:27.829Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',memcached_defaults} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
 {max_connections,65000},
 {system_connections,5000},
 {connection_idle_time,0},
 {verbosity,0},
 {privilege_debug,false},
 {breakpad_enabled,true},
 {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
 {dedupe_nmvb_maps,false},
 {je_malloc_conf,undefined},
 {tracing_enabled,true},
 {datatype_snappy,true},
 {num_reader_threads,<<"default">>},
 {num_writer_threads,<<"default">>},
 {num_auxio_threads,<<"default">>},
 {num_nonio_threads,<<"default">>},
 {num_storage_threads,<<"default">>},
 {tcp_keepalive_idle,360},
 {tcp_keepalive_interval,10},
 {tcp_keepalive_probes,3},
 {tcp_user_timeout,30},
 {always_collect_trace_info,true},
 {connection_limit_mode,<<"disconnect">>},
 {free_connection_pool_size,0},
 {max_client_connection_details,0}]
[ns_server:debug,2025-05-15T18:47:27.829Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',memcached_prometheus} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|11280]
[ns_server:debug,2025-05-15T18:47:27.829Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',n2n_client_cert_auth} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|false]
[ns_server:debug,2025-05-15T18:47:27.829Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',node_cert} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{2,63914554046}}]},
 {certs_epoch,0},
 {subject,<<"CN=Couchbase Server Node (db3.lan)">>},
 {not_after,63985747645},
 {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
 {load_timestamp,63914554045},
 {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hX"...>>},
 {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDOjCCAiKgAwIBAgIIGD/HyL2BZrYwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjVaFw0y\nNzA4MTcxODQ3MjVaMCoxKDAmBgNVBAMTH0NvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nZGIzLmxhbikwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCoBie+jXEG\nM9mqURIO0hHIXr1th5jf7TJD7f69DGF3znRjw1VTjnaMZAQOk8Gg44nwI1o1KtJj\nUmf"...>>},
 {pkey_passphrase_settings,[]},
 {type,generated},
 {hostname,"db3.lan"}]
[ns_server:debug,2025-05-15T18:47:27.829Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',node_encryption} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|false]
[ns_server:debug,2025-05-15T18:47:27.830Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',ns_log} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
 {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]
[ns_server:debug,2025-05-15T18:47:27.830Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',port_servers} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}]
[ns_server:debug,2025-05-15T18:47:27.830Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',projector_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9999]
[ns_server:debug,2025-05-15T18:47:27.830Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',projector_ssl_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9999]
[ns_server:debug,2025-05-15T18:47:27.830Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',prometheus_auth_info} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{2,63914554047}}]}|
 {"@prometheus",
  {auth,
   [{<<"hash">>,
     {[{<<"hashes">>,
        {sanitized,<<"K8k/NfCHEgbJA5Gjm/Uf6LIWuV5wdCnmlHHxqgxuFcg=">>}},
       {<<"algorithm">>,<<"argon2id">>},
       {<<"salt">>,<<"DJB2Ptc3EvRFRaPMQY8lxQ==">>},
       {<<"parallelism">>,1},
       {<<"time">>,3},
       {<<"memory">>,524288}]}},
    {<<"scram-sha-512">>,
     {[{<<"salt">>,
        <<"kQZBb8aKY+QE2LMIih5sI+B6I8bZhl0CgtizD2Fc7mLeSYkH0HpqLrT4l2I+g3feMZ02/xAzulfE7pW7LKeNOg==">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"3t2jsPPn+9DcO1shgR+zoz6ZswYB3twV7AiZzEoEZaw=">>}}]}},
    {<<"scram-sha-256">>,
     {[{<<"salt">>,<<"t1Mu2m4fYVtuDH4+kaa1tY0l5MgkTK5scos+czMBlSU=">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"3/WNUnYG1nDOZfZnux37H+3kYuv6M9rzdGe6kGBsDos=">>}}]}},
    {<<"scram-sha-1">>,
     {[{<<"salt">>,<<"ejCbw2caHZmnb35/ZwMKng0kuTY=">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"koVlXdNIWFlL5fpQq2C/7QKV+kuonc55IuSVFkJpchc=">>}}]}}]}}]
[ns_server:debug,2025-05-15T18:47:27.830Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',prometheus_http_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9123]
[ns_server:debug,2025-05-15T18:47:27.830Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',query_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|8093]
[ns_server:debug,2025-05-15T18:47:27.830Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',rest} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
 {port,8091},
 {port_meta,global}]
[ns_server:debug,2025-05-15T18:47:27.830Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',saslauthd_enabled} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|true]
[ns_server:debug,2025-05-15T18:47:27.830Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',ssl_capi_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|18092]
[ns_server:debug,2025-05-15T18:47:27.830Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',ssl_query_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|18093]
[ns_server:debug,2025-05-15T18:47:27.830Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',ssl_rest_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|18091]
[ns_server:debug,2025-05-15T18:47:27.830Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',uuid} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
 <<"80960cc3730024bccd4fc49444efdc14">>]
[ns_server:debug,2025-05-15T18:47:27.830Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',xdcr_rest_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9998]
[ns_server:debug,2025-05-15T18:47:27.830Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',{project_intact,is_vulnerable}} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|false]
[ns_server:debug,2025-05-15T18:47:27.831Z,ns_1@db2.lan:<0.2717.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":68,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"indexAdmin\":9100,\"indexHttp\":9102,\"indexHttps\":19102,\"indexScan\":9101,\"indexStreamCatchup\":9104,\"indexStreamInit\":9103,\"indexStreamMaint\":9105,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"hostname\":\"172.19.0.4\",\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[ns_server:debug,2025-05-15T18:47:27.838Z,ns_1@db2.lan:ns_ssl_services_setup<0.2373.0>:ns_ssl_services_setup:notify_services:1146]Going to notify following services: [capi_ssl_service,memcached]
[ns_server:info,2025-05-15T18:47:27.838Z,ns_1@db2.lan:<0.2962.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service memcached
[ns_server:info,2025-05-15T18:47:27.838Z,ns_1@db2.lan:memcached_config_mgr<0.2713.0>:memcached_config_mgr:push_tls_config:233]Pushing TLS config to memcached:
{[{<<"private key">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem">>},
  {<<"certificate chain">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem">>},
  {<<"CA file">>,<<"/opt/couchbase/var/lib/couchbase/config/certs/ca.pem">>},
  {<<"minimum version">>,<<"TLS 1.2">>},
  {<<"cipher list">>,
   {[{<<"TLS 1.2">>,<<"HIGH">>},
     {<<"TLS 1.3">>,
      <<"TLS_AES_256_GCM_SHA384:TLS_AES_128_GCM_SHA256:TLS_CHACHA20_POLY1305_SHA256">>}]}},
  {<<"cipher order">>,true},
  {<<"client cert auth">>,<<"disabled">>}]}
[ns_server:info,2025-05-15T18:47:27.842Z,ns_1@db2.lan:memcached_config_mgr<0.2713.0>:memcached_config_mgr:push_tls_config:237]Successfully pushed TLS config to memcached
[ns_server:info,2025-05-15T18:47:27.844Z,ns_1@db2.lan:<0.2961.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service capi_ssl_service
[ns_server:info,2025-05-15T18:47:27.844Z,ns_1@db2.lan:ns_ssl_services_setup<0.2373.0>:ns_ssl_services_setup:notify_services:1162]Succesfully notified services [memcached,capi_ssl_service]
[ns_server:debug,2025-05-15T18:47:27.845Z,ns_1@db2.lan:ns_ssl_services_setup<0.2373.0>:ns_ssl_services_setup:restart_regenerate_client_cert_timer:1447]Time left before client cert regeneration: 70588795000
[ns_server:debug,2025-05-15T18:47:28.106Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: counters, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,26})
[{rebalance_start,{1747334848,1}}]
[ns_server:debug,2025-05-15T18:47:28.107Z,ns_1@db2.lan:<0.2717.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":69,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"indexAdmin\":9100,\"indexHttp\":9102,\"indexHttps\":19102,\"indexScan\":9101,\"indexStreamCatchup\":9104,\"indexStreamInit\":9103,\"indexStreamMaint\":9105,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"hostname\":\"172.19.0.4\",\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[ns_server:debug,2025-05-15T18:47:28.133Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: rebalance_status, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                     27})
running
[ns_server:debug,2025-05-15T18:47:28.133Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: rebalance_status_uuid, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                          27})
<<"1827da82069c38267f07251824989d63">>
[ns_server:debug,2025-05-15T18:47:28.133Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: rebalancer_pid, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,27})
<34787.2826.0>
[ns_server:debug,2025-05-15T18:47:28.133Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: rebalance_type, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,27})
rebalance
[error_logger:info,2025-05-15T18:47:29.819Z,ns_1@db2.lan:net_kernel<0.2175.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.2180.0>,setup_timer_timeout}}
[error_logger:info,2025-05-15T18:47:29.819Z,ns_1@db2.lan:net_kernel<0.2175.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:47:29.819Z,ns_1@db2.lan:cb_dist<0.2173.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3735524962.3650355207.241408>,
                               inet_tcp_dist,<0.2180.0>,
                               #Ref<0.3735524962.3650355201.245714>}
[ns_server:debug,2025-05-15T18:47:30.114Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/indexing/rebalance/RebalanceToken">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554050}}]}|
 '_deleted']
[ns_server:debug,2025-05-15T18:47:30.133Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/indexing/info/versionToken">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554050}}]}|
 <<"{\"Version\":9}">>]
[ns_server:debug,2025-05-15T18:47:32.669Z,ns_1@db2.lan:compiled_roles_cache<0.2444.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@prometheus",stats_reader}
[ns_server:debug,2025-05-15T18:47:36.817Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@172.19.0.4',recovery_type}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                           30})
none
[ns_server:debug,2025-05-15T18:47:36.818Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@db2.lan',recovery_type}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                        30})
none
[ns_server:debug,2025-05-15T18:47:36.818Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@db3.lan',recovery_type}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                        30})
none
[ns_server:debug,2025-05-15T18:47:36.818Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@172.19.0.4',failover_vbuckets}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                               30})
[]
[ns_server:debug,2025-05-15T18:47:36.818Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@db2.lan',failover_vbuckets}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                            30})
[]
[ns_server:debug,2025-05-15T18:47:36.818Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@db3.lan',failover_vbuckets}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                            30})
[]
[ns_server:debug,2025-05-15T18:47:36.848Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: {service_map,n1ql}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                       31})
['ns_1@172.19.0.4']
[ns_server:debug,2025-05-15T18:47:36.856Z,ns_1@db2.lan:<0.2717.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":76,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"indexAdmin\":9100,\"indexHttp\":9102,\"indexHttps\":19102,\"indexScan\":9101,\"indexStreamCatchup\":9104,\"indexStreamInit\":9103,\"indexStreamMaint\":9105,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"n1ql\":8093,\"n1qlSSL\":18093,\"projector\":9999},\"hostname\":\"172.19.0.4\",\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[ns_server:debug,2025-05-15T18:47:36.884Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: {service_map,n1ql}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                       32})
['ns_1@172.19.0.4']
[ns_server:debug,2025-05-15T18:47:36.885Z,ns_1@db2.lan:<0.2717.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":77,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"indexAdmin\":9100,\"indexHttp\":9102,\"indexHttps\":19102,\"indexScan\":9101,\"indexStreamCatchup\":9104,\"indexStreamInit\":9103,\"indexStreamMaint\":9105,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"n1ql\":8093,\"n1qlSSL\":18093,\"projector\":9999},\"hostname\":\"172.19.0.4\",\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[ns_server:debug,2025-05-15T18:47:36.956Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: unfinished_topology_operation, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                  35})
{regular,{activate_nodes,['ns_1@172.19.0.4','ns_1@db2.lan','ns_1@db3.lan']},
         <<"2c2b97b761cc8e298503d744e5749bcb">>,
         #Ref<34787.2327127311.3650355203.217418>}
[ns_server:debug,2025-05-15T18:47:36.957Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@172.19.0.4',membership}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                        35})
active
[ns_server:debug,2025-05-15T18:47:36.958Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@db2.lan',membership}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                     35})
active
[ns_server:debug,2025-05-15T18:47:36.959Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@db3.lan',membership}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                     35})
active
[ns_server:debug,2025-05-15T18:47:36.961Z,ns_1@db2.lan:<0.2717.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":80,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"indexAdmin\":9100,\"indexHttp\":9102,\"indexHttps\":19102,\"indexScan\":9101,\"indexStreamCatchup\":9104,\"indexStreamInit\":9103,\"indexStreamMaint\":9105,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"n1ql\":8093,\"n1qlSSL\":18093,\"projector\":9999},\"hostname\":\"172.19.0.4\",\"serverGroup\":\"Group 1\"},{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true,\"hostname\":\"db2.lan\",\"serverGroup\":\"Group 1\"},{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"hostname\":\"db3.lan\",\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[error_logger:info,2025-05-15T18:47:36.966Z,ns_1@db2.lan:service_stats_children_sup<0.2741.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_stats_children_sup}
    started: [{pid,<0.3369.0>},
              {id,{service_index,stats_reader,"@index"}},
              {mfargs,{stats_reader,start_link,["@index"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:36.968Z,ns_1@db2.lan:service_monitor_children_sup<0.2797.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_monitor_children_sup}
    started: [{pid,<0.3371.0>},
              {id,{kv,dcp_traffic_monitor}},
              {mfargs,{dcp_traffic_monitor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:36.973Z,ns_1@db2.lan:menelaus_cbauth<0.2662.0>:menelaus_cbauth:handle_cast:203]Observed json rpc process {"goxdcr-cbauth",[{internal,true}],<0.2848.0>} needs_update
[error_logger:info,2025-05-15T18:47:36.973Z,ns_1@db2.lan:service_agent_children_sup<0.2675.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_agent_children_sup}
    started: [{pid,<0.3373.0>},
              {id,{service_agent,index}},
              {mfargs,{service_agent,start_link,[index]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:36.974Z,ns_1@db2.lan:menelaus_cbauth<0.2662.0>:menelaus_cbauth:handle_cast:203]Observed json rpc process {"goxdcr-cbauth",[{internal,true}],<0.2848.0>} needs_update
[error_logger:info,2025-05-15T18:47:36.974Z,ns_1@db2.lan:service_agent_children_sup<0.2675.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_agent_children_sup}
    started: [{pid,<0.3377.0>},
              {id,{service_agent,n1ql}},
              {mfargs,{service_agent,start_link,[n1ql]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:36.977Z,ns_1@db2.lan:service_monitor_children_sup<0.2797.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_monitor_children_sup}
    started: [{pid,<0.3381.0>},
              {id,{kv,kv_stats_monitor}},
              {mfargs,{kv_stats_monitor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:36.977Z,ns_1@db2.lan:service_monitor_children_sup<0.2797.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_monitor_children_sup}
    started: [{pid,<0.3385.0>},
              {id,{kv,kv_monitor}},
              {mfargs,{kv_monitor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[chronicle:info,2025-05-15T18:47:36.980Z,ns_1@db2.lan:chronicle_leader<0.2330.0>:chronicle_leader:handle_new_config:525]Our electability (the new value is true) changed. Becoming an observer.
[ns_server:debug,2025-05-15T18:47:37.011Z,ns_1@db2.lan:ns_ports_setup<0.2671.0>:ns_ports_manager:set_dynamic_children:48]Setting children [memcached,saslauthd_port,goxdcr,index,n1ql,projector]
[ns_server:debug,2025-05-15T18:47:37.021Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:handle_info:42]delete (key: unfinished_topology_operation, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                  39})
[ns_server:debug,2025-05-15T18:47:37.204Z,ns_1@db2.lan:json_rpc_connection-index-cbauth<0.3407.0>:json_rpc_connection:init:71]Observed revrpc connection: label "index-cbauth", handling process <0.3407.0>
[ns_server:debug,2025-05-15T18:47:37.204Z,ns_1@db2.lan:menelaus_cbauth<0.2662.0>:menelaus_cbauth:handle_cast:203]Observed json rpc process {"index-cbauth",[{internal,true}],<0.3407.0>} started
[ns_server:debug,2025-05-15T18:47:37.206Z,ns_1@db2.lan:compiled_roles_cache<0.2444.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@index",admin}
[ns_server:debug,2025-05-15T18:47:37.310Z,ns_1@db2.lan:json_rpc_connection-index-service_api<0.3454.0>:json_rpc_connection:init:71]Observed revrpc connection: label "index-service_api", handling process <0.3454.0>
[ns_server:debug,2025-05-15T18:47:37.311Z,ns_1@db2.lan:<0.3376.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {json_rpc_events,<0.3374.0>} exited with reason normal
[ns_server:debug,2025-05-15T18:47:37.311Z,ns_1@db2.lan:service_agent-index<0.3373.0>:service_agent:do_handle_connection:410]Observed new json rpc connection for index: <0.3454.0>
[ns_server:debug,2025-05-15T18:47:37.455Z,ns_1@db2.lan:json_rpc_connection-cbq-engine-cbauth<0.3465.0>:json_rpc_connection:init:71]Observed revrpc connection: label "cbq-engine-cbauth", handling process <0.3465.0>
[ns_server:debug,2025-05-15T18:47:37.455Z,ns_1@db2.lan:menelaus_cbauth<0.2662.0>:menelaus_cbauth:handle_cast:203]Observed json rpc process {"cbq-engine-cbauth",[{internal,true}],<0.3465.0>} started
[ns_server:debug,2025-05-15T18:47:37.458Z,ns_1@db2.lan:compiled_roles_cache<0.2444.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@cbq-engine",admin}
[ns_server:debug,2025-05-15T18:47:37.465Z,ns_1@db2.lan:<0.3476.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.3442.0>} exited with reason normal
[ns_server:debug,2025-05-15T18:47:37.537Z,ns_1@db2.lan:json_rpc_connection-projector-cbauth<0.3487.0>:json_rpc_connection:init:71]Observed revrpc connection: label "projector-cbauth", handling process <0.3487.0>
[ns_server:debug,2025-05-15T18:47:37.537Z,ns_1@db2.lan:menelaus_cbauth<0.2662.0>:menelaus_cbauth:handle_cast:203]Observed json rpc process {"projector-cbauth",[{internal,true}],<0.3487.0>} started
[ns_server:debug,2025-05-15T18:47:37.539Z,ns_1@db2.lan:compiled_roles_cache<0.2444.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@projector",admin}
[ns_server:info,2025-05-15T18:47:39.298Z,ns_1@db2.lan:ns_config_rep<0.2526.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@172.19.0.4'
[ns_server:debug,2025-05-15T18:47:39.489Z,ns_1@db2.lan:leader_lease_agent<0.2776.0>:leader_lease_agent:handle_lease_expired:277]Lease held by {lease_holder,<<"445e053598a21dcd845301efeef2f950">>,
                            'ns_1@cb.local'} expired. Starting expirer.
[ns_server:debug,2025-05-15T18:47:39.492Z,ns_1@db2.lan:leader_lease_agent<0.2776.0>:leader_lease_agent:do_handle_acquire_lease:140]Granting lease to {lease_holder,<<"60878074b48f15926e51cf002c4824e4">>,
                                'ns_1@172.19.0.4'} for 15000ms
[ns_server:debug,2025-05-15T18:47:39.503Z,ns_1@db2.lan:ns_config_rep<0.2526.0>:ns_config_rep:handle_call:149]Got full synchronization request from 'ns_1@172.19.0.4'
[ns_server:debug,2025-05-15T18:47:39.509Z,ns_1@db2.lan:ns_config_rep<0.2526.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 5604 us
[ns_server:debug,2025-05-15T18:47:39.509Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
quorum_nodes ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554059}}]},
 'ns_1@172.19.0.4','ns_1@db2.lan','ns_1@db3.lan']
[ns_server:debug,2025-05-15T18:47:39.657Z,ns_1@db2.lan:json_rpc_connection-cbq-engine-service_api<0.3584.0>:json_rpc_connection:init:71]Observed revrpc connection: label "cbq-engine-service_api", handling process <0.3584.0>
[ns_server:debug,2025-05-15T18:47:39.658Z,ns_1@db2.lan:service_agent-n1ql<0.3377.0>:service_agent:do_handle_connection:410]Observed new json rpc connection for n1ql: <0.3584.0>
[ns_server:debug,2025-05-15T18:47:39.658Z,ns_1@db2.lan:<0.3380.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {json_rpc_events,<0.3378.0>} exited with reason normal
[ns_server:debug,2025-05-15T18:47:39.688Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: {service_map,n1ql}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                       40})
['ns_1@172.19.0.4','ns_1@db2.lan','ns_1@db3.lan']
[ns_server:debug,2025-05-15T18:47:39.689Z,ns_1@db2.lan:<0.2717.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":86,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"indexAdmin\":9100,\"indexHttp\":9102,\"indexHttps\":19102,\"indexScan\":9101,\"indexStreamCatchup\":9104,\"indexStreamInit\":9103,\"indexStreamMaint\":9105,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"n1ql\":8093,\"n1qlSSL\":18093,\"projector\":9999},\"hostname\":\"172.19.0.4\",\"serverGroup\":\"Group 1\"},{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"n1ql\":8093,\"n1qlSSL\":18093,\"projector\":9999},\"thisNode\":true,\"hostname\":\"db2.lan\",\"serverGroup\":\"Group 1\"},{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"n1ql\":8093,\"n1qlSSL\":18093,\"projector\":9999},\"hostname\":\"db3.lan\",\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[ns_server:debug,2025-05-15T18:47:39.729Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: {service_map,index}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                        41})
['ns_1@172.19.0.4','ns_1@db2.lan','ns_1@db3.lan']
[ns_server:debug,2025-05-15T18:47:39.730Z,ns_1@db2.lan:<0.2717.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":87,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"indexAdmin\":9100,\"indexHttp\":9102,\"indexHttps\":19102,\"indexScan\":9101,\"indexStreamCatchup\":9104,\"indexStreamInit\":9103,\"indexStreamMaint\":9105,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"n1ql\":8093,\"n1qlSSL\":18093,\"projector\":9999},\"hostname\":\"172.19.0.4\",\"serverGroup\":\"Group 1\"},{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"indexAdmin\":9100,\"indexHttp\":9102,\"indexHttps\":19102,\"indexScan\":9101,\"indexStreamCatchup\":9104,\"indexStreamInit\":9103,\"indexStreamMaint\":9105,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"n1ql\":8093,\"n1qlSSL\":18093,\"projector\":9999},\"thisNode\":true,\"hostname\":\"db2.lan\",\"serverGroup\":\"Group 1\"},{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"indexAdmin\":9100,\"indexHttp\":9102,\"indexHttps\":19102,\"indexScan\":9101,\"indexStreamCatchup\":9104,\"indexStreamInit\":9103,\"indexStreamMaint\":9105,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"n1ql\":8093,\"n1qlSSL\":18093,\"projector\":9999},\"hostname\":\"db3.lan\",\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[ns_server:debug,2025-05-15T18:47:39.757Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/indexing/rebalance/RebalanceToken">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{3,63914554059}}]}|
 <<"{\"MasterId\":\"28569ac00b9c1d7c50e39741027d428c\",\"RebalId\":\"62c17460b6c2fb91a6a08c24ea499705\",\"Source\":0,\"Error\":\"\",\"MasterIP\":\"172.19.0.4\",\"Version\":0,\"RebalPhase\":0,\"ActiveRebalancer\":1}">>]
[ns_server:error,2025-05-15T18:47:40.217Z,ns_1@db2.lan:service_status_keeper_worker<0.2743.0>:rest_utils:get_json:62]Request to (indexer) getIndexStatus with headers [] failed: {ok,
                                                             {{500,
                                                               "Internal Server Error"},
                                                              [{"Content-Length",
                                                                "130"},
                                                               {"Date",
                                                                "Thu, 15 May 2025 18:47:40 GMT"},
                                                               {"Content-Type",
                                                                "application/json"}],
                                                              <<"{\"code\":\"error\",\"error\":\"Fail to retrieve cluster-wide metadata from index service\",\"failedNodes\":[\"db2.lan:8091\",\"db3.lan:8091\"]}">>}}
[ns_server:error,2025-05-15T18:47:40.217Z,ns_1@db2.lan:service_status_keeper-index<0.2744.0>:service_status_keeper:handle_cast:103]Service service_index returned incorrect status
[ns_server:debug,2025-05-15T18:47:41.094Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{4,63914554061}}]}|
 <<"{\"indexer.plasma.backIndex.enablePageBloomFilter\":true,\"indexer.settings.allow_large_keys\":true,\"indexer.settings.bufferPoolBlockSize\":16384,\"indexer.settings.build.batch_size\":5,\"indexer.settings.compaction.abort_exceed_interval\":false,\"indexer.settings.compaction.check_period\":30,\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.days_of_week\":\"Sund"...>>]
[ns_server:debug,2025-05-15T18:47:41.096Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/indexing/settings/config/features/PlasmaEnablePageBloomFilterBackIndex">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554061}}]}|
 <<"{}">>]
[ns_server:debug,2025-05-15T18:47:49.779Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/indexing/rebalance/RebalanceToken">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{4,63914554069}}]}|
 '_deleted']
[ns_server:debug,2025-05-15T18:47:49.794Z,ns_1@db2.lan:service_agent-index<0.3373.0>:service_agent:cleanup_service:641]Cleaning up stale tasks:
[[{<<"rev">>,<<"AAAAAAAAAAA=">>},
  {<<"id">>,<<"prepare/62c17460b6c2fb91a6a08c24ea499705">>},
  {<<"type">>,<<"task-prepared">>},
  {<<"status">>,<<"task-running">>},
  {<<"isCancelable">>,true},
  {<<"progress">>,0},
  {<<"extra">>,
   {[{<<"rebalanceId">>,<<"62c17460b6c2fb91a6a08c24ea499705">>}]}}]]
[ns_server:debug,2025-05-15T18:47:49.795Z,ns_1@db2.lan:<0.3608.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.3406.0>} exited with reason normal
[ns_server:debug,2025-05-15T18:47:49.828Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: counters, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,44})
[{rebalance_success,{1747334869,1}},{rebalance_start,{1747334848,1}}]
[ns_server:debug,2025-05-15T18:47:49.829Z,ns_1@db2.lan:<0.2717.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":94,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"indexAdmin\":9100,\"indexHttp\":9102,\"indexHttps\":19102,\"indexScan\":9101,\"indexStreamCatchup\":9104,\"indexStreamInit\":9103,\"indexStreamMaint\":9105,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"n1ql\":8093,\"n1qlSSL\":18093,\"projector\":9999},\"hostname\":\"172.19.0.4\",\"serverGroup\":\"Group 1\"},{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"indexAdmin\":9100,\"indexHttp\":9102,\"indexHttps\":19102,\"indexScan\":9101,\"indexStreamCatchup\":9104,\"indexStreamInit\":9103,\"indexStreamMaint\":9105,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"n1ql\":8093,\"n1qlSSL\":18093,\"projector\":9999},\"thisNode\":true,\"hostname\":\"db2.lan\",\"serverGroup\":\"Group 1\"},{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"indexAdmin\":9100,\"indexHttp\":9102,\"indexHttps\":19102,\"indexScan\":9101,\"indexStreamCatchup\":9104,\"indexStreamInit\":9103,\"indexStreamMaint\":9105,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"n1ql\":8093,\"n1qlSSL\":18093,\"projector\":9999},\"hostname\":\"db3.lan\",\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[ns_server:debug,2025-05-15T18:47:49.836Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
rebalance_reports ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554069}}]},
 {<<"c1b1e82bc3018faaa44cc124f5be1eeb">>,
  [{node,'ns_1@172.19.0.4'},
   {filename,"rebalance_report_20250515T184749.json"}]}]
[ns_server:debug,2025-05-15T18:47:49.857Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: rebalance_status, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                     45})
none
[ns_server:debug,2025-05-15T18:47:49.857Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: rebalance_status_uuid, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                          45})
<<"e30f0e0de45bc0da5422537090074df6">>
[ns_server:debug,2025-05-15T18:47:49.857Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: rebalancer_pid, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,45})
undefined
[ns_server:debug,2025-05-15T18:47:49.857Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: rebalance_type, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,45})
rebalance
[ns_server:debug,2025-05-15T18:47:50.300Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: bucket_names, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,46})
["doom-scrolling"]
[ns_server:debug,2025-05-15T18:47:50.300Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: {bucket,"doom-scrolling",props}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                    46})
[{replica_index,true},
 {ram_quota,536870912},
 {durability_min_level,none},
 {num_vbuckets,1024},
 {cross_cluster_versioning_enabled,false},
 {version_pruning_window_hrs,720},
 {pitr_enabled,false},
 {pitr_granularity,600},
 {pitr_max_history_age,86400},
 {flush_enabled,false},
 {num_threads,3},
 {eviction_policy,value_only},
 {conflict_resolution_type,seqno},
 {storage_mode,couchstore},
 {max_ttl,0},
 {compression_mode,passive},
 {rank,0},
 {type,membase},
 {num_replicas,1},
 {replication_topology,star},
 {repl_type,dcp},
 {servers,[]}]
[ns_server:debug,2025-05-15T18:47:50.300Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: {bucket,"doom-scrolling",uuid}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                   46})
<<"f70f8d64d14cf7513107bff35eb6561d">>
[ns_server:debug,2025-05-15T18:47:50.300Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: {bucket,"doom-scrolling",collections}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                          46})
[{uid,1},
 {next_uid,2},
 {next_scope_uid,9},
 {next_coll_uid,10},
 {num_scopes,0},
 {num_collections,0},
 {scopes,[{"_default",
           [{uid,0},{collections,[{"_default",[{uid,0},{maxTTL,0}]}]}]},
          {"_system",
           [{uid,8},
            {collections,[{"_query",[{uid,9},{maxTTL,-1},{history,false}]},
                          {"_mobile",
                           [{uid,8},{maxTTL,-1},{history,false}]}]}]}]}]
[ns_server:debug,2025-05-15T18:47:50.300Z,ns_1@db2.lan:roles_cache<0.2447.0>:active_cache:clean:181]Clearing the roles_cache cache
[ns_server:debug,2025-05-15T18:47:50.301Z,ns_1@db2.lan:ns_bucket_worker<0.2721.0>:ns_bucket_worker:start_one_uploader:153]Starting uploader for bucket: "doom-scrolling"
[ns_server:debug,2025-05-15T18:47:50.301Z,ns_1@db2.lan:roles_cache<0.2447.0>:active_cache:clean:181]Clearing the roles_cache cache
[ns_server:debug,2025-05-15T18:47:50.301Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@172.19.0.4',
                   {"doom-scrolling",last_seen_collection_ids}}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                                       46})
[2,9,10]
[ns_server:debug,2025-05-15T18:47:50.301Z,ns_1@db2.lan:compiled_roles_cache<0.2444.0>:versioned_cache:handle_info:89]Flushing cache compiled_roles_cache due to version change from undefined to {[7,
                                                                              6],
                                                                             {0,
                                                                              1041567676},
                                                                             {0,
                                                                              1041567676},
                                                                             true,
                                                                             [{"doom-scrolling",
                                                                               <<"f70f8d64d14cf7513107bff35eb6561d">>,
                                                                               1}]}
[ns_server:debug,2025-05-15T18:47:50.301Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@db2.lan',{"doom-scrolling",last_seen_collection_ids}}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                                                      46})
[2,9,10]
[ns_server:debug,2025-05-15T18:47:50.301Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@db3.lan',{"doom-scrolling",last_seen_collection_ids}}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                                                      46})
[2,9,10]
[ns_server:debug,2025-05-15T18:47:50.303Z,ns_1@db2.lan:memcached_permissions<0.2514.0>:memcached_cfg:write_cfg:156]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2025-05-15T18:47:50.306Z,ns_1@db2.lan:memcached_permissions<0.2514.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,{user,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2025-05-15T18:47:50.308Z,ns_1@db2.lan:memcached_refresh<0.2369.0>:memcached_refresh:handle_call:57]File rename from "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac" is requested
[ns_server:debug,2025-05-15T18:47:50.310Z,ns_1@db2.lan:memcached_permissions<0.2514.0>:memcached_cfg:rename_and_refresh:178]Successfully renamed "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2025-05-15T18:47:50.310Z,ns_1@db2.lan:memcached_refresh<0.2369.0>:memcached_refresh:handle_cast:67]Refresh of rbac requested
[ns_server:debug,2025-05-15T18:47:50.311Z,ns_1@db2.lan:memcached_permissions<0.2514.0>:memcached_cfg:write_cfg:156]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2025-05-15T18:47:50.313Z,ns_1@db2.lan:memcached_permissions<0.2514.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,{user,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2025-05-15T18:47:50.316Z,ns_1@db2.lan:memcached_refresh<0.2369.0>:memcached_refresh:handle_call:57]File rename from "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac" is requested
[ns_server:debug,2025-05-15T18:47:50.317Z,ns_1@db2.lan:terse_bucket_info_uploader-doom-scrolling<0.4114.0>:memcached_config_mgr:memcached_port_pid:154]waiting for completion of initial ns_ports_setup round
[error_logger:info,2025-05-15T18:47:50.317Z,ns_1@db2.lan:ns_bucket_sup<0.2720.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_bucket_sup}
    started: [{pid,<0.4114.0>},
              {id,{terse_bucket_info_uploader,"doom-scrolling"}},
              {mfargs,
                  {terse_bucket_info_uploader,start_link,["doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:50.317Z,ns_1@db2.lan:memcached_permissions<0.2514.0>:memcached_cfg:rename_and_refresh:178]Successfully renamed "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2025-05-15T18:47:50.317Z,ns_1@db2.lan:memcached_refresh<0.2369.0>:memcached_refresh:handle_cast:67]Refresh of rbac requested
[ns_server:debug,2025-05-15T18:47:50.317Z,ns_1@db2.lan:<0.4113.0>:memcached_refresh:do_refresh:153]Successfully connected to memcached, Trying to refresh [rbac]
[ns_server:debug,2025-05-15T18:47:50.320Z,ns_1@db2.lan:memcached_refresh<0.2369.0>:memcached_refresh:update_refresh_state:116]Refresh of [rbac] succeeded
[ns_server:debug,2025-05-15T18:47:50.322Z,ns_1@db2.lan:<0.4118.0>:memcached_refresh:do_refresh:153]Successfully connected to memcached, Trying to refresh [rbac]
[ns_server:debug,2025-05-15T18:47:50.323Z,ns_1@db2.lan:memcached_refresh<0.2369.0>:memcached_refresh:update_refresh_state:116]Refresh of [rbac] succeeded
[ns_server:debug,2025-05-15T18:47:50.341Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: {bucket,"doom-scrolling",props}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                    47})
[{replica_index,true},
 {ram_quota,536870912},
 {durability_min_level,none},
 {num_vbuckets,1024},
 {cross_cluster_versioning_enabled,false},
 {version_pruning_window_hrs,720},
 {pitr_enabled,false},
 {pitr_granularity,600},
 {pitr_max_history_age,86400},
 {flush_enabled,false},
 {num_threads,3},
 {eviction_policy,value_only},
 {conflict_resolution_type,seqno},
 {storage_mode,couchstore},
 {max_ttl,0},
 {compression_mode,passive},
 {rank,0},
 {type,membase},
 {num_replicas,1},
 {replication_topology,star},
 {repl_type,dcp},
 {servers,['ns_1@172.19.0.4','ns_1@db2.lan','ns_1@db3.lan']},
 {map_diff,[]},
 {fastForwardMap_diff,[]}]
[ns_server:debug,2025-05-15T18:47:50.341Z,ns_1@db2.lan:roles_cache<0.2447.0>:active_cache:clean:181]Clearing the roles_cache cache
[ns_server:debug,2025-05-15T18:47:50.341Z,ns_1@db2.lan:ns_bucket_worker<0.2721.0>:ns_bucket_worker:start_one_bucket:125]Starting new bucket: "doom-scrolling"
[ns_server:debug,2025-05-15T18:47:50.343Z,ns_1@db2.lan:single_bucket_kv_sup-doom-scrolling<0.4121.0>:single_bucket_kv_sup:sync_config_to_couchdb_node:64]Syncing config to couchdb node
[ns_server:debug,2025-05-15T18:47:50.344Z,ns_1@db2.lan:ns_config_rep<0.2526.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 26 us
[ns_server:debug,2025-05-15T18:47:50.344Z,ns_1@db2.lan:single_bucket_kv_sup-doom-scrolling<0.4121.0>:single_bucket_kv_sup:sync_config_to_couchdb_node:70]Synced config to couchdb node successfully
[ns_server:debug,2025-05-15T18:47:50.351Z,ns_1@db2.lan:compiled_roles_cache<0.2444.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@projector",admin}
[ns_server:debug,2025-05-15T18:47:50.351Z,ns_1@db2.lan:compiled_roles_cache<0.2444.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@index",admin}
[ns_server:debug,2025-05-15T18:47:50.352Z,ns_1@db2.lan:compiled_roles_cache<0.2444.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@goxdcr",admin}
[ns_server:debug,2025-05-15T18:47:50.352Z,ns_1@db2.lan:capi_doc_replicator-doom-scrolling<0.4128.0>:replicated_storage:wait_for_startup:47]Start waiting for startup
[error_logger:info,2025-05-15T18:47:50.352Z,ns_1@db2.lan:<0.4127.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.4127.0>,docs_kv_sup}
    started: [{pid,<0.4128.0>},
              {id,doc_replicator},
              {mfargs,{capi_ddoc_manager,start_replicator,["doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:50.352Z,ns_1@db2.lan:compiled_roles_cache<0.2444.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@cbq-engine",admin}
[ns_server:debug,2025-05-15T18:47:50.352Z,ns_1@db2.lan:capi_ddoc_replication_srv-doom-scrolling<0.4129.0>:replicated_storage:wait_for_startup:47]Start waiting for startup
[error_logger:info,2025-05-15T18:47:50.352Z,ns_1@db2.lan:<0.4127.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.4127.0>,docs_kv_sup}
    started: [{pid,<0.4129.0>},
              {id,doc_replication_srv},
              {mfargs,{doc_replication_srv,start_link,["doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:50.353Z,ns_1@db2.lan:terse_bucket_info_uploader-doom-scrolling<0.4114.0>:memcached_config_mgr:memcached_port_pid:156]ns_ports_setup seems to be ready
[ns_server:debug,2025-05-15T18:47:50.355Z,ns_1@db2.lan:terse_bucket_info_uploader-doom-scrolling<0.4114.0>:memcached_config_mgr:find_port_pid_loop:164]Found memcached port <16971.139.0>
[ns_server:debug,2025-05-15T18:47:50.355Z,ns_1@db2.lan:terse_bucket_info_uploader-doom-scrolling<0.4114.0>:terse_bucket_info_uploader:flush_refresh_msgs:94]Flushed 1 refresh messages
[ns_server:debug,2025-05-15T18:47:50.356Z,ns_1@db2.lan:terse_bucket_info_uploader-doom-scrolling<0.4114.0>:terse_bucket_info_uploader:maybe_set_cluster_config:144]Bucket 'doom-scrolling' needs vbmap before setting cluster config
[error_logger:info,2025-05-15T18:47:50.379Z,ns_1@db2.lan:logger_proxy<0.70.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'capi_ddoc_manager_sup-doom-scrolling'}
    started: [{pid,<16972.650.0>},
              {id,capi_ddoc_manager_events},
              {mfargs,
                  {capi_ddoc_manager,start_link_event_manager,
                      ["doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:50.386Z,ns_1@db2.lan:capi_doc_replicator-doom-scrolling<0.4128.0>:replicated_storage:wait_for_startup:50]Received replicated storage registration from <16972.651.0>
[ns_server:debug,2025-05-15T18:47:50.386Z,ns_1@db2.lan:capi_ddoc_replication_srv-doom-scrolling<0.4129.0>:replicated_storage:wait_for_startup:50]Received replicated storage registration from <16972.651.0>
[error_logger:info,2025-05-15T18:47:50.387Z,ns_1@db2.lan:<0.4127.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.4127.0>,docs_kv_sup}
    started: [{pid,<16972.649.0>},
              {id,capi_ddoc_manager_sup},
              {mfargs,
                  {capi_ddoc_manager_sup,start_link_remote,
                      ['couchdb_ns_1@cb.local',"doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:50.387Z,ns_1@db2.lan:logger_proxy<0.70.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'capi_ddoc_manager_sup-doom-scrolling'}
    started: [{pid,<16972.651.0>},
              {id,capi_ddoc_manager},
              {mfargs,
                  {capi_ddoc_manager,start_link,
                      ["doom-scrolling",<0.4128.0>,<0.4129.0>]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:50.397Z,ns_1@db2.lan:<0.4127.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.4127.0>,docs_kv_sup}
    started: [{pid,<16972.653.0>},
              {id,capi_set_view_manager},
              {mfargs,
                  {capi_set_view_manager,start_link_remote,
                      ['couchdb_ns_1@cb.local',"doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:50.406Z,ns_1@db2.lan:<0.4127.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.4127.0>,docs_kv_sup}
    started: [{pid,<16972.656.0>},
              {id,couch_stats_reader},
              {mfargs,
                  {couch_stats_reader,start_link_remote,
                      ['couchdb_ns_1@cb.local',"doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:50.406Z,ns_1@db2.lan:single_bucket_kv_sup-doom-scrolling<0.4121.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'single_bucket_kv_sup-doom-scrolling'}
    started: [{pid,<0.4127.0>},
              {id,{docs_kv_sup,"doom-scrolling"}},
              {mfargs,{docs_kv_sup,start_link,["doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:47:50.410Z,ns_1@db2.lan:ns_memcached-doom-scrolling<0.4157.0>:ns_memcached:init:161]Starting ns_memcached
[ns_server:debug,2025-05-15T18:47:50.410Z,ns_1@db2.lan:<0.4158.0>:ns_memcached:run_connect_phase:188]Started 'connecting' phase of ns_memcached-doom-scrolling. Parent is <0.4157.0>
[error_logger:info,2025-05-15T18:47:50.410Z,ns_1@db2.lan:<0.4156.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.4156.0>,ns_memcached_sup}
    started: [{pid,<0.4157.0>},
              {id,{ns_memcached,"doom-scrolling"}},
              {mfargs,{ns_memcached,start_link,["doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,86400000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:50.410Z,ns_1@db2.lan:single_bucket_kv_sup-doom-scrolling<0.4121.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'single_bucket_kv_sup-doom-scrolling'}
    started: [{pid,<0.4156.0>},
              {id,{ns_memcached_sup,"doom-scrolling"}},
              {mfargs,{ns_memcached_sup,start_link,["doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:50.416Z,ns_1@db2.lan:single_bucket_kv_sup-doom-scrolling<0.4121.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'single_bucket_kv_sup-doom-scrolling'}
    started: [{pid,<0.4161.0>},
              {id,{dcp_sup,"doom-scrolling"}},
              {mfargs,{dcp_sup,start_link,["doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:50.422Z,ns_1@db2.lan:single_bucket_kv_sup-doom-scrolling<0.4121.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'single_bucket_kv_sup-doom-scrolling'}
    started: [{pid,<0.4162.0>},
              {id,{dcp_replication_manager,"doom-scrolling"}},
              {mfargs,{dcp_replication_manager,start_link,["doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:50.433Z,ns_1@db2.lan:single_bucket_kv_sup-doom-scrolling<0.4121.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'single_bucket_kv_sup-doom-scrolling'}
    started: [{pid,<0.4163.0>},
              {id,{replication_manager,"doom-scrolling"}},
              {mfargs,{replication_manager,start_link,["doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:50.447Z,ns_1@db2.lan:janitor_agent_sup-doom-scrolling<0.4164.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'janitor_agent_sup-doom-scrolling'}
    started: [{pid,<0.4165.0>},
              {id,rebalance_subprocesses_registry},
              {mfargs,
                  {ns_process_registry,start_link,
                      ['rebalance_subprocesses_registry-doom-scrolling',
                       [{terminate_command,kill}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,86400000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:50.455Z,ns_1@db2.lan:janitor_agent_sup-doom-scrolling<0.4164.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'janitor_agent_sup-doom-scrolling'}
    started: [{pid,<0.4166.0>},
              {id,janitor_agent},
              {mfargs,{janitor_agent,start_link,["doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:50.455Z,ns_1@db2.lan:single_bucket_kv_sup-doom-scrolling<0.4121.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'single_bucket_kv_sup-doom-scrolling'}
    started: [{pid,<0.4164.0>},
              {id,{janitor_agent_sup,"doom-scrolling"}},
              {mfargs,{janitor_agent_sup,start_link,["doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:50.455Z,ns_1@db2.lan:single_bucket_kv_sup-doom-scrolling<0.4121.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'single_bucket_kv_sup-doom-scrolling'}
    started: [{pid,<0.4167.0>},
              {id,{stats_reader,"doom-scrolling"}},
              {mfargs,{stats_reader,start_link,["doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:50.455Z,ns_1@db2.lan:single_bucket_kv_sup-doom-scrolling<0.4121.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'single_bucket_kv_sup-doom-scrolling'}
    started: [{pid,<0.4169.0>},
              {id,{goxdcr_stats_reader,"doom-scrolling"}},
              {mfargs,{stats_reader,start_link,["@xdcr-doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:50.455Z,ns_1@db2.lan:ns_bucket_sup<0.2720.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_bucket_sup}
    started: [{pid,<0.4121.0>},
              {id,{single_bucket_kv_sup,"doom-scrolling"}},
              {mfargs,{single_bucket_kv_sup,start_link,["doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:47:50.456Z,ns_1@db2.lan:janitor_agent-doom-scrolling<0.4166.0>:dcp_sup:nuke:110]Nuking DCP replicators for bucket "doom-scrolling":
[]
[ns_server:debug,2025-05-15T18:47:50.463Z,ns_1@db2.lan:capi_doc_replicator-doom-scrolling<0.4128.0>:doc_replicator:loop:79]Replicating all docs to new nodes: ['ns_1@172.19.0.4','ns_1@db3.lan']
[ns_server:info,2025-05-15T18:47:50.466Z,ns_1@db2.lan:ns_memcached-doom-scrolling<0.4157.0>:ns_memcached:do_ensure_bucket:1592]Created bucket "doom-scrolling" with config string "max_size=536870912;dbname=/opt/couchbase/var/lib/couchbase/data/doom-scrolling;backend=couchdb;couch_bucket=doom-scrolling;max_vbuckets=1024;alog_path=/opt/couchbase/var/lib/couchbase/data/doom-scrolling/access.log;data_traffic_enabled=false;max_num_workers=3;uuid=f70f8d64d14cf7513107bff35eb6561d;conflict_resolution_type=seqno;bucket_type=persistent;durability_min_level=none;pitr_enabled=false;pitr_granularity=600;pitr_max_history_age=86400;item_eviction_policy=value_only;persistent_metadata_purge_age=259200;max_ttl=0;ht_locks=47;compression_mode=passive;max_num_shards=0;failpartialwarmup=false"
[ns_server:debug,2025-05-15T18:47:50.468Z,ns_1@db2.lan:ns_memcached-doom-scrolling<0.4157.0>:memcached_bucket_config:ensure_collections:291]Applying collection manifest to bucket "doom-scrolling" due to id change from <<"0">> to <<"1">>.
[ns_server:info,2025-05-15T18:47:50.474Z,ns_1@db2.lan:ns_memcached-doom-scrolling<0.4157.0>:ns_memcached:handle_info:837]Main ns_memcached connection established: {ok,#Port<0.207>}
[ns_server:debug,2025-05-15T18:47:50.475Z,ns_1@db2.lan:terse_bucket_info_uploader-doom-scrolling<0.4114.0>:terse_bucket_info_uploader:maybe_set_cluster_config:144]Bucket 'doom-scrolling' needs vbmap before setting cluster config
[user:info,2025-05-15T18:47:50.477Z,ns_1@db2.lan:ns_memcached-doom-scrolling<0.4157.0>:ns_memcached:handle_cast:806]Bucket "doom-scrolling" loaded on node 'ns_1@db2.lan' in 0 seconds.
[ns_server:info,2025-05-15T18:47:50.480Z,ns_1@db2.lan:janitor_agent-doom-scrolling<0.4166.0>:janitor_agent:read_flush_counter:969]Loading flushseq failed: {error,enoent}. Assuming it's equal to global config.
[ns_server:info,2025-05-15T18:47:50.481Z,ns_1@db2.lan:janitor_agent-doom-scrolling<0.4166.0>:janitor_agent:read_flush_counter_from_config:977]Initialized flushseq 0 from bucket config
[ns_server:debug,2025-05-15T18:47:50.628Z,ns_1@db2.lan:compiled_roles_cache<0.2444.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@",admin}
[ns_server:debug,2025-05-15T18:47:51.323Z,ns_1@db2.lan:compiled_roles_cache<0.2444.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@prometheus",stats_reader}
[ns_server:debug,2025-05-15T18:47:51.358Z,ns_1@db2.lan:terse_bucket_info_uploader-doom-scrolling<0.4114.0>:terse_bucket_info_uploader:maybe_set_cluster_config:144]Bucket 'doom-scrolling' needs vbmap before setting cluster config
[ns_server:debug,2025-05-15T18:47:51.492Z,ns_1@db2.lan:terse_bucket_info_uploader-doom-scrolling<0.4114.0>:terse_bucket_info_uploader:maybe_set_cluster_config:144]Bucket 'doom-scrolling' needs vbmap before setting cluster config
[ns_server:debug,2025-05-15T18:47:51.990Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: {bucket,"doom-scrolling",last_balanced_vbmap}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                                  48})
{[['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4'|...],
  [...]|...],
 [{replication_topology,star},
  {tags,undefined},
  {use_vbmap_greedy_optimization,true},
  {max_slaves,10}]}
[ns_server:debug,2025-05-15T18:47:51.991Z,ns_1@db2.lan:terse_bucket_info_uploader-doom-scrolling<0.4114.0>:terse_bucket_info_uploader:maybe_set_cluster_config:144]Bucket 'doom-scrolling' needs vbmap before setting cluster config
[ns_server:debug,2025-05-15T18:47:52.015Z,ns_1@db2.lan:roles_cache<0.2447.0>:active_cache:clean:181]Clearing the roles_cache cache
[ns_server:debug,2025-05-15T18:47:52.015Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: {bucket,"doom-scrolling",props}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                    49})
[{map_diff,[{0,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {1,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {2,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {3,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {4,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {5,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {6,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {7,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {8,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {9,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {10,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {11,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {12,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {13,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {14,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {15,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {16,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {17,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {18,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {19,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {20,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {21,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {22,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {23,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {24,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {25,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {26,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {27,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {28,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {29,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {30,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {31,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {32,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {33,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {34,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {35,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {36,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {37,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {38,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {39,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {40,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {41,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {42,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {43,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {44,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {45,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {46,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {47,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {48,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {49,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {50,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {51,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {52,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {53,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {54,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {55,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {56,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {57,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {58,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {59,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {60,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {61,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {62,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {63,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {64,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {65,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {66,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {67,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {68,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {69,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {70,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {71,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {72,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {73,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {74,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {75,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {76,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {77,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {78,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {79,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {80,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {81,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {82,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {83,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {84,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {85,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {86,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {87,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {88,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {89,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {90,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {91,[],['ns_1@172.19.0.4'|...]},
            {92,[],[...]},
            {93,[],...},
            {94,...},
            {...}|...]},
 {map_opts_hash,42591107},
 {replica_index,true},
 {ram_quota,536870912},
 {durability_min_level,none},
 {num_vbuckets,1024},
 {cross_cluster_versioning_enabled,false},
 {version_pruning_window_hrs,720},
 {pitr_enabled,false},
 {pitr_granularity,600},
 {pitr_max_history_age,86400},
 {flush_enabled,false},
 {num_threads,3},
 {eviction_policy,value_only},
 {conflict_resolution_type,seqno},
 {storage_mode,couchstore},
 {max_ttl,0},
 {compression_mode,passive},
 {rank,0},
 {type,membase},
 {num_replicas,1},
 {replication_topology,star},
 {repl_type,dcp},
 {servers,['ns_1@172.19.0.4','ns_1@db2.lan','ns_1@db3.lan']},
 {fastForwardMap_diff,[]}]
[ns_server:info,2025-05-15T18:47:52.230Z,ns_1@db2.lan:<0.4176.0>:ns_memcached:do_handle_call:710]Changed vbucket state 
[{1023,replica,undefined},
 {1022,replica,undefined},
 {1021,replica,undefined},
 {1020,replica,undefined},
 {1019,replica,undefined},
 {1018,replica,undefined},
 {1017,replica,undefined},
 {1016,replica,undefined},
 {1015,replica,undefined},
 {1014,replica,undefined},
 {1013,replica,undefined},
 {1012,replica,undefined},
 {1011,replica,undefined},
 {1010,replica,undefined},
 {1009,replica,undefined},
 {1008,replica,undefined},
 {1007,replica,undefined},
 {1006,replica,undefined},
 {1005,replica,undefined},
 {1004,replica,undefined},
 {1003,replica,undefined},
 {1002,replica,undefined},
 {1001,replica,undefined},
 {1000,replica,undefined},
 {999,replica,undefined},
 {998,replica,undefined},
 {997,replica,undefined},
 {996,replica,undefined},
 {995,replica,undefined},
 {994,replica,undefined},
 {993,replica,undefined},
 {992,replica,undefined},
 {991,replica,undefined},
 {990,replica,undefined},
 {989,replica,undefined},
 {988,replica,undefined},
 {987,replica,undefined},
 {986,replica,undefined},
 {985,replica,undefined},
 {984,replica,undefined},
 {983,replica,undefined},
 {982,replica,undefined},
 {981,replica,undefined},
 {980,replica,undefined},
 {979,replica,undefined},
 {978,replica,undefined},
 {977,replica,undefined},
 {976,replica,undefined},
 {975,replica,undefined},
 {974,replica,undefined},
 {973,replica,undefined},
 {972,replica,undefined},
 {971,replica,undefined},
 {970,replica,undefined},
 {969,replica,undefined},
 {968,replica,undefined},
 {967,replica,undefined},
 {966,replica,undefined},
 {965,replica,undefined},
 {964,replica,undefined},
 {963,replica,undefined},
 {962,replica,undefined},
 {961,replica,undefined},
 {960,replica,undefined},
 {959,replica,undefined},
 {958,replica,undefined},
 {957,replica,undefined},
 {956,replica,undefined},
 {955,replica,undefined},
 {954,replica,undefined},
 {953,replica,undefined},
 {952,replica,undefined},
 {951,replica,undefined},
 {950,replica,undefined},
 {949,replica,undefined},
 {948,replica,undefined},
 {947,replica,undefined},
 {946,replica,undefined},
 {945,replica,undefined},
 {944,replica,undefined},
 {943,replica,undefined},
 {942,replica,undefined},
 {941,replica,undefined},
 {940,replica,undefined},
 {939,replica,undefined},
 {938,replica,undefined},
 {937,replica,undefined},
 {936,replica,undefined},
 {935,replica,undefined},
 {934,replica,undefined},
 {933,replica,undefined},
 {932,replica,undefined},
 {931,replica,undefined},
 {930,replica,undefined},
 {929,replica,undefined},
 {928,replica,undefined},
 {927,replica,undefined},
 {926,replica,undefined},
 {925,replica,undefined},
 {924,replica,undefined},
 {923,replica,undefined},
 {922,replica,undefined},
 {921,replica,undefined},
 {920,replica,undefined},
 {919,replica,undefined},
 {918,replica,undefined},
 {917,replica,undefined},
 {916,replica,undefined},
 {915,replica,undefined},
 {914,replica,undefined},
 {913,replica,undefined},
 {912,replica,undefined},
 {911,replica,undefined},
 {910,replica,undefined},
 {909,replica,undefined},
 {908,replica,undefined},
 {907,replica,undefined},
 {906,replica,undefined},
 {905,replica,undefined},
 {904,replica,undefined},
 {903,replica,undefined},
 {902,replica,undefined},
 {901,replica,undefined},
 {900,replica,undefined},
 {899,replica,undefined},
 {898,replica,undefined},
 {897,replica,undefined},
 {896,replica,undefined},
 {895,replica,undefined},
 {894,replica,undefined},
 {893,replica,undefined},
 {892,replica,undefined},
 {891,replica,undefined},
 {890,replica,undefined},
 {889,replica,undefined},
 {888,replica,undefined},
 {887,replica,undefined},
 {886,replica,undefined},
 {885,replica,undefined},
 {884,replica,undefined},
 {883,replica,undefined},
 {882,replica,undefined},
 {881,replica,undefined},
 {880,replica,undefined},
 {879,replica,undefined},
 {878,replica,undefined},
 {877,replica,undefined},
 {876,replica,undefined},
 {875,replica,undefined},
 {874,replica,undefined},
 {873,replica,undefined},
 {872,replica,undefined},
 {871,replica,undefined},
 {870,replica,undefined},
 {869,replica,undefined},
 {868,replica,undefined},
 {867,replica,undefined},
 {866,replica,undefined},
 {865,replica,undefined},
 {864,replica,undefined},
 {863,replica,undefined},
 {862,replica,undefined},
 {861,replica,undefined},
 {860,replica,undefined},
 {859,replica,undefined},
 {858,replica,undefined},
 {857,replica,undefined},
 {856,replica,undefined},
 {855,replica,undefined},
 {854,replica,undefined},
 {853,replica,undefined},
 {681,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {680,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {679,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {678,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {677,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {676,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {675,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {674,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {673,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {672,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {671,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {670,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {669,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {668,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {667,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {666,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {665,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {664,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {663,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {662,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {661,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {660,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {659,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {658,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {657,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {656,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {655,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {654,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {653,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {652,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {651,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {650,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {649,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {648,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {647,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {646,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {645,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {644,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {643,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {642,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {641,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {640,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {639,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {638,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {637,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {636,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {635,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {634,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {633,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {632,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {631,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {630,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {629,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {628,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {627,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {626,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {625,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {624,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {623,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {622,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {621,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {620,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {619,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {618,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {617,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {616,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {615,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {614,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {613,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {612,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {611,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {610,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {609,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {608,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {607,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {606,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {605,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {604,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {603,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {602,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {601,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {600,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {599,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {598,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {597,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {596,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {595,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {594,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {593,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {592,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {591,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {590,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {589,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {588,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {587,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {586,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {585,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {584,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {583,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {582,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {581,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {580,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {579,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {578,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {577,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {576,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {575,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {574,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {573,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {572,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {571,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {570,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {569,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {568,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {567,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {566,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {565,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {564,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {563,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {562,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {561,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {560,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {559,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {558,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {557,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {556,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {555,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {554,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {553,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {552,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {551,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {550,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {549,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {548,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {547,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {546,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {545,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {544,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {543,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {542,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {541,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {540,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {539,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {538,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {537,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {536,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {535,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {534,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {533,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {532,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {531,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {530,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {529,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {528,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {527,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {526,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {525,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {524,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {523,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {522,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {521,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {520,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {519,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {518,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {517,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {516,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {515,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {514,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {513,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {512,active,{[{topology,[['ns_1@db2.lan','ns_1@db3.lan']]}]}},
 {511,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {510,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {509,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {508,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {507,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {506,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {505,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {504,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {503,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {502,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {501,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {500,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {499,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {498,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {497,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {496,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {495,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {494,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {493,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {492,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {491,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {490,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {489,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {488,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {487,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {486,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {485,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {484,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {483,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {482,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {481,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {480,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {479,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {478,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {477,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {476,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {475,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {474,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {473,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {472,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {471,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {470,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {469,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {468,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {467,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {466,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {465,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {464,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {463,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {462,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {461,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {460,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {459,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {458,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {457,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {456,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {455,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {454,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {453,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {452,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {451,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {450,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {449,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {448,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {447,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {446,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {445,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {444,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {443,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {442,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {441,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {440,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {439,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {438,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {437,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {436,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {435,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {434,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {433,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {432,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {431,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {430,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {429,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {428,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {427,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {426,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {425,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {424,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {423,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {422,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {421,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {420,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {419,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {418,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {417,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {416,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {415,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {414,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {413,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {412,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {411,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {410,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {409,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {408,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {407,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {406,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {405,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {404,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {403,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {402,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {401,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {400,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {399,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {398,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {397,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {396,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {395,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {394,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {393,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {392,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {391,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {390,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {389,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {388,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {387,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {386,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {385,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {384,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {383,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {382,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {381,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {380,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {379,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {378,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {377,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {376,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {375,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {374,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {373,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {372,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {371,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {370,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {369,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {368,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {367,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {366,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {365,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {364,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {363,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {362,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {361,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {360,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {359,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {358,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {357,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {356,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {355,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {354,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {353,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {352,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {351,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {350,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {349,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {348,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {347,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {346,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {345,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {344,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {343,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {342,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {341,active,{[{topology,[['ns_1@db2.lan','ns_1@172.19.0.4']]}]}},
 {169,replica,undefined},
 {168,replica,undefined},
 {167,replica,undefined},
 {166,replica,undefined},
 {165,replica,undefined},
 {164,replica,undefined},
 {163,replica,undefined},
 {162,replica,undefined},
 {161,replica,undefined},
 {160,replica,undefined},
 {159,replica,undefined},
 {158,replica,undefined},
 {157,replica,undefined},
 {156,replica,undefined},
 {155,replica,undefined},
 {154,replica,undefined},
 {153,replica,undefined},
 {152,replica,undefined},
 {151,replica,undefined},
 {150,replica,undefined},
 {149,replica,undefined},
 {148,replica,undefined},
 {147,replica,undefined},
 {146,replica,undefined},
 {145,replica,undefined},
 {144,replica,undefined},
 {143,replica,undefined},
 {142,replica,undefined},
 {141,replica,undefined},
 {140,replica,undefined},
 {139,replica,undefined},
 {138,replica,undefined},
 {137,replica,undefined},
 {136,replica,undefined},
 {135,replica,undefined},
 {134,replica,undefined},
 {133,replica,undefined},
 {132,replica,undefined},
 {131,replica,undefined},
 {130,replica,undefined},
 {129,replica,undefined},
 {128,replica,undefined},
 {127,replica,undefined},
 {126,replica,undefined},
 {125,replica,undefined},
 {124,replica,undefined},
 {123,replica,undefined},
 {122,replica,undefined},
 {121,replica,undefined},
 {120,replica,undefined},
 {119,replica,undefined},
 {118,replica,undefined},
 {117,replica,undefined},
 {116,replica,undefined},
 {115,replica,undefined},
 {114,replica,undefined},
 {113,replica,undefined},
 {112,replica,undefined},
 {111,replica,undefined},
 {110,replica,undefined},
 {109,replica,undefined},
 {108,replica,undefined},
 {107,replica,undefined},
 {106,replica,undefined},
 {105,replica,undefined},
 {104,replica,undefined},
 {103,replica,undefined},
 {102,replica,undefined},
 {101,replica,undefined},
 {100,replica,undefined},
 {99,replica,undefined},
 {98,replica,undefined},
 {97,replica,undefined},
 {96,replica,undefined},
 {95,replica,undefined},
 {94,replica,undefined},
 {93,replica,undefined},
 {92,replica,undefined},
 {91,replica,undefined},
 {90,replica,undefined},
 {89,replica,undefined},
 {88,replica,undefined},
 {87,replica,undefined},
 {86,replica,undefined},
 {85,replica,undefined},
 {84,replica,undefined},
 {83,replica,undefined},
 {82,replica,undefined},
 {81,replica,undefined},
 {80,replica,undefined},
 {79,replica,undefined},
 {78,replica,undefined},
 {77,replica,undefined},
 {76,replica,undefined},
 {75,replica,undefined},
 {74,replica,undefined},
 {73,replica,undefined},
 {72,replica,undefined},
 {71,replica,undefined},
 {70,replica,undefined},
 {69,replica,undefined},
 {68,replica,undefined},
 {67,replica,undefined},
 {66,replica,undefined},
 {65,replica,undefined},
 {64,replica,undefined},
 {63,replica,undefined},
 {62,replica,undefined},
 {61,replica,undefined},
 {60,replica,undefined},
 {59,replica,undefined},
 {58,replica,undefined},
 {57,replica,undefined},
 {56,replica,undefined},
 {55,replica,undefined},
 {54,replica,undefined},
 {53,replica,undefined},
 {52,replica,undefined},
 {51,replica,undefined},
 {50,replica,undefined},
 {49,replica,undefined},
 {48,replica,undefined},
 {47,replica,undefined},
 {46,replica,undefined},
 {45,replica,undefined},
 {44,replica,undefined},
 {43,replica,undefined},
 {42,replica,undefined},
 {41,replica,undefined},
 {40,replica,undefined},
 {39,replica,undefined},
 {38,replica,undefined},
 {37,replica,undefined},
 {36,replica,undefined},
 {35,replica,undefined},
 {34,replica,undefined},
 {33,replica,undefined},
 {32,replica,undefined},
 {31,replica,undefined},
 {30,replica,undefined},
 {29,replica,undefined},
 {28,replica,undefined},
 {27,replica,undefined},
 {26,replica,undefined},
 {25,replica,undefined},
 {24,replica,undefined},
 {23,replica,undefined},
 {22,replica,undefined},
 {21,replica,undefined},
 {20,replica,undefined},
 {19,replica,undefined},
 {18,replica,undefined},
 {17,replica,undefined},
 {16,replica,undefined},
 {15,replica,undefined},
 {14,replica,undefined},
 {13,replica,undefined},
 {12,replica,undefined},
 {11,replica,undefined},
 {10,replica,undefined},
 {9,replica,undefined},
 {8,replica,undefined},
 {7,replica,undefined},
 {6,replica,undefined},
 {5,replica,undefined},
 {4,replica,undefined},
 {3,replica,undefined},
 {2,replica,undefined},
 {1,replica,undefined},
 {0,replica,undefined}]
[ns_server:debug,2025-05-15T18:47:52.261Z,ns_1@db2.lan:dcp_replication_manager-doom-scrolling<0.4162.0>:dcp_sup:start_replicator:48]Starting DCP replication from 'ns_1@172.19.0.4' for bucket "doom-scrolling" (Features = [collections,
                                                                                         del_times,
                                                                                         del_user_xattr,
                                                                                         json,
                                                                                         set_consumer_name,
                                                                                         snappy,
                                                                                         xattr])
[ns_server:debug,2025-05-15T18:47:52.275Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:open_connection:72]Open consumer connection "replication:ns_1@172.19.0.4->ns_1@db2.lan:doom-scrolling" on socket #Port<0.212>: Body <<"{\"consumer_name\":\"ns_1@db2.lan\"}">>
[ns_server:debug,2025-05-15T18:47:52.276Z,ns_1@db2.lan:dcp_replicator-doom-scrolling-ns_1@172.19.0.4<0.4264.0>:dcp_replicator:init:53]Opened connection to local memcached <0.4265.0>
[error_logger:info,2025-05-15T18:47:52.280Z,ns_1@db2.lan:dcp_sup-doom-scrolling<0.4161.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'dcp_sup-doom-scrolling'}
    started: [{pid,<0.4264.0>},
              {id,{'ns_1@172.19.0.4',[collections,del_times,del_user_xattr,
                                      json,set_consumer_name,snappy,xattr]}},
              {mfargs,{dcp_replicator,start_link,
                                      ['ns_1@172.19.0.4',"doom-scrolling",
                                       [collections,del_times,del_user_xattr,
                                        json,set_consumer_name,snappy,
                                        xattr]]}},
              {restart_type,temporary},
              {significant,false},
              {shutdown,60000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:52.280Z,ns_1@db2.lan:dcp_replication_manager-doom-scrolling<0.4162.0>:dcp_sup:start_replicator:48]Starting DCP replication from 'ns_1@db3.lan' for bucket "doom-scrolling" (Features = [collections,
                                                                                      del_times,
                                                                                      del_user_xattr,
                                                                                      json,
                                                                                      set_consumer_name,
                                                                                      snappy,
                                                                                      xattr])
[ns_server:debug,2025-05-15T18:47:52.285Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:open_connection:72]Open consumer connection "replication:ns_1@db3.lan->ns_1@db2.lan:doom-scrolling" on socket #Port<0.214>: Body <<"{\"consumer_name\":\"ns_1@db2.lan\"}">>
[ns_server:debug,2025-05-15T18:47:52.286Z,ns_1@db2.lan:dcp_replicator-doom-scrolling-ns_1@db3.lan<0.4276.0>:dcp_replicator:init:53]Opened connection to local memcached <0.4277.0>
[error_logger:info,2025-05-15T18:47:52.286Z,ns_1@db2.lan:dcp_sup-doom-scrolling<0.4161.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'dcp_sup-doom-scrolling'}
    started: [{pid,<0.4276.0>},
              {id,{'ns_1@db3.lan',[collections,del_times,del_user_xattr,json,
                                   set_consumer_name,snappy,xattr]}},
              {mfargs,{dcp_replicator,start_link,
                                      ['ns_1@db3.lan',"doom-scrolling",
                                       [collections,del_times,del_user_xattr,
                                        json,set_consumer_name,snappy,
                                        xattr]]}},
              {restart_type,temporary},
              {significant,false},
              {shutdown,60000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:52.286Z,ns_1@db2.lan:<0.4274.0>:dcp_commands:open_connection:72]Open producer connection "replication:ns_1@172.19.0.4->ns_1@db2.lan:doom-scrolling" on socket #Port<0.213>: Body undefined
[ns_server:debug,2025-05-15T18:47:52.287Z,ns_1@db2.lan:<0.4275.0>:dcp_replicator:connect_to_producer:81]initiated new dcp replication with consumer side: <0.4265.0> and producer side: <0.4274.0>
[ns_server:debug,2025-05-15T18:47:52.287Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 0, opaque = 0x0, type = add
[ns_server:debug,2025-05-15T18:47:52.287Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 1, opaque = 0x1, type = add
[ns_server:debug,2025-05-15T18:47:52.287Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 2, opaque = 0x2, type = add
[ns_server:debug,2025-05-15T18:47:52.287Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 3, opaque = 0x3, type = add
[ns_server:debug,2025-05-15T18:47:52.287Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 4, opaque = 0x4, type = add
[ns_server:debug,2025-05-15T18:47:52.288Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 5, opaque = 0x5, type = add
[ns_server:debug,2025-05-15T18:47:52.288Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 6, opaque = 0x6, type = add
[ns_server:debug,2025-05-15T18:47:52.288Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 7, opaque = 0x7, type = add
[ns_server:debug,2025-05-15T18:47:52.288Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 8, opaque = 0x8, type = add
[ns_server:debug,2025-05-15T18:47:52.288Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 9, opaque = 0x9, type = add
[ns_server:debug,2025-05-15T18:47:52.289Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 10, opaque = 0xA, type = add
[ns_server:debug,2025-05-15T18:47:52.289Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 11, opaque = 0xB, type = add
[ns_server:debug,2025-05-15T18:47:52.289Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 12, opaque = 0xC, type = add
[ns_server:debug,2025-05-15T18:47:52.289Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 13, opaque = 0xD, type = add
[ns_server:debug,2025-05-15T18:47:52.289Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 14, opaque = 0xE, type = add
[ns_server:debug,2025-05-15T18:47:52.289Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 15, opaque = 0xF, type = add
[ns_server:debug,2025-05-15T18:47:52.289Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 16, opaque = 0x10, type = add
[ns_server:debug,2025-05-15T18:47:52.289Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 17, opaque = 0x11, type = add
[ns_server:debug,2025-05-15T18:47:52.289Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 18, opaque = 0x12, type = add
[ns_server:debug,2025-05-15T18:47:52.289Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 19, opaque = 0x13, type = add
[ns_server:debug,2025-05-15T18:47:52.289Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 20, opaque = 0x14, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 21, opaque = 0x15, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 22, opaque = 0x16, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 23, opaque = 0x17, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 24, opaque = 0x18, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 25, opaque = 0x19, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 26, opaque = 0x1A, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 27, opaque = 0x1B, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 28, opaque = 0x1C, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 29, opaque = 0x1D, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 30, opaque = 0x1E, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 31, opaque = 0x1F, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 32, opaque = 0x20, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 33, opaque = 0x21, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 34, opaque = 0x22, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 35, opaque = 0x23, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 36, opaque = 0x24, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 37, opaque = 0x25, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 38, opaque = 0x26, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 39, opaque = 0x27, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 40, opaque = 0x28, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 41, opaque = 0x29, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 42, opaque = 0x2A, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 43, opaque = 0x2B, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 44, opaque = 0x2C, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 45, opaque = 0x2D, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 46, opaque = 0x2E, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 47, opaque = 0x2F, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 48, opaque = 0x30, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 49, opaque = 0x31, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 50, opaque = 0x32, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 51, opaque = 0x33, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 52, opaque = 0x34, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 53, opaque = 0x35, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 54, opaque = 0x36, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 55, opaque = 0x37, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 56, opaque = 0x38, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 57, opaque = 0x39, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 58, opaque = 0x3A, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 59, opaque = 0x3B, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 60, opaque = 0x3C, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 61, opaque = 0x3D, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 62, opaque = 0x3E, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 63, opaque = 0x3F, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 64, opaque = 0x40, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 65, opaque = 0x41, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 66, opaque = 0x42, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 67, opaque = 0x43, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 68, opaque = 0x44, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 69, opaque = 0x45, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 70, opaque = 0x46, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 71, opaque = 0x47, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 72, opaque = 0x48, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 73, opaque = 0x49, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 74, opaque = 0x4A, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 75, opaque = 0x4B, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 76, opaque = 0x4C, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 77, opaque = 0x4D, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 78, opaque = 0x4E, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 79, opaque = 0x4F, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 80, opaque = 0x50, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 81, opaque = 0x51, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 82, opaque = 0x52, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 83, opaque = 0x53, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 84, opaque = 0x54, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 85, opaque = 0x55, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 86, opaque = 0x56, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 87, opaque = 0x57, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 88, opaque = 0x58, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 89, opaque = 0x59, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 90, opaque = 0x5A, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 91, opaque = 0x5B, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 92, opaque = 0x5C, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 93, opaque = 0x5D, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 94, opaque = 0x5E, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 95, opaque = 0x5F, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 96, opaque = 0x60, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 97, opaque = 0x61, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 98, opaque = 0x62, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 99, opaque = 0x63, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 100, opaque = 0x64, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 101, opaque = 0x65, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 102, opaque = 0x66, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 103, opaque = 0x67, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 104, opaque = 0x68, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 105, opaque = 0x69, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 106, opaque = 0x6A, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 107, opaque = 0x6B, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 108, opaque = 0x6C, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 109, opaque = 0x6D, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 110, opaque = 0x6E, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 111, opaque = 0x6F, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 112, opaque = 0x70, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 113, opaque = 0x71, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 114, opaque = 0x72, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 115, opaque = 0x73, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 116, opaque = 0x74, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 117, opaque = 0x75, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 118, opaque = 0x76, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 119, opaque = 0x77, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 120, opaque = 0x78, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 121, opaque = 0x79, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 122, opaque = 0x7A, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 123, opaque = 0x7B, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 124, opaque = 0x7C, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 125, opaque = 0x7D, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 126, opaque = 0x7E, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 127, opaque = 0x7F, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 128, opaque = 0x80, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 129, opaque = 0x81, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 130, opaque = 0x82, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 131, opaque = 0x83, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 132, opaque = 0x84, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 133, opaque = 0x85, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 134, opaque = 0x86, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 135, opaque = 0x87, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 136, opaque = 0x88, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 137, opaque = 0x89, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 138, opaque = 0x8A, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 139, opaque = 0x8B, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 140, opaque = 0x8C, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 141, opaque = 0x8D, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 142, opaque = 0x8E, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 143, opaque = 0x8F, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 144, opaque = 0x90, type = add
[ns_server:debug,2025-05-15T18:47:52.297Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 145, opaque = 0x91, type = add
[ns_server:debug,2025-05-15T18:47:52.297Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 146, opaque = 0x92, type = add
[ns_server:debug,2025-05-15T18:47:52.297Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 147, opaque = 0x93, type = add
[ns_server:debug,2025-05-15T18:47:52.297Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 148, opaque = 0x94, type = add
[ns_server:debug,2025-05-15T18:47:52.297Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 149, opaque = 0x95, type = add
[ns_server:debug,2025-05-15T18:47:52.298Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 150, opaque = 0x96, type = add
[ns_server:debug,2025-05-15T18:47:52.298Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 151, opaque = 0x97, type = add
[ns_server:debug,2025-05-15T18:47:52.298Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 152, opaque = 0x98, type = add
[ns_server:debug,2025-05-15T18:47:52.299Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 153, opaque = 0x99, type = add
[ns_server:debug,2025-05-15T18:47:52.299Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 154, opaque = 0x9A, type = add
[ns_server:debug,2025-05-15T18:47:52.299Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 155, opaque = 0x9B, type = add
[ns_server:debug,2025-05-15T18:47:52.299Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 156, opaque = 0x9C, type = add
[ns_server:debug,2025-05-15T18:47:52.300Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 157, opaque = 0x9D, type = add
[ns_server:debug,2025-05-15T18:47:52.300Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 158, opaque = 0x9E, type = add
[ns_server:debug,2025-05-15T18:47:52.300Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 159, opaque = 0x9F, type = add
[ns_server:debug,2025-05-15T18:47:52.300Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 160, opaque = 0xA0, type = add
[ns_server:debug,2025-05-15T18:47:52.300Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 161, opaque = 0xA1, type = add
[ns_server:debug,2025-05-15T18:47:52.300Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 162, opaque = 0xA2, type = add
[ns_server:debug,2025-05-15T18:47:52.300Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 163, opaque = 0xA3, type = add
[ns_server:debug,2025-05-15T18:47:52.300Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 164, opaque = 0xA4, type = add
[ns_server:debug,2025-05-15T18:47:52.301Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 165, opaque = 0xA5, type = add
[ns_server:debug,2025-05-15T18:47:52.301Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 166, opaque = 0xA6, type = add
[ns_server:debug,2025-05-15T18:47:52.301Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 167, opaque = 0xA7, type = add
[ns_server:debug,2025-05-15T18:47:52.301Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 168, opaque = 0xA8, type = add
[ns_server:debug,2025-05-15T18:47:52.301Z,ns_1@db2.lan:<0.4265.0>:dcp_commands:add_stream:83]Add stream for partition 169, opaque = 0xA9, type = add
[ns_server:debug,2025-05-15T18:47:52.301Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_call:206]Setup DCP streams:
Current []
Streams to open [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169]
Streams to close []

[ns_server:debug,2025-05-15T18:47:52.301Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0x4000000
80 5E 00 16
00 00 00 00
00 00 00 1E
04 00 00 00
00 00 00 00
00 00 00 00
63 6F 6E 6E
65 63 74 69
6F 6E 5F 62
75 66 66 65
72 5F 73 69
7A 65 31 33
34 32 31 37
37 33 
[ns_server:debug,2025-05-15T18:47:52.301Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0xFE (cmd_get_error_map) vbucket = 0 opaque = 0x5000000
80 FE 00 00
00 00 00 00
00 00 00 02
05 00 00 00
00 00 00 00
00 00 00 00
00 00 
[ns_server:debug,2025-05-15T18:47:52.302Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0x4000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
04 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.302Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0xFE (cmd_get_error_map) vbucket = 0 opaque = 0x5000000 status = 0x1 (key_enoent)
81 FE 00 00
00 00 00 01
00 00 00 00
05 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.302Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xAD000000
80 5E 00 0B
00 00 00 00
00 00 00 0F
AD 00 00 00
00 00 00 00
00 00 00 00
65 6E 61 62
6C 65 5F 6E
6F 6F 70 74
72 75 65 
[ns_server:debug,2025-05-15T18:47:52.302Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xAE000000
80 5E 00 11
00 00 00 00
00 00 00 19
AE 00 00 00
00 00 00 00
00 00 00 00
73 65 74 5F
6E 6F 6F 70
5F 69 6E 74
65 72 76 61
6C 30 2E 31
30 30 30 30
30 
[ns_server:debug,2025-05-15T18:47:52.303Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xAD000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
AD 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.303Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xAE000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
AE 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.304Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xAF000000
80 5E 00 0C
00 00 00 00
00 00 00 10
AF 00 00 00
00 00 00 00
00 00 00 00
73 65 74 5F
70 72 69 6F
72 69 74 79
68 69 67 68

[ns_server:debug,2025-05-15T18:47:52.305Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB0000000
80 5E 00 1F
00 00 00 00
00 00 00 23
B0 00 00 00
00 00 00 00
00 00 00 00
73 75 70 70
6F 72 74 73
5F 63 75 72
73 6F 72 5F
64 72 6F 70
70 69 6E 67
5F 76 75 6C
63 61 6E 74
72 75 65 
[ns_server:debug,2025-05-15T18:47:52.305Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB1000000
80 5E 00 11
00 00 00 00
00 00 00 15
B1 00 00 00
00 00 00 00
00 00 00 00
73 75 70 70
6F 72 74 73
5F 68 69 66
69 5F 4D 46
55 74 72 75
65 
[ns_server:debug,2025-05-15T18:47:52.305Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB2000000
80 5E 00 26
00 00 00 00
00 00 00 2A
B2 00 00 00
00 00 00 00
00 00 00 00
73 65 6E 64
5F 73 74 72
65 61 6D 5F
65 6E 64 5F
6F 6E 5F 63
6C 69 65 6E
74 5F 63 6C
6F 73 65 5F
73 74 72 65
61 6D 74 72
75 65 
[ns_server:debug,2025-05-15T18:47:52.305Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB3000000
80 5E 00 14
00 00 00 00
00 00 00 18
B3 00 00 00
00 00 00 00
00 00 00 00
65 6E 61 62
6C 65 5F 65
78 70 69 72
79 5F 6F 70
63 6F 64 65
74 72 75 65

[ns_server:debug,2025-05-15T18:47:52.305Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB4000000
80 5E 00 12
00 00 00 00
00 00 00 16
B4 00 00 00
00 00 00 00
00 00 00 00
65 6E 61 62
6C 65 5F 73
79 6E 63 5F
77 72 69 74
65 73 74 72
75 65 
[ns_server:debug,2025-05-15T18:47:52.306Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xAF000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
AF 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.306Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB0000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B0 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.306Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB1000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B1 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.306Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB2000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B2 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.306Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB3000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B3 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.306Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB4000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B4 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.306Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB5000000
80 5E 00 0D
00 00 00 00
00 00 00 19
B5 00 00 00
00 00 00 00
00 00 00 00
63 6F 6E 73
75 6D 65 72
5F 6E 61 6D
65 6E 73 5F
31 40 64 62
32 2E 6C 61
6E 
[ns_server:debug,2025-05-15T18:47:52.306Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB6000000
80 5E 00 1B
00 00 00 00
00 00 00 1F
B6 00 00 00
00 00 00 00
00 00 00 00
69 6E 63 6C
75 64 65 5F
64 65 6C 65
74 65 64 5F
75 73 65 72
5F 78 61 74
74 72 73 74
72 75 65 
[ns_server:debug,2025-05-15T18:47:52.307Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB5000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B5 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.307Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB6000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B6 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.308Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB7000000
80 5E 00 13
00 00 00 00
00 00 00 17
B7 00 00 00
00 00 00 00
00 00 00 00
76 37 5F 64
63 70 5F 73
74 61 74 75
73 5F 63 6F
64 65 73 74
72 75 65 
[ns_server:debug,2025-05-15T18:47:52.309Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB7000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B7 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.309Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB8000000
80 5E 00 19
00 00 00 00
00 00 00 1D
B8 00 00 00
00 00 00 00
00 00 00 00
66 6C 61 74
62 75 66 66
65 72 73 5F
73 79 73 74
65 6D 5F 65
76 65 6E 74
73 74 72 75
65 
[ns_server:debug,2025-05-15T18:47:52.310Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB8000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B8 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.311Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB9000000
80 5E 00 0E
00 00 00 00
00 00 00 12
B9 00 00 00
00 00 00 00
00 00 00 00
63 68 61 6E
67 65 5F 73
74 72 65 61
6D 73 74 72
75 65 
[ns_server:debug,2025-05-15T18:47:52.312Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB9000000 status = 0x83 (not_supported)
81 5E 00 00
00 00 00 83
00 00 00 00
B9 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.313Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x1000000
80 53 00 00
30 00 00 00
00 00 00 30
01 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.313Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 1 opaque = 0x2000000
80 53 00 00
30 00 00 01
00 00 00 30
02 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.314Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 2 opaque = 0x3000000
80 53 00 00
30 00 00 02
00 00 00 30
03 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.314Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 3 opaque = 0x6000000
80 53 00 00
30 00 00 03
00 00 00 30
06 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.314Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 4 opaque = 0x7000000
80 53 00 00
30 00 00 04
00 00 00 30
07 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.315Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 5 opaque = 0x8000000
80 53 00 00
30 00 00 05
00 00 00 30
08 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.315Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 6 opaque = 0x9000000
80 53 00 00
30 00 00 06
00 00 00 30
09 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.315Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 7 opaque = 0xA000000
80 53 00 00
30 00 00 07
00 00 00 30
0A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.315Z,ns_1@db2.lan:<0.4284.0>:dcp_commands:open_connection:72]Open producer connection "replication:ns_1@db3.lan->ns_1@db2.lan:doom-scrolling" on socket #Port<0.215>: Body undefined
[ns_server:debug,2025-05-15T18:47:52.316Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 8 opaque = 0xB000000
80 53 00 00
30 00 00 08
00 00 00 30
0B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.316Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 9 opaque = 0xC000000
80 53 00 00
30 00 00 09
00 00 00 30
0C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.316Z,ns_1@db2.lan:<0.4285.0>:dcp_replicator:connect_to_producer:81]initiated new dcp replication with consumer side: <0.4277.0> and producer side: <0.4284.0>
[ns_server:debug,2025-05-15T18:47:52.316Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 10 opaque = 0xD000000
80 53 00 00
30 00 00 0A
00 00 00 30
0D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.316Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 11 opaque = 0xE000000
80 53 00 00
30 00 00 0B
00 00 00 30
0E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.316Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 12 opaque = 0xF000000
80 53 00 00
30 00 00 0C
00 00 00 30
0F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.317Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 13 opaque = 0x10000000
80 53 00 00
30 00 00 0D
00 00 00 30
10 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.317Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 14 opaque = 0x11000000
80 53 00 00
30 00 00 0E
00 00 00 30
11 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.317Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 15 opaque = 0x12000000
80 53 00 00
30 00 00 0F
00 00 00 30
12 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.317Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 16 opaque = 0x13000000
80 53 00 00
30 00 00 10
00 00 00 30
13 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.317Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 17 opaque = 0x14000000
80 53 00 00
30 00 00 11
00 00 00 30
14 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.317Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 18 opaque = 0x15000000
80 53 00 00
30 00 00 12
00 00 00 30
15 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.318Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 19 opaque = 0x16000000
80 53 00 00
30 00 00 13
00 00 00 30
16 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.318Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 20 opaque = 0x17000000
80 53 00 00
30 00 00 14
00 00 00 30
17 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.318Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 21 opaque = 0x18000000
80 53 00 00
30 00 00 15
00 00 00 30
18 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.318Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 22 opaque = 0x19000000
80 53 00 00
30 00 00 16
00 00 00 30
19 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.318Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 23 opaque = 0x1A000000
80 53 00 00
30 00 00 17
00 00 00 30
1A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.318Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 24 opaque = 0x1B000000
80 53 00 00
30 00 00 18
00 00 00 30
1B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.318Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 25 opaque = 0x1C000000
80 53 00 00
30 00 00 19
00 00 00 30
1C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.318Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 26 opaque = 0x1D000000
80 53 00 00
30 00 00 1A
00 00 00 30
1D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.318Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 27 opaque = 0x1E000000
80 53 00 00
30 00 00 1B
00 00 00 30
1E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.319Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 28 opaque = 0x1F000000
80 53 00 00
30 00 00 1C
00 00 00 30
1F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.319Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 29 opaque = 0x20000000
80 53 00 00
30 00 00 1D
00 00 00 30
20 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.319Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 30 opaque = 0x21000000
80 53 00 00
30 00 00 1E
00 00 00 30
21 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.319Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 31 opaque = 0x22000000
80 53 00 00
30 00 00 1F
00 00 00 30
22 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.319Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 32 opaque = 0x23000000
80 53 00 00
30 00 00 20
00 00 00 30
23 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.319Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 33 opaque = 0x24000000
80 53 00 00
30 00 00 21
00 00 00 30
24 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.319Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 34 opaque = 0x25000000
80 53 00 00
30 00 00 22
00 00 00 30
25 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.319Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 35 opaque = 0x26000000
80 53 00 00
30 00 00 23
00 00 00 30
26 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.320Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 36 opaque = 0x27000000
80 53 00 00
30 00 00 24
00 00 00 30
27 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.320Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 37 opaque = 0x28000000
80 53 00 00
30 00 00 25
00 00 00 30
28 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.320Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 38 opaque = 0x29000000
80 53 00 00
30 00 00 26
00 00 00 30
29 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.320Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 39 opaque = 0x2A000000
80 53 00 00
30 00 00 27
00 00 00 30
2A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.321Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 40 opaque = 0x2B000000
80 53 00 00
30 00 00 28
00 00 00 30
2B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.321Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 41 opaque = 0x2C000000
80 53 00 00
30 00 00 29
00 00 00 30
2C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.321Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 42 opaque = 0x2D000000
80 53 00 00
30 00 00 2A
00 00 00 30
2D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.321Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 43 opaque = 0x2E000000
80 53 00 00
30 00 00 2B
00 00 00 30
2E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.321Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 44 opaque = 0x2F000000
80 53 00 00
30 00 00 2C
00 00 00 30
2F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.321Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 45 opaque = 0x30000000
80 53 00 00
30 00 00 2D
00 00 00 30
30 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.321Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 46 opaque = 0x31000000
80 53 00 00
30 00 00 2E
00 00 00 30
31 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.322Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 47 opaque = 0x32000000
80 53 00 00
30 00 00 2F
00 00 00 30
32 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.322Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 48 opaque = 0x33000000
80 53 00 00
30 00 00 30
00 00 00 30
33 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.322Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 49 opaque = 0x34000000
80 53 00 00
30 00 00 31
00 00 00 30
34 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.322Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 50 opaque = 0x35000000
80 53 00 00
30 00 00 32
00 00 00 30
35 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.322Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 51 opaque = 0x36000000
80 53 00 00
30 00 00 33
00 00 00 30
36 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.322Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 52 opaque = 0x37000000
80 53 00 00
30 00 00 34
00 00 00 30
37 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.322Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 53 opaque = 0x38000000
80 53 00 00
30 00 00 35
00 00 00 30
38 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.323Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 54 opaque = 0x39000000
80 53 00 00
30 00 00 36
00 00 00 30
39 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.323Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 55 opaque = 0x3A000000
80 53 00 00
30 00 00 37
00 00 00 30
3A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.323Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 56 opaque = 0x3B000000
80 53 00 00
30 00 00 38
00 00 00 30
3B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.323Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 57 opaque = 0x3C000000
80 53 00 00
30 00 00 39
00 00 00 30
3C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.323Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 58 opaque = 0x3D000000
80 53 00 00
30 00 00 3A
00 00 00 30
3D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.323Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 59 opaque = 0x3E000000
80 53 00 00
30 00 00 3B
00 00 00 30
3E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.323Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 60 opaque = 0x3F000000
80 53 00 00
30 00 00 3C
00 00 00 30
3F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.323Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 61 opaque = 0x40000000
80 53 00 00
30 00 00 3D
00 00 00 30
40 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.324Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 62 opaque = 0x41000000
80 53 00 00
30 00 00 3E
00 00 00 30
41 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.324Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 63 opaque = 0x42000000
80 53 00 00
30 00 00 3F
00 00 00 30
42 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.324Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 64 opaque = 0x43000000
80 53 00 00
30 00 00 40
00 00 00 30
43 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.324Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 65 opaque = 0x44000000
80 53 00 00
30 00 00 41
00 00 00 30
44 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.324Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 66 opaque = 0x45000000
80 53 00 00
30 00 00 42
00 00 00 30
45 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.324Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 67 opaque = 0x46000000
80 53 00 00
30 00 00 43
00 00 00 30
46 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.324Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 68 opaque = 0x47000000
80 53 00 00
30 00 00 44
00 00 00 30
47 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.324Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 69 opaque = 0x48000000
80 53 00 00
30 00 00 45
00 00 00 30
48 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.325Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 70 opaque = 0x49000000
80 53 00 00
30 00 00 46
00 00 00 30
49 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.325Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 71 opaque = 0x4A000000
80 53 00 00
30 00 00 47
00 00 00 30
4A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.325Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 72 opaque = 0x4B000000
80 53 00 00
30 00 00 48
00 00 00 30
4B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.325Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 73 opaque = 0x4C000000
80 53 00 00
30 00 00 49
00 00 00 30
4C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.325Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 74 opaque = 0x4D000000
80 53 00 00
30 00 00 4A
00 00 00 30
4D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.325Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 75 opaque = 0x4E000000
80 53 00 00
30 00 00 4B
00 00 00 30
4E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.325Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 76 opaque = 0x4F000000
80 53 00 00
30 00 00 4C
00 00 00 30
4F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.325Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 77 opaque = 0x50000000
80 53 00 00
30 00 00 4D
00 00 00 30
50 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.326Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 78 opaque = 0x51000000
80 53 00 00
30 00 00 4E
00 00 00 30
51 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.326Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 79 opaque = 0x52000000
80 53 00 00
30 00 00 4F
00 00 00 30
52 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.326Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 80 opaque = 0x53000000
80 53 00 00
30 00 00 50
00 00 00 30
53 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.326Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 81 opaque = 0x54000000
80 53 00 00
30 00 00 51
00 00 00 30
54 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.326Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 82 opaque = 0x55000000
80 53 00 00
30 00 00 52
00 00 00 30
55 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.326Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 83 opaque = 0x56000000
80 53 00 00
30 00 00 53
00 00 00 30
56 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.326Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 84 opaque = 0x57000000
80 53 00 00
30 00 00 54
00 00 00 30
57 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.326Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 85 opaque = 0x58000000
80 53 00 00
30 00 00 55
00 00 00 30
58 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.327Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 86 opaque = 0x59000000
80 53 00 00
30 00 00 56
00 00 00 30
59 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.327Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 87 opaque = 0x5A000000
80 53 00 00
30 00 00 57
00 00 00 30
5A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.327Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 88 opaque = 0x5B000000
80 53 00 00
30 00 00 58
00 00 00 30
5B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.327Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 89 opaque = 0x5C000000
80 53 00 00
30 00 00 59
00 00 00 30
5C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.327Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 90 opaque = 0x5D000000
80 53 00 00
30 00 00 5A
00 00 00 30
5D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.327Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 91 opaque = 0x5E000000
80 53 00 00
30 00 00 5B
00 00 00 30
5E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.327Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 92 opaque = 0x5F000000
80 53 00 00
30 00 00 5C
00 00 00 30
5F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.327Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 93 opaque = 0x60000000
80 53 00 00
30 00 00 5D
00 00 00 30
60 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.327Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 94 opaque = 0x61000000
80 53 00 00
30 00 00 5E
00 00 00 30
61 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.328Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 95 opaque = 0x62000000
80 53 00 00
30 00 00 5F
00 00 00 30
62 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.328Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 96 opaque = 0x63000000
80 53 00 00
30 00 00 60
00 00 00 30
63 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.328Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 97 opaque = 0x64000000
80 53 00 00
30 00 00 61
00 00 00 30
64 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.328Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 98 opaque = 0x65000000
80 53 00 00
30 00 00 62
00 00 00 30
65 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.328Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 99 opaque = 0x66000000
80 53 00 00
30 00 00 63
00 00 00 30
66 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.328Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 100 opaque = 0x67000000
80 53 00 00
30 00 00 64
00 00 00 30
67 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.328Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 101 opaque = 0x68000000
80 53 00 00
30 00 00 65
00 00 00 30
68 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.328Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 102 opaque = 0x69000000
80 53 00 00
30 00 00 66
00 00 00 30
69 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.328Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 103 opaque = 0x6A000000
80 53 00 00
30 00 00 67
00 00 00 30
6A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.328Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 104 opaque = 0x6B000000
80 53 00 00
30 00 00 68
00 00 00 30
6B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.329Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 105 opaque = 0x6C000000
80 53 00 00
30 00 00 69
00 00 00 30
6C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.329Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 106 opaque = 0x6D000000
80 53 00 00
30 00 00 6A
00 00 00 30
6D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.329Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 107 opaque = 0x6E000000
80 53 00 00
30 00 00 6B
00 00 00 30
6E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.329Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 108 opaque = 0x6F000000
80 53 00 00
30 00 00 6C
00 00 00 30
6F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.329Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 109 opaque = 0x70000000
80 53 00 00
30 00 00 6D
00 00 00 30
70 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.329Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 110 opaque = 0x71000000
80 53 00 00
30 00 00 6E
00 00 00 30
71 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.329Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 111 opaque = 0x72000000
80 53 00 00
30 00 00 6F
00 00 00 30
72 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.329Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 112 opaque = 0x73000000
80 53 00 00
30 00 00 70
00 00 00 30
73 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.329Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 113 opaque = 0x74000000
80 53 00 00
30 00 00 71
00 00 00 30
74 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.330Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 114 opaque = 0x75000000
80 53 00 00
30 00 00 72
00 00 00 30
75 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.330Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 115 opaque = 0x76000000
80 53 00 00
30 00 00 73
00 00 00 30
76 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.330Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 116 opaque = 0x77000000
80 53 00 00
30 00 00 74
00 00 00 30
77 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.330Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 117 opaque = 0x78000000
80 53 00 00
30 00 00 75
00 00 00 30
78 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.330Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 118 opaque = 0x79000000
80 53 00 00
30 00 00 76
00 00 00 30
79 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.330Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 119 opaque = 0x7A000000
80 53 00 00
30 00 00 77
00 00 00 30
7A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.330Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 120 opaque = 0x7B000000
80 53 00 00
30 00 00 78
00 00 00 30
7B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.330Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 121 opaque = 0x7C000000
80 53 00 00
30 00 00 79
00 00 00 30
7C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.330Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 122 opaque = 0x7D000000
80 53 00 00
30 00 00 7A
00 00 00 30
7D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.331Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 123 opaque = 0x7E000000
80 53 00 00
30 00 00 7B
00 00 00 30
7E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.331Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 124 opaque = 0x7F000000
80 53 00 00
30 00 00 7C
00 00 00 30
7F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.331Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 125 opaque = 0x80000000
80 53 00 00
30 00 00 7D
00 00 00 30
80 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.331Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 126 opaque = 0x81000000
80 53 00 00
30 00 00 7E
00 00 00 30
81 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.331Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 127 opaque = 0x82000000
80 53 00 00
30 00 00 7F
00 00 00 30
82 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.331Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 128 opaque = 0x83000000
80 53 00 00
30 00 00 80
00 00 00 30
83 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.331Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 129 opaque = 0x84000000
80 53 00 00
30 00 00 81
00 00 00 30
84 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.331Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 130 opaque = 0x85000000
80 53 00 00
30 00 00 82
00 00 00 30
85 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.332Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 131 opaque = 0x86000000
80 53 00 00
30 00 00 83
00 00 00 30
86 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.332Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 132 opaque = 0x87000000
80 53 00 00
30 00 00 84
00 00 00 30
87 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.332Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 133 opaque = 0x88000000
80 53 00 00
30 00 00 85
00 00 00 30
88 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.332Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 134 opaque = 0x89000000
80 53 00 00
30 00 00 86
00 00 00 30
89 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.332Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 135 opaque = 0x8A000000
80 53 00 00
30 00 00 87
00 00 00 30
8A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.332Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 136 opaque = 0x8B000000
80 53 00 00
30 00 00 88
00 00 00 30
8B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.332Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 137 opaque = 0x8C000000
80 53 00 00
30 00 00 89
00 00 00 30
8C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.333Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 138 opaque = 0x8D000000
80 53 00 00
30 00 00 8A
00 00 00 30
8D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.333Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 139 opaque = 0x8E000000
80 53 00 00
30 00 00 8B
00 00 00 30
8E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.333Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 140 opaque = 0x8F000000
80 53 00 00
30 00 00 8C
00 00 00 30
8F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.333Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 141 opaque = 0x90000000
80 53 00 00
30 00 00 8D
00 00 00 30
90 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.333Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 142 opaque = 0x91000000
80 53 00 00
30 00 00 8E
00 00 00 30
91 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.333Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 143 opaque = 0x92000000
80 53 00 00
30 00 00 8F
00 00 00 30
92 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.333Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 144 opaque = 0x93000000
80 53 00 00
30 00 00 90
00 00 00 30
93 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.333Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 145 opaque = 0x94000000
80 53 00 00
30 00 00 91
00 00 00 30
94 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.333Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 146 opaque = 0x95000000
80 53 00 00
30 00 00 92
00 00 00 30
95 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.334Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 147 opaque = 0x96000000
80 53 00 00
30 00 00 93
00 00 00 30
96 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.334Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 148 opaque = 0x97000000
80 53 00 00
30 00 00 94
00 00 00 30
97 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.334Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 149 opaque = 0x98000000
80 53 00 00
30 00 00 95
00 00 00 30
98 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.334Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 150 opaque = 0x99000000
80 53 00 00
30 00 00 96
00 00 00 30
99 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.334Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 151 opaque = 0x9A000000
80 53 00 00
30 00 00 97
00 00 00 30
9A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.334Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 152 opaque = 0x9B000000
80 53 00 00
30 00 00 98
00 00 00 30
9B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.334Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 153 opaque = 0x9C000000
80 53 00 00
30 00 00 99
00 00 00 30
9C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.334Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 154 opaque = 0x9D000000
80 53 00 00
30 00 00 9A
00 00 00 30
9D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.335Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 155 opaque = 0x9E000000
80 53 00 00
30 00 00 9B
00 00 00 30
9E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.335Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 156 opaque = 0x9F000000
80 53 00 00
30 00 00 9C
00 00 00 30
9F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.335Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 157 opaque = 0xA0000000
80 53 00 00
30 00 00 9D
00 00 00 30
A0 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.335Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 158 opaque = 0xA1000000
80 53 00 00
30 00 00 9E
00 00 00 30
A1 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.335Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 159 opaque = 0xA2000000
80 53 00 00
30 00 00 9F
00 00 00 30
A2 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.335Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 160 opaque = 0xA3000000
80 53 00 00
30 00 00 A0
00 00 00 30
A3 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.336Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 161 opaque = 0xA4000000
80 53 00 00
30 00 00 A1
00 00 00 30
A4 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.336Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 162 opaque = 0xA5000000
80 53 00 00
30 00 00 A2
00 00 00 30
A5 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.336Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 163 opaque = 0xA6000000
80 53 00 00
30 00 00 A3
00 00 00 30
A6 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.336Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 164 opaque = 0xA7000000
80 53 00 00
30 00 00 A4
00 00 00 30
A7 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.337Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 165 opaque = 0xA8000000
80 53 00 00
30 00 00 A5
00 00 00 30
A8 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.337Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 166 opaque = 0xA9000000
80 53 00 00
30 00 00 A6
00 00 00 30
A9 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.337Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 167 opaque = 0xAA000000
80 53 00 00
30 00 00 A7
00 00 00 30
AA 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.337Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 168 opaque = 0xAB000000
80 53 00 00
30 00 00 A8
00 00 00 30
AB 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.337Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 169 opaque = 0xAC000000
80 53 00 00
30 00 00 A9
00 00 00 30
AC 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.339Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x1000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
01 00 00 00
00 00 00 00
00 00 00 00
00 00 A4 F4
BA 4E 3E 8E
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.339Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x2000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
02 00 00 00
00 00 00 00
00 00 00 00
00 00 AB AC
4F 28 28 5D
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.339Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x3000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
03 00 00 00
00 00 00 00
00 00 00 00
00 00 FD BA
31 B6 B1 D9
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.339Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x6000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
06 00 00 00
00 00 00 00
00 00 00 00
00 00 B4 0D
9D EA 41 1F
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.339Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x7000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
07 00 00 00
00 00 00 00
00 00 00 00
00 00 93 9A
88 AA 16 5C
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.339Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x8000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
08 00 00 00
00 00 00 00
00 00 00 00
00 00 EC 66
20 AF FC 05
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.339Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x9000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
09 00 00 00
00 00 00 00
00 00 00 00
00 00 F9 E8
21 20 0B A3
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.339Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
0A 00 00 00
00 00 00 00
00 00 00 00
00 00 4B 97
EE 05 55 77
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.339Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xB000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
0B 00 00 00
00 00 00 00
00 00 00 00
00 00 0C 7C
7F 83 E4 56
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.339Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xC000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
0C 00 00 00
00 00 00 00
00 00 00 00
00 00 B9 F2
64 52 B6 09
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.339Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xD000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
0D 00 00 00
00 00 00 00
00 00 00 00
00 00 D1 D1
7A 16 87 C1
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.339Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xE000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
0E 00 00 00
00 00 00 00
00 00 00 00
00 00 39 A9
14 F3 1D C8
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.340Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xF000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
0F 00 00 00
00 00 00 00
00 00 00 00
00 00 24 9B
5B A8 DA EC
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.340Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x10000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
10 00 00 00
00 00 00 00
00 00 00 00
00 00 11 18
03 5C 49 75
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.340Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x11000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
11 00 00 00
00 00 00 00
00 00 00 00
00 00 F1 B9
0F 7D 3F 5F
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.340Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x12000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
12 00 00 00
00 00 00 00
00 00 00 00
00 00 B4 AA
4B 50 AF D0
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.340Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x13000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
13 00 00 00
00 00 00 00
00 00 00 00
00 00 C4 B4
C6 E4 8F 29
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.341Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x14000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
14 00 00 00
00 00 00 00
00 00 00 00
00 00 1D 59
71 C2 B3 DC
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.341Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x15000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
15 00 00 00
00 00 00 00
00 00 00 00
00 00 19 58
EB 0F CA AD
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.341Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x16000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
16 00 00 00
00 00 00 00
00 00 00 00
00 00 32 52
3C B9 95 A2
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.341Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x17000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
17 00 00 00
00 00 00 00
00 00 00 00
00 00 6C 12
4C 97 80 AB
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.341Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x18000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
18 00 00 00
00 00 00 00
00 00 00 00
00 00 89 9B
FA C6 FB 4E
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.341Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x19000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
19 00 00 00
00 00 00 00
00 00 00 00
00 00 30 D0
EE 1F 81 12
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.341Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x1A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
1A 00 00 00
00 00 00 00
00 00 00 00
00 00 DF BD
CF 33 65 F2
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.341Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x1B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
1B 00 00 00
00 00 00 00
00 00 00 00
00 00 E1 AF
1C 15 94 CC
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.341Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x1C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
1C 00 00 00
00 00 00 00
00 00 00 00
00 00 E1 65
D3 FE 49 F4
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.341Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x1D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
1D 00 00 00
00 00 00 00
00 00 00 00
00 00 36 67
1D D4 8F 21
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.341Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x1E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
1E 00 00 00
00 00 00 00
00 00 00 00
00 00 B1 40
C9 2B C2 EE
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.342Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x1F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
1F 00 00 00
00 00 00 00
00 00 00 00
00 00 06 60
EE 8F 0A 55
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.342Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x20000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
20 00 00 00
00 00 00 00
00 00 00 00
00 00 A0 74
59 D8 2E 26
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.342Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x21000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
21 00 00 00
00 00 00 00
00 00 00 00
00 00 26 F1
E3 A3 D1 6E
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.342Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x22000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
22 00 00 00
00 00 00 00
00 00 00 00
00 00 DB 66
6C ED FE D9
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.342Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x23000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
23 00 00 00
00 00 00 00
00 00 00 00
00 00 6D DC
66 3A E9 6B
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.342Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x24000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
24 00 00 00
00 00 00 00
00 00 00 00
00 00 AC 14
05 9C 10 93
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.342Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x25000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
25 00 00 00
00 00 00 00
00 00 00 00
00 00 A4 B6
31 CB DA E0
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.342Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x26000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
26 00 00 00
00 00 00 00
00 00 00 00
00 00 44 B6
53 50 61 EA
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.343Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x27000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
27 00 00 00
00 00 00 00
00 00 00 00
00 00 42 C8
B6 21 04 A9
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.343Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x28000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
28 00 00 00
00 00 00 00
00 00 00 00
00 00 40 FB
26 2C CD 4A
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.343Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x29000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
29 00 00 00
00 00 00 00
00 00 00 00
00 00 BA A3
6E 3D 50 D4
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.343Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x2A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
2A 00 00 00
00 00 00 00
00 00 00 00
00 00 18 D9
86 BC EF 68
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.344Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x2B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
2B 00 00 00
00 00 00 00
00 00 00 00
00 00 49 BB
CA 54 9A 2B
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.344Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x2C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
2C 00 00 00
00 00 00 00
00 00 00 00
00 00 35 4B
A1 8B E4 7F
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.344Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x2D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
2D 00 00 00
00 00 00 00
00 00 00 00
00 00 C9 9D
D5 EE 3E C2
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.344Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x2E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
2E 00 00 00
00 00 00 00
00 00 00 00
00 00 64 5A
B6 F7 09 10
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.344Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x2F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
2F 00 00 00
00 00 00 00
00 00 00 00
00 00 0E 9C
C6 BA C5 41
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.344Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x30000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
30 00 00 00
00 00 00 00
00 00 00 00
00 00 2F D9
92 25 05 24
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.345Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x31000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
31 00 00 00
00 00 00 00
00 00 00 00
00 00 8B 9E
95 B2 C7 AB
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.345Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x32000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
32 00 00 00
00 00 00 00
00 00 00 00
00 00 A0 69
09 7C 9B 86
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.345Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x33000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
33 00 00 00
00 00 00 00
00 00 00 00
00 00 57 C9
6B E1 46 A0
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.345Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x34000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
34 00 00 00
00 00 00 00
00 00 00 00
00 00 C0 06
22 BF 60 B5
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.345Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x35000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
35 00 00 00
00 00 00 00
00 00 00 00
00 00 D1 7D
24 82 D2 1E
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.345Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x36000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
36 00 00 00
00 00 00 00
00 00 00 00
00 00 16 8C
16 C7 71 23
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.345Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x37000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
37 00 00 00
00 00 00 00
00 00 00 00
00 00 86 74
C5 14 5C A6
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.345Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x38000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
38 00 00 00
00 00 00 00
00 00 00 00
00 00 C0 60
F0 9F CF 49
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.345Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x39000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
39 00 00 00
00 00 00 00
00 00 00 00
00 00 48 D0
E6 48 B3 7C
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.346Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x3A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
3A 00 00 00
00 00 00 00
00 00 00 00
00 00 6C EE
D2 D0 91 16
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.346Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x3B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
3B 00 00 00
00 00 00 00
00 00 00 00
00 00 FB 56
28 13 3B 81
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.346Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x3C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
3C 00 00 00
00 00 00 00
00 00 00 00
00 00 EA CA
A7 89 AA 18
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.346Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x3D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
3D 00 00 00
00 00 00 00
00 00 00 00
00 00 67 60
9F A9 71 37
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.346Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x3E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
3E 00 00 00
00 00 00 00
00 00 00 00
00 00 33 9E
38 E6 56 4B
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.346Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x3F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
3F 00 00 00
00 00 00 00
00 00 00 00
00 00 C4 1B
38 6B DA CB
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.346Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x40000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
40 00 00 00
00 00 00 00
00 00 00 00
00 00 7B CE
7B 8D F5 67
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.346Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x41000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
41 00 00 00
00 00 00 00
00 00 00 00
00 00 FA A2
BB EE 2D C4
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.346Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x42000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
42 00 00 00
00 00 00 00
00 00 00 00
00 00 8D 9A
5D 85 66 71
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.346Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x43000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
43 00 00 00
00 00 00 00
00 00 00 00
00 00 F4 EF
4E B9 70 B9
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.346Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x44000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
44 00 00 00
00 00 00 00
00 00 00 00
00 00 43 6F
5D 55 3A 1C
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.346Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x45000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
45 00 00 00
00 00 00 00
00 00 00 00
00 00 79 BC
57 E5 50 7A
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.347Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x46000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
46 00 00 00
00 00 00 00
00 00 00 00
00 00 93 A6
C4 C3 40 9C
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.347Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x47000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
47 00 00 00
00 00 00 00
00 00 00 00
00 00 04 55
B6 F3 22 C3
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.347Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x48000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
48 00 00 00
00 00 00 00
00 00 00 00
00 00 CD 08
8E B6 64 EF
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.347Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x0 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 01

[ns_server:debug,2025-05-15T18:47:52.347Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x49000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
49 00 00 00
00 00 00 00
00 00 00 00
00 00 23 6A
03 E0 0D 50
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.347Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 0, stream opaque = 0x1000000
[ns_server:debug,2025-05-15T18:47:52.347Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x4A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
4A 00 00 00
00 00 00 00
00 00 00 00
00 00 E9 E5
40 5F D2 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.347Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 01
00 00 00 00
00 00 00 00
00 00 00 02

[rebalance:debug,2025-05-15T18:47:52.347Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 1, stream opaque = 0x2000000
[ns_server:debug,2025-05-15T18:47:52.347Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x4B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
4B 00 00 00
00 00 00 00
00 00 00 00
00 00 58 BB
67 7F 06 9D
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.347Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 02
00 00 00 00
00 00 00 00
00 00 00 03

[rebalance:debug,2025-05-15T18:47:52.347Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 2, stream opaque = 0x3000000
[ns_server:debug,2025-05-15T18:47:52.347Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x4C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
4C 00 00 00
00 00 00 00
00 00 00 00
00 00 4E AB
BF 2F EF A1
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.347Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 03
00 00 00 00
00 00 00 00
00 00 00 06

[rebalance:debug,2025-05-15T18:47:52.347Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 3, stream opaque = 0x6000000
[ns_server:debug,2025-05-15T18:47:52.347Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x4D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
4D 00 00 00
00 00 00 00
00 00 00 00
00 00 E2 FD
F5 E2 25 B5
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.348Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x4E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
4E 00 00 00
00 00 00 00
00 00 00 00
00 00 0A 59
89 EE E8 9A
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.348Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x4F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
4F 00 00 00
00 00 00 00
00 00 00 00
00 00 49 66
0B F6 66 48
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.348Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x50000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
50 00 00 00
00 00 00 00
00 00 00 00
00 00 BD AB
49 B4 62 CE
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.348Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x51000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
51 00 00 00
00 00 00 00
00 00 00 00
00 00 8F 0A
6E FA F9 4B
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.348Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x4 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 04
00 00 00 00
00 00 00 00
00 00 00 07

[rebalance:debug,2025-05-15T18:47:52.348Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 4, stream opaque = 0x7000000
[ns_server:debug,2025-05-15T18:47:52.348Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x52000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
52 00 00 00
00 00 00 00
00 00 00 00
00 00 C3 64
1F ED 52 52
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.348Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x5 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 05
00 00 00 00
00 00 00 00
00 00 00 08

[ns_server:debug,2025-05-15T18:47:52.348Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x53000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
53 00 00 00
00 00 00 00
00 00 00 00
00 00 C2 21
DE B3 02 D7
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.348Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x54000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
54 00 00 00
00 00 00 00
00 00 00 00
00 00 BD FC
94 AE 43 25
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.348Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x55000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
55 00 00 00
00 00 00 00
00 00 00 00
00 00 A5 84
DC 68 F8 E8
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.348Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 5, stream opaque = 0x8000000
[ns_server:debug,2025-05-15T18:47:52.348Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x56000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
56 00 00 00
00 00 00 00
00 00 00 00
00 00 54 8E
EF A2 4E 91
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.348Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x6 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 06
00 00 00 00
00 00 00 00
00 00 00 09

[rebalance:debug,2025-05-15T18:47:52.348Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 6, stream opaque = 0x9000000
[ns_server:debug,2025-05-15T18:47:52.348Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x57000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
57 00 00 00
00 00 00 00
00 00 00 00
00 00 F0 CF
1F 49 3D 64
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.348Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x7 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 07
00 00 00 00
00 00 00 00
00 00 00 0A

[rebalance:debug,2025-05-15T18:47:52.348Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 7, stream opaque = 0xA000000
[ns_server:debug,2025-05-15T18:47:52.348Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x58000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
58 00 00 00
00 00 00 00
00 00 00 00
00 00 07 5F
96 2B C5 01
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.348Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x8 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 08
00 00 00 00
00 00 00 00
00 00 00 0B

[ns_server:debug,2025-05-15T18:47:52.348Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x59000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
59 00 00 00
00 00 00 00
00 00 00 00
00 00 64 81
E7 36 70 19
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.349Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 8, stream opaque = 0xB000000
[ns_server:debug,2025-05-15T18:47:52.349Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x5A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
5A 00 00 00
00 00 00 00
00 00 00 00
00 00 2B EF
64 80 94 4C
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.349Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x9 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 09
00 00 00 00
00 00 00 00
00 00 00 0C

[ns_server:debug,2025-05-15T18:47:52.349Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x5B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
5B 00 00 00
00 00 00 00
00 00 00 00
00 00 57 3E
CB 7E B1 A2
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.349Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 9, stream opaque = 0xC000000
[ns_server:debug,2025-05-15T18:47:52.349Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xA status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 0A
00 00 00 00
00 00 00 00
00 00 00 0D

[rebalance:debug,2025-05-15T18:47:52.349Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 10, stream opaque = 0xD000000
[ns_server:debug,2025-05-15T18:47:52.349Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xB status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 0B
00 00 00 00
00 00 00 00
00 00 00 0E

[rebalance:debug,2025-05-15T18:47:52.349Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 11, stream opaque = 0xE000000
[ns_server:debug,2025-05-15T18:47:52.349Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xC status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 0C
00 00 00 00
00 00 00 00
00 00 00 0F

[rebalance:debug,2025-05-15T18:47:52.349Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 12, stream opaque = 0xF000000
[ns_server:debug,2025-05-15T18:47:52.349Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xD status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 0D
00 00 00 00
00 00 00 00
00 00 00 10

[ns_server:debug,2025-05-15T18:47:52.349Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x5C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
5C 00 00 00
00 00 00 00
00 00 00 00
00 00 E9 6D
12 C9 F1 CB
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.349Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 13, stream opaque = 0x10000000
[ns_server:debug,2025-05-15T18:47:52.349Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xE status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 0E
00 00 00 00
00 00 00 00
00 00 00 11

[rebalance:debug,2025-05-15T18:47:52.349Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 14, stream opaque = 0x11000000
[ns_server:debug,2025-05-15T18:47:52.349Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xF status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 0F
00 00 00 00
00 00 00 00
00 00 00 12

[rebalance:debug,2025-05-15T18:47:52.350Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 15, stream opaque = 0x12000000
[ns_server:debug,2025-05-15T18:47:52.350Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x10 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 10
00 00 00 00
00 00 00 00
00 00 00 13

[rebalance:debug,2025-05-15T18:47:52.350Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 16, stream opaque = 0x13000000
[ns_server:debug,2025-05-15T18:47:52.350Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x11 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 11
00 00 00 00
00 00 00 00
00 00 00 14

[rebalance:debug,2025-05-15T18:47:52.350Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 17, stream opaque = 0x14000000
[ns_server:debug,2025-05-15T18:47:52.350Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x12 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 12
00 00 00 00
00 00 00 00
00 00 00 15

[rebalance:debug,2025-05-15T18:47:52.350Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 18, stream opaque = 0x15000000
[ns_server:debug,2025-05-15T18:47:52.350Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x13 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 13
00 00 00 00
00 00 00 00
00 00 00 16

[rebalance:debug,2025-05-15T18:47:52.350Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 19, stream opaque = 0x16000000
[ns_server:debug,2025-05-15T18:47:52.350Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x14 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 14
00 00 00 00
00 00 00 00
00 00 00 17

[rebalance:debug,2025-05-15T18:47:52.350Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 20, stream opaque = 0x17000000
[ns_server:debug,2025-05-15T18:47:52.349Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x5D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
5D 00 00 00
00 00 00 00
00 00 00 00
00 00 6E 38
AB 80 CD 92
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.350Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x15 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 15
00 00 00 00
00 00 00 00
00 00 00 18

[ns_server:debug,2025-05-15T18:47:52.350Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x5E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
5E 00 00 00
00 00 00 00
00 00 00 00
00 00 21 5A
BE 78 7C 86
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.350Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 21, stream opaque = 0x18000000
[ns_server:debug,2025-05-15T18:47:52.350Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x5F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
5F 00 00 00
00 00 00 00
00 00 00 00
00 00 47 A5
EC D2 06 E4
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.350Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x60000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
60 00 00 00
00 00 00 00
00 00 00 00
00 00 84 F7
04 55 63 40
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.350Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x61000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
61 00 00 00
00 00 00 00
00 00 00 00
00 00 81 10
2D 51 80 A4
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.350Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x62000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
62 00 00 00
00 00 00 00
00 00 00 00
00 00 0F C6
3A B4 B5 2A
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.350Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x63000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
63 00 00 00
00 00 00 00
00 00 00 00
00 00 25 53
1B 72 2D D5
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.350Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x64000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
64 00 00 00
00 00 00 00
00 00 00 00
00 00 21 61
21 45 F4 FA
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.351Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x65000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
65 00 00 00
00 00 00 00
00 00 00 00
00 00 85 FA
63 48 AC 61
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.351Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x66000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
66 00 00 00
00 00 00 00
00 00 00 00
00 00 74 92
A6 56 8D 22
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.351Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x67000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
67 00 00 00
00 00 00 00
00 00 00 00
00 00 4B 5A
04 75 67 05
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.351Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x68000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
68 00 00 00
00 00 00 00
00 00 00 00
00 00 DE BD
D5 28 94 5D
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.351Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x69000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
69 00 00 00
00 00 00 00
00 00 00 00
00 00 96 E4
04 0B 89 BB
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.351Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x6A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
6A 00 00 00
00 00 00 00
00 00 00 00
00 00 CE 62
73 9A BE CE
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.351Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x6B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
6B 00 00 00
00 00 00 00
00 00 00 00
00 00 EB EF
0C F4 79 43
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.351Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x6C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
6C 00 00 00
00 00 00 00
00 00 00 00
00 00 B5 DE
1D AF FA 44
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.351Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x6D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
6D 00 00 00
00 00 00 00
00 00 00 00
00 00 31 E1
4F AD 7B F0
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.351Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x6E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
6E 00 00 00
00 00 00 00
00 00 00 00
00 00 02 68
8D 7B 24 CE
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.351Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x6F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
6F 00 00 00
00 00 00 00
00 00 00 00
00 00 CE 07
E2 48 16 C3
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.351Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x70000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
70 00 00 00
00 00 00 00
00 00 00 00
00 00 7A 15
81 6E 44 F0
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.351Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x71000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
71 00 00 00
00 00 00 00
00 00 00 00
00 00 70 FB
4A 93 7A 67
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.351Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x72000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
72 00 00 00
00 00 00 00
00 00 00 00
00 00 55 EE
CD F3 FA A0
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.351Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x73000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
73 00 00 00
00 00 00 00
00 00 00 00
00 00 46 70
B5 10 1A 2A
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.352Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x74000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
74 00 00 00
00 00 00 00
00 00 00 00
00 00 78 5F
7C 92 32 5F
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.352Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x75000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
75 00 00 00
00 00 00 00
00 00 00 00
00 00 7D 23
E1 F6 ED B4
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.352Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x76000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
76 00 00 00
00 00 00 00
00 00 00 00
00 00 ED 29
14 40 06 1E
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.352Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x77000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
77 00 00 00
00 00 00 00
00 00 00 00
00 00 F3 EB
B8 18 CF 5B
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.352Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x78000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
78 00 00 00
00 00 00 00
00 00 00 00
00 00 42 0B
FC 42 34 5C
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.352Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x79000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
79 00 00 00
00 00 00 00
00 00 00 00
00 00 B6 29
82 B5 53 23
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.352Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x7A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
7A 00 00 00
00 00 00 00
00 00 00 00
00 00 B4 E8
FE 67 C9 99
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.352Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x7B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
7B 00 00 00
00 00 00 00
00 00 00 00
00 00 38 6D
F6 B4 20 85
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.352Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x7C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
7C 00 00 00
00 00 00 00
00 00 00 00
00 00 7A 05
3A 06 97 EB
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.352Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x7D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
7D 00 00 00
00 00 00 00
00 00 00 00
00 00 F0 54
7D 1D C9 CD
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.352Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x7E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
7E 00 00 00
00 00 00 00
00 00 00 00
00 00 96 C1
86 00 97 1C
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.352Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x7F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
7F 00 00 00
00 00 00 00
00 00 00 00
00 00 E5 F9
38 AB 5F DD
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.352Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x80000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
80 00 00 00
00 00 00 00
00 00 00 00
00 00 58 59
27 44 EE C3
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.352Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x81000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
81 00 00 00
00 00 00 00
00 00 00 00
00 00 49 2D
3B C4 16 BB
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.353Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x82000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
82 00 00 00
00 00 00 00
00 00 00 00
00 00 81 87
E4 94 D6 AE
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.353Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x83000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
83 00 00 00
00 00 00 00
00 00 00 00
00 00 DA 03
74 3E C8 AE
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.353Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x84000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
84 00 00 00
00 00 00 00
00 00 00 00
00 00 D5 53
EE E7 7B 9D
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.353Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x85000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
85 00 00 00
00 00 00 00
00 00 00 00
00 00 E9 CC
4D A3 40 D9
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.353Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x86000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
86 00 00 00
00 00 00 00
00 00 00 00
00 00 4E 9C
89 CD 3A 35
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.353Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x87000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
87 00 00 00
00 00 00 00
00 00 00 00
00 00 C9 FD
DA 35 F3 B2
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.353Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x88000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
88 00 00 00
00 00 00 00
00 00 00 00
00 00 F1 01
C8 09 70 5E
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.353Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x89000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
89 00 00 00
00 00 00 00
00 00 00 00
00 00 68 35
E7 3C 05 D3
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.353Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x8A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
8A 00 00 00
00 00 00 00
00 00 00 00
00 00 73 56
A7 2C 43 9D
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.353Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x8B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
8B 00 00 00
00 00 00 00
00 00 00 00
00 00 83 83
D4 B2 2D C2
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.353Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x8C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
8C 00 00 00
00 00 00 00
00 00 00 00
00 00 DF 1A
63 EA 68 84
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.353Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x8D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
8D 00 00 00
00 00 00 00
00 00 00 00
00 00 A0 EE
4D A5 E2 6D
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.353Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x8E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
8E 00 00 00
00 00 00 00
00 00 00 00
00 00 74 86
A4 C8 81 4D
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.350Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x16 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 16
00 00 00 00
00 00 00 00
00 00 00 19

[rebalance:debug,2025-05-15T18:47:52.353Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 22, stream opaque = 0x19000000
[ns_server:debug,2025-05-15T18:47:52.354Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x17 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 17
00 00 00 00
00 00 00 00
00 00 00 1A

[rebalance:debug,2025-05-15T18:47:52.354Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 23, stream opaque = 0x1A000000
[ns_server:debug,2025-05-15T18:47:52.354Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x18 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 18
00 00 00 00
00 00 00 00
00 00 00 1B

[rebalance:debug,2025-05-15T18:47:52.354Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 24, stream opaque = 0x1B000000
[ns_server:debug,2025-05-15T18:47:52.354Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x19 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 19
00 00 00 00
00 00 00 00
00 00 00 1C

[rebalance:debug,2025-05-15T18:47:52.354Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 25, stream opaque = 0x1C000000
[ns_server:debug,2025-05-15T18:47:52.353Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x8F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
8F 00 00 00
00 00 00 00
00 00 00 00
00 00 FD 47
B7 0A E5 62
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.354Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1A status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 1A
00 00 00 00
00 00 00 00
00 00 00 1D

[rebalance:debug,2025-05-15T18:47:52.354Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 26, stream opaque = 0x1D000000
[ns_server:debug,2025-05-15T18:47:52.354Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1B status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 1B
00 00 00 00
00 00 00 00
00 00 00 1E

[rebalance:debug,2025-05-15T18:47:52.354Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 27, stream opaque = 0x1E000000
[ns_server:debug,2025-05-15T18:47:52.354Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1C status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 1C
00 00 00 00
00 00 00 00
00 00 00 1F

[rebalance:debug,2025-05-15T18:47:52.354Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 28, stream opaque = 0x1F000000
[ns_server:debug,2025-05-15T18:47:52.354Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1D status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 1D
00 00 00 00
00 00 00 00
00 00 00 20

[rebalance:debug,2025-05-15T18:47:52.354Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 29, stream opaque = 0x20000000
[ns_server:debug,2025-05-15T18:47:52.354Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1E status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 1E
00 00 00 00
00 00 00 00
00 00 00 21

[rebalance:debug,2025-05-15T18:47:52.354Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 30, stream opaque = 0x21000000
[ns_server:debug,2025-05-15T18:47:52.354Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1F status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 1F
00 00 00 00
00 00 00 00
00 00 00 22

[rebalance:debug,2025-05-15T18:47:52.354Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 31, stream opaque = 0x22000000
[ns_server:debug,2025-05-15T18:47:52.356Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x20 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 20
00 00 00 00
00 00 00 00
00 00 00 23

[rebalance:debug,2025-05-15T18:47:52.356Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 32, stream opaque = 0x23000000
[ns_server:debug,2025-05-15T18:47:52.356Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x21 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 21
00 00 00 00
00 00 00 00
00 00 00 24

[rebalance:debug,2025-05-15T18:47:52.356Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 33, stream opaque = 0x24000000
[ns_server:debug,2025-05-15T18:47:52.356Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x90000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
90 00 00 00
00 00 00 00
00 00 00 00
00 00 C2 C2
37 DD CB 90
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.356Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x22 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 22
00 00 00 00
00 00 00 00
00 00 00 25

[rebalance:debug,2025-05-15T18:47:52.356Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 34, stream opaque = 0x25000000
[ns_server:debug,2025-05-15T18:47:52.356Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x91000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
91 00 00 00
00 00 00 00
00 00 00 00
00 00 C4 FD
88 60 06 BB
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.356Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x92000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
92 00 00 00
00 00 00 00
00 00 00 00
00 00 3E 42
0F 33 5C 50
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.356Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x93000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
93 00 00 00
00 00 00 00
00 00 00 00
00 00 E6 76
61 12 9A 59
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.356Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x23 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 23
00 00 00 00
00 00 00 00
00 00 00 26

[rebalance:debug,2025-05-15T18:47:52.356Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 35, stream opaque = 0x26000000
[ns_server:debug,2025-05-15T18:47:52.356Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x24 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 24
00 00 00 00
00 00 00 00
00 00 00 27

[ns_server:debug,2025-05-15T18:47:52.356Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x94000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
94 00 00 00
00 00 00 00
00 00 00 00
00 00 80 12
08 40 AF 49
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.356Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 36, stream opaque = 0x27000000
[ns_server:debug,2025-05-15T18:47:52.357Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x95000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
95 00 00 00
00 00 00 00
00 00 00 00
00 00 E0 54
BC 68 94 64
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.357Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x25 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 25
00 00 00 00
00 00 00 00
00 00 00 28

[rebalance:debug,2025-05-15T18:47:52.357Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 37, stream opaque = 0x28000000
[ns_server:debug,2025-05-15T18:47:52.357Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x26 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 26
00 00 00 00
00 00 00 00
00 00 00 29

[ns_server:debug,2025-05-15T18:47:52.357Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x96000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
96 00 00 00
00 00 00 00
00 00 00 00
00 00 D4 78
29 DD 61 70
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.357Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 38, stream opaque = 0x29000000
[ns_server:debug,2025-05-15T18:47:52.357Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x27 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 27
00 00 00 00
00 00 00 00
00 00 00 2A

[rebalance:debug,2025-05-15T18:47:52.357Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 39, stream opaque = 0x2A000000
[ns_server:debug,2025-05-15T18:47:52.357Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x97000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
97 00 00 00
00 00 00 00
00 00 00 00
00 00 FA EA
14 54 10 4C
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.357Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x28 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 28
00 00 00 00
00 00 00 00
00 00 00 2B

[rebalance:debug,2025-05-15T18:47:52.357Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 40, stream opaque = 0x2B000000
[ns_server:debug,2025-05-15T18:47:52.357Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x98000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
98 00 00 00
00 00 00 00
00 00 00 00
00 00 26 FD
29 22 3E 77
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.357Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x29 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 29
00 00 00 00
00 00 00 00
00 00 00 2C

[rebalance:debug,2025-05-15T18:47:52.357Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 41, stream opaque = 0x2C000000
[ns_server:debug,2025-05-15T18:47:52.357Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2A status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 2A
00 00 00 00
00 00 00 00
00 00 00 2D

[ns_server:debug,2025-05-15T18:47:52.357Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x99000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
99 00 00 00
00 00 00 00
00 00 00 00
00 00 1D CD
21 8E F5 12
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.357Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 42, stream opaque = 0x2D000000
[ns_server:debug,2025-05-15T18:47:52.357Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x9A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
9A 00 00 00
00 00 00 00
00 00 00 00
00 00 01 81
91 4E 7A 7A
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.357Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x9B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
9B 00 00 00
00 00 00 00
00 00 00 00
00 00 DE 33
FA 06 06 5A
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.357Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2B status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 2B
00 00 00 00
00 00 00 00
00 00 00 2E

[ns_server:debug,2025-05-15T18:47:52.357Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x9C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
9C 00 00 00
00 00 00 00
00 00 00 00
00 00 86 4C
C5 38 6E CA
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.357Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 43, stream opaque = 0x2E000000
[ns_server:debug,2025-05-15T18:47:52.357Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2C status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 2C
00 00 00 00
00 00 00 00
00 00 00 2F

[ns_server:debug,2025-05-15T18:47:52.358Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x9D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
9D 00 00 00
00 00 00 00
00 00 00 00
00 00 C2 15
A7 85 C7 24
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.358Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 44, stream opaque = 0x2F000000
[ns_server:debug,2025-05-15T18:47:52.358Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2D status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 2D
00 00 00 00
00 00 00 00
00 00 00 30

[rebalance:debug,2025-05-15T18:47:52.358Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 45, stream opaque = 0x30000000
[ns_server:debug,2025-05-15T18:47:52.358Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2E status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 2E
00 00 00 00
00 00 00 00
00 00 00 31

[rebalance:debug,2025-05-15T18:47:52.358Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 46, stream opaque = 0x31000000
[ns_server:debug,2025-05-15T18:47:52.358Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x9E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
9E 00 00 00
00 00 00 00
00 00 00 00
00 00 68 B7
D8 A2 C2 2A
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.358Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2F status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 2F
00 00 00 00
00 00 00 00
00 00 00 32

[rebalance:debug,2025-05-15T18:47:52.358Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 47, stream opaque = 0x32000000
[ns_server:debug,2025-05-15T18:47:52.358Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x30 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 30
00 00 00 00
00 00 00 00
00 00 00 33

[rebalance:debug,2025-05-15T18:47:52.358Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 48, stream opaque = 0x33000000
[ns_server:debug,2025-05-15T18:47:52.358Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x9F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
9F 00 00 00
00 00 00 00
00 00 00 00
00 00 9E BF
CB DC D2 51
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.358Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x31 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 31
00 00 00 00
00 00 00 00
00 00 00 34

[ns_server:debug,2025-05-15T18:47:52.358Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA0000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A0 00 00 00
00 00 00 00
00 00 00 00
00 00 A9 86
1E 9D 39 D3
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.358Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 49, stream opaque = 0x34000000
[ns_server:debug,2025-05-15T18:47:52.358Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA1000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A1 00 00 00
00 00 00 00
00 00 00 00
00 00 51 10
7F FB A6 3A
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.358Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x32 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 32
00 00 00 00
00 00 00 00
00 00 00 35

[rebalance:debug,2025-05-15T18:47:52.358Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 50, stream opaque = 0x35000000
[ns_server:debug,2025-05-15T18:47:52.358Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA2000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A2 00 00 00
00 00 00 00
00 00 00 00
00 00 87 24
2D 4E E5 6B
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.358Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x33 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 33
00 00 00 00
00 00 00 00
00 00 00 36

[ns_server:debug,2025-05-15T18:47:52.358Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA3000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A3 00 00 00
00 00 00 00
00 00 00 00
00 00 9F CB
13 93 26 6D
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.359Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA4000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A4 00 00 00
00 00 00 00
00 00 00 00
00 00 CC 0C
44 89 4A 9B
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.360Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA5000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A5 00 00 00
00 00 00 00
00 00 00 00
00 00 82 5A
CA E7 93 A4
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.358Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 51, stream opaque = 0x36000000
[ns_server:debug,2025-05-15T18:47:52.360Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA6000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A6 00 00 00
00 00 00 00
00 00 00 00
00 00 D9 12
21 D7 5D FE
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.360Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x34 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 34
00 00 00 00
00 00 00 00
00 00 00 37

[ns_server:debug,2025-05-15T18:47:52.360Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA7000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A7 00 00 00
00 00 00 00
00 00 00 00
00 00 93 58
CC 6D D5 34
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.360Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 52, stream opaque = 0x37000000
[ns_server:debug,2025-05-15T18:47:52.360Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x35 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 35
00 00 00 00
00 00 00 00
00 00 00 38

[ns_server:debug,2025-05-15T18:47:52.360Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA8000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A8 00 00 00
00 00 00 00
00 00 00 00
00 00 5D 41
29 0D 5D 75
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.360Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 53, stream opaque = 0x38000000
[ns_server:debug,2025-05-15T18:47:52.360Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x36 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 36
00 00 00 00
00 00 00 00
00 00 00 39

[ns_server:debug,2025-05-15T18:47:52.360Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA9000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A9 00 00 00
00 00 00 00
00 00 00 00
00 00 27 CD
0F E9 06 31
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.360Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 54, stream opaque = 0x39000000
[ns_server:debug,2025-05-15T18:47:52.360Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xAA000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
AA 00 00 00
00 00 00 00
00 00 00 00
00 00 7F 9C
36 DB 01 54
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.360Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xAB000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
AB 00 00 00
00 00 00 00
00 00 00 00
00 00 9E 1F
1E 21 FF 72
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.360Z,ns_1@db2.lan:<0.4274.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xAC000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
AC 00 00 00
00 00 00 00
00 00 00 00
00 00 3B DC
14 83 80 74
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.360Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x37 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 37
00 00 00 00
00 00 00 00
00 00 00 3A

[rebalance:debug,2025-05-15T18:47:52.374Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 55, stream opaque = 0x3A000000
[ns_server:debug,2025-05-15T18:47:52.374Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x38 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 38
00 00 00 00
00 00 00 00
00 00 00 3B

[rebalance:debug,2025-05-15T18:47:52.374Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 56, stream opaque = 0x3B000000
[ns_server:debug,2025-05-15T18:47:52.376Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x39 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 39
00 00 00 00
00 00 00 00
00 00 00 3C

[rebalance:debug,2025-05-15T18:47:52.377Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 57, stream opaque = 0x3C000000
[ns_server:debug,2025-05-15T18:47:52.377Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3A status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 3A
00 00 00 00
00 00 00 00
00 00 00 3D

[rebalance:debug,2025-05-15T18:47:52.377Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 58, stream opaque = 0x3D000000
[ns_server:debug,2025-05-15T18:47:52.377Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3B status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 3B
00 00 00 00
00 00 00 00
00 00 00 3E

[rebalance:debug,2025-05-15T18:47:52.377Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 59, stream opaque = 0x3E000000
[ns_server:debug,2025-05-15T18:47:52.377Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3C status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 3C
00 00 00 00
00 00 00 00
00 00 00 3F

[rebalance:debug,2025-05-15T18:47:52.377Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 60, stream opaque = 0x3F000000
[ns_server:debug,2025-05-15T18:47:52.377Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3D status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 3D
00 00 00 00
00 00 00 00
00 00 00 40

[rebalance:debug,2025-05-15T18:47:52.377Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 61, stream opaque = 0x40000000
[ns_server:debug,2025-05-15T18:47:52.378Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3E status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 3E
00 00 00 00
00 00 00 00
00 00 00 41

[rebalance:debug,2025-05-15T18:47:52.378Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 62, stream opaque = 0x41000000
[ns_server:debug,2025-05-15T18:47:52.378Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3F status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 3F
00 00 00 00
00 00 00 00
00 00 00 42

[rebalance:debug,2025-05-15T18:47:52.378Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 63, stream opaque = 0x42000000
[ns_server:debug,2025-05-15T18:47:52.378Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x40 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 40
00 00 00 00
00 00 00 00
00 00 00 43

[rebalance:debug,2025-05-15T18:47:52.378Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 64, stream opaque = 0x43000000
[ns_server:debug,2025-05-15T18:47:52.378Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x41 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 41
00 00 00 00
00 00 00 00
00 00 00 44

[rebalance:debug,2025-05-15T18:47:52.378Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 65, stream opaque = 0x44000000
[ns_server:debug,2025-05-15T18:47:52.378Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x42 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 42
00 00 00 00
00 00 00 00
00 00 00 45

[rebalance:debug,2025-05-15T18:47:52.378Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 66, stream opaque = 0x45000000
[ns_server:debug,2025-05-15T18:47:52.378Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x43 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 43
00 00 00 00
00 00 00 00
00 00 00 46

[rebalance:debug,2025-05-15T18:47:52.378Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 67, stream opaque = 0x46000000
[ns_server:debug,2025-05-15T18:47:52.378Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x44 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 44
00 00 00 00
00 00 00 00
00 00 00 47

[rebalance:debug,2025-05-15T18:47:52.379Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 68, stream opaque = 0x47000000
[ns_server:debug,2025-05-15T18:47:52.379Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x45 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 45
00 00 00 00
00 00 00 00
00 00 00 48

[rebalance:debug,2025-05-15T18:47:52.379Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 69, stream opaque = 0x48000000
[ns_server:debug,2025-05-15T18:47:52.379Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x46 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 46
00 00 00 00
00 00 00 00
00 00 00 49

[rebalance:debug,2025-05-15T18:47:52.379Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 70, stream opaque = 0x49000000
[ns_server:debug,2025-05-15T18:47:52.379Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x47 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 47
00 00 00 00
00 00 00 00
00 00 00 4A

[rebalance:debug,2025-05-15T18:47:52.379Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 71, stream opaque = 0x4A000000
[ns_server:debug,2025-05-15T18:47:52.379Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x48 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 48
00 00 00 00
00 00 00 00
00 00 00 4B

[rebalance:debug,2025-05-15T18:47:52.379Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 72, stream opaque = 0x4B000000
[ns_server:debug,2025-05-15T18:47:52.379Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x49 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 49
00 00 00 00
00 00 00 00
00 00 00 4C

[rebalance:debug,2025-05-15T18:47:52.379Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 73, stream opaque = 0x4C000000
[ns_server:debug,2025-05-15T18:47:52.379Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x4A status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 4A
00 00 00 00
00 00 00 00
00 00 00 4D

[rebalance:debug,2025-05-15T18:47:52.379Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 74, stream opaque = 0x4D000000
[ns_server:debug,2025-05-15T18:47:52.379Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x4B status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 4B
00 00 00 00
00 00 00 00
00 00 00 4E

[rebalance:debug,2025-05-15T18:47:52.379Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 75, stream opaque = 0x4E000000
[ns_server:debug,2025-05-15T18:47:52.379Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x4C status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 4C
00 00 00 00
00 00 00 00
00 00 00 4F

[rebalance:debug,2025-05-15T18:47:52.379Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 76, stream opaque = 0x4F000000
[ns_server:debug,2025-05-15T18:47:52.379Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x4D status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 4D
00 00 00 00
00 00 00 00
00 00 00 50

[rebalance:debug,2025-05-15T18:47:52.379Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 77, stream opaque = 0x50000000
[ns_server:debug,2025-05-15T18:47:52.379Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x4E status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 4E
00 00 00 00
00 00 00 00
00 00 00 51

[rebalance:debug,2025-05-15T18:47:52.379Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 78, stream opaque = 0x51000000
[ns_server:debug,2025-05-15T18:47:52.379Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x4F status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 4F
00 00 00 00
00 00 00 00
00 00 00 52

[rebalance:debug,2025-05-15T18:47:52.379Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 79, stream opaque = 0x52000000
[ns_server:debug,2025-05-15T18:47:52.379Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x50 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 50
00 00 00 00
00 00 00 00
00 00 00 53

[rebalance:debug,2025-05-15T18:47:52.379Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 80, stream opaque = 0x53000000
[ns_server:debug,2025-05-15T18:47:52.379Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x51 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 51
00 00 00 00
00 00 00 00
00 00 00 54

[rebalance:debug,2025-05-15T18:47:52.379Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 81, stream opaque = 0x54000000
[ns_server:debug,2025-05-15T18:47:52.379Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x52 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 52
00 00 00 00
00 00 00 00
00 00 00 55

[rebalance:debug,2025-05-15T18:47:52.380Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 82, stream opaque = 0x55000000
[ns_server:debug,2025-05-15T18:47:52.380Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x53 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 53
00 00 00 00
00 00 00 00
00 00 00 56

[rebalance:debug,2025-05-15T18:47:52.380Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 83, stream opaque = 0x56000000
[ns_server:debug,2025-05-15T18:47:52.380Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x54 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 54
00 00 00 00
00 00 00 00
00 00 00 57

[rebalance:debug,2025-05-15T18:47:52.380Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 84, stream opaque = 0x57000000
[ns_server:debug,2025-05-15T18:47:52.380Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x55 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 55
00 00 00 00
00 00 00 00
00 00 00 58

[rebalance:debug,2025-05-15T18:47:52.380Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 85, stream opaque = 0x58000000
[ns_server:debug,2025-05-15T18:47:52.380Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x56 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 56
00 00 00 00
00 00 00 00
00 00 00 59

[rebalance:debug,2025-05-15T18:47:52.380Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 86, stream opaque = 0x59000000
[ns_server:debug,2025-05-15T18:47:52.380Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x57 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 57
00 00 00 00
00 00 00 00
00 00 00 5A

[rebalance:debug,2025-05-15T18:47:52.380Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 87, stream opaque = 0x5A000000
[ns_server:debug,2025-05-15T18:47:52.380Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x58 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 58
00 00 00 00
00 00 00 00
00 00 00 5B

[rebalance:debug,2025-05-15T18:47:52.380Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 88, stream opaque = 0x5B000000
[ns_server:debug,2025-05-15T18:47:52.380Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x59 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 59
00 00 00 00
00 00 00 00
00 00 00 5C

[rebalance:debug,2025-05-15T18:47:52.380Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 89, stream opaque = 0x5C000000
[ns_server:debug,2025-05-15T18:47:52.380Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x5A status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 5A
00 00 00 00
00 00 00 00
00 00 00 5D

[rebalance:debug,2025-05-15T18:47:52.380Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 90, stream opaque = 0x5D000000
[ns_server:debug,2025-05-15T18:47:52.380Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x5B status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 5B
00 00 00 00
00 00 00 00
00 00 00 5E

[rebalance:debug,2025-05-15T18:47:52.380Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 91, stream opaque = 0x5E000000
[ns_server:debug,2025-05-15T18:47:52.380Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x5C status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 5C
00 00 00 00
00 00 00 00
00 00 00 5F

[rebalance:debug,2025-05-15T18:47:52.380Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 92, stream opaque = 0x5F000000
[ns_server:debug,2025-05-15T18:47:52.380Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x5D status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 5D
00 00 00 00
00 00 00 00
00 00 00 60

[rebalance:debug,2025-05-15T18:47:52.380Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 93, stream opaque = 0x60000000
[ns_server:debug,2025-05-15T18:47:52.380Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x5E status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 5E
00 00 00 00
00 00 00 00
00 00 00 61

[rebalance:debug,2025-05-15T18:47:52.380Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 94, stream opaque = 0x61000000
[ns_server:debug,2025-05-15T18:47:52.380Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x5F status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 5F
00 00 00 00
00 00 00 00
00 00 00 62

[rebalance:debug,2025-05-15T18:47:52.380Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 95, stream opaque = 0x62000000
[ns_server:debug,2025-05-15T18:47:52.380Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x60 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 60
00 00 00 00
00 00 00 00
00 00 00 63

[rebalance:debug,2025-05-15T18:47:52.380Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 96, stream opaque = 0x63000000
[ns_server:debug,2025-05-15T18:47:52.380Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x61 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 61
00 00 00 00
00 00 00 00
00 00 00 64

[rebalance:debug,2025-05-15T18:47:52.380Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 97, stream opaque = 0x64000000
[ns_server:debug,2025-05-15T18:47:52.381Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x62 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 62
00 00 00 00
00 00 00 00
00 00 00 65

[rebalance:debug,2025-05-15T18:47:52.381Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 98, stream opaque = 0x65000000
[ns_server:debug,2025-05-15T18:47:52.381Z,ns_1@db2.lan:dcp_traffic_monitor<0.3371.0>:dcp_traffic_monitor:update_bucket:148]Saw that bucket "doom-scrolling" became alive on node 'ns_1@172.19.0.4'
[ns_server:debug,2025-05-15T18:47:52.381Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x63 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 63
00 00 00 00
00 00 00 00
00 00 00 66

[rebalance:debug,2025-05-15T18:47:52.381Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 99, stream opaque = 0x66000000
[ns_server:debug,2025-05-15T18:47:52.381Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x64 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 64
00 00 00 00
00 00 00 00
00 00 00 67

[rebalance:debug,2025-05-15T18:47:52.381Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 100, stream opaque = 0x67000000
[ns_server:debug,2025-05-15T18:47:52.381Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x65 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 65
00 00 00 00
00 00 00 00
00 00 00 68

[rebalance:debug,2025-05-15T18:47:52.381Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 101, stream opaque = 0x68000000
[ns_server:debug,2025-05-15T18:47:52.381Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x66 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 66
00 00 00 00
00 00 00 00
00 00 00 69

[rebalance:debug,2025-05-15T18:47:52.381Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 102, stream opaque = 0x69000000
[ns_server:debug,2025-05-15T18:47:52.381Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x67 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 67
00 00 00 00
00 00 00 00
00 00 00 6A

[rebalance:debug,2025-05-15T18:47:52.381Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 103, stream opaque = 0x6A000000
[ns_server:debug,2025-05-15T18:47:52.381Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x68 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 68
00 00 00 00
00 00 00 00
00 00 00 6B

[rebalance:debug,2025-05-15T18:47:52.381Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 104, stream opaque = 0x6B000000
[ns_server:debug,2025-05-15T18:47:52.381Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x69 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 69
00 00 00 00
00 00 00 00
00 00 00 6C

[rebalance:debug,2025-05-15T18:47:52.381Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 105, stream opaque = 0x6C000000
[ns_server:debug,2025-05-15T18:47:52.381Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x6A status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 6A
00 00 00 00
00 00 00 00
00 00 00 6D

[rebalance:debug,2025-05-15T18:47:52.381Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 106, stream opaque = 0x6D000000
[ns_server:debug,2025-05-15T18:47:52.382Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x6B status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 6B
00 00 00 00
00 00 00 00
00 00 00 6E

[rebalance:debug,2025-05-15T18:47:52.382Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 107, stream opaque = 0x6E000000
[ns_server:debug,2025-05-15T18:47:52.382Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x6C status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 6C
00 00 00 00
00 00 00 00
00 00 00 6F

[rebalance:debug,2025-05-15T18:47:52.382Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 108, stream opaque = 0x6F000000
[ns_server:debug,2025-05-15T18:47:52.382Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x6D status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 6D
00 00 00 00
00 00 00 00
00 00 00 70

[rebalance:debug,2025-05-15T18:47:52.382Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 109, stream opaque = 0x70000000
[ns_server:debug,2025-05-15T18:47:52.382Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x6E status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 6E
00 00 00 00
00 00 00 00
00 00 00 71

[rebalance:debug,2025-05-15T18:47:52.382Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 110, stream opaque = 0x71000000
[ns_server:debug,2025-05-15T18:47:52.382Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x6F status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 6F
00 00 00 00
00 00 00 00
00 00 00 72

[rebalance:debug,2025-05-15T18:47:52.382Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 111, stream opaque = 0x72000000
[ns_server:debug,2025-05-15T18:47:52.382Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x70 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 70
00 00 00 00
00 00 00 00
00 00 00 73

[rebalance:debug,2025-05-15T18:47:52.382Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 112, stream opaque = 0x73000000
[ns_server:debug,2025-05-15T18:47:52.382Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x71 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 71
00 00 00 00
00 00 00 00
00 00 00 74

[rebalance:debug,2025-05-15T18:47:52.382Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 113, stream opaque = 0x74000000
[ns_server:debug,2025-05-15T18:47:52.382Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x72 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 72
00 00 00 00
00 00 00 00
00 00 00 75

[rebalance:debug,2025-05-15T18:47:52.382Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 114, stream opaque = 0x75000000
[ns_server:debug,2025-05-15T18:47:52.382Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x73 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 73
00 00 00 00
00 00 00 00
00 00 00 76

[rebalance:debug,2025-05-15T18:47:52.382Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 115, stream opaque = 0x76000000
[ns_server:debug,2025-05-15T18:47:52.382Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x74 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 74
00 00 00 00
00 00 00 00
00 00 00 77

[rebalance:debug,2025-05-15T18:47:52.383Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 116, stream opaque = 0x77000000
[ns_server:debug,2025-05-15T18:47:52.383Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x75 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 75
00 00 00 00
00 00 00 00
00 00 00 78

[rebalance:debug,2025-05-15T18:47:52.383Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 117, stream opaque = 0x78000000
[ns_server:debug,2025-05-15T18:47:52.383Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x76 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 76
00 00 00 00
00 00 00 00
00 00 00 79

[rebalance:debug,2025-05-15T18:47:52.383Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 118, stream opaque = 0x79000000
[ns_server:debug,2025-05-15T18:47:52.383Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x77 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 77
00 00 00 00
00 00 00 00
00 00 00 7A

[rebalance:debug,2025-05-15T18:47:52.383Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 119, stream opaque = 0x7A000000
[ns_server:debug,2025-05-15T18:47:52.383Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x78 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 78
00 00 00 00
00 00 00 00
00 00 00 7B

[rebalance:debug,2025-05-15T18:47:52.383Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 120, stream opaque = 0x7B000000
[ns_server:debug,2025-05-15T18:47:52.383Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x79 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 79
00 00 00 00
00 00 00 00
00 00 00 7C

[rebalance:debug,2025-05-15T18:47:52.383Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 121, stream opaque = 0x7C000000
[ns_server:debug,2025-05-15T18:47:52.383Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x7A status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 7A
00 00 00 00
00 00 00 00
00 00 00 7D

[rebalance:debug,2025-05-15T18:47:52.383Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 122, stream opaque = 0x7D000000
[ns_server:debug,2025-05-15T18:47:52.383Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x7B status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 7B
00 00 00 00
00 00 00 00
00 00 00 7E

[rebalance:debug,2025-05-15T18:47:52.383Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 123, stream opaque = 0x7E000000
[ns_server:debug,2025-05-15T18:47:52.383Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x7C status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 7C
00 00 00 00
00 00 00 00
00 00 00 7F

[rebalance:debug,2025-05-15T18:47:52.383Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 124, stream opaque = 0x7F000000
[ns_server:debug,2025-05-15T18:47:52.383Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x7D status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 7D
00 00 00 00
00 00 00 00
00 00 00 80

[rebalance:debug,2025-05-15T18:47:52.383Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 125, stream opaque = 0x80000000
[ns_server:debug,2025-05-15T18:47:52.383Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x7E status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 7E
00 00 00 00
00 00 00 00
00 00 00 81

[rebalance:debug,2025-05-15T18:47:52.383Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 126, stream opaque = 0x81000000
[ns_server:debug,2025-05-15T18:47:52.383Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x7F status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 7F
00 00 00 00
00 00 00 00
00 00 00 82

[rebalance:debug,2025-05-15T18:47:52.384Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 127, stream opaque = 0x82000000
[ns_server:debug,2025-05-15T18:47:52.384Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x80 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 80
00 00 00 00
00 00 00 00
00 00 00 83

[rebalance:debug,2025-05-15T18:47:52.384Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 128, stream opaque = 0x83000000
[ns_server:debug,2025-05-15T18:47:52.384Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x81 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 81
00 00 00 00
00 00 00 00
00 00 00 84

[rebalance:debug,2025-05-15T18:47:52.384Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 129, stream opaque = 0x84000000
[ns_server:debug,2025-05-15T18:47:52.384Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x82 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 82
00 00 00 00
00 00 00 00
00 00 00 85

[rebalance:debug,2025-05-15T18:47:52.384Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 130, stream opaque = 0x85000000
[ns_server:debug,2025-05-15T18:47:52.384Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x83 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 83
00 00 00 00
00 00 00 00
00 00 00 86

[rebalance:debug,2025-05-15T18:47:52.384Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 131, stream opaque = 0x86000000
[ns_server:debug,2025-05-15T18:47:52.384Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x84 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 84
00 00 00 00
00 00 00 00
00 00 00 87

[rebalance:debug,2025-05-15T18:47:52.384Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 132, stream opaque = 0x87000000
[ns_server:debug,2025-05-15T18:47:52.384Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x85 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 85
00 00 00 00
00 00 00 00
00 00 00 88

[rebalance:debug,2025-05-15T18:47:52.384Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 133, stream opaque = 0x88000000
[ns_server:debug,2025-05-15T18:47:52.384Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x86 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 86
00 00 00 00
00 00 00 00
00 00 00 89

[rebalance:debug,2025-05-15T18:47:52.384Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 134, stream opaque = 0x89000000
[ns_server:debug,2025-05-15T18:47:52.384Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x87 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 87
00 00 00 00
00 00 00 00
00 00 00 8A

[rebalance:debug,2025-05-15T18:47:52.384Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 135, stream opaque = 0x8A000000
[ns_server:debug,2025-05-15T18:47:52.384Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x88 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 88
00 00 00 00
00 00 00 00
00 00 00 8B

[rebalance:debug,2025-05-15T18:47:52.384Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 136, stream opaque = 0x8B000000
[ns_server:debug,2025-05-15T18:47:52.384Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x89 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 89
00 00 00 00
00 00 00 00
00 00 00 8C

[rebalance:debug,2025-05-15T18:47:52.384Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 137, stream opaque = 0x8C000000
[ns_server:debug,2025-05-15T18:47:52.385Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x8A status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 8A
00 00 00 00
00 00 00 00
00 00 00 8D

[rebalance:debug,2025-05-15T18:47:52.385Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 138, stream opaque = 0x8D000000
[ns_server:debug,2025-05-15T18:47:52.385Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x8B status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 8B
00 00 00 00
00 00 00 00
00 00 00 8E

[rebalance:debug,2025-05-15T18:47:52.385Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 139, stream opaque = 0x8E000000
[ns_server:debug,2025-05-15T18:47:52.385Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x8C status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 8C
00 00 00 00
00 00 00 00
00 00 00 8F

[rebalance:debug,2025-05-15T18:47:52.385Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 140, stream opaque = 0x8F000000
[ns_server:debug,2025-05-15T18:47:52.385Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x8D status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 8D
00 00 00 00
00 00 00 00
00 00 00 90

[rebalance:debug,2025-05-15T18:47:52.385Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 141, stream opaque = 0x90000000
[ns_server:debug,2025-05-15T18:47:52.385Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x8E status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 8E
00 00 00 00
00 00 00 00
00 00 00 91

[rebalance:debug,2025-05-15T18:47:52.385Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 142, stream opaque = 0x91000000
[ns_server:debug,2025-05-15T18:47:52.385Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x8F status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 8F
00 00 00 00
00 00 00 00
00 00 00 92

[rebalance:debug,2025-05-15T18:47:52.385Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 143, stream opaque = 0x92000000
[ns_server:debug,2025-05-15T18:47:52.385Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x90 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 90
00 00 00 00
00 00 00 00
00 00 00 93

[rebalance:debug,2025-05-15T18:47:52.385Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 144, stream opaque = 0x93000000
[ns_server:debug,2025-05-15T18:47:52.385Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x91 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 91
00 00 00 00
00 00 00 00
00 00 00 94

[rebalance:debug,2025-05-15T18:47:52.385Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 145, stream opaque = 0x94000000
[ns_server:debug,2025-05-15T18:47:52.385Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x92 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 92
00 00 00 00
00 00 00 00
00 00 00 95

[rebalance:debug,2025-05-15T18:47:52.385Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 146, stream opaque = 0x95000000
[ns_server:debug,2025-05-15T18:47:52.385Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x93 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 93
00 00 00 00
00 00 00 00
00 00 00 96

[rebalance:debug,2025-05-15T18:47:52.385Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 147, stream opaque = 0x96000000
[ns_server:debug,2025-05-15T18:47:52.385Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x94 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 94
00 00 00 00
00 00 00 00
00 00 00 97

[rebalance:debug,2025-05-15T18:47:52.385Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 148, stream opaque = 0x97000000
[ns_server:debug,2025-05-15T18:47:52.386Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x95 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 95
00 00 00 00
00 00 00 00
00 00 00 98

[rebalance:debug,2025-05-15T18:47:52.386Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 149, stream opaque = 0x98000000
[ns_server:debug,2025-05-15T18:47:52.386Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x96 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 96
00 00 00 00
00 00 00 00
00 00 00 99

[rebalance:debug,2025-05-15T18:47:52.386Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 150, stream opaque = 0x99000000
[ns_server:debug,2025-05-15T18:47:52.386Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x97 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 97
00 00 00 00
00 00 00 00
00 00 00 9A

[rebalance:debug,2025-05-15T18:47:52.386Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 151, stream opaque = 0x9A000000
[ns_server:debug,2025-05-15T18:47:52.386Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x98 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 98
00 00 00 00
00 00 00 00
00 00 00 9B

[rebalance:debug,2025-05-15T18:47:52.386Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 152, stream opaque = 0x9B000000
[ns_server:debug,2025-05-15T18:47:52.386Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x99 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 99
00 00 00 00
00 00 00 00
00 00 00 9C

[rebalance:debug,2025-05-15T18:47:52.386Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 153, stream opaque = 0x9C000000
[ns_server:debug,2025-05-15T18:47:52.386Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x9A status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 9A
00 00 00 00
00 00 00 00
00 00 00 9D

[rebalance:debug,2025-05-15T18:47:52.386Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 154, stream opaque = 0x9D000000
[ns_server:debug,2025-05-15T18:47:52.386Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x9B status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 9B
00 00 00 00
00 00 00 00
00 00 00 9E

[rebalance:debug,2025-05-15T18:47:52.386Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 155, stream opaque = 0x9E000000
[ns_server:debug,2025-05-15T18:47:52.386Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x9C status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 9C
00 00 00 00
00 00 00 00
00 00 00 9F

[rebalance:debug,2025-05-15T18:47:52.386Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 156, stream opaque = 0x9F000000
[ns_server:debug,2025-05-15T18:47:52.386Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x9D status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 9D
00 00 00 00
00 00 00 00
00 00 00 A0

[rebalance:debug,2025-05-15T18:47:52.386Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 157, stream opaque = 0xA0000000
[ns_server:debug,2025-05-15T18:47:52.386Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x9E status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 9E
00 00 00 00
00 00 00 00
00 00 00 A1

[rebalance:debug,2025-05-15T18:47:52.386Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 158, stream opaque = 0xA1000000
[ns_server:debug,2025-05-15T18:47:52.386Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x9F status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 9F
00 00 00 00
00 00 00 00
00 00 00 A2

[rebalance:debug,2025-05-15T18:47:52.386Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 159, stream opaque = 0xA2000000
[ns_server:debug,2025-05-15T18:47:52.387Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xA0 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 A0
00 00 00 00
00 00 00 00
00 00 00 A3

[rebalance:debug,2025-05-15T18:47:52.387Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 160, stream opaque = 0xA3000000
[ns_server:debug,2025-05-15T18:47:52.387Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xA1 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 A1
00 00 00 00
00 00 00 00
00 00 00 A4

[rebalance:debug,2025-05-15T18:47:52.387Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 161, stream opaque = 0xA4000000
[ns_server:debug,2025-05-15T18:47:52.387Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xA2 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 A2
00 00 00 00
00 00 00 00
00 00 00 A5

[rebalance:debug,2025-05-15T18:47:52.387Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 162, stream opaque = 0xA5000000
[ns_server:debug,2025-05-15T18:47:52.387Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xA3 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 A3
00 00 00 00
00 00 00 00
00 00 00 A6

[rebalance:debug,2025-05-15T18:47:52.387Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 163, stream opaque = 0xA6000000
[ns_server:debug,2025-05-15T18:47:52.387Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xA4 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 A4
00 00 00 00
00 00 00 00
00 00 00 A7

[rebalance:debug,2025-05-15T18:47:52.387Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 164, stream opaque = 0xA7000000
[ns_server:debug,2025-05-15T18:47:52.387Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xA5 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 A5
00 00 00 00
00 00 00 00
00 00 00 A8

[rebalance:debug,2025-05-15T18:47:52.387Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 165, stream opaque = 0xA8000000
[ns_server:debug,2025-05-15T18:47:52.387Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xA6 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 A6
00 00 00 00
00 00 00 00
00 00 00 A9

[rebalance:debug,2025-05-15T18:47:52.387Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 166, stream opaque = 0xA9000000
[ns_server:debug,2025-05-15T18:47:52.387Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xA7 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 A7
00 00 00 00
00 00 00 00
00 00 00 AA

[rebalance:debug,2025-05-15T18:47:52.387Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 167, stream opaque = 0xAA000000
[ns_server:debug,2025-05-15T18:47:52.387Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xA8 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 A8
00 00 00 00
00 00 00 00
00 00 00 AB

[rebalance:debug,2025-05-15T18:47:52.387Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 168, stream opaque = 0xAB000000
[ns_server:debug,2025-05-15T18:47:52.387Z,ns_1@db2.lan:<0.4265.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0xA9 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 00 A9
00 00 00 00
00 00 00 00
00 00 00 AC

[rebalance:debug,2025-05-15T18:47:52.387Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 169, stream opaque = 0xAC000000
[ns_server:debug,2025-05-15T18:47:52.387Z,ns_1@db2.lan:<0.4265.0>:dcp_consumer_conn:maybe_reply_setup_streams:499]Setup stream request completed with ok. Moving to idle state
[ns_server:debug,2025-05-15T18:47:52.387Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 853, opaque = 0x355, type = add
[ns_server:debug,2025-05-15T18:47:52.387Z,ns_1@db2.lan:dcp_traffic_monitor<0.3371.0>:dcp_traffic_monitor:update_bucket:148]Saw that bucket "doom-scrolling" became alive on node 'ns_1@db2.lan'
[ns_server:debug,2025-05-15T18:47:52.387Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 854, opaque = 0x356, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 855, opaque = 0x357, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 856, opaque = 0x358, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 857, opaque = 0x359, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 858, opaque = 0x35A, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 859, opaque = 0x35B, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 860, opaque = 0x35C, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 861, opaque = 0x35D, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 862, opaque = 0x35E, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 863, opaque = 0x35F, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 864, opaque = 0x360, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 865, opaque = 0x361, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 866, opaque = 0x362, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 867, opaque = 0x363, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 868, opaque = 0x364, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 869, opaque = 0x365, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 870, opaque = 0x366, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 871, opaque = 0x367, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 872, opaque = 0x368, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 873, opaque = 0x369, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 874, opaque = 0x36A, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 875, opaque = 0x36B, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 876, opaque = 0x36C, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 877, opaque = 0x36D, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 878, opaque = 0x36E, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 879, opaque = 0x36F, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 880, opaque = 0x370, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 881, opaque = 0x371, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 882, opaque = 0x372, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 883, opaque = 0x373, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 884, opaque = 0x374, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 885, opaque = 0x375, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 886, opaque = 0x376, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 887, opaque = 0x377, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 888, opaque = 0x378, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 889, opaque = 0x379, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 890, opaque = 0x37A, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 891, opaque = 0x37B, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 892, opaque = 0x37C, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 893, opaque = 0x37D, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 894, opaque = 0x37E, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 895, opaque = 0x37F, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 896, opaque = 0x380, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 897, opaque = 0x381, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 898, opaque = 0x382, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 899, opaque = 0x383, type = add
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 900, opaque = 0x384, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 901, opaque = 0x385, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 902, opaque = 0x386, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 903, opaque = 0x387, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 904, opaque = 0x388, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 905, opaque = 0x389, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 906, opaque = 0x38A, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 907, opaque = 0x38B, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 908, opaque = 0x38C, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 909, opaque = 0x38D, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 910, opaque = 0x38E, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 911, opaque = 0x38F, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 912, opaque = 0x390, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 913, opaque = 0x391, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 914, opaque = 0x392, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 915, opaque = 0x393, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 916, opaque = 0x394, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 917, opaque = 0x395, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 918, opaque = 0x396, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 919, opaque = 0x397, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 920, opaque = 0x398, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 921, opaque = 0x399, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 922, opaque = 0x39A, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 923, opaque = 0x39B, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 924, opaque = 0x39C, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 925, opaque = 0x39D, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 926, opaque = 0x39E, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 927, opaque = 0x39F, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 928, opaque = 0x3A0, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 929, opaque = 0x3A1, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 930, opaque = 0x3A2, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 931, opaque = 0x3A3, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 932, opaque = 0x3A4, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 933, opaque = 0x3A5, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 934, opaque = 0x3A6, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 935, opaque = 0x3A7, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 936, opaque = 0x3A8, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 937, opaque = 0x3A9, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 938, opaque = 0x3AA, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 939, opaque = 0x3AB, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 940, opaque = 0x3AC, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 941, opaque = 0x3AD, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 942, opaque = 0x3AE, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 943, opaque = 0x3AF, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 944, opaque = 0x3B0, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 945, opaque = 0x3B1, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 946, opaque = 0x3B2, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 947, opaque = 0x3B3, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 948, opaque = 0x3B4, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 949, opaque = 0x3B5, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 950, opaque = 0x3B6, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 951, opaque = 0x3B7, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 952, opaque = 0x3B8, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 953, opaque = 0x3B9, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 954, opaque = 0x3BA, type = add
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 955, opaque = 0x3BB, type = add
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 956, opaque = 0x3BC, type = add
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 957, opaque = 0x3BD, type = add
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 958, opaque = 0x3BE, type = add
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 959, opaque = 0x3BF, type = add
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 960, opaque = 0x3C0, type = add
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 961, opaque = 0x3C1, type = add
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 962, opaque = 0x3C2, type = add
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 963, opaque = 0x3C3, type = add
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 964, opaque = 0x3C4, type = add
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 965, opaque = 0x3C5, type = add
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 966, opaque = 0x3C6, type = add
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 967, opaque = 0x3C7, type = add
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 968, opaque = 0x3C8, type = add
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 969, opaque = 0x3C9, type = add
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 970, opaque = 0x3CA, type = add
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 971, opaque = 0x3CB, type = add
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 972, opaque = 0x3CC, type = add
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 973, opaque = 0x3CD, type = add
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 974, opaque = 0x3CE, type = add
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 975, opaque = 0x3CF, type = add
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 976, opaque = 0x3D0, type = add
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 977, opaque = 0x3D1, type = add
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 978, opaque = 0x3D2, type = add
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 979, opaque = 0x3D3, type = add
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 980, opaque = 0x3D4, type = add
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 981, opaque = 0x3D5, type = add
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 982, opaque = 0x3D6, type = add
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 983, opaque = 0x3D7, type = add
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 984, opaque = 0x3D8, type = add
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 985, opaque = 0x3D9, type = add
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 986, opaque = 0x3DA, type = add
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 987, opaque = 0x3DB, type = add
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 988, opaque = 0x3DC, type = add
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 989, opaque = 0x3DD, type = add
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 990, opaque = 0x3DE, type = add
[ns_server:debug,2025-05-15T18:47:52.391Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 991, opaque = 0x3DF, type = add
[ns_server:debug,2025-05-15T18:47:52.391Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 992, opaque = 0x3E0, type = add
[ns_server:debug,2025-05-15T18:47:52.391Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 993, opaque = 0x3E1, type = add
[ns_server:debug,2025-05-15T18:47:52.391Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 994, opaque = 0x3E2, type = add
[ns_server:debug,2025-05-15T18:47:52.391Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 995, opaque = 0x3E3, type = add
[ns_server:debug,2025-05-15T18:47:52.391Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 996, opaque = 0x3E4, type = add
[ns_server:debug,2025-05-15T18:47:52.391Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 997, opaque = 0x3E5, type = add
[ns_server:debug,2025-05-15T18:47:52.393Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 998, opaque = 0x3E6, type = add
[ns_server:debug,2025-05-15T18:47:52.394Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 999, opaque = 0x3E7, type = add
[ns_server:debug,2025-05-15T18:47:52.394Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 1000, opaque = 0x3E8, type = add
[ns_server:debug,2025-05-15T18:47:52.394Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 1001, opaque = 0x3E9, type = add
[ns_server:debug,2025-05-15T18:47:52.394Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 1002, opaque = 0x3EA, type = add
[ns_server:debug,2025-05-15T18:47:52.394Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 1003, opaque = 0x3EB, type = add
[ns_server:debug,2025-05-15T18:47:52.394Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 1004, opaque = 0x3EC, type = add
[ns_server:debug,2025-05-15T18:47:52.394Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 1005, opaque = 0x3ED, type = add
[ns_server:debug,2025-05-15T18:47:52.395Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 1006, opaque = 0x3EE, type = add
[ns_server:debug,2025-05-15T18:47:52.395Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 1007, opaque = 0x3EF, type = add
[ns_server:debug,2025-05-15T18:47:52.395Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 1008, opaque = 0x3F0, type = add
[ns_server:debug,2025-05-15T18:47:52.395Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 1009, opaque = 0x3F1, type = add
[ns_server:debug,2025-05-15T18:47:52.395Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 1010, opaque = 0x3F2, type = add
[ns_server:debug,2025-05-15T18:47:52.395Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 1011, opaque = 0x3F3, type = add
[ns_server:debug,2025-05-15T18:47:52.395Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 1012, opaque = 0x3F4, type = add
[ns_server:debug,2025-05-15T18:47:52.395Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 1013, opaque = 0x3F5, type = add
[ns_server:debug,2025-05-15T18:47:52.395Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 1014, opaque = 0x3F6, type = add
[ns_server:debug,2025-05-15T18:47:52.396Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 1015, opaque = 0x3F7, type = add
[ns_server:debug,2025-05-15T18:47:52.396Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 1016, opaque = 0x3F8, type = add
[ns_server:debug,2025-05-15T18:47:52.396Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 1017, opaque = 0x3F9, type = add
[ns_server:debug,2025-05-15T18:47:52.396Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 1018, opaque = 0x3FA, type = add
[ns_server:debug,2025-05-15T18:47:52.396Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 1019, opaque = 0x3FB, type = add
[ns_server:debug,2025-05-15T18:47:52.396Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 1020, opaque = 0x3FC, type = add
[ns_server:debug,2025-05-15T18:47:52.396Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 1021, opaque = 0x3FD, type = add
[ns_server:debug,2025-05-15T18:47:52.396Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 1022, opaque = 0x3FE, type = add
[ns_server:debug,2025-05-15T18:47:52.396Z,ns_1@db2.lan:<0.4277.0>:dcp_commands:add_stream:83]Add stream for partition 1023, opaque = 0x3FF, type = add
[ns_server:debug,2025-05-15T18:47:52.396Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_call:206]Setup DCP streams:
Current []
Streams to open [853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023]
Streams to close []

[ns_server:debug,2025-05-15T18:47:52.396Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0x4000000
80 5E 00 16
00 00 00 00
00 00 00 1E
04 00 00 00
00 00 00 00
00 00 00 00
63 6F 6E 6E
65 63 74 69
6F 6E 5F 62
75 66 66 65
72 5F 73 69
7A 65 31 33
34 32 31 37
37 33 
[ns_server:debug,2025-05-15T18:47:52.396Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0xFE (cmd_get_error_map) vbucket = 0 opaque = 0x5000000
80 FE 00 00
00 00 00 00
00 00 00 02
05 00 00 00
00 00 00 00
00 00 00 00
00 00 
[ns_server:debug,2025-05-15T18:47:52.397Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0x4000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
04 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.397Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0xFE (cmd_get_error_map) vbucket = 0 opaque = 0x5000000 status = 0x1 (key_enoent)
81 FE 00 00
00 00 00 01
00 00 00 00
05 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.397Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xAE000000
80 5E 00 0B
00 00 00 00
00 00 00 0F
AE 00 00 00
00 00 00 00
00 00 00 00
65 6E 61 62
6C 65 5F 6E
6F 6F 70 74
72 75 65 
[ns_server:debug,2025-05-15T18:47:52.397Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xAF000000
80 5E 00 11
00 00 00 00
00 00 00 19
AF 00 00 00
00 00 00 00
00 00 00 00
73 65 74 5F
6E 6F 6F 70
5F 69 6E 74
65 72 76 61
6C 30 2E 31
30 30 30 30
30 
[ns_server:debug,2025-05-15T18:47:52.397Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xAE000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
AE 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.397Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xAF000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
AF 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.398Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB0000000
80 5E 00 0C
00 00 00 00
00 00 00 10
B0 00 00 00
00 00 00 00
00 00 00 00
73 65 74 5F
70 72 69 6F
72 69 74 79
68 69 67 68

[ns_server:debug,2025-05-15T18:47:52.398Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB1000000
80 5E 00 1F
00 00 00 00
00 00 00 23
B1 00 00 00
00 00 00 00
00 00 00 00
73 75 70 70
6F 72 74 73
5F 63 75 72
73 6F 72 5F
64 72 6F 70
70 69 6E 67
5F 76 75 6C
63 61 6E 74
72 75 65 
[ns_server:debug,2025-05-15T18:47:52.398Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB2000000
80 5E 00 11
00 00 00 00
00 00 00 15
B2 00 00 00
00 00 00 00
00 00 00 00
73 75 70 70
6F 72 74 73
5F 68 69 66
69 5F 4D 46
55 74 72 75
65 
[ns_server:debug,2025-05-15T18:47:52.398Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB3000000
80 5E 00 26
00 00 00 00
00 00 00 2A
B3 00 00 00
00 00 00 00
00 00 00 00
73 65 6E 64
5F 73 74 72
65 61 6D 5F
65 6E 64 5F
6F 6E 5F 63
6C 69 65 6E
74 5F 63 6C
6F 73 65 5F
73 74 72 65
61 6D 74 72
75 65 
[ns_server:debug,2025-05-15T18:47:52.398Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB4000000
80 5E 00 14
00 00 00 00
00 00 00 18
B4 00 00 00
00 00 00 00
00 00 00 00
65 6E 61 62
6C 65 5F 65
78 70 69 72
79 5F 6F 70
63 6F 64 65
74 72 75 65

[ns_server:debug,2025-05-15T18:47:52.398Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB5000000
80 5E 00 12
00 00 00 00
00 00 00 16
B5 00 00 00
00 00 00 00
00 00 00 00
65 6E 61 62
6C 65 5F 73
79 6E 63 5F
77 72 69 74
65 73 74 72
75 65 
[ns_server:debug,2025-05-15T18:47:52.398Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB0000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B0 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.399Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB1000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B1 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.399Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB2000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B2 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.399Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB3000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B3 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.399Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB4000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B4 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.399Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB5000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B5 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.399Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB6000000
80 5E 00 0D
00 00 00 00
00 00 00 19
B6 00 00 00
00 00 00 00
00 00 00 00
63 6F 6E 73
75 6D 65 72
5F 6E 61 6D
65 6E 73 5F
31 40 64 62
32 2E 6C 61
6E 
[ns_server:debug,2025-05-15T18:47:52.399Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB7000000
80 5E 00 1B
00 00 00 00
00 00 00 1F
B7 00 00 00
00 00 00 00
00 00 00 00
69 6E 63 6C
75 64 65 5F
64 65 6C 65
74 65 64 5F
75 73 65 72
5F 78 61 74
74 72 73 74
72 75 65 
[ns_server:debug,2025-05-15T18:47:52.400Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB6000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B6 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.400Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB7000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B7 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.402Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB8000000
80 5E 00 13
00 00 00 00
00 00 00 17
B8 00 00 00
00 00 00 00
00 00 00 00
76 37 5F 64
63 70 5F 73
74 61 74 75
73 5F 63 6F
64 65 73 74
72 75 65 
[ns_server:debug,2025-05-15T18:47:52.402Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB8000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B8 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.402Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB9000000
80 5E 00 19
00 00 00 00
00 00 00 1D
B9 00 00 00
00 00 00 00
00 00 00 00
66 6C 61 74
62 75 66 66
65 72 73 5F
73 79 73 74
65 6D 5F 65
76 65 6E 74
73 74 72 75
65 
[ns_server:debug,2025-05-15T18:47:52.403Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB9000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B9 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.403Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xBA000000
80 5E 00 0E
00 00 00 00
00 00 00 12
BA 00 00 00
00 00 00 00
00 00 00 00
63 68 61 6E
67 65 5F 73
74 72 65 61
6D 73 74 72
75 65 
[ns_server:debug,2025-05-15T18:47:52.404Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xBA000000 status = 0x83 (not_supported)
81 5E 00 00
00 00 00 83
00 00 00 00
BA 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.404Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 853 opaque = 0x1000000
80 53 00 00
30 00 03 55
00 00 00 30
01 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.404Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 854 opaque = 0x2000000
80 53 00 00
30 00 03 56
00 00 00 30
02 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.404Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 855 opaque = 0x3000000
80 53 00 00
30 00 03 57
00 00 00 30
03 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.405Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 856 opaque = 0x6000000
80 53 00 00
30 00 03 58
00 00 00 30
06 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.405Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 857 opaque = 0x7000000
80 53 00 00
30 00 03 59
00 00 00 30
07 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.405Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 858 opaque = 0x8000000
80 53 00 00
30 00 03 5A
00 00 00 30
08 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.405Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 859 opaque = 0x9000000
80 53 00 00
30 00 03 5B
00 00 00 30
09 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.405Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 860 opaque = 0xA000000
80 53 00 00
30 00 03 5C
00 00 00 30
0A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.405Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 861 opaque = 0xB000000
80 53 00 00
30 00 03 5D
00 00 00 30
0B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.405Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 862 opaque = 0xC000000
80 53 00 00
30 00 03 5E
00 00 00 30
0C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.405Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 863 opaque = 0xD000000
80 53 00 00
30 00 03 5F
00 00 00 30
0D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.406Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 864 opaque = 0xE000000
80 53 00 00
30 00 03 60
00 00 00 30
0E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.406Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 865 opaque = 0xF000000
80 53 00 00
30 00 03 61
00 00 00 30
0F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.406Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 866 opaque = 0x10000000
80 53 00 00
30 00 03 62
00 00 00 30
10 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.406Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 867 opaque = 0x11000000
80 53 00 00
30 00 03 63
00 00 00 30
11 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.406Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 868 opaque = 0x12000000
80 53 00 00
30 00 03 64
00 00 00 30
12 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.406Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 869 opaque = 0x13000000
80 53 00 00
30 00 03 65
00 00 00 30
13 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.406Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 870 opaque = 0x14000000
80 53 00 00
30 00 03 66
00 00 00 30
14 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.406Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 871 opaque = 0x15000000
80 53 00 00
30 00 03 67
00 00 00 30
15 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.406Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 872 opaque = 0x16000000
80 53 00 00
30 00 03 68
00 00 00 30
16 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.407Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 873 opaque = 0x17000000
80 53 00 00
30 00 03 69
00 00 00 30
17 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.407Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 874 opaque = 0x18000000
80 53 00 00
30 00 03 6A
00 00 00 30
18 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.407Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 875 opaque = 0x19000000
80 53 00 00
30 00 03 6B
00 00 00 30
19 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.407Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 876 opaque = 0x1A000000
80 53 00 00
30 00 03 6C
00 00 00 30
1A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.407Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 877 opaque = 0x1B000000
80 53 00 00
30 00 03 6D
00 00 00 30
1B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.407Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 878 opaque = 0x1C000000
80 53 00 00
30 00 03 6E
00 00 00 30
1C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.407Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 879 opaque = 0x1D000000
80 53 00 00
30 00 03 6F
00 00 00 30
1D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.407Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 880 opaque = 0x1E000000
80 53 00 00
30 00 03 70
00 00 00 30
1E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.408Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 881 opaque = 0x1F000000
80 53 00 00
30 00 03 71
00 00 00 30
1F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.408Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 882 opaque = 0x20000000
80 53 00 00
30 00 03 72
00 00 00 30
20 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.408Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 883 opaque = 0x21000000
80 53 00 00
30 00 03 73
00 00 00 30
21 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.408Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 884 opaque = 0x22000000
80 53 00 00
30 00 03 74
00 00 00 30
22 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.408Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 885 opaque = 0x23000000
80 53 00 00
30 00 03 75
00 00 00 30
23 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.408Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 886 opaque = 0x24000000
80 53 00 00
30 00 03 76
00 00 00 30
24 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.408Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 887 opaque = 0x25000000
80 53 00 00
30 00 03 77
00 00 00 30
25 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.408Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 888 opaque = 0x26000000
80 53 00 00
30 00 03 78
00 00 00 30
26 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.409Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 889 opaque = 0x27000000
80 53 00 00
30 00 03 79
00 00 00 30
27 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.409Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 890 opaque = 0x28000000
80 53 00 00
30 00 03 7A
00 00 00 30
28 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.409Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 891 opaque = 0x29000000
80 53 00 00
30 00 03 7B
00 00 00 30
29 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.409Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 892 opaque = 0x2A000000
80 53 00 00
30 00 03 7C
00 00 00 30
2A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.409Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 893 opaque = 0x2B000000
80 53 00 00
30 00 03 7D
00 00 00 30
2B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.409Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 894 opaque = 0x2C000000
80 53 00 00
30 00 03 7E
00 00 00 30
2C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.409Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 895 opaque = 0x2D000000
80 53 00 00
30 00 03 7F
00 00 00 30
2D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.409Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 896 opaque = 0x2E000000
80 53 00 00
30 00 03 80
00 00 00 30
2E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.409Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 897 opaque = 0x2F000000
80 53 00 00
30 00 03 81
00 00 00 30
2F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.410Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 898 opaque = 0x30000000
80 53 00 00
30 00 03 82
00 00 00 30
30 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.410Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 899 opaque = 0x31000000
80 53 00 00
30 00 03 83
00 00 00 30
31 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.410Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 900 opaque = 0x32000000
80 53 00 00
30 00 03 84
00 00 00 30
32 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.410Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 901 opaque = 0x33000000
80 53 00 00
30 00 03 85
00 00 00 30
33 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.410Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 902 opaque = 0x34000000
80 53 00 00
30 00 03 86
00 00 00 30
34 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.410Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 903 opaque = 0x35000000
80 53 00 00
30 00 03 87
00 00 00 30
35 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.410Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 904 opaque = 0x36000000
80 53 00 00
30 00 03 88
00 00 00 30
36 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.411Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 905 opaque = 0x37000000
80 53 00 00
30 00 03 89
00 00 00 30
37 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.411Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 906 opaque = 0x38000000
80 53 00 00
30 00 03 8A
00 00 00 30
38 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.411Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 907 opaque = 0x39000000
80 53 00 00
30 00 03 8B
00 00 00 30
39 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.411Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 908 opaque = 0x3A000000
80 53 00 00
30 00 03 8C
00 00 00 30
3A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.411Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 909 opaque = 0x3B000000
80 53 00 00
30 00 03 8D
00 00 00 30
3B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.412Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 910 opaque = 0x3C000000
80 53 00 00
30 00 03 8E
00 00 00 30
3C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.412Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 911 opaque = 0x3D000000
80 53 00 00
30 00 03 8F
00 00 00 30
3D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.412Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 912 opaque = 0x3E000000
80 53 00 00
30 00 03 90
00 00 00 30
3E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.412Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 913 opaque = 0x3F000000
80 53 00 00
30 00 03 91
00 00 00 30
3F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.412Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 914 opaque = 0x40000000
80 53 00 00
30 00 03 92
00 00 00 30
40 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.412Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 915 opaque = 0x41000000
80 53 00 00
30 00 03 93
00 00 00 30
41 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.412Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 916 opaque = 0x42000000
80 53 00 00
30 00 03 94
00 00 00 30
42 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.412Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 917 opaque = 0x43000000
80 53 00 00
30 00 03 95
00 00 00 30
43 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.412Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 918 opaque = 0x44000000
80 53 00 00
30 00 03 96
00 00 00 30
44 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.413Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 919 opaque = 0x45000000
80 53 00 00
30 00 03 97
00 00 00 30
45 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.413Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 920 opaque = 0x46000000
80 53 00 00
30 00 03 98
00 00 00 30
46 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.413Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 921 opaque = 0x47000000
80 53 00 00
30 00 03 99
00 00 00 30
47 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.413Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 922 opaque = 0x48000000
80 53 00 00
30 00 03 9A
00 00 00 30
48 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.413Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 923 opaque = 0x49000000
80 53 00 00
30 00 03 9B
00 00 00 30
49 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.413Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 924 opaque = 0x4A000000
80 53 00 00
30 00 03 9C
00 00 00 30
4A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.413Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 925 opaque = 0x4B000000
80 53 00 00
30 00 03 9D
00 00 00 30
4B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.413Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 926 opaque = 0x4C000000
80 53 00 00
30 00 03 9E
00 00 00 30
4C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.413Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 927 opaque = 0x4D000000
80 53 00 00
30 00 03 9F
00 00 00 30
4D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.413Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 928 opaque = 0x4E000000
80 53 00 00
30 00 03 A0
00 00 00 30
4E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.414Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 929 opaque = 0x4F000000
80 53 00 00
30 00 03 A1
00 00 00 30
4F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.414Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 930 opaque = 0x50000000
80 53 00 00
30 00 03 A2
00 00 00 30
50 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.414Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 931 opaque = 0x51000000
80 53 00 00
30 00 03 A3
00 00 00 30
51 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.414Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 932 opaque = 0x52000000
80 53 00 00
30 00 03 A4
00 00 00 30
52 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.414Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 933 opaque = 0x53000000
80 53 00 00
30 00 03 A5
00 00 00 30
53 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.414Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 934 opaque = 0x54000000
80 53 00 00
30 00 03 A6
00 00 00 30
54 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.414Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 935 opaque = 0x55000000
80 53 00 00
30 00 03 A7
00 00 00 30
55 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.414Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 936 opaque = 0x56000000
80 53 00 00
30 00 03 A8
00 00 00 30
56 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.415Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 937 opaque = 0x57000000
80 53 00 00
30 00 03 A9
00 00 00 30
57 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.415Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 938 opaque = 0x58000000
80 53 00 00
30 00 03 AA
00 00 00 30
58 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.415Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 939 opaque = 0x59000000
80 53 00 00
30 00 03 AB
00 00 00 30
59 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.415Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 940 opaque = 0x5A000000
80 53 00 00
30 00 03 AC
00 00 00 30
5A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.415Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 941 opaque = 0x5B000000
80 53 00 00
30 00 03 AD
00 00 00 30
5B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.416Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 942 opaque = 0x5C000000
80 53 00 00
30 00 03 AE
00 00 00 30
5C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.416Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 943 opaque = 0x5D000000
80 53 00 00
30 00 03 AF
00 00 00 30
5D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.416Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 944 opaque = 0x5E000000
80 53 00 00
30 00 03 B0
00 00 00 30
5E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.416Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 945 opaque = 0x5F000000
80 53 00 00
30 00 03 B1
00 00 00 30
5F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.416Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 946 opaque = 0x60000000
80 53 00 00
30 00 03 B2
00 00 00 30
60 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.416Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 947 opaque = 0x61000000
80 53 00 00
30 00 03 B3
00 00 00 30
61 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.416Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 948 opaque = 0x62000000
80 53 00 00
30 00 03 B4
00 00 00 30
62 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.417Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 949 opaque = 0x63000000
80 53 00 00
30 00 03 B5
00 00 00 30
63 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.417Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 950 opaque = 0x64000000
80 53 00 00
30 00 03 B6
00 00 00 30
64 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.417Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 951 opaque = 0x65000000
80 53 00 00
30 00 03 B7
00 00 00 30
65 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.417Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 952 opaque = 0x66000000
80 53 00 00
30 00 03 B8
00 00 00 30
66 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.417Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 953 opaque = 0x67000000
80 53 00 00
30 00 03 B9
00 00 00 30
67 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.417Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 954 opaque = 0x68000000
80 53 00 00
30 00 03 BA
00 00 00 30
68 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.417Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 955 opaque = 0x69000000
80 53 00 00
30 00 03 BB
00 00 00 30
69 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.417Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 956 opaque = 0x6A000000
80 53 00 00
30 00 03 BC
00 00 00 30
6A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.418Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 957 opaque = 0x6B000000
80 53 00 00
30 00 03 BD
00 00 00 30
6B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.418Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 958 opaque = 0x6C000000
80 53 00 00
30 00 03 BE
00 00 00 30
6C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.418Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 959 opaque = 0x6D000000
80 53 00 00
30 00 03 BF
00 00 00 30
6D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.418Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 960 opaque = 0x6E000000
80 53 00 00
30 00 03 C0
00 00 00 30
6E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.418Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 961 opaque = 0x6F000000
80 53 00 00
30 00 03 C1
00 00 00 30
6F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.418Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 962 opaque = 0x70000000
80 53 00 00
30 00 03 C2
00 00 00 30
70 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.418Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 963 opaque = 0x71000000
80 53 00 00
30 00 03 C3
00 00 00 30
71 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.418Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 964 opaque = 0x72000000
80 53 00 00
30 00 03 C4
00 00 00 30
72 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.418Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 965 opaque = 0x73000000
80 53 00 00
30 00 03 C5
00 00 00 30
73 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.419Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 966 opaque = 0x74000000
80 53 00 00
30 00 03 C6
00 00 00 30
74 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.419Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 967 opaque = 0x75000000
80 53 00 00
30 00 03 C7
00 00 00 30
75 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.419Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 968 opaque = 0x76000000
80 53 00 00
30 00 03 C8
00 00 00 30
76 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.419Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 969 opaque = 0x77000000
80 53 00 00
30 00 03 C9
00 00 00 30
77 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.419Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 970 opaque = 0x78000000
80 53 00 00
30 00 03 CA
00 00 00 30
78 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.419Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 971 opaque = 0x79000000
80 53 00 00
30 00 03 CB
00 00 00 30
79 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.419Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 972 opaque = 0x7A000000
80 53 00 00
30 00 03 CC
00 00 00 30
7A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.419Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 973 opaque = 0x7B000000
80 53 00 00
30 00 03 CD
00 00 00 30
7B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.420Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 974 opaque = 0x7C000000
80 53 00 00
30 00 03 CE
00 00 00 30
7C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.420Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 975 opaque = 0x7D000000
80 53 00 00
30 00 03 CF
00 00 00 30
7D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.420Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 976 opaque = 0x7E000000
80 53 00 00
30 00 03 D0
00 00 00 30
7E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.420Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 977 opaque = 0x7F000000
80 53 00 00
30 00 03 D1
00 00 00 30
7F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.420Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 978 opaque = 0x80000000
80 53 00 00
30 00 03 D2
00 00 00 30
80 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.420Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 979 opaque = 0x81000000
80 53 00 00
30 00 03 D3
00 00 00 30
81 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.420Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 980 opaque = 0x82000000
80 53 00 00
30 00 03 D4
00 00 00 30
82 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.420Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 981 opaque = 0x83000000
80 53 00 00
30 00 03 D5
00 00 00 30
83 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.421Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 982 opaque = 0x84000000
80 53 00 00
30 00 03 D6
00 00 00 30
84 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.421Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 983 opaque = 0x85000000
80 53 00 00
30 00 03 D7
00 00 00 30
85 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.421Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 984 opaque = 0x86000000
80 53 00 00
30 00 03 D8
00 00 00 30
86 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.421Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 985 opaque = 0x87000000
80 53 00 00
30 00 03 D9
00 00 00 30
87 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.421Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 986 opaque = 0x88000000
80 53 00 00
30 00 03 DA
00 00 00 30
88 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.421Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 987 opaque = 0x89000000
80 53 00 00
30 00 03 DB
00 00 00 30
89 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.422Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 988 opaque = 0x8A000000
80 53 00 00
30 00 03 DC
00 00 00 30
8A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.422Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 989 opaque = 0x8B000000
80 53 00 00
30 00 03 DD
00 00 00 30
8B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.422Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 990 opaque = 0x8C000000
80 53 00 00
30 00 03 DE
00 00 00 30
8C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.422Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 991 opaque = 0x8D000000
80 53 00 00
30 00 03 DF
00 00 00 30
8D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.422Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 992 opaque = 0x8E000000
80 53 00 00
30 00 03 E0
00 00 00 30
8E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.422Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 993 opaque = 0x8F000000
80 53 00 00
30 00 03 E1
00 00 00 30
8F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.422Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 994 opaque = 0x90000000
80 53 00 00
30 00 03 E2
00 00 00 30
90 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.422Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 995 opaque = 0x91000000
80 53 00 00
30 00 03 E3
00 00 00 30
91 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.422Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 996 opaque = 0x92000000
80 53 00 00
30 00 03 E4
00 00 00 30
92 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.422Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 997 opaque = 0x93000000
80 53 00 00
30 00 03 E5
00 00 00 30
93 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.423Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 998 opaque = 0x94000000
80 53 00 00
30 00 03 E6
00 00 00 30
94 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.423Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 999 opaque = 0x95000000
80 53 00 00
30 00 03 E7
00 00 00 30
95 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.423Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 1000 opaque = 0x96000000
80 53 00 00
30 00 03 E8
00 00 00 30
96 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.423Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 1001 opaque = 0x97000000
80 53 00 00
30 00 03 E9
00 00 00 30
97 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.423Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 1002 opaque = 0x98000000
80 53 00 00
30 00 03 EA
00 00 00 30
98 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.423Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 1003 opaque = 0x99000000
80 53 00 00
30 00 03 EB
00 00 00 30
99 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.423Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 1004 opaque = 0x9A000000
80 53 00 00
30 00 03 EC
00 00 00 30
9A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.424Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 1005 opaque = 0x9B000000
80 53 00 00
30 00 03 ED
00 00 00 30
9B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.424Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 1006 opaque = 0x9C000000
80 53 00 00
30 00 03 EE
00 00 00 30
9C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.424Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 1007 opaque = 0x9D000000
80 53 00 00
30 00 03 EF
00 00 00 30
9D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.424Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 1008 opaque = 0x9E000000
80 53 00 00
30 00 03 F0
00 00 00 30
9E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.424Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 1009 opaque = 0x9F000000
80 53 00 00
30 00 03 F1
00 00 00 30
9F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.424Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 1010 opaque = 0xA0000000
80 53 00 00
30 00 03 F2
00 00 00 30
A0 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.424Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 1011 opaque = 0xA1000000
80 53 00 00
30 00 03 F3
00 00 00 30
A1 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.424Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 1012 opaque = 0xA2000000
80 53 00 00
30 00 03 F4
00 00 00 30
A2 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.425Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 1013 opaque = 0xA3000000
80 53 00 00
30 00 03 F5
00 00 00 30
A3 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.425Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 1014 opaque = 0xA4000000
80 53 00 00
30 00 03 F6
00 00 00 30
A4 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.425Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 1015 opaque = 0xA5000000
80 53 00 00
30 00 03 F7
00 00 00 30
A5 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.425Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 1016 opaque = 0xA6000000
80 53 00 00
30 00 03 F8
00 00 00 30
A6 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.425Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 1017 opaque = 0xA7000000
80 53 00 00
30 00 03 F9
00 00 00 30
A7 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.425Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 1018 opaque = 0xA8000000
80 53 00 00
30 00 03 FA
00 00 00 30
A8 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.425Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 1019 opaque = 0xA9000000
80 53 00 00
30 00 03 FB
00 00 00 30
A9 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.425Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 1020 opaque = 0xAA000000
80 53 00 00
30 00 03 FC
00 00 00 30
AA 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.425Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 1021 opaque = 0xAB000000
80 53 00 00
30 00 03 FD
00 00 00 30
AB 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.425Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 1022 opaque = 0xAC000000
80 53 00 00
30 00 03 FE
00 00 00 30
AC 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.426Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 1023 opaque = 0xAD000000
80 53 00 00
30 00 03 FF
00 00 00 30
AD 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.429Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x1000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
01 00 00 00
00 00 00 00
00 00 00 00
00 00 CE EA
FD 07 92 69
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.429Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x2000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
02 00 00 00
00 00 00 00
00 00 00 00
00 00 B1 AF
38 60 65 63
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.429Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x3000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
03 00 00 00
00 00 00 00
00 00 00 00
00 00 04 D7
51 61 19 73
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.429Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x6000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
06 00 00 00
00 00 00 00
00 00 00 00
00 00 27 60
33 F1 3F 75
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.429Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x7000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
07 00 00 00
00 00 00 00
00 00 00 00
00 00 C2 03
16 C8 06 85
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.429Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x8000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
08 00 00 00
00 00 00 00
00 00 00 00
00 00 E2 69
9B 65 00 E8
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.429Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x9000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
09 00 00 00
00 00 00 00
00 00 00 00
00 00 C3 6C
AB 6B 44 ED
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.429Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
0A 00 00 00
00 00 00 00
00 00 00 00
00 00 72 D8
64 1A 38 C2
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.429Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xB000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
0B 00 00 00
00 00 00 00
00 00 00 00
00 00 44 F7
C8 0C 54 EC
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.429Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xC000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
0C 00 00 00
00 00 00 00
00 00 00 00
00 00 18 7D
9A B7 75 7E
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.429Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xD000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
0D 00 00 00
00 00 00 00
00 00 00 00
00 00 C5 B3
EC F0 5E C6
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.429Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xE000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
0E 00 00 00
00 00 00 00
00 00 00 00
00 00 9C 2E
5B CC B7 AC
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.430Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xF000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
0F 00 00 00
00 00 00 00
00 00 00 00
00 00 0F 63
EF 6B 27 B3
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.430Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x10000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
10 00 00 00
00 00 00 00
00 00 00 00
00 00 62 AE
FF D0 EA 70
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.430Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x11000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
11 00 00 00
00 00 00 00
00 00 00 00
00 00 0E 7C
2A E1 CE 45
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.430Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x12000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
12 00 00 00
00 00 00 00
00 00 00 00
00 00 5E 42
B1 11 72 A6
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.430Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x13000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
13 00 00 00
00 00 00 00
00 00 00 00
00 00 E8 D9
C1 05 58 25
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.430Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x14000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
14 00 00 00
00 00 00 00
00 00 00 00
00 00 AE C9
79 45 D5 DC
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.430Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x15000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
15 00 00 00
00 00 00 00
00 00 00 00
00 00 0B FB
A0 4B FE 35
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.430Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x16000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
16 00 00 00
00 00 00 00
00 00 00 00
00 00 88 8E
EB 45 7A 2E
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.430Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x17000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
17 00 00 00
00 00 00 00
00 00 00 00
00 00 16 DE
DA 5D BA 69
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.430Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x18000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
18 00 00 00
00 00 00 00
00 00 00 00
00 00 36 E6
B8 1C 68 C2
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.430Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x19000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
19 00 00 00
00 00 00 00
00 00 00 00
00 00 F0 F9
02 0B 20 CD
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.430Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x1A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
1A 00 00 00
00 00 00 00
00 00 00 00
00 00 89 C0
76 20 14 4F
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.430Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x1B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
1B 00 00 00
00 00 00 00
00 00 00 00
00 00 CA F9
66 A2 19 AB
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.430Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x1C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
1C 00 00 00
00 00 00 00
00 00 00 00
00 00 B8 6B
EC F0 07 90
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.430Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x1D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
1D 00 00 00
00 00 00 00
00 00 00 00
00 00 A0 BD
3F A7 DE 5E
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.431Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x1E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
1E 00 00 00
00 00 00 00
00 00 00 00
00 00 60 C2
48 BB 70 38
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.431Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x1F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
1F 00 00 00
00 00 00 00
00 00 00 00
00 00 8E 81
DD 0E 1A 74
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.431Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x20000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
20 00 00 00
00 00 00 00
00 00 00 00
00 00 AA DF
97 91 1E 4E
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.431Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x21000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
21 00 00 00
00 00 00 00
00 00 00 00
00 00 68 AD
79 99 E7 C0
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.431Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x22000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
22 00 00 00
00 00 00 00
00 00 00 00
00 00 6D 5E
8B 1F 50 A5
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.431Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x23000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
23 00 00 00
00 00 00 00
00 00 00 00
00 00 96 AE
F5 BC 43 2A
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.431Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x24000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
24 00 00 00
00 00 00 00
00 00 00 00
00 00 42 12
AC B2 B2 C5
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.431Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x25000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
25 00 00 00
00 00 00 00
00 00 00 00
00 00 EE E9
88 1F 27 F5
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.431Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x26000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
26 00 00 00
00 00 00 00
00 00 00 00
00 00 36 0E
4F 9D 81 8F
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.431Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x27000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
27 00 00 00
00 00 00 00
00 00 00 00
00 00 42 43
73 D1 D5 77
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.431Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x28000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
28 00 00 00
00 00 00 00
00 00 00 00
00 00 05 BB
F7 CC D5 B3
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.431Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x29000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
29 00 00 00
00 00 00 00
00 00 00 00
00 00 CD 0F
04 0E D2 46
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.431Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x2A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
2A 00 00 00
00 00 00 00
00 00 00 00
00 00 5A D2
64 29 B0 9D
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.431Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x2B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
2B 00 00 00
00 00 00 00
00 00 00 00
00 00 FD 2E
55 73 7C 9B
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.431Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x2C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
2C 00 00 00
00 00 00 00
00 00 00 00
00 00 A2 3F
17 0D FB 5A
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.432Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x2D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
2D 00 00 00
00 00 00 00
00 00 00 00
00 00 1A AF
7C 62 8C 88
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.432Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x2E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
2E 00 00 00
00 00 00 00
00 00 00 00
00 00 3E FE
8C 92 94 99
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.432Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x2F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
2F 00 00 00
00 00 00 00
00 00 00 00
00 00 E6 8E
C0 64 3F 87
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.432Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x30000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
30 00 00 00
00 00 00 00
00 00 00 00
00 00 39 0A
75 54 A3 F4
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.432Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x31000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
31 00 00 00
00 00 00 00
00 00 00 00
00 00 96 69
8B 66 4C 21
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.432Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x32000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
32 00 00 00
00 00 00 00
00 00 00 00
00 00 D1 DE
8C FB E7 99
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.432Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x33000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
33 00 00 00
00 00 00 00
00 00 00 00
00 00 A3 44
20 4C 78 5F
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.432Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x34000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
34 00 00 00
00 00 00 00
00 00 00 00
00 00 B7 B6
94 C9 DE E5
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.432Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x35000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
35 00 00 00
00 00 00 00
00 00 00 00
00 00 8E 79
25 D2 A4 01
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.432Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x36000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
36 00 00 00
00 00 00 00
00 00 00 00
00 00 81 C4
1B 85 80 6C
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.432Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x37000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
37 00 00 00
00 00 00 00
00 00 00 00
00 00 71 A9
9D 66 2D 68
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.432Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x38000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
38 00 00 00
00 00 00 00
00 00 00 00
00 00 96 6A
EB 45 39 9D
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.433Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x39000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
39 00 00 00
00 00 00 00
00 00 00 00
00 00 67 AA
65 A6 88 EC
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.433Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x3A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
3A 00 00 00
00 00 00 00
00 00 00 00
00 00 72 2D
A5 30 09 2E
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.433Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x3B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
3B 00 00 00
00 00 00 00
00 00 00 00
00 00 F8 D2
79 30 5E 11
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.433Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x3C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
3C 00 00 00
00 00 00 00
00 00 00 00
00 00 42 4E
E3 8B 28 93
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.433Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x3D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
3D 00 00 00
00 00 00 00
00 00 00 00
00 00 DF D8
79 B5 3F 44
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.433Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x3E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
3E 00 00 00
00 00 00 00
00 00 00 00
00 00 84 83
49 DA 60 9C
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.433Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x3F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
3F 00 00 00
00 00 00 00
00 00 00 00
00 00 60 19
BB 84 9E 9A
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.433Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x40000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
40 00 00 00
00 00 00 00
00 00 00 00
00 00 5B CC
A8 A9 2B 08
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.433Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x41000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
41 00 00 00
00 00 00 00
00 00 00 00
00 00 BE 81
CB 82 E0 F5
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.433Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x42000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
42 00 00 00
00 00 00 00
00 00 00 00
00 00 C2 C4
F4 F9 68 D6
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.433Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x43000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
43 00 00 00
00 00 00 00
00 00 00 00
00 00 BD 63
D5 2A 87 44
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.433Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x44000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
44 00 00 00
00 00 00 00
00 00 00 00
00 00 B4 F2
5E 95 3A 37
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.433Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x45000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
45 00 00 00
00 00 00 00
00 00 00 00
00 00 A6 B9
11 64 6E 27
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.433Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x46000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
46 00 00 00
00 00 00 00
00 00 00 00
00 00 DC AF
2B 8C FF 8E
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.433Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x47000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
47 00 00 00
00 00 00 00
00 00 00 00
00 00 83 84
61 47 56 96
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.434Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x48000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
48 00 00 00
00 00 00 00
00 00 00 00
00 00 6C 9E
B4 79 6E 50
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.434Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x49000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
49 00 00 00
00 00 00 00
00 00 00 00
00 00 A9 66
86 E5 32 26
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.434Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x4A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
4A 00 00 00
00 00 00 00
00 00 00 00
00 00 2C 4A
40 C9 24 78
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.434Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x4B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
4B 00 00 00
00 00 00 00
00 00 00 00
00 00 C2 41
E5 39 C5 70
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.434Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x4C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
4C 00 00 00
00 00 00 00
00 00 00 00
00 00 88 2F
29 40 BE 3B
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.434Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x4D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
4D 00 00 00
00 00 00 00
00 00 00 00
00 00 87 D1
5C B7 F5 B3
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.434Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x4E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
4E 00 00 00
00 00 00 00
00 00 00 00
00 00 A4 4E
FB 06 52 35
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.434Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x4F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
4F 00 00 00
00 00 00 00
00 00 00 00
00 00 51 B0
86 4E 32 BA
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.434Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x50000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
50 00 00 00
00 00 00 00
00 00 00 00
00 00 6B CA
C9 8C DA 09
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.434Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x51000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
51 00 00 00
00 00 00 00
00 00 00 00
00 00 FE 7A
39 E0 63 26
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.434Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x52000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
52 00 00 00
00 00 00 00
00 00 00 00
00 00 FA A0
62 2B 4E 09
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.434Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x53000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
53 00 00 00
00 00 00 00
00 00 00 00
00 00 3A 37
4B 26 57 F6
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.434Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x54000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
54 00 00 00
00 00 00 00
00 00 00 00
00 00 31 2F
C2 BA 2A BD
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.434Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x55000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
55 00 00 00
00 00 00 00
00 00 00 00
00 00 55 F9
B5 93 6F FE
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.434Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x56000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
56 00 00 00
00 00 00 00
00 00 00 00
00 00 DD 0F
E4 31 4A 8F
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.434Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x57000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
57 00 00 00
00 00 00 00
00 00 00 00
00 00 CE 6B
6C 04 AE 4D
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.435Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x58000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
58 00 00 00
00 00 00 00
00 00 00 00
00 00 B9 2B
B5 B9 58 3C
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.435Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x59000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
59 00 00 00
00 00 00 00
00 00 00 00
00 00 DA CC
43 DC B1 2C
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.435Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x5A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
5A 00 00 00
00 00 00 00
00 00 00 00
00 00 96 D5
4E A3 FE E4
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.435Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x5B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
5B 00 00 00
00 00 00 00
00 00 00 00
00 00 CE 42
13 C5 50 D1
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.435Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x5C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
5C 00 00 00
00 00 00 00
00 00 00 00
00 00 49 56
8D F4 C8 A0
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.435Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x355 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 55
00 00 00 00
00 00 00 00
00 00 00 01

[ns_server:debug,2025-05-15T18:47:52.435Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x5D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
5D 00 00 00
00 00 00 00
00 00 00 00
00 00 B5 1F
84 84 65 F8
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.435Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 853, stream opaque = 0x1000000
[ns_server:debug,2025-05-15T18:47:52.435Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x356 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 56
00 00 00 00
00 00 00 00
00 00 00 02

[ns_server:debug,2025-05-15T18:47:52.435Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x5E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
5E 00 00 00
00 00 00 00
00 00 00 00
00 00 99 C4
6C E3 AA EF
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.435Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 854, stream opaque = 0x2000000
[ns_server:debug,2025-05-15T18:47:52.435Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x5F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
5F 00 00 00
00 00 00 00
00 00 00 00
00 00 CF CC
50 1F C1 30
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.435Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x60000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
60 00 00 00
00 00 00 00
00 00 00 00
00 00 EA 7D
B8 D4 C0 1F
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.436Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x357 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 57
00 00 00 00
00 00 00 00
00 00 00 03

[rebalance:debug,2025-05-15T18:47:52.436Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 855, stream opaque = 0x3000000
[ns_server:debug,2025-05-15T18:47:52.436Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x358 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 58
00 00 00 00
00 00 00 00
00 00 00 06

[rebalance:debug,2025-05-15T18:47:52.436Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 856, stream opaque = 0x6000000
[ns_server:debug,2025-05-15T18:47:52.436Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x61000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
61 00 00 00
00 00 00 00
00 00 00 00
00 00 80 D6
AD EA DF 5C
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.436Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x62000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
62 00 00 00
00 00 00 00
00 00 00 00
00 00 E5 7C
54 DC E8 47
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.436Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x359 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 59
00 00 00 00
00 00 00 00
00 00 00 07

[ns_server:debug,2025-05-15T18:47:52.436Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x63000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
63 00 00 00
00 00 00 00
00 00 00 00
00 00 60 E8
D6 A2 E1 67
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.436Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 857, stream opaque = 0x7000000
[ns_server:debug,2025-05-15T18:47:52.436Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x64000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
64 00 00 00
00 00 00 00
00 00 00 00
00 00 91 57
F3 5C D4 BE
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.436Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x35A status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 5A
00 00 00 00
00 00 00 00
00 00 00 08

[ns_server:debug,2025-05-15T18:47:52.436Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x65000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
65 00 00 00
00 00 00 00
00 00 00 00
00 00 5E BC
97 00 91 4F
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.436Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 858, stream opaque = 0x8000000
[ns_server:debug,2025-05-15T18:47:52.436Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x66000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
66 00 00 00
00 00 00 00
00 00 00 00
00 00 9A 6F
81 D0 F5 A1
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.437Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x35B status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 5B
00 00 00 00
00 00 00 00
00 00 00 09

[ns_server:debug,2025-05-15T18:47:52.437Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x67000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
67 00 00 00
00 00 00 00
00 00 00 00
00 00 18 45
93 8C 73 BA
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.437Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 859, stream opaque = 0x9000000
[ns_server:debug,2025-05-15T18:47:52.437Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x68000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
68 00 00 00
00 00 00 00
00 00 00 00
00 00 4C 7F
2C 9B 2B D6
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.437Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x69000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
69 00 00 00
00 00 00 00
00 00 00 00
00 00 BC E3
49 D5 42 4F
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.437Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x35C status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 5C
00 00 00 00
00 00 00 00
00 00 00 0A

[ns_server:debug,2025-05-15T18:47:52.437Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x6A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
6A 00 00 00
00 00 00 00
00 00 00 00
00 00 3B 72
08 A6 E7 C3
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.437Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 860, stream opaque = 0xA000000
[ns_server:debug,2025-05-15T18:47:52.437Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x6B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
6B 00 00 00
00 00 00 00
00 00 00 00
00 00 78 9E
30 91 5A BB
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.437Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x35D status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 5D
00 00 00 00
00 00 00 00
00 00 00 0B

[ns_server:debug,2025-05-15T18:47:52.437Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x6C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
6C 00 00 00
00 00 00 00
00 00 00 00
00 00 11 87
5E 08 09 AC
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.437Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 861, stream opaque = 0xB000000
[ns_server:debug,2025-05-15T18:47:52.438Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x6D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
6D 00 00 00
00 00 00 00
00 00 00 00
00 00 6E 0B
94 DE D4 A5
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.438Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x6E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
6E 00 00 00
00 00 00 00
00 00 00 00
00 00 9C E1
FC 17 A5 87
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.438Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x35E status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 5E
00 00 00 00
00 00 00 00
00 00 00 0C

[ns_server:debug,2025-05-15T18:47:52.438Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x6F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
6F 00 00 00
00 00 00 00
00 00 00 00
00 00 75 BE
5C F6 1D 9A
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.438Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 862, stream opaque = 0xC000000
[ns_server:debug,2025-05-15T18:47:52.438Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x35F status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 5F
00 00 00 00
00 00 00 00
00 00 00 0D

[rebalance:debug,2025-05-15T18:47:52.438Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 863, stream opaque = 0xD000000
[ns_server:debug,2025-05-15T18:47:52.438Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x360 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 60
00 00 00 00
00 00 00 00
00 00 00 0E

[rebalance:debug,2025-05-15T18:47:52.439Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 864, stream opaque = 0xE000000
[ns_server:debug,2025-05-15T18:47:52.439Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x361 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 61
00 00 00 00
00 00 00 00
00 00 00 0F

[rebalance:debug,2025-05-15T18:47:52.439Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 865, stream opaque = 0xF000000
[ns_server:debug,2025-05-15T18:47:52.439Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x362 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 62
00 00 00 00
00 00 00 00
00 00 00 10

[ns_server:debug,2025-05-15T18:47:52.438Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x70000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
70 00 00 00
00 00 00 00
00 00 00 00
00 00 97 AF
D9 7A 31 83
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.439Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 866, stream opaque = 0x10000000
[ns_server:debug,2025-05-15T18:47:52.439Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x363 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 63
00 00 00 00
00 00 00 00
00 00 00 11

[rebalance:debug,2025-05-15T18:47:52.439Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 867, stream opaque = 0x11000000
[ns_server:debug,2025-05-15T18:47:52.439Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x364 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 64
00 00 00 00
00 00 00 00
00 00 00 12

[rebalance:debug,2025-05-15T18:47:52.439Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 868, stream opaque = 0x12000000
[ns_server:debug,2025-05-15T18:47:52.439Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x365 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 65
00 00 00 00
00 00 00 00
00 00 00 13

[rebalance:debug,2025-05-15T18:47:52.439Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 869, stream opaque = 0x13000000
[ns_server:debug,2025-05-15T18:47:52.439Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x366 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 66
00 00 00 00
00 00 00 00
00 00 00 14

[rebalance:debug,2025-05-15T18:47:52.439Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 870, stream opaque = 0x14000000
[ns_server:debug,2025-05-15T18:47:52.439Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x71000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
71 00 00 00
00 00 00 00
00 00 00 00
00 00 C2 D6
B7 E3 26 0F
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.439Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x367 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 67
00 00 00 00
00 00 00 00
00 00 00 15

[rebalance:debug,2025-05-15T18:47:52.440Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 871, stream opaque = 0x15000000
[ns_server:debug,2025-05-15T18:47:52.440Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x368 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 68
00 00 00 00
00 00 00 00
00 00 00 16

[rebalance:debug,2025-05-15T18:47:52.440Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 872, stream opaque = 0x16000000
[ns_server:debug,2025-05-15T18:47:52.440Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x72000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
72 00 00 00
00 00 00 00
00 00 00 00
00 00 11 22
36 4F 8A 45
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.440Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x369 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 69
00 00 00 00
00 00 00 00
00 00 00 17

[ns_server:debug,2025-05-15T18:47:52.440Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x73000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
73 00 00 00
00 00 00 00
00 00 00 00
00 00 19 4C
7A B2 5A 0D
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.440Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 873, stream opaque = 0x17000000
[ns_server:debug,2025-05-15T18:47:52.440Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x36A status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 6A
00 00 00 00
00 00 00 00
00 00 00 18

[rebalance:debug,2025-05-15T18:47:52.440Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 874, stream opaque = 0x18000000
[ns_server:debug,2025-05-15T18:47:52.440Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x36B status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 6B
00 00 00 00
00 00 00 00
00 00 00 19

[rebalance:debug,2025-05-15T18:47:52.440Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 875, stream opaque = 0x19000000
[ns_server:debug,2025-05-15T18:47:52.440Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x36C status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 6C
00 00 00 00
00 00 00 00
00 00 00 1A

[rebalance:debug,2025-05-15T18:47:52.440Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 876, stream opaque = 0x1A000000
[ns_server:debug,2025-05-15T18:47:52.440Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x36D status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 6D
00 00 00 00
00 00 00 00
00 00 00 1B

[rebalance:debug,2025-05-15T18:47:52.440Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 877, stream opaque = 0x1B000000
[ns_server:debug,2025-05-15T18:47:52.440Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x74000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
74 00 00 00
00 00 00 00
00 00 00 00
00 00 9F 7C
38 B4 4A 60
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.440Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x36E status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 6E
00 00 00 00
00 00 00 00
00 00 00 1C

[ns_server:debug,2025-05-15T18:47:52.441Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x75000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
75 00 00 00
00 00 00 00
00 00 00 00
00 00 F0 91
1C 87 FA 2D
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.441Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 878, stream opaque = 0x1C000000
[ns_server:debug,2025-05-15T18:47:52.441Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x76000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
76 00 00 00
00 00 00 00
00 00 00 00
00 00 E3 AE
14 27 D8 EC
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.441Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x36F status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 6F
00 00 00 00
00 00 00 00
00 00 00 1D

[rebalance:debug,2025-05-15T18:47:52.441Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 879, stream opaque = 0x1D000000
[ns_server:debug,2025-05-15T18:47:52.441Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x370 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 70
00 00 00 00
00 00 00 00
00 00 00 1E

[ns_server:debug,2025-05-15T18:47:52.441Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x77000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
77 00 00 00
00 00 00 00
00 00 00 00
00 00 D7 56
64 33 CB B4
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.441Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 880, stream opaque = 0x1E000000
[ns_server:debug,2025-05-15T18:47:52.441Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x78000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
78 00 00 00
00 00 00 00
00 00 00 00
00 00 98 2C
ED 96 4D 6A
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.441Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x371 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 71
00 00 00 00
00 00 00 00
00 00 00 1F

[ns_server:debug,2025-05-15T18:47:52.441Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x79000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
79 00 00 00
00 00 00 00
00 00 00 00
00 00 26 AA
C2 7D 84 3D
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.441Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 881, stream opaque = 0x1F000000
[ns_server:debug,2025-05-15T18:47:52.441Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x7A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
7A 00 00 00
00 00 00 00
00 00 00 00
00 00 3A A1
18 AA F2 85
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.441Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x372 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 72
00 00 00 00
00 00 00 00
00 00 00 20

[rebalance:debug,2025-05-15T18:47:52.441Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 882, stream opaque = 0x20000000
[ns_server:debug,2025-05-15T18:47:52.441Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x7B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
7B 00 00 00
00 00 00 00
00 00 00 00
00 00 ED 26
F6 25 B9 35
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.441Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x373 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 73
00 00 00 00
00 00 00 00
00 00 00 21

[rebalance:debug,2025-05-15T18:47:52.441Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 883, stream opaque = 0x21000000
[ns_server:debug,2025-05-15T18:47:52.441Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x7C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
7C 00 00 00
00 00 00 00
00 00 00 00
00 00 4C 14
05 66 39 63
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.441Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x374 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 74
00 00 00 00
00 00 00 00
00 00 00 22

[rebalance:debug,2025-05-15T18:47:52.441Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 884, stream opaque = 0x22000000
[ns_server:debug,2025-05-15T18:47:52.442Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x7D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
7D 00 00 00
00 00 00 00
00 00 00 00
00 00 0D 79
0C 69 BF EE
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.442Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x375 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 75
00 00 00 00
00 00 00 00
00 00 00 23

[rebalance:debug,2025-05-15T18:47:52.442Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 885, stream opaque = 0x23000000
[ns_server:debug,2025-05-15T18:47:52.442Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x376 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 76
00 00 00 00
00 00 00 00
00 00 00 24

[ns_server:debug,2025-05-15T18:47:52.442Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x7E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
7E 00 00 00
00 00 00 00
00 00 00 00
00 00 E8 EF
D1 D7 97 DC
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.442Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 886, stream opaque = 0x24000000
[ns_server:debug,2025-05-15T18:47:52.442Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x377 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 77
00 00 00 00
00 00 00 00
00 00 00 25

[ns_server:debug,2025-05-15T18:47:52.442Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x7F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
7F 00 00 00
00 00 00 00
00 00 00 00
00 00 7B 49
22 05 1E 49
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.442Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 887, stream opaque = 0x25000000
[ns_server:debug,2025-05-15T18:47:52.442Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x80000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
80 00 00 00
00 00 00 00
00 00 00 00
00 00 CB 7D
DD 3C 24 D7
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.442Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x378 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 78
00 00 00 00
00 00 00 00
00 00 00 26

[rebalance:debug,2025-05-15T18:47:52.442Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 888, stream opaque = 0x26000000
[ns_server:debug,2025-05-15T18:47:52.442Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x81000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
81 00 00 00
00 00 00 00
00 00 00 00
00 00 73 4A
F1 0A AA E7
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.442Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x379 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 79
00 00 00 00
00 00 00 00
00 00 00 27

[rebalance:debug,2025-05-15T18:47:52.442Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 889, stream opaque = 0x27000000
[ns_server:debug,2025-05-15T18:47:52.442Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x82000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
82 00 00 00
00 00 00 00
00 00 00 00
00 00 58 AF
E1 35 F3 50
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.442Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x37A status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 7A
00 00 00 00
00 00 00 00
00 00 00 28

[rebalance:debug,2025-05-15T18:47:52.442Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 890, stream opaque = 0x28000000
[ns_server:debug,2025-05-15T18:47:52.442Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x37B status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 7B
00 00 00 00
00 00 00 00
00 00 00 29

[ns_server:debug,2025-05-15T18:47:52.442Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x83000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
83 00 00 00
00 00 00 00
00 00 00 00
00 00 AD AD
63 B1 83 0D
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.442Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 891, stream opaque = 0x29000000
[ns_server:debug,2025-05-15T18:47:52.442Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x37C status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 7C
00 00 00 00
00 00 00 00
00 00 00 2A

[ns_server:debug,2025-05-15T18:47:52.442Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x84000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
84 00 00 00
00 00 00 00
00 00 00 00
00 00 A4 7D
90 69 22 55
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.442Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 892, stream opaque = 0x2A000000
[ns_server:debug,2025-05-15T18:47:52.442Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x37D status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 7D
00 00 00 00
00 00 00 00
00 00 00 2B

[ns_server:debug,2025-05-15T18:47:52.442Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x85000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
85 00 00 00
00 00 00 00
00 00 00 00
00 00 1A 60
F2 EA 9B A9
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.442Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 893, stream opaque = 0x2B000000
[ns_server:debug,2025-05-15T18:47:52.442Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x37E status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 7E
00 00 00 00
00 00 00 00
00 00 00 2C

[rebalance:debug,2025-05-15T18:47:52.442Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 894, stream opaque = 0x2C000000
[ns_server:debug,2025-05-15T18:47:52.442Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x37F status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 7F
00 00 00 00
00 00 00 00
00 00 00 2D

[rebalance:debug,2025-05-15T18:47:52.442Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 895, stream opaque = 0x2D000000
[ns_server:debug,2025-05-15T18:47:52.442Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x380 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 80
00 00 00 00
00 00 00 00
00 00 00 2E

[rebalance:debug,2025-05-15T18:47:52.442Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 896, stream opaque = 0x2E000000
[ns_server:debug,2025-05-15T18:47:52.442Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x381 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 81
00 00 00 00
00 00 00 00
00 00 00 2F

[rebalance:debug,2025-05-15T18:47:52.442Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 897, stream opaque = 0x2F000000
[ns_server:debug,2025-05-15T18:47:52.443Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x382 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 82
00 00 00 00
00 00 00 00
00 00 00 30

[rebalance:debug,2025-05-15T18:47:52.443Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 898, stream opaque = 0x30000000
[ns_server:debug,2025-05-15T18:47:52.443Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x383 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 83
00 00 00 00
00 00 00 00
00 00 00 31

[rebalance:debug,2025-05-15T18:47:52.443Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 899, stream opaque = 0x31000000
[ns_server:debug,2025-05-15T18:47:52.443Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x384 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 84
00 00 00 00
00 00 00 00
00 00 00 32

[rebalance:debug,2025-05-15T18:47:52.443Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 900, stream opaque = 0x32000000
[ns_server:debug,2025-05-15T18:47:52.443Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x385 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 85
00 00 00 00
00 00 00 00
00 00 00 33

[rebalance:debug,2025-05-15T18:47:52.443Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 901, stream opaque = 0x33000000
[ns_server:debug,2025-05-15T18:47:52.443Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x386 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 86
00 00 00 00
00 00 00 00
00 00 00 34

[rebalance:debug,2025-05-15T18:47:52.443Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 902, stream opaque = 0x34000000
[ns_server:debug,2025-05-15T18:47:52.443Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x387 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 87
00 00 00 00
00 00 00 00
00 00 00 35

[rebalance:debug,2025-05-15T18:47:52.443Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 903, stream opaque = 0x35000000
[ns_server:debug,2025-05-15T18:47:52.443Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x388 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 88
00 00 00 00
00 00 00 00
00 00 00 36

[rebalance:debug,2025-05-15T18:47:52.443Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 904, stream opaque = 0x36000000
[ns_server:debug,2025-05-15T18:47:52.443Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x389 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 89
00 00 00 00
00 00 00 00
00 00 00 37

[rebalance:debug,2025-05-15T18:47:52.443Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 905, stream opaque = 0x37000000
[ns_server:debug,2025-05-15T18:47:52.443Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x38A status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 8A
00 00 00 00
00 00 00 00
00 00 00 38

[rebalance:debug,2025-05-15T18:47:52.443Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 906, stream opaque = 0x38000000
[ns_server:debug,2025-05-15T18:47:52.443Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x38B status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 8B
00 00 00 00
00 00 00 00
00 00 00 39

[rebalance:debug,2025-05-15T18:47:52.443Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 907, stream opaque = 0x39000000
[ns_server:debug,2025-05-15T18:47:52.443Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x38C status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 8C
00 00 00 00
00 00 00 00
00 00 00 3A

[rebalance:debug,2025-05-15T18:47:52.443Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 908, stream opaque = 0x3A000000
[ns_server:debug,2025-05-15T18:47:52.442Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x86000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
86 00 00 00
00 00 00 00
00 00 00 00
00 00 4F 02
B6 6F CA AC
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.444Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x87000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
87 00 00 00
00 00 00 00
00 00 00 00
00 00 D1 64
BC 7A D5 8B
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.444Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x88000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
88 00 00 00
00 00 00 00
00 00 00 00
00 00 A1 1B
21 8A ED 5B
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.444Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x89000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
89 00 00 00
00 00 00 00
00 00 00 00
00 00 97 45
AE 71 9C 42
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.444Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x8A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
8A 00 00 00
00 00 00 00
00 00 00 00
00 00 0A 56
68 47 3A B9
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.444Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x8B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
8B 00 00 00
00 00 00 00
00 00 00 00
00 00 DC 2B
DC 0B 55 85
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.444Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x8C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
8C 00 00 00
00 00 00 00
00 00 00 00
00 00 93 C6
FF 73 57 18
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.444Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x8D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
8D 00 00 00
00 00 00 00
00 00 00 00
00 00 92 D2
EA AC 1F C2
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.444Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x8E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
8E 00 00 00
00 00 00 00
00 00 00 00
00 00 13 7A
00 48 BF 05
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.444Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x8F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
8F 00 00 00
00 00 00 00
00 00 00 00
00 00 04 AA
52 CE 94 55
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.444Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x90000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
90 00 00 00
00 00 00 00
00 00 00 00
00 00 00 B6
29 C6 52 5B
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.444Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x91000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
91 00 00 00
00 00 00 00
00 00 00 00
00 00 2A 27
84 CD A0 AC
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.444Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x92000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
92 00 00 00
00 00 00 00
00 00 00 00
00 00 9E F4
D5 BE 1A 4E
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.444Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x93000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
93 00 00 00
00 00 00 00
00 00 00 00
00 00 E6 3F
0C B9 87 01
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.445Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x94000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
94 00 00 00
00 00 00 00
00 00 00 00
00 00 04 8A
7B F2 84 D3
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.445Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x95000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
95 00 00 00
00 00 00 00
00 00 00 00
00 00 5A 87
04 27 E3 72
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.445Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x96000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
96 00 00 00
00 00 00 00
00 00 00 00
00 00 B4 6E
34 11 28 1D
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.445Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x97000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
97 00 00 00
00 00 00 00
00 00 00 00
00 00 E3 79
B8 68 B8 AF
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.445Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x98000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
98 00 00 00
00 00 00 00
00 00 00 00
00 00 DF C2
8B D2 A5 9B
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.445Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x99000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
99 00 00 00
00 00 00 00
00 00 00 00
00 00 D0 F9
BC DD 93 B1
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.445Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x9A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
9A 00 00 00
00 00 00 00
00 00 00 00
00 00 E2 58
05 B6 7D D7
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.445Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x9B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
9B 00 00 00
00 00 00 00
00 00 00 00
00 00 7D 53
8E FE FE 27
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.445Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x9C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
9C 00 00 00
00 00 00 00
00 00 00 00
00 00 62 D2
C2 2B BB B7
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.445Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x9D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
9D 00 00 00
00 00 00 00
00 00 00 00
00 00 FF 72
49 FB C0 13
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.445Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x9E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
9E 00 00 00
00 00 00 00
00 00 00 00
00 00 86 5D
2D B1 89 A7
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.446Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x9F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
9F 00 00 00
00 00 00 00
00 00 00 00
00 00 E5 62
86 0C A6 A5
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.446Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA0000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A0 00 00 00
00 00 00 00
00 00 00 00
00 00 C1 00
07 29 B6 CF
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.446Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA1000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A1 00 00 00
00 00 00 00
00 00 00 00
00 00 87 14
52 F1 31 40
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.446Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA2000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A2 00 00 00
00 00 00 00
00 00 00 00
00 00 B5 9D
4E 02 F8 93
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.446Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA3000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A3 00 00 00
00 00 00 00
00 00 00 00
00 00 B2 90
97 11 12 B7
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.446Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA4000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A4 00 00 00
00 00 00 00
00 00 00 00
00 00 A3 0F
56 8E 1D E7
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.446Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA5000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A5 00 00 00
00 00 00 00
00 00 00 00
00 00 4B 24
C4 CF F9 78
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.446Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA6000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A6 00 00 00
00 00 00 00
00 00 00 00
00 00 1D 5A
BB EF 65 26
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.446Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA7000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A7 00 00 00
00 00 00 00
00 00 00 00
00 00 71 A7
89 08 8E 34
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.446Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA8000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A8 00 00 00
00 00 00 00
00 00 00 00
00 00 D7 BE
8B 1C 56 4D
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.446Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA9000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A9 00 00 00
00 00 00 00
00 00 00 00
00 00 3B FA
7E 75 1D A6
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.446Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xAA000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
AA 00 00 00
00 00 00 00
00 00 00 00
00 00 4F EF
31 56 22 4A
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.446Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xAB000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
AB 00 00 00
00 00 00 00
00 00 00 00
00 00 77 CB
24 EB AE 1B
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.447Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xAC000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
AC 00 00 00
00 00 00 00
00 00 00 00
00 00 82 BA
9B 66 D9 29
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.447Z,ns_1@db2.lan:<0.4284.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xAD000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
AD 00 00 00
00 00 00 00
00 00 00 00
00 00 82 00
C5 DE 44 BE
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.448Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x38D status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 8D
00 00 00 00
00 00 00 00
00 00 00 3B

[rebalance:debug,2025-05-15T18:47:52.448Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 909, stream opaque = 0x3B000000
[ns_server:debug,2025-05-15T18:47:52.448Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x38E status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 8E
00 00 00 00
00 00 00 00
00 00 00 3C

[rebalance:debug,2025-05-15T18:47:52.448Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 910, stream opaque = 0x3C000000
[ns_server:debug,2025-05-15T18:47:52.448Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x38F status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 8F
00 00 00 00
00 00 00 00
00 00 00 3D

[rebalance:debug,2025-05-15T18:47:52.448Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 911, stream opaque = 0x3D000000
[ns_server:debug,2025-05-15T18:47:52.448Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x390 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 90
00 00 00 00
00 00 00 00
00 00 00 3E

[rebalance:debug,2025-05-15T18:47:52.448Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 912, stream opaque = 0x3E000000
[ns_server:debug,2025-05-15T18:47:52.449Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x391 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 91
00 00 00 00
00 00 00 00
00 00 00 3F

[rebalance:debug,2025-05-15T18:47:52.449Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 913, stream opaque = 0x3F000000
[ns_server:debug,2025-05-15T18:47:52.449Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x392 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 92
00 00 00 00
00 00 00 00
00 00 00 40

[rebalance:debug,2025-05-15T18:47:52.449Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 914, stream opaque = 0x40000000
[ns_server:debug,2025-05-15T18:47:52.449Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x393 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 93
00 00 00 00
00 00 00 00
00 00 00 41

[rebalance:debug,2025-05-15T18:47:52.449Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 915, stream opaque = 0x41000000
[ns_server:debug,2025-05-15T18:47:52.449Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x394 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 94
00 00 00 00
00 00 00 00
00 00 00 42

[rebalance:debug,2025-05-15T18:47:52.450Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 916, stream opaque = 0x42000000
[ns_server:debug,2025-05-15T18:47:52.450Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x395 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 95
00 00 00 00
00 00 00 00
00 00 00 43

[rebalance:debug,2025-05-15T18:47:52.450Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 917, stream opaque = 0x43000000
[ns_server:debug,2025-05-15T18:47:52.450Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x396 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 96
00 00 00 00
00 00 00 00
00 00 00 44

[rebalance:debug,2025-05-15T18:47:52.450Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 918, stream opaque = 0x44000000
[ns_server:debug,2025-05-15T18:47:52.450Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x397 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 97
00 00 00 00
00 00 00 00
00 00 00 45

[rebalance:debug,2025-05-15T18:47:52.450Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 919, stream opaque = 0x45000000
[ns_server:debug,2025-05-15T18:47:52.450Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x398 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 98
00 00 00 00
00 00 00 00
00 00 00 46

[rebalance:debug,2025-05-15T18:47:52.450Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 920, stream opaque = 0x46000000
[ns_server:debug,2025-05-15T18:47:52.450Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x399 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 99
00 00 00 00
00 00 00 00
00 00 00 47

[rebalance:debug,2025-05-15T18:47:52.450Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 921, stream opaque = 0x47000000
[ns_server:debug,2025-05-15T18:47:52.451Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x39A status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 9A
00 00 00 00
00 00 00 00
00 00 00 48

[rebalance:debug,2025-05-15T18:47:52.451Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 922, stream opaque = 0x48000000
[ns_server:debug,2025-05-15T18:47:52.451Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x39B status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 9B
00 00 00 00
00 00 00 00
00 00 00 49

[rebalance:debug,2025-05-15T18:47:52.451Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 923, stream opaque = 0x49000000
[ns_server:debug,2025-05-15T18:47:52.451Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x39C status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 9C
00 00 00 00
00 00 00 00
00 00 00 4A

[rebalance:debug,2025-05-15T18:47:52.451Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 924, stream opaque = 0x4A000000
[ns_server:debug,2025-05-15T18:47:52.451Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x39D status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 9D
00 00 00 00
00 00 00 00
00 00 00 4B

[rebalance:debug,2025-05-15T18:47:52.451Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 925, stream opaque = 0x4B000000
[ns_server:debug,2025-05-15T18:47:52.451Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x39E status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 9E
00 00 00 00
00 00 00 00
00 00 00 4C

[rebalance:debug,2025-05-15T18:47:52.452Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 926, stream opaque = 0x4C000000
[ns_server:debug,2025-05-15T18:47:52.452Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x39F status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 9F
00 00 00 00
00 00 00 00
00 00 00 4D

[rebalance:debug,2025-05-15T18:47:52.452Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 927, stream opaque = 0x4D000000
[ns_server:debug,2025-05-15T18:47:52.452Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3A0 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 A0
00 00 00 00
00 00 00 00
00 00 00 4E

[rebalance:debug,2025-05-15T18:47:52.452Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 928, stream opaque = 0x4E000000
[ns_server:debug,2025-05-15T18:47:52.452Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3A1 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 A1
00 00 00 00
00 00 00 00
00 00 00 4F

[rebalance:debug,2025-05-15T18:47:52.452Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 929, stream opaque = 0x4F000000
[ns_server:debug,2025-05-15T18:47:52.453Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3A2 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 A2
00 00 00 00
00 00 00 00
00 00 00 50

[rebalance:debug,2025-05-15T18:47:52.453Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 930, stream opaque = 0x50000000
[ns_server:debug,2025-05-15T18:47:52.453Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3A3 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 A3
00 00 00 00
00 00 00 00
00 00 00 51

[rebalance:debug,2025-05-15T18:47:52.453Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 931, stream opaque = 0x51000000
[ns_server:debug,2025-05-15T18:47:52.453Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3A4 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 A4
00 00 00 00
00 00 00 00
00 00 00 52

[rebalance:debug,2025-05-15T18:47:52.453Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 932, stream opaque = 0x52000000
[ns_server:debug,2025-05-15T18:47:52.453Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3A5 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 A5
00 00 00 00
00 00 00 00
00 00 00 53

[rebalance:debug,2025-05-15T18:47:52.453Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 933, stream opaque = 0x53000000
[ns_server:debug,2025-05-15T18:47:52.453Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3A6 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 A6
00 00 00 00
00 00 00 00
00 00 00 54

[rebalance:debug,2025-05-15T18:47:52.454Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 934, stream opaque = 0x54000000
[ns_server:debug,2025-05-15T18:47:52.454Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3A7 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 A7
00 00 00 00
00 00 00 00
00 00 00 55

[rebalance:debug,2025-05-15T18:47:52.454Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 935, stream opaque = 0x55000000
[ns_server:debug,2025-05-15T18:47:52.454Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3A8 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 A8
00 00 00 00
00 00 00 00
00 00 00 56

[rebalance:debug,2025-05-15T18:47:52.454Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 936, stream opaque = 0x56000000
[ns_server:debug,2025-05-15T18:47:52.454Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3A9 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 A9
00 00 00 00
00 00 00 00
00 00 00 57

[rebalance:debug,2025-05-15T18:47:52.454Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 937, stream opaque = 0x57000000
[ns_server:debug,2025-05-15T18:47:52.455Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3AA status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 AA
00 00 00 00
00 00 00 00
00 00 00 58

[rebalance:debug,2025-05-15T18:47:52.455Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 938, stream opaque = 0x58000000
[ns_server:debug,2025-05-15T18:47:52.455Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3AB status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 AB
00 00 00 00
00 00 00 00
00 00 00 59

[rebalance:debug,2025-05-15T18:47:52.455Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 939, stream opaque = 0x59000000
[ns_server:debug,2025-05-15T18:47:52.455Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3AC status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 AC
00 00 00 00
00 00 00 00
00 00 00 5A

[rebalance:debug,2025-05-15T18:47:52.455Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 940, stream opaque = 0x5A000000
[ns_server:debug,2025-05-15T18:47:52.455Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3AD status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 AD
00 00 00 00
00 00 00 00
00 00 00 5B

[rebalance:debug,2025-05-15T18:47:52.455Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 941, stream opaque = 0x5B000000
[ns_server:debug,2025-05-15T18:47:52.455Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3AE status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 AE
00 00 00 00
00 00 00 00
00 00 00 5C

[rebalance:debug,2025-05-15T18:47:52.455Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 942, stream opaque = 0x5C000000
[ns_server:debug,2025-05-15T18:47:52.455Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3AF status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 AF
00 00 00 00
00 00 00 00
00 00 00 5D

[rebalance:debug,2025-05-15T18:47:52.455Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 943, stream opaque = 0x5D000000
[ns_server:debug,2025-05-15T18:47:52.456Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3B0 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 B0
00 00 00 00
00 00 00 00
00 00 00 5E

[rebalance:debug,2025-05-15T18:47:52.456Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 944, stream opaque = 0x5E000000
[ns_server:debug,2025-05-15T18:47:52.456Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3B1 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 B1
00 00 00 00
00 00 00 00
00 00 00 5F

[rebalance:debug,2025-05-15T18:47:52.456Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 945, stream opaque = 0x5F000000
[ns_server:debug,2025-05-15T18:47:52.456Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3B2 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 B2
00 00 00 00
00 00 00 00
00 00 00 60

[rebalance:debug,2025-05-15T18:47:52.456Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 946, stream opaque = 0x60000000
[ns_server:debug,2025-05-15T18:47:52.456Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3B3 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 B3
00 00 00 00
00 00 00 00
00 00 00 61

[rebalance:debug,2025-05-15T18:47:52.456Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 947, stream opaque = 0x61000000
[ns_server:debug,2025-05-15T18:47:52.456Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3B4 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 B4
00 00 00 00
00 00 00 00
00 00 00 62

[rebalance:debug,2025-05-15T18:47:52.456Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 948, stream opaque = 0x62000000
[ns_server:debug,2025-05-15T18:47:52.456Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3B5 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 B5
00 00 00 00
00 00 00 00
00 00 00 63

[rebalance:debug,2025-05-15T18:47:52.456Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 949, stream opaque = 0x63000000
[ns_server:debug,2025-05-15T18:47:52.456Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3B6 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 B6
00 00 00 00
00 00 00 00
00 00 00 64

[rebalance:debug,2025-05-15T18:47:52.456Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 950, stream opaque = 0x64000000
[ns_server:debug,2025-05-15T18:47:52.456Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3B7 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 B7
00 00 00 00
00 00 00 00
00 00 00 65

[rebalance:debug,2025-05-15T18:47:52.457Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 951, stream opaque = 0x65000000
[ns_server:debug,2025-05-15T18:47:52.457Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3B8 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 B8
00 00 00 00
00 00 00 00
00 00 00 66

[rebalance:debug,2025-05-15T18:47:52.457Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 952, stream opaque = 0x66000000
[ns_server:debug,2025-05-15T18:47:52.457Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3B9 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 B9
00 00 00 00
00 00 00 00
00 00 00 67

[rebalance:debug,2025-05-15T18:47:52.457Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 953, stream opaque = 0x67000000
[ns_server:debug,2025-05-15T18:47:52.457Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3BA status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 BA
00 00 00 00
00 00 00 00
00 00 00 68

[rebalance:debug,2025-05-15T18:47:52.457Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 954, stream opaque = 0x68000000
[ns_server:debug,2025-05-15T18:47:52.457Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3BB status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 BB
00 00 00 00
00 00 00 00
00 00 00 69

[rebalance:debug,2025-05-15T18:47:52.457Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 955, stream opaque = 0x69000000
[ns_server:debug,2025-05-15T18:47:52.457Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3BC status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 BC
00 00 00 00
00 00 00 00
00 00 00 6A

[rebalance:debug,2025-05-15T18:47:52.457Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 956, stream opaque = 0x6A000000
[ns_server:debug,2025-05-15T18:47:52.457Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3BD status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 BD
00 00 00 00
00 00 00 00
00 00 00 6B

[rebalance:debug,2025-05-15T18:47:52.457Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 957, stream opaque = 0x6B000000
[ns_server:debug,2025-05-15T18:47:52.457Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3BE status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 BE
00 00 00 00
00 00 00 00
00 00 00 6C

[rebalance:debug,2025-05-15T18:47:52.457Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 958, stream opaque = 0x6C000000
[ns_server:debug,2025-05-15T18:47:52.457Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3BF status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 BF
00 00 00 00
00 00 00 00
00 00 00 6D

[rebalance:debug,2025-05-15T18:47:52.457Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 959, stream opaque = 0x6D000000
[ns_server:debug,2025-05-15T18:47:52.458Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3C0 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 C0
00 00 00 00
00 00 00 00
00 00 00 6E

[rebalance:debug,2025-05-15T18:47:52.458Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 960, stream opaque = 0x6E000000
[ns_server:debug,2025-05-15T18:47:52.458Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3C1 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 C1
00 00 00 00
00 00 00 00
00 00 00 6F

[rebalance:debug,2025-05-15T18:47:52.458Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 961, stream opaque = 0x6F000000
[ns_server:debug,2025-05-15T18:47:52.458Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3C2 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 C2
00 00 00 00
00 00 00 00
00 00 00 70

[rebalance:debug,2025-05-15T18:47:52.458Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 962, stream opaque = 0x70000000
[ns_server:debug,2025-05-15T18:47:52.458Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3C3 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 C3
00 00 00 00
00 00 00 00
00 00 00 71

[rebalance:debug,2025-05-15T18:47:52.458Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 963, stream opaque = 0x71000000
[ns_server:debug,2025-05-15T18:47:52.458Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3C4 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 C4
00 00 00 00
00 00 00 00
00 00 00 72

[rebalance:debug,2025-05-15T18:47:52.458Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 964, stream opaque = 0x72000000
[ns_server:debug,2025-05-15T18:47:52.458Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3C5 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 C5
00 00 00 00
00 00 00 00
00 00 00 73

[rebalance:debug,2025-05-15T18:47:52.458Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 965, stream opaque = 0x73000000
[ns_server:debug,2025-05-15T18:47:52.458Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3C6 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 C6
00 00 00 00
00 00 00 00
00 00 00 74

[rebalance:debug,2025-05-15T18:47:52.458Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 966, stream opaque = 0x74000000
[ns_server:debug,2025-05-15T18:47:52.459Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3C7 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 C7
00 00 00 00
00 00 00 00
00 00 00 75

[rebalance:debug,2025-05-15T18:47:52.459Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 967, stream opaque = 0x75000000
[ns_server:debug,2025-05-15T18:47:52.459Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3C8 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 C8
00 00 00 00
00 00 00 00
00 00 00 76

[rebalance:debug,2025-05-15T18:47:52.459Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 968, stream opaque = 0x76000000
[ns_server:debug,2025-05-15T18:47:52.459Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3C9 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 C9
00 00 00 00
00 00 00 00
00 00 00 77

[rebalance:debug,2025-05-15T18:47:52.459Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 969, stream opaque = 0x77000000
[ns_server:debug,2025-05-15T18:47:52.459Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3CA status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 CA
00 00 00 00
00 00 00 00
00 00 00 78

[rebalance:debug,2025-05-15T18:47:52.459Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 970, stream opaque = 0x78000000
[ns_server:debug,2025-05-15T18:47:52.459Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3CB status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 CB
00 00 00 00
00 00 00 00
00 00 00 79

[rebalance:debug,2025-05-15T18:47:52.459Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 971, stream opaque = 0x79000000
[ns_server:debug,2025-05-15T18:47:52.459Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3CC status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 CC
00 00 00 00
00 00 00 00
00 00 00 7A

[rebalance:debug,2025-05-15T18:47:52.459Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 972, stream opaque = 0x7A000000
[ns_server:debug,2025-05-15T18:47:52.459Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3CD status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 CD
00 00 00 00
00 00 00 00
00 00 00 7B

[rebalance:debug,2025-05-15T18:47:52.459Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 973, stream opaque = 0x7B000000
[ns_server:debug,2025-05-15T18:47:52.459Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3CE status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 CE
00 00 00 00
00 00 00 00
00 00 00 7C

[rebalance:debug,2025-05-15T18:47:52.459Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 974, stream opaque = 0x7C000000
[ns_server:debug,2025-05-15T18:47:52.459Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3CF status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 CF
00 00 00 00
00 00 00 00
00 00 00 7D

[rebalance:debug,2025-05-15T18:47:52.459Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 975, stream opaque = 0x7D000000
[ns_server:debug,2025-05-15T18:47:52.459Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3D0 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 D0
00 00 00 00
00 00 00 00
00 00 00 7E

[rebalance:debug,2025-05-15T18:47:52.459Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 976, stream opaque = 0x7E000000
[ns_server:debug,2025-05-15T18:47:52.459Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3D1 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 D1
00 00 00 00
00 00 00 00
00 00 00 7F

[rebalance:debug,2025-05-15T18:47:52.459Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 977, stream opaque = 0x7F000000
[ns_server:debug,2025-05-15T18:47:52.459Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3D2 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 D2
00 00 00 00
00 00 00 00
00 00 00 80

[rebalance:debug,2025-05-15T18:47:52.459Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 978, stream opaque = 0x80000000
[ns_server:debug,2025-05-15T18:47:52.460Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3D3 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 D3
00 00 00 00
00 00 00 00
00 00 00 81

[rebalance:debug,2025-05-15T18:47:52.460Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 979, stream opaque = 0x81000000
[ns_server:debug,2025-05-15T18:47:52.460Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3D4 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 D4
00 00 00 00
00 00 00 00
00 00 00 82

[rebalance:debug,2025-05-15T18:47:52.460Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 980, stream opaque = 0x82000000
[ns_server:debug,2025-05-15T18:47:52.460Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3D5 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 D5
00 00 00 00
00 00 00 00
00 00 00 83

[rebalance:debug,2025-05-15T18:47:52.460Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 981, stream opaque = 0x83000000
[ns_server:debug,2025-05-15T18:47:52.460Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3D6 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 D6
00 00 00 00
00 00 00 00
00 00 00 84

[rebalance:debug,2025-05-15T18:47:52.460Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 982, stream opaque = 0x84000000
[ns_server:debug,2025-05-15T18:47:52.460Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3D7 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 D7
00 00 00 00
00 00 00 00
00 00 00 85

[rebalance:debug,2025-05-15T18:47:52.460Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 983, stream opaque = 0x85000000
[ns_server:debug,2025-05-15T18:47:52.460Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3D8 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 D8
00 00 00 00
00 00 00 00
00 00 00 86

[rebalance:debug,2025-05-15T18:47:52.460Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 984, stream opaque = 0x86000000
[ns_server:debug,2025-05-15T18:47:52.460Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3D9 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 D9
00 00 00 00
00 00 00 00
00 00 00 87

[rebalance:debug,2025-05-15T18:47:52.460Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 985, stream opaque = 0x87000000
[ns_server:debug,2025-05-15T18:47:52.461Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3DA status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 DA
00 00 00 00
00 00 00 00
00 00 00 88

[rebalance:debug,2025-05-15T18:47:52.461Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 986, stream opaque = 0x88000000
[ns_server:debug,2025-05-15T18:47:52.461Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3DB status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 DB
00 00 00 00
00 00 00 00
00 00 00 89

[rebalance:debug,2025-05-15T18:47:52.461Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 987, stream opaque = 0x89000000
[ns_server:debug,2025-05-15T18:47:52.461Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3DC status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 DC
00 00 00 00
00 00 00 00
00 00 00 8A

[rebalance:debug,2025-05-15T18:47:52.461Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 988, stream opaque = 0x8A000000
[ns_server:debug,2025-05-15T18:47:52.461Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3DD status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 DD
00 00 00 00
00 00 00 00
00 00 00 8B

[rebalance:debug,2025-05-15T18:47:52.461Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 989, stream opaque = 0x8B000000
[ns_server:debug,2025-05-15T18:47:52.461Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3DE status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 DE
00 00 00 00
00 00 00 00
00 00 00 8C

[rebalance:debug,2025-05-15T18:47:52.461Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 990, stream opaque = 0x8C000000
[ns_server:debug,2025-05-15T18:47:52.461Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3DF status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 DF
00 00 00 00
00 00 00 00
00 00 00 8D

[rebalance:debug,2025-05-15T18:47:52.461Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 991, stream opaque = 0x8D000000
[ns_server:debug,2025-05-15T18:47:52.461Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3E0 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 E0
00 00 00 00
00 00 00 00
00 00 00 8E

[rebalance:debug,2025-05-15T18:47:52.462Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 992, stream opaque = 0x8E000000
[ns_server:debug,2025-05-15T18:47:52.462Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3E1 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 E1
00 00 00 00
00 00 00 00
00 00 00 8F

[rebalance:debug,2025-05-15T18:47:52.462Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 993, stream opaque = 0x8F000000
[ns_server:debug,2025-05-15T18:47:52.462Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3E2 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 E2
00 00 00 00
00 00 00 00
00 00 00 90

[rebalance:debug,2025-05-15T18:47:52.462Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 994, stream opaque = 0x90000000
[ns_server:debug,2025-05-15T18:47:52.462Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3E3 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 E3
00 00 00 00
00 00 00 00
00 00 00 91

[rebalance:debug,2025-05-15T18:47:52.462Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 995, stream opaque = 0x91000000
[ns_server:debug,2025-05-15T18:47:52.462Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3E4 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 E4
00 00 00 00
00 00 00 00
00 00 00 92

[rebalance:debug,2025-05-15T18:47:52.462Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 996, stream opaque = 0x92000000
[ns_server:debug,2025-05-15T18:47:52.462Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3E5 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 E5
00 00 00 00
00 00 00 00
00 00 00 93

[rebalance:debug,2025-05-15T18:47:52.462Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 997, stream opaque = 0x93000000
[ns_server:debug,2025-05-15T18:47:52.463Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3E6 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 E6
00 00 00 00
00 00 00 00
00 00 00 94

[rebalance:debug,2025-05-15T18:47:52.463Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 998, stream opaque = 0x94000000
[ns_server:debug,2025-05-15T18:47:52.463Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3E7 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 E7
00 00 00 00
00 00 00 00
00 00 00 95

[rebalance:debug,2025-05-15T18:47:52.463Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 999, stream opaque = 0x95000000
[ns_server:debug,2025-05-15T18:47:52.463Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3E8 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 E8
00 00 00 00
00 00 00 00
00 00 00 96

[rebalance:debug,2025-05-15T18:47:52.463Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 1000, stream opaque = 0x96000000
[ns_server:debug,2025-05-15T18:47:52.463Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3E9 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 E9
00 00 00 00
00 00 00 00
00 00 00 97

[rebalance:debug,2025-05-15T18:47:52.463Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 1001, stream opaque = 0x97000000
[ns_server:debug,2025-05-15T18:47:52.463Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3EA status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 EA
00 00 00 00
00 00 00 00
00 00 00 98

[rebalance:debug,2025-05-15T18:47:52.463Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 1002, stream opaque = 0x98000000
[ns_server:debug,2025-05-15T18:47:52.463Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3EB status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 EB
00 00 00 00
00 00 00 00
00 00 00 99

[rebalance:debug,2025-05-15T18:47:52.463Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 1003, stream opaque = 0x99000000
[ns_server:debug,2025-05-15T18:47:52.463Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3EC status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 EC
00 00 00 00
00 00 00 00
00 00 00 9A

[rebalance:debug,2025-05-15T18:47:52.463Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 1004, stream opaque = 0x9A000000
[ns_server:debug,2025-05-15T18:47:52.463Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3ED status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 ED
00 00 00 00
00 00 00 00
00 00 00 9B

[rebalance:debug,2025-05-15T18:47:52.463Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 1005, stream opaque = 0x9B000000
[ns_server:debug,2025-05-15T18:47:52.463Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3EE status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 EE
00 00 00 00
00 00 00 00
00 00 00 9C

[rebalance:debug,2025-05-15T18:47:52.463Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 1006, stream opaque = 0x9C000000
[ns_server:debug,2025-05-15T18:47:52.464Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3EF status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 EF
00 00 00 00
00 00 00 00
00 00 00 9D

[rebalance:debug,2025-05-15T18:47:52.464Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 1007, stream opaque = 0x9D000000
[ns_server:debug,2025-05-15T18:47:52.464Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3F0 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 F0
00 00 00 00
00 00 00 00
00 00 00 9E

[rebalance:debug,2025-05-15T18:47:52.464Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 1008, stream opaque = 0x9E000000
[ns_server:debug,2025-05-15T18:47:52.464Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3F1 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 F1
00 00 00 00
00 00 00 00
00 00 00 9F

[rebalance:debug,2025-05-15T18:47:52.464Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 1009, stream opaque = 0x9F000000
[ns_server:debug,2025-05-15T18:47:52.464Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3F2 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 F2
00 00 00 00
00 00 00 00
00 00 00 A0

[rebalance:debug,2025-05-15T18:47:52.464Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 1010, stream opaque = 0xA0000000
[ns_server:debug,2025-05-15T18:47:52.464Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3F3 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 F3
00 00 00 00
00 00 00 00
00 00 00 A1

[rebalance:debug,2025-05-15T18:47:52.464Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 1011, stream opaque = 0xA1000000
[ns_server:debug,2025-05-15T18:47:52.464Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3F4 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 F4
00 00 00 00
00 00 00 00
00 00 00 A2

[rebalance:debug,2025-05-15T18:47:52.464Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 1012, stream opaque = 0xA2000000
[ns_server:debug,2025-05-15T18:47:52.464Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3F5 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 F5
00 00 00 00
00 00 00 00
00 00 00 A3

[rebalance:debug,2025-05-15T18:47:52.464Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 1013, stream opaque = 0xA3000000
[ns_server:debug,2025-05-15T18:47:52.464Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3F6 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 F6
00 00 00 00
00 00 00 00
00 00 00 A4

[rebalance:debug,2025-05-15T18:47:52.464Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 1014, stream opaque = 0xA4000000
[ns_server:debug,2025-05-15T18:47:52.464Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3F7 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 F7
00 00 00 00
00 00 00 00
00 00 00 A5

[rebalance:debug,2025-05-15T18:47:52.464Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 1015, stream opaque = 0xA5000000
[ns_server:debug,2025-05-15T18:47:52.465Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3F8 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 F8
00 00 00 00
00 00 00 00
00 00 00 A6

[rebalance:debug,2025-05-15T18:47:52.465Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 1016, stream opaque = 0xA6000000
[ns_server:debug,2025-05-15T18:47:52.465Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3F9 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 F9
00 00 00 00
00 00 00 00
00 00 00 A7

[rebalance:debug,2025-05-15T18:47:52.465Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 1017, stream opaque = 0xA7000000
[ns_server:debug,2025-05-15T18:47:52.465Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3FA status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 FA
00 00 00 00
00 00 00 00
00 00 00 A8

[rebalance:debug,2025-05-15T18:47:52.465Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 1018, stream opaque = 0xA8000000
[ns_server:debug,2025-05-15T18:47:52.465Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3FB status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 FB
00 00 00 00
00 00 00 00
00 00 00 A9

[rebalance:debug,2025-05-15T18:47:52.465Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 1019, stream opaque = 0xA9000000
[ns_server:debug,2025-05-15T18:47:52.465Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3FC status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 FC
00 00 00 00
00 00 00 00
00 00 00 AA

[rebalance:debug,2025-05-15T18:47:52.465Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 1020, stream opaque = 0xAA000000
[ns_server:debug,2025-05-15T18:47:52.465Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3FD status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 FD
00 00 00 00
00 00 00 00
00 00 00 AB

[rebalance:debug,2025-05-15T18:47:52.465Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 1021, stream opaque = 0xAB000000
[ns_server:debug,2025-05-15T18:47:52.465Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3FE status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 FE
00 00 00 00
00 00 00 00
00 00 00 AC

[rebalance:debug,2025-05-15T18:47:52.465Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 1022, stream opaque = 0xAC000000
[ns_server:debug,2025-05-15T18:47:52.465Z,ns_1@db2.lan:<0.4277.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x3FF status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 FF
00 00 00 00
00 00 00 00
00 00 00 AD

[rebalance:debug,2025-05-15T18:47:52.465Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 1023, stream opaque = 0xAD000000
[ns_server:debug,2025-05-15T18:47:52.465Z,ns_1@db2.lan:<0.4277.0>:dcp_consumer_conn:maybe_reply_setup_streams:499]Setup stream request completed with ok. Moving to idle state
[ns_server:info,2025-05-15T18:47:52.476Z,ns_1@db2.lan:ns_memcached-doom-scrolling<0.4157.0>:ns_memcached:handle_call:370]Enabling traffic to bucket "doom-scrolling"
[ns_server:info,2025-05-15T18:47:52.477Z,ns_1@db2.lan:ns_memcached-doom-scrolling<0.4157.0>:ns_memcached:handle_call:374]Bucket "doom-scrolling" marked as warmed in 2 seconds
[ns_server:debug,2025-05-15T18:47:52.488Z,ns_1@db2.lan:dcp_traffic_monitor<0.3371.0>:dcp_traffic_monitor:update_bucket:148]Saw that bucket "doom-scrolling" became alive on node 'ns_1@db3.lan'
[ns_server:debug,2025-05-15T18:47:52.503Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@172.19.0.4',buckets_with_data}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                               51})
[{"doom-scrolling",<<"f70f8d64d14cf7513107bff35eb6561d">>}]
[ns_server:debug,2025-05-15T18:47:52.504Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@db3.lan',buckets_with_data}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                            52})
[{"doom-scrolling",<<"f70f8d64d14cf7513107bff35eb6561d">>}]
[ns_server:debug,2025-05-15T18:47:52.504Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@db2.lan',buckets_with_data}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                            53})
[{"doom-scrolling",<<"f70f8d64d14cf7513107bff35eb6561d">>}]
[ns_server:info,2025-05-15T18:47:52.701Z,ns_1@db2.lan:ns_doctor<0.2571.0>:ns_doctor:update_status:316]The following buckets became ready on node 'ns_1@172.19.0.4': ["doom-scrolling"]
[ns_server:info,2025-05-15T18:47:52.704Z,ns_1@db2.lan:ns_doctor<0.2571.0>:ns_doctor:update_status:316]The following buckets became ready on node 'ns_1@db3.lan': ["doom-scrolling"]
[ns_server:info,2025-05-15T18:47:52.710Z,ns_1@db2.lan:ns_doctor<0.2571.0>:ns_doctor:update_status:316]The following buckets became ready on node 'ns_1@db2.lan': ["doom-scrolling"]
[ns_server:debug,2025-05-15T18:47:52.917Z,ns_1@db2.lan:cb_saml<0.2435.0>:cb_saml:refresh_metadata:526]Refreshing metadata
[ns_server:debug,2025-05-15T18:47:52.917Z,ns_1@db2.lan:cb_saml<0.2435.0>:cb_saml:restart_refresh_timer:583]Disabling refresh timer
[ns_server:debug,2025-05-15T18:47:54.494Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/query/migration/CBO_STATS/state">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554074}}]}|
 <<"{\"node\":\"172.19.0.4:8091\",\"state\":\"migrated\",\"when\":\"2025-05-15T18:47:54.475750678Z\"}">>]
[ns_server:debug,2025-05-15T18:47:54.494Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/query/migration/UDF/state">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554074}}]}|
 <<"{\"node\":\"172.19.0.4:8091\",\"state\":\"migrated\",\"when\":\"2025-05-15T18:47:54.475987011Z\"}">>]
[ns_server:debug,2025-05-15T18:47:54.823Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/indexing/ddl/commandToken/create/17514355370907628042/0">>} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554074}}]}|
 <<"{\"DefnId\":17514355370907628042,\"BucketUUID\":\"f70f8d64d14cf7513107bff35eb6561d\",\"ScopeId\":\"8\",\"CollectionId\":\"9\",\"Definitions\":{\"7a33d46fd1976e65466c5efc8b45ef2e\":[{\"defnId\":17514355370907628042,\"name\":\"#primary\",\"using\":\"GSI\",\"bucket\":\"doom-scrolling\",\"isPrimary\":true,\"exprType\":\"N1QL\",\"partitionScheme\":\"SINGLE\",\"numReplica\":1,\"NumReplica2\":{\"HasValue\":true,\"Base\":1,\"Incr\":0,\"Decr\":0},"...>>]
[ns_server:debug,2025-05-15T18:47:54.823Z,ns_1@db2.lan:ns_config_rep<0.2526.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([{metakv,
                                   <<"/indexing/ddl/commandToken/create/17514355370907628042/0">>}]..)
[ns_server:debug,2025-05-15T18:47:55.108Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/indexing/ddl/commandToken/create/17514355370907628042/0">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554075}},
             {<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554074}}]}|
 '_deleted']
[ns_server:debug,2025-05-15T18:47:55.202Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:debug,2025-05-15T18:47:55.202Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:47:55.202Z,ns_1@db2.lan:<0.4523.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:47:55.204Z,ns_1@db2.lan:<0.4526.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 777499, disk size is 6436798
[ns_server:debug,2025-05-15T18:47:55.204Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:47:55.204Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:47:55.206Z,ns_1@db2.lan:<0.4524.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:47:55.206Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:47:55.206Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:48:14.314Z,ns_1@db2.lan:ns_config_rep<0.2526.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@db3.lan'
[ns_server:debug,2025-05-15T18:48:25.096Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{5,63914554105}}]}|
 <<"{\"indexer.plasma.backIndex.enableInMemoryCompression\":true,\"indexer.plasma.backIndex.enablePageBloomFilter\":true,\"indexer.plasma.mainIndex.enableInMemoryCompression\":true,\"indexer.settings.allow_large_keys\":true,\"indexer.settings.bufferPoolBlockSize\":16384,\"indexer.settings.build.batch_size\":5,\"indexer.settings.compaction.abort_exceed_interval\":false,\"indexer.settings.compaction.check_"...>>]
[ns_server:debug,2025-05-15T18:48:25.096Z,ns_1@db2.lan:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/indexing/settings/config/features/PlasmaInMemoryCompression">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554105}}]}|
 <<"{}">>]
[user:info,2025-05-15T18:48:25.192Z,ns_1@db2.lan:<0.5920.0>:menelaus_web_alerts_srv:global_alert:227]Approaching full disk warning. Usage of disk "/opt/couchbase/var" on node "db2.lan" is around 92%.
[ns_server:debug,2025-05-15T18:48:25.207Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:48:25.209Z,ns_1@db2.lan:<0.5924.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:48:25.209Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:debug,2025-05-15T18:48:25.212Z,ns_1@db2.lan:<0.5926.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 777499, disk size is 6436798
[ns_server:debug,2025-05-15T18:48:25.213Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:48:25.213Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:48:25.214Z,ns_1@db2.lan:<0.5927.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:48:25.214Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:48:25.214Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:48:28.331Z,ns_1@db2.lan:ns_config_rep<0.2526.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@db3.lan'
[ns_server:debug,2025-05-15T18:48:38.212Z,ns_1@db2.lan:ldap_auth_cache<0.2433.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[error_logger:info,2025-05-15T18:48:41.992Z,ns_1@db2.lan:alarm_handler<0.133.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
    alarm_handler: {set,{system_memory_high_watermark,[]}}
[ns_server:debug,2025-05-15T18:48:55.215Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:debug,2025-05-15T18:48:55.222Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:48:55.223Z,ns_1@db2.lan:<0.7284.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:48:55.227Z,ns_1@db2.lan:<0.7287.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 777499, disk size is 6436798
[ns_server:debug,2025-05-15T18:48:55.228Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:48:55.228Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:48:55.228Z,ns_1@db2.lan:<0.7286.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:48:55.229Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:48:55.229Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:48:57.791Z,ns_1@db2.lan:ns_config_rep<0.2526.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@172.19.0.4'
[ns_server:info,2025-05-15T18:49:12.429Z,ns_1@db2.lan:ns_config_rep<0.2526.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@db3.lan'
[ns_server:debug,2025-05-15T18:49:25.230Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:debug,2025-05-15T18:49:25.231Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:49:25.231Z,ns_1@db2.lan:<0.8649.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:49:25.232Z,ns_1@db2.lan:<0.8651.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 777499, disk size is 6436798
[ns_server:debug,2025-05-15T18:49:25.232Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:49:25.232Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:49:25.233Z,ns_1@db2.lan:<0.8652.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:49:25.234Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:49:25.234Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:49:53.214Z,ns_1@db2.lan:ldap_auth_cache<0.2433.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2025-05-15T18:49:55.234Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:debug,2025-05-15T18:49:55.239Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:49:55.240Z,ns_1@db2.lan:<0.10006.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:49:55.241Z,ns_1@db2.lan:<0.10008.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 777499, disk size is 6436798
[ns_server:debug,2025-05-15T18:49:55.241Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:49:55.242Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:49:55.250Z,ns_1@db2.lan:<0.10009.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:49:55.251Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:49:55.251Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:50:01.884Z,ns_1@db2.lan:ns_config_rep<0.2526.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@172.19.0.4'
[ns_server:debug,2025-05-15T18:50:25.245Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:50:25.246Z,ns_1@db2.lan:<0.11369.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:50:25.247Z,ns_1@db2.lan:<0.11371.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 777499, disk size is 6436798
[ns_server:debug,2025-05-15T18:50:25.247Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:50:25.247Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:50:25.254Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:50:25.257Z,ns_1@db2.lan:<0.11373.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:50:25.257Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:50:25.257Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:50:33.069Z,ns_1@db2.lan:ns_gc_runner<0.937.0>:ns_gc_runner:handle_info:125]GC populating new pid list of size=689, prevMaxGcDuration=5368 us
[ns_server:debug,2025-05-15T18:50:55.249Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:50:55.252Z,ns_1@db2.lan:<0.12727.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:50:55.253Z,ns_1@db2.lan:<0.12729.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 777499, disk size is 6436798
[ns_server:debug,2025-05-15T18:50:55.254Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:50:55.254Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:50:55.260Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:50:55.267Z,ns_1@db2.lan:<0.12730.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:50:55.268Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:50:55.268Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:50:55.866Z,ns_1@db2.lan:ns_config_rep<0.2526.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@db3.lan'
[ns_server:debug,2025-05-15T18:51:08.217Z,ns_1@db2.lan:ldap_auth_cache<0.2433.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2025-05-15T18:51:25.257Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:51:25.258Z,ns_1@db2.lan:<0.14092.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:51:25.259Z,ns_1@db2.lan:<0.14094.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 777499, disk size is 6436798
[ns_server:debug,2025-05-15T18:51:25.259Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:51:25.259Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:51:25.271Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:51:25.274Z,ns_1@db2.lan:<0.14095.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:51:25.274Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:51:25.274Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:51:33.999Z,ns_1@db2.lan:ns_config_rep<0.2526.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@db3.lan'
[ns_server:debug,2025-05-15T18:51:55.261Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:51:55.262Z,ns_1@db2.lan:<0.15449.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:51:55.263Z,ns_1@db2.lan:<0.15451.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 777499, disk size is 6436798
[ns_server:debug,2025-05-15T18:51:55.263Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:51:55.263Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:51:55.277Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:51:55.280Z,ns_1@db2.lan:<0.15452.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:51:55.281Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:51:55.281Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:52:00.252Z,ns_1@db2.lan:ns_config_rep<0.2526.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@db3.lan'
[ns_server:debug,2025-05-15T18:52:22.324Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: ns_config_vclock_ts, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                        65})
63914554042
[ns_server:debug,2025-05-15T18:52:22.324Z,ns_1@db2.lan:tombstone_keeper<0.277.0>:tombstone_keeper:handle_call:49]Refreshed with timestamps [{ns_config_vclock_ts,63914554042}]
[ns_server:debug,2025-05-15T18:52:22.324Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: ns_config_purger, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                     65})
'ns_1@172.19.0.4'
[ns_server:debug,2025-05-15T18:52:22.324Z,ns_1@db2.lan:ns_config_rep<0.2526.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 64 us
[ns_server:debug,2025-05-15T18:52:22.327Z,ns_1@db2.lan:ns_config_rep<0.2526.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([rbac_upgrade,
                               {metakv,
                                   <<"/indexing/ddl/commandToken/create/17514355370907628042/0">>},
                               {metakv,
                                   <<"/indexing/rebalance/RebalanceToken">>}]..)
[ns_server:debug,2025-05-15T18:52:22.327Z,ns_1@db2.lan:ns_config_rep<0.2526.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 54 us
[ns_server:debug,2025-05-15T18:52:22.333Z,ns_1@db2.lan:ns_config_rep<0.2526.0>:ns_config_rep:handle_call:149]Got full synchronization request from 'ns_1@172.19.0.4'
[ns_server:debug,2025-05-15T18:52:22.333Z,ns_1@db2.lan:ns_config_rep<0.2526.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 2 us
[ns_server:debug,2025-05-15T18:52:22.361Z,ns_1@db2.lan:tombstone_keeper<0.277.0>:tombstone_keeper:handle_call:49]Refreshed with timestamps [{ns_config_purge_ts,63914554042},
                           {ns_config_vclock_ts,63914554042}]
[ns_server:debug,2025-05-15T18:52:22.361Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: ns_config_purge_ts, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                       66})
63914554042
[ns_server:debug,2025-05-15T18:52:22.362Z,ns_1@db2.lan:tombstone_agent<0.2547.0>:tombstone_agent:purge:182]Purged 1 ns_config tombstone(s) up to timestamp 63914554042. Tombstones:
[rbac_upgrade]
[ns_server:debug,2025-05-15T18:52:23.220Z,ns_1@db2.lan:ldap_auth_cache<0.2433.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:info,2025-05-15T18:52:24.512Z,ns_1@db2.lan:ns_config_rep<0.2526.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@172.19.0.4'
[ns_server:debug,2025-05-15T18:52:25.266Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:52:25.267Z,ns_1@db2.lan:<0.16825.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:52:25.268Z,ns_1@db2.lan:<0.16827.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 777499, disk size is 6436798
[ns_server:debug,2025-05-15T18:52:25.269Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:52:25.269Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:52:25.284Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:52:25.287Z,ns_1@db2.lan:<0.16828.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:52:25.287Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:52:25.287Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:52:52.390Z,ns_1@db2.lan:ns_config_rep<0.2526.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@172.19.0.4'
[ns_server:debug,2025-05-15T18:52:55.272Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:52:55.273Z,ns_1@db2.lan:<0.18187.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:52:55.275Z,ns_1@db2.lan:<0.18189.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 777499, disk size is 6436798
[ns_server:debug,2025-05-15T18:52:55.275Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:52:55.275Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:52:55.290Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:52:55.295Z,ns_1@db2.lan:<0.18190.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:52:55.296Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:52:55.296Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:52:58.624Z,ns_1@db2.lan:ns_config_rep<0.2526.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@db3.lan'
[ns_server:debug,2025-05-15T18:53:22.386Z,ns_1@db2.lan:tombstone_keeper<0.277.0>:tombstone_keeper:handle_call:49]Refreshed with timestamps [{ns_config_purge_ts,63914554042},
                           {ns_config_vclock_ts,63914554102}]
[ns_server:debug,2025-05-15T18:53:22.387Z,ns_1@db2.lan:ns_config_rep<0.2526.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 80 us
[ns_server:debug,2025-05-15T18:53:22.387Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: ns_config_vclock_ts, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                        69})
63914554102
[ns_server:debug,2025-05-15T18:53:22.387Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: ns_config_purger, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                     69})
'ns_1@172.19.0.4'
[ns_server:debug,2025-05-15T18:53:22.388Z,ns_1@db2.lan:ns_config_rep<0.2526.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([{metakv,
                                   <<"/indexing/ddl/commandToken/create/17514355370907628042/0">>},
                               {metakv,
                                   <<"/indexing/rebalance/RebalanceToken">>}]..)
[ns_server:debug,2025-05-15T18:53:22.389Z,ns_1@db2.lan:ns_config_rep<0.2526.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 4 us
[ns_server:debug,2025-05-15T18:53:22.395Z,ns_1@db2.lan:ns_config_rep<0.2526.0>:ns_config_rep:handle_call:149]Got full synchronization request from 'ns_1@172.19.0.4'
[ns_server:debug,2025-05-15T18:53:22.395Z,ns_1@db2.lan:ns_config_rep<0.2526.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 13 us
[ns_server:debug,2025-05-15T18:53:22.419Z,ns_1@db2.lan:tombstone_keeper<0.277.0>:tombstone_keeper:handle_call:49]Refreshed with timestamps [{ns_config_purge_ts,63914554102},
                           {ns_config_vclock_ts,63914554102}]
[ns_server:debug,2025-05-15T18:53:22.419Z,ns_1@db2.lan:chronicle_kv_log<0.2485.0>:chronicle_kv_log:log:59]update (key: ns_config_purge_ts, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                       70})
63914554102
[ns_server:debug,2025-05-15T18:53:22.420Z,ns_1@db2.lan:tombstone_agent<0.2547.0>:tombstone_agent:purge:182]Purged 2 ns_config tombstone(s) up to timestamp 63914554102. Tombstones:
[{metakv,<<"/indexing/rebalance/RebalanceToken">>},{metakv,<<"/indexing/ddl/commandToken/create/17514355370907628042/0">>}]
[ns_server:info,2025-05-15T18:53:24.764Z,ns_1@db2.lan:ns_config_rep<0.2526.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@172.19.0.4'
[ns_server:debug,2025-05-15T18:53:25.277Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:53:25.278Z,ns_1@db2.lan:<0.19538.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:53:25.279Z,ns_1@db2.lan:<0.19540.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 777499, disk size is 6436798
[ns_server:debug,2025-05-15T18:53:25.279Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:53:25.279Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:53:25.296Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:53:25.301Z,ns_1@db2.lan:<0.19541.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:53:25.301Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:53:25.301Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:53:31.668Z,ns_1@db2.lan:ns_config_rep<0.2526.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@172.19.0.4'
[ns_server:debug,2025-05-15T18:53:38.223Z,ns_1@db2.lan:ldap_auth_cache<0.2433.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2025-05-15T18:53:55.282Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:53:55.283Z,ns_1@db2.lan:<0.20897.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:53:55.285Z,ns_1@db2.lan:<0.20899.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 777499, disk size is 6436798
[ns_server:debug,2025-05-15T18:53:55.285Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:53:55.286Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:53:55.307Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:53:55.314Z,ns_1@db2.lan:<0.20901.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:53:55.315Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:53:55.315Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:54:18.256Z,ns_1@db2.lan:ns_config_rep<0.2526.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@db3.lan'
[ns_server:debug,2025-05-15T18:54:25.286Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:54:25.287Z,ns_1@db2.lan:<0.22249.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:54:25.289Z,ns_1@db2.lan:<0.22251.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 777499, disk size is 6436798
[ns_server:debug,2025-05-15T18:54:25.289Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:54:25.289Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:54:25.316Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:54:25.319Z,ns_1@db2.lan:<0.22266.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:54:25.319Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:54:25.319Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:54:37.968Z,ns_1@db2.lan:ns_config_rep<0.2526.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@172.19.0.4'
[ns_server:debug,2025-05-15T18:54:53.214Z,ns_1@db2.lan:roles_cache<0.2447.0>:active_cache:renew:238]Starting roles_cache cache renewal
[ns_server:debug,2025-05-15T18:54:53.214Z,ns_1@db2.lan:roles_cache<0.2447.0>:active_cache:renew:244]roles_cache cache renewal process originated 0 queries
[ns_server:debug,2025-05-15T18:54:53.226Z,ns_1@db2.lan:ldap_auth_cache<0.2433.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2025-05-15T18:54:55.293Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:54:55.297Z,ns_1@db2.lan:<0.23607.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:54:55.301Z,ns_1@db2.lan:<0.23609.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 777499, disk size is 6436798
[ns_server:debug,2025-05-15T18:54:55.302Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:54:55.302Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:54:55.320Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:54:55.324Z,ns_1@db2.lan:<0.23610.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:54:55.325Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:54:55.325Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:55:14.468Z,ns_1@db2.lan:ns_config_rep<0.2526.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@db3.lan'
[ns_server:debug,2025-05-15T18:55:25.303Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:55:25.304Z,ns_1@db2.lan:<0.24972.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:55:25.309Z,ns_1@db2.lan:<0.24974.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 777499, disk size is 6436798
[ns_server:debug,2025-05-15T18:55:25.309Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:55:25.309Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:55:25.325Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:55:25.328Z,ns_1@db2.lan:<0.24975.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:55:25.329Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:55:25.329Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:55:55.312Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:55:55.313Z,ns_1@db2.lan:<0.26318.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:55:55.314Z,ns_1@db2.lan:<0.26320.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 777499, disk size is 6436798
[ns_server:debug,2025-05-15T18:55:55.315Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:55:55.315Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:55:55.332Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:55:55.337Z,ns_1@db2.lan:<0.26332.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:55:55.338Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:55:55.338Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:56:08.228Z,ns_1@db2.lan:ldap_auth_cache<0.2433.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:info,2025-05-15T18:56:14.051Z,ns_1@db2.lan:ns_config_rep<0.2526.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@db3.lan'
[ns_server:debug,2025-05-15T18:56:25.319Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:56:25.326Z,ns_1@db2.lan:<0.27682.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:56:25.328Z,ns_1@db2.lan:<0.27684.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 777499, disk size is 6436798
[ns_server:debug,2025-05-15T18:56:25.329Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:56:25.329Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:56:25.343Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:56:25.348Z,ns_1@db2.lan:<0.27685.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:56:25.348Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:56:25.348Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:56:55.332Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:56:55.334Z,ns_1@db2.lan:<0.29041.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:56:55.335Z,ns_1@db2.lan:<0.29043.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 777499, disk size is 6436798
[ns_server:debug,2025-05-15T18:56:55.336Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:56:55.336Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:56:55.350Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:56:55.354Z,ns_1@db2.lan:<0.29044.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:56:55.354Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:56:55.354Z,ns_1@db2.lan:compaction_daemon<0.2762.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:56:56.812Z,ns_1@db2.lan:ns_config_rep<0.2526.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@db3.lan'
