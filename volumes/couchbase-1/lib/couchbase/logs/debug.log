[ns_server:info,2025-05-15T18:46:45.152Z,nonode@nohost:<0.154.0>:ns_server:init_logging:180]Started & configured logging
[ns_server:warn,2025-05-15T18:46:45.169Z,nonode@nohost:<0.154.0>:config_profile:load:123]Could not load profile file ("/etc/couchbase.d/config_profile") because it does not exist
[ns_server:debug,2025-05-15T18:46:45.170Z,nonode@nohost:<0.154.0>:ns_server:setup_server_profile:108]Using profile 'default': [{name,"default"},
                          {{indexer,disable_shard_affinity},true}]
[ns_server:warn,2025-05-15T18:46:45.173Z,nonode@nohost:<0.154.0>:ns_server:config_profile_continuity_checker:129]Writing config_profile '"default"' to disk.
[ns_server:info,2025-05-15T18:46:45.179Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]Static config terms:
[{error_logger_mf_dir,"/opt/couchbase/var/lib/couchbase/logs"},
 {path_config_bindir,"/opt/couchbase/bin"},
 {path_config_etcdir,"/opt/couchbase/etc/couchbase"},
 {path_config_libdir,"/opt/couchbase/lib"},
 {path_config_datadir,"/opt/couchbase/var/lib/couchbase"},
 {path_config_tmpdir,"/opt/couchbase/var/lib/couchbase/tmp"},
 {path_config_secdir,"/opt/couchbase/etc/security"},
 {nodefile,"/opt/couchbase/var/lib/couchbase/couchbase-server.node"},
 {loglevel_default,debug},
 {loglevel_couchdb,info},
 {loglevel_ns_server,debug},
 {loglevel_error_logger,debug},
 {loglevel_user,debug},
 {loglevel_menelaus,debug},
 {loglevel_ns_doctor,debug},
 {loglevel_stats,debug},
 {loglevel_rebalance,debug},
 {loglevel_cluster,debug},
 {loglevel_views,debug},
 {loglevel_mapreduce_errors,debug},
 {loglevel_xdcr,debug},
 {loglevel_access,info},
 {loglevel_cbas,debug},
 {disk_sink_opts,
     [{rotation,
          [{compress,true},
           {size,41943040},
           {num_files,10},
           {buffer_size_max,52428800}]}]},
 {disk_sink_opts_disk_json_rpc,
     [{rotation,
          [{compress,true},
           {size,41943040},
           {num_files,2},
           {buffer_size_max,52428800}]}]},
 {disk_sink_opts_disk_tls_key_log,
     [{rotation,
          [{compress,true},
           {size,10485760},
           {num_files,1},
           {buffer_size_max,13107200}]}]},
 {net_kernel_verbosity,10}]
[ns_server:warn,2025-05-15T18:46:45.179Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter error_logger_mf_dir, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.179Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter path_config_bindir, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.179Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter path_config_etcdir, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.179Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter path_config_libdir, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.179Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter path_config_datadir, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.179Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter path_config_tmpdir, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.179Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter path_config_secdir, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.179Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter nodefile, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.179Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_default, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.179Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_couchdb, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.179Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_ns_server, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.179Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_error_logger, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.179Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_user, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.179Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_menelaus, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.180Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_ns_doctor, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.180Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_stats, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.180Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_rebalance, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.180Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_cluster, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.180Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_views, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.180Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_mapreduce_errors, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.180Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_xdcr, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.180Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_access, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.180Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter loglevel_cbas, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.180Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter disk_sink_opts, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.180Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter disk_sink_opts_disk_json_rpc, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.180Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter disk_sink_opts_disk_tls_key_log, which is given from command line
[ns_server:warn,2025-05-15T18:46:45.180Z,nonode@nohost:<0.154.0>:ns_server:log_pending:30]not overriding parameter net_kernel_verbosity, which is given from command line
[ns_server:info,2025-05-15T18:46:45.193Z,nonode@nohost:dist_manager<0.215.0>:dist_manager:read_address_config_from_path:83]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip_start"
[ns_server:info,2025-05-15T18:46:45.193Z,nonode@nohost:dist_manager<0.215.0>:dist_manager:read_address_config_from_path:83]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2025-05-15T18:46:45.193Z,nonode@nohost:dist_manager<0.215.0>:dist_manager:init:180]ip config not found. Looks like we're brand new node
[ns_server:info,2025-05-15T18:46:45.200Z,nonode@nohost:dist_manager<0.215.0>:dist_manager:bringup:246]Attempting to bring up net_kernel with name 'ns_1@cb.local'
[error_logger:info,2025-05-15T18:46:45.216Z,nonode@nohost:ssl_dist_admin_sup<0.218.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_admin_sup}
    started: [{pid,<0.219.0>},
              {id,ssl_pem_cache_dist},
              {mfargs,{ssl_pem_cache,start_link_dist,[[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.216Z,nonode@nohost:ssl_dist_admin_sup<0.218.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_admin_sup}
    started: [{pid,<0.220.0>},
              {id,ssl_dist_manager},
              {mfargs,{ssl_manager,start_link_dist,[[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.217Z,nonode@nohost:ssl_dist_sup<0.217.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_sup}
    started: [{pid,<0.218.0>},
              {id,ssl_dist_admin_sup},
              {mfargs,{ssl_dist_admin_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:45.222Z,nonode@nohost:tls_dist_sup<0.221.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_sup}
    started: [{pid,<0.222.0>},
              {id,dist_tls_connection_sup},
              {mfargs,{tls_connection_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:45.226Z,nonode@nohost:tls_dist_server_sup<0.223.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_server_sup}
    started: [{pid,<0.224.0>},
              {id,dist_ssl_listen_tracker_sup},
              {mfargs,{ssl_listen_tracker_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:45.226Z,nonode@nohost:tls_dist_server_sup<0.223.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_server_sup}
    started: [{pid,<0.225.0>},
              {id,dist_tls_server_session_ticket},
              {mfargs,{tls_server_session_ticket_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:45.226Z,nonode@nohost:tls_dist_server_sup<0.223.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_server_sup}
    started: [{pid,<0.226.0>},
              {id,dist_ssl_upgrade_server_session_cache_sup},
              {mfargs,
                  {ssl_upgrade_server_session_cache_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:45.226Z,nonode@nohost:tls_dist_sup<0.221.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_sup}
    started: [{pid,<0.223.0>},
              {id,tls_dist_server_sup},
              {mfargs,{tls_dist_server_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:45.226Z,nonode@nohost:ssl_dist_sup<0.217.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_sup}
    started: [{pid,<0.221.0>},
              {id,tls_dist_sup},
              {mfargs,{tls_dist_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:45.226Z,nonode@nohost:net_sup<0.216.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.217.0>},
              {id,ssl_dist_sup},
              {mfargs,{ssl_dist_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:45.226Z,nonode@nohost:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Starting cb_dist with config [{config_vsn,2}]
[error_logger:info,2025-05-15T18:46:45.227Z,nonode@nohost:net_sup<0.216.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.227.0>},
              {id,cb_dist},
              {mfargs,{cb_dist,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.229Z,nonode@nohost:net_sup<0.216.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.228.0>},
              {id,auth},
              {mfargs,{auth,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,2000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:45.230Z,nonode@nohost:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Initial protos: [{external,inet_tcp_dist}], required protos: [{external,
                                                                        inet_tcp_dist}]
[ns_server:debug,2025-05-15T18:46:45.231Z,nonode@nohost:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Starting {external,inet_tcp_dist} listener on 21100...
[ns_server:debug,2025-05-15T18:46:45.232Z,nonode@nohost:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Started listener: inet_tcp_dist
[ns_server:debug,2025-05-15T18:46:45.290Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated server ssl_dist_opts (keep_secrets) - false
[ns_server:debug,2025-05-15T18:46:45.290Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Started acceptor inet_tcp_dist: <0.231.0>
[ns_server:debug,2025-05-15T18:46:45.290Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Ensure config is going to change listeners. Will be stopped: [], will be started: [], will be restarted: []
[error_logger:info,2025-05-15T18:46:45.291Z,ns_1@cb.local:net_sup<0.216.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.229.0>},
              {id,net_kernel},
              {mfargs,{net_kernel,start_link,
                                  [#{clean_halt => false,
                                     name => 'ns_1@cb.local',
                                     name_domain => longnames,
                                     net_tickintensity => 4,
                                     net_ticktime => 60,
                                     supervisor => net_sup_dynamic}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,2000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.291Z,ns_1@cb.local:kernel_sup<0.49.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,kernel_sup}
    started: [{pid,<0.216.0>},
              {id,net_sup_dynamic},
              {mfargs,
                  {erl_distribution,start_link,
                      [#{clean_halt => false,name => 'ns_1@cb.local',
                         name_domain => longnames,net_tickintensity => 4,
                         net_ticktime => 60,supervisor => net_sup_dynamic}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,60000},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:45.292Z,ns_1@cb.local:dist_manager<0.215.0>:dist_manager:configure_net_kernel:302]Set net_kernel vebosity to 10 -> 0
[ns_server:info,2025-05-15T18:46:45.294Z,ns_1@cb.local:dist_manager<0.215.0>:dist_manager:save_node:160]saving node name '"ns_1@cb.local"' to "/opt/couchbase/var/lib/couchbase/couchbase-server.node"
[ns_server:debug,2025-05-15T18:46:45.296Z,ns_1@cb.local:dist_manager<0.215.0>:dist_manager:bringup:269]Attempted to save node name to disk: ok
[ns_server:debug,2025-05-15T18:46:45.296Z,ns_1@cb.local:dist_manager<0.215.0>:dist_manager:wait_for_node:276]Waiting for connection to node 'babysitter_of_ns_1@cb.local' to be established
[error_logger:info,2025-05-15T18:46:45.296Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{connect,normal,'babysitter_of_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:46:45.302Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'babysitter_of_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:46:45.304Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.2327127311.3650355203.215446>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:46:45.305Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.2327127311.3650355203.215446>,
                                  inet_tcp_dist,<0.233.0>,
                                  #Ref<0.2327127311.3650355203.215449>}
[ns_server:debug,2025-05-15T18:46:45.353Z,ns_1@cb.local:dist_manager<0.215.0>:dist_manager:wait_for_node:288]Observed node 'babysitter_of_ns_1@cb.local' to come up
[ns_server:info,2025-05-15T18:46:45.356Z,ns_1@cb.local:dist_manager<0.215.0>:dist_manager:save_address_config:147]Deleting irrelevant ip file "/opt/couchbase/var/lib/couchbase/ip_start": {error,
                                                                          enoent}
[ns_server:info,2025-05-15T18:46:45.357Z,ns_1@cb.local:dist_manager<0.215.0>:dist_manager:save_address_config:148]saving ip config to "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2025-05-15T18:46:45.362Z,ns_1@cb.local:dist_manager<0.215.0>:dist_manager:save_address_config:151]Persisted the address successfully
[error_logger:info,2025-05-15T18:46:45.370Z,ns_1@cb.local:root_sup<0.214.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,root_sup}
    started: [{pid,<0.215.0>},
              {id,dist_manager},
              {mfargs,{dist_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.378Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.236.0>},
              {id,local_tasks},
              {mfargs,{local_tasks,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:45.384Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:log_os_info:start_link:19]OS type: {unix,linux} Version: {6,10,14}
Runtime info: [{otp_release,"25"},
               {erl_version,"13.2.2.3"},
               {erl_version_long,
                   "Erlang/OTP 25 [erts-13.2.2.3] [source-15104f9619] [64-bit] [smp:10:10] [ds:10:10:10] [async-threads:16] [jit]\n"},
               {system_arch_raw,"aarch64-unknown-linux-gnu"},
               {system_arch,"aarch64-unknown-linux-gnu"},
               {localtime,{{2025,5,15},{18,46,45}}},
               {memory,
                   [{total,44122360},
                    {processes,9962912},
                    {processes_used,9958744},
                    {system,34159448},
                    {atom,540873},
                    {atom_used,522143},
                    {binary,130168},
                    {code,9676526},
                    {ets,2639600}]},
               {loaded,
                   [ns_info,log_os_info,local_tasks,restartable,
                    ns_server_cluster_sup,ns_cluster,dist_util,ns_node_disco,
                    crypto,inet_tcp,re,auth,tls_dist_server_sup,tls_dist_sup,
                    ssl_dist_admin_sup,ssl_dist_sup,inet_tls_dist,
                    inet_tcp_dist,cb_epmd,gen_udp,inet_hosts,dist_manager,
                    root_sup,cb_dist,path_config,config_profile,
                    ns_server_stats,calendar,ale_default_formatter,
                    'ale_logger-tls_key','ale_logger-metakv',
                    'ale_logger-rebalance','ale_logger-chronicle',
                    'ale_logger-ns_server_trace','ale_logger-menelaus',
                    'ale_logger-stats','ale_logger-json_rpc',
                    'ale_logger-access','ale_logger-ns_server',
                    'ale_logger-user','ale_logger-ns_doctor',
                    'ale_logger-cluster','ale_logger-xdcr',erl_bits,
                    otp_internal,cb_log_counter_sink,ns_log_sink,
                    ale_disk_sink,misc,couch_util,ns_server,esaml_util,esaml,
                    ale_error_logger_handler,cpu_sup,timer,filelib,memsup,
                    disksup,os_mon,unicode_util,string,io,release_handler,
                    alarm_handler,sasl,httpd_sup,httpc_handler_sup,
                    httpc_cookie,inets_trace,httpc_manager,httpc,
                    httpc_profile_sup,httpc_sup,inets_sup,inets_app,ssl,
                    lhttpc_manager,lhttpc_sup,lhttpc,
                    dtls_server_session_cache_sup,dtls_listener_sup,
                    dtls_server_sup,dtls_connection_sup,dtls_sup,
                    ssl_upgrade_server_session_cache_sup,
                    ssl_server_session_cache_sup,
                    tls_server_session_ticket_sup,ssl_listen_tracker_sup,
                    tls_server_sup,tls_connection_sup,tls_sup,
                    ssl_connection_sup,tls_client_ticket_store,
                    ssl_client_session_cache_db,ssl_config,ssl_manager,
                    ssl_pkix_db,ssl_pem_cache,ssl_admin_sup,ssl_sup,
                    logger_h_common,logger_std_h,ssl_logger,ssl_app,
                    'ale_logger-trace_logger','ale_logger-ale_logger',
                    'ale_logger-error_logger',beam_opcodes,beam_dict,beam_asm,
                    beam_z,beam_flatten,beam_trim,beam_clean,beam_block,
                    beam_utils,beam_jump,beam_a,beam_validator,
                    beam_ssa_codegen,beam_ssa_pre_codegen,beam_ssa_throw,
                    beam_ssa_dead,beam_call_types,beam_types,beam_ssa_type,
                    beam_ssa_bc_size,beam_ssa_opt,beam_ssa_bsm,beam_ssa_recv,
                    beam_ssa_share,beam_ssa_bool,beam_ssa,beam_kernel_to_ssa,
                    v3_kernel,sys_core_bsm,sys_core_alias,erl_bifs,
                    cerl_clauses,sys_core_fold,sys_core_inline,cerl_trees,
                    core_lib,cerl,sets,v3_core,erl_expand_records,sofs,
                    erl_internal,ordsets,compile,dynamic_compile,ale_utils,
                    io_lib_pretty,io_lib_format,io_lib,ale_codegen,dict,ale,
                    ale_dynamic_sup,ale_sup,ale_app,ns_bootstrap,child_erlang,
                    raw_file_io,orddict,c,erl_signal_handler,
                    logger_handler_watcher,logger_sup,kernel_refc,
                    kernel_config,user_io,user_sup,supervisor_bridge,
                    standard_error,erpc,global_group,erl_distribution,maps,
                    rand,net_kernel,global,rpc,epp,inet_parse,inet,inet_udp,
                    inet_config,inet_db,unicode,os,gb_trees,gb_sets,binary,
                    beam_lib,peer,erl_anno,erl_features,proplists,erl_scan,
                    queue,logger_olp,logger_proxy,error_handler,application,
                    erl_lint,application_controller,application_master,file,
                    file_io_server,filename,gen_event,kernel,ets,heart,gen,
                    logger_server,gen_server,logger_backend,logger_config,
                    file_server,logger_simple_h,logger_filters,logger,
                    error_logger,lists,code,code_server,proc_lib,erl_eval,
                    supervisor,erl_parse,persistent_term,counters,atomics,
                    erts_dirty_process_signal_handler,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,prim_net,prim_socket,
                    socket_registry,zlib,prim_file,prim_inet,prim_eval,
                    prim_buffer,init,erl_init,erts_code_purger]},
               {applications,
                   [{stdlib,"ERTS  CXC 138 10","4.3.1.2"},
                    {public_key,"Public key infrastructure","1.13.3.1"},
                    {crypto,"CRYPTO","5.1.4.1"},
                    {lhttpc,"Lightweight HTTP Client","1.3.0"},
                    {asn1,"The Erlang ASN1 compiler version 5.0.21","5.0.21"},
                    {sasl,"SASL  CXC 138 11","4.2"},
                    {ssl,"Erlang/OTP SSL application","10.9.1.2"},
                    {esaml,"SAML Server Provider library for erlang","4.4.0"},
                    {ale,"Another Logger for Erlang","0.0.0"},
                    {xmerl,"XML parser","1.3.31.1"},
                    {inets,"INETS  CXC 138 49","8.3.1.2"},
                    {os_mon,"CPO  CXC 138 46","2.8.2"},
                    {ns_server,"Couchbase server","7.6.2-3721-enterprise"},
                    {kernel,"ERTS  CXC 138 10","8.5.4.2"}]},
               {pre_loaded,
                   [persistent_term,counters,atomics,
                    erts_dirty_process_signal_handler,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,prim_net,prim_socket,
                    socket_registry,zlib,prim_file,prim_inet,prim_eval,
                    prim_buffer,init,erl_init,erts_code_purger]},
               {process_count,159},
               {node,'ns_1@cb.local'},
               {nodes,[]},
               {registered,
                   [ssl_connection_sup,standard_error,net_kernel,
                    ssl_upgrade_server_session_cache_sup,kernel_refc,
                    'sink-disk_debug',ale_sup,httpd_sup,
                    ssl_upgrade_server_session_cache_sup_dist,
                    tls_client_ticket_store,dtls_server_session_cache_sup,
                    'sink-disk_trace',inets_sup,tls_sup,erts_code_purger,
                    'sink-disk_tls_key_log',dist_manager,ssl_pem_cache_dist,
                    cb_dist,ale,erl_prim_loader,global_group,cpu_sup,init,
                    application_controller,ns_server_cluster_sup,
                    httpc_manager,erl_signal_server,ssl_dist_sup,
                    tls_dist_connection_sup,ssl_sup,esaml,logger_sup,
                    'sink-disk_error',tls_server_session_ticket_sup_dist,
                    ssl_listen_tracker_sup,os_mon_sup,'sink-disk_reports',
                    dtls_sup,rex,tls_dist_sup,tls_server_session_ticket_sup,
                    'sink-disk_metakv',disksup,ssl_manager,'sink-ns_log',
                    kernel_safe_sup,logger_proxy,'sink-disk_default',
                    ale_dynamic_sup,ssl_listen_tracker_sup_dist,auth,
                    logger_std_h_ssl_handler,standard_error_sup,
                    'sink-disk_stats',httpc_profile_sup,kernel_sup,
                    ssl_admin_sup,inet_db,user,sasl_safe_sup,release_handler,
                    'sink-disk_access_int',ssl_server_session_cache_sup,
                    root_sup,logger,alarm_handler,lhttpc_manager,
                    dtls_server_sup,logger_handler_watcher,httpc_sup,
                    tls_dist_server_sup,global_group_check,
                    'sink-disk_json_rpc',local_tasks,memsup,ssl_pem_cache,
                    'sink-disk_xdcr',dtls_connection_sup,global_name_server,
                    file_server_2,httpc_handler_sup,sasl_sup,
                    tls_connection_sup,ssl_manager_dist,lhttpc_sup,
                    'sink-disk_access',dtls_listener_sup,ale_stats_events,
                    socket_registry,code_server,net_sup,esaml_ets_table_owner,
                    'sink-cb_log_counter',tls_server_sup,ssl_dist_admin_sup]},
               {cookie,nocookie},
               {wordsize,8},
               {wall_clock,4}]
[ns_server:info,2025-05-15T18:46:45.404Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:log_os_info:start_link:21]Manifest:
["<?xml version=\"1.0\" encoding=\"UTF-8\"?>","<manifest>",
 "  <remote name=\"blevesearch\" fetch=\"https://github.com/blevesearch/\"/>",
 "  <remote name=\"couchbase\" fetch=\"https://github.com/couchbase/\" review=\"review.couchbase.org\"/>",
 "  <remote name=\"couchbase-priv\" fetch=\"ssh://git@github.com/couchbase/\" review=\"review.couchbase.org\"/>",
 "  <remote name=\"couchbasedeps\" fetch=\"https://github.com/couchbasedeps/\" review=\"review.couchbase.org\"/>",
 "  <remote name=\"couchbaselabs\" fetch=\"https://github.com/couchbaselabs/\" review=\"review.couchbase.org\"/>",
 "  ","  <default remote=\"couchbase\" revision=\"master\"/>","  ",
 "  <project name=\"HdrHistogram_c\" path=\"third_party/HdrHistogram_c\" remote=\"couchbasedeps\" revision=\"9ead6b88adbf8d6131e5ae7a3a699c477a3b4195\" groups=\"kv\"/>",
 "  <project name=\"analytics-dcp-client\" path=\"analytics/java-dcp-client\" revision=\"081d9934d4a28b4abdadcd13891792ea423416c0\" upstream=\"trinity\" dest-branch=\"trinity\" groups=\"notdefault,enterprise,analytics\"/>",
 "  <project name=\"asterixdb\" path=\"analytics/asterixdb\" revision=\"6a10f3f81db977c706447ece476b487cbe56414c\" groups=\"notdefault,enterprise,analytics\"/>",
 "  <project name=\"backup\" remote=\"couchbase-priv\" revision=\"192d7500ba2a7b5281d2c61af126c8027bbb858d\" groups=\"backup,notdefault,enterprise\"/>",
 "  <project name=\"build\" path=\"cbbuild\" revision=\"cedf9d4ec929eac7e61f8e86488aeac5402c8563\" groups=\"notdefault,build\">",
 "    <annotation name=\"RELEASE\" value=\"trinity\"/>",
 "    <annotation name=\"PRODUCT\" value=\"couchbase-server\"/>",
 "    <annotation name=\"BLD_NUM\" value=\"3721\"/>",
 "    <annotation name=\"VERSION\" value=\"7.6.2\"/>","  </project>",
 "  <project name=\"cbas\" path=\"goproj/src/github.com/couchbase/cbas\" remote=\"couchbase-priv\" revision=\"2db6eb59fd5af47a1ce81d53c8f4e58c7a14df3a\" groups=\"notdefault,enterprise,analytics\"/>",
 "  <project name=\"cbas-core\" path=\"analytics\" remote=\"couchbase-priv\" revision=\"2c1d23ee3aba4c80196d9d94ceaca3917b8ea8a7\" upstream=\"7.6.2\" dest-branch=\"7.6.2\" groups=\"notdefault,enterprise,analytics\"/>",
 "  <project name=\"cbas-ui\" revision=\"704db180d01de15f70cacc9fc11c5d8d8d4ff965\" groups=\"notdefault,enterprise,analytics\"/>",
 "  <project name=\"cbauth\" path=\"goproj/src/github.com/couchbase/cbauth\" revision=\"a9992170165a1d330cb5a9918a29d5bd417c5e46\" groups=\"backup\"/>",
 "  <project name=\"cbbs\" remote=\"couchbase-priv\" revision=\"f1d0272decc7f1b445b08e56e1f75e99f743aa90\" groups=\"backup,notdefault,enterprise\"/>",
 "  <project name=\"cbft\" revision=\"69d32cca4a8eca6e5aad5dad689795ab72ecdd6e\"/>",
 "  <project name=\"cbftx\" remote=\"couchbase-priv\" revision=\"258e3829db59f06a202ea2435c776a351a590eba\" groups=\"notdefault,enterprise\"/>",
 "  <project name=\"cbgt\" revision=\"b7dd01a11c5c56fbca88b9b950d8eca4dacce36f\"/>",
 "  <project name=\"cbsummary\" path=\"goproj/src/github.com/couchbase/cbsummary\" revision=\"fb656c91554a97318c44f58e3cc7f166f1eef4fc\"/>",
 "  <project name=\"chronicle\" path=\"ns_server/deps/chronicle\" revision=\"8d1feeb0d8b15e2b6a4c1a417addfd159b422a71\"/>",
 "  <project name=\"client_golang\" path=\"godeps/src/github.com/prometheus/client_golang\" remote=\"couchbasedeps\" revision=\"2e1c4818ccfdcf953ce399cadad615ff2bed968c\" upstream=\"refs/tags/v1.12.1\" dest-branch=\"refs/tags/v1.12.1\"/>",
 "  <project name=\"client_model\" path=\"godeps/src/github.com/prometheus/client_model\" remote=\"couchbasedeps\" revision=\"6dc836ede0b5b08c61893c3ffeb474498b18bb83\"/>",
 "  <project name=\"clog\" path=\"godeps/src/github.com/couchbase/clog\" revision=\"f935d1fdfc36541b505cf86fea4822e4067f9c39\"/>",
 "  <project name=\"common\" path=\"godeps/src/github.com/prometheus/common\" remote=\"couchbasedeps\" revision=\"902cb39e6c079571d32c2db8da220da13c11b562\" upstream=\"refs/tags/v0.33.0\" dest-branch=\"refs/tags/v0.33.0\"/>",
 "  <project name=\"couchbase-cli\" revision=\"941f6d7bbac8f8a42870c3f5459376b9f19ef1fd\" groups=\"kv\"/>",
 "  <project name=\"couchdb\" revision=\"3e5b8f248d77dd9317b36b50eed2567bcfb5f4cf\" dest-branch=\"unstable\"/>",
 "  <project name=\"couchdbx-app\" revision=\"702647dd015e7443de9cdb789806351774e85463\" groups=\"notdefault,packaging\"/>",
 "  <project name=\"couchstore\" revision=\"ce7305bab3feb64bd2504f34d24a1419008e8bda\" groups=\"kv\"/>",
 "  <project name=\"crypto\" path=\"godeps/src/golang.org/x/crypto\" remote=\"couchbasedeps\" revision=\"eb61739cd99fb244c7cd188d3c5bae54824e781d\" upstream=\"refs/tags/v0.15.0\" dest-branch=\"refs/tags/v0.15.0\"/>",
 "  <project name=\"docloader\" path=\"goproj/src/github.com/couchbase/docloader\" revision=\"cf3254d7dfb042192c9a23bd2e64a281c32a29d8\"/>",
 "  <project name=\"errors\" path=\"godeps/src/github.com/pkg/errors\" remote=\"couchbasedeps\" revision=\"30136e27e2ac8d167177e8a583aa4c3fea5be833\"/>",
 "  <project name=\"eventing\" path=\"goproj/src/github.com/couchbase/eventing\" revision=\"047b756132464b8f756cc35e02a15b5f498f80d5\" dest-branch=\"unstable\" groups=\"notdefault,enterprise\"/>",
 "  <project name=\"eventing-ee\" path=\"goproj/src/github.com/couchbase/eventing-ee\" remote=\"couchbase-priv\" revision=\"5425f180a0756868524081f889ab224cfc10b70d\" dest-branch=\"unstable\" groups=\"notdefault,enterprise\"/>",
 "  <project name=\"flatbuffers\" path=\"godeps/src/github.com/google/flatbuffers\" remote=\"couchbasedeps\" revision=\"1a8968225130caeddd16e227678e6f8af1926303\"/>",
 "  <project name=\"forestdb\" revision=\"9efe6d75d7d61e742af70fb47fe97ad1d04ba86f\" groups=\"backup\"/>",
 "  <project name=\"geocouch\" revision=\"68f3b9d36630682d17ca5232770f1693b9b8fa18\"/>",
 "  <project name=\"go-couchbase\" path=\"goproj/src/github.com/couchbase/go-couchbase\" revision=\"959eaf944140a6c660990f38b1db310ddd6d8e42\" groups=\"backup\"/>",
 "  <project name=\"go-metrics\" path=\"godeps/src/github.com/rcrowley/go-metrics\" remote=\"couchbasedeps\" revision=\"cf1acfcdf4751e0554ffa765d03e479ec491cad6\"/>",
 "  <project name=\"go_json\" path=\"goproj/src/github.com/couchbase/go_json\" revision=\"d6e17ad2b9a218e82569e09b761c226fa8df726a\"/>",
 "  <project name=\"gocb\" path=\"godeps/src/github.com/couchbase/gocb/v2\" revision=\"40020eef484873e507745107edbf99c421927a93\" upstream=\"refs/tags/v2.2.5\" dest-branch=\"refs/tags/v2.2.5\"/>",
 "  <project name=\"gocbcore\" path=\"godeps/src/github.com/couchbase/gocbcore/v9\" revision=\"0ece206041d8cf5f5fcd919767446603691bdb69\" upstream=\"refs/tags/v9.1.6\" dest-branch=\"refs/tags/v9.1.6\"/>",
 "  <project name=\"godbc\" path=\"goproj/src/github.com/couchbase/godbc\" revision=\"2d3ecc3de903a5e4d0bc9181adedb5e637f83435\"/>",
 "  <project name=\"gojsonsm\" path=\"godeps/src/github.com/couchbaselabs/gojsonsm\" remote=\"couchbaselabs\" revision=\"8db06ae62940835d35db4de075bd68f0e00ea6b7\" groups=\"bsl\"/>",
 "  <project name=\"golang\" remote=\"couchbaselabs\" revision=\"4dd1b189981c94835b61c1607ca765e88604ce5a\" groups=\"kv\"/>",
 "  <project name=\"golang-pkg-pcre\" path=\"godeps/src/github.com/glenn-brown/golang-pkg-pcre\" remote=\"couchbasedeps\" revision=\"48bb82a8b8ceea98f4e97825b43870f6ba1970d6\"/>",
 "  <project name=\"golang-snappy\" path=\"godeps/src/github.com/golang/snappy\" remote=\"couchbasedeps\" revision=\"723cc1e459b8eea2dea4583200fd60757d40097a\"/>",
 "  <project name=\"golang-tools\" path=\"godeps/src/golang.org/x/tools\" remote=\"couchbasedeps\" revision=\"a28dfb48e06b2296b66678872c2cb638f0304f20\"/>",
 "  <project name=\"golang_protobuf_extensions\" path=\"godeps/src/github.com/matttproud/golang_protobuf_extensions\" remote=\"couchbasedeps\" revision=\"c182affec369e30f25d3eb8cd8a478dee585ae7d\"/>",
 "  <project name=\"gomemcached\" path=\"goproj/src/github.com/couchbase/gomemcached\" revision=\"689b8f03386ba2e7bac304bfd3a525b1e1427675\" groups=\"backup\"/>",
 "  <project name=\"gometa\" path=\"goproj/src/github.com/couchbase/gometa\" revision=\"816f7d6346c9fc5473c4a11e3efe9ed29a2f7f72\"/>",
 "  <project name=\"goutils\" path=\"goproj/src/github.com/couchbase/goutils\" revision=\"30adfca73d8113b5b217097414d7c3adeeef849a\" groups=\"bsl\"/>",
 "  <project name=\"goxdcr\" path=\"goproj/src/github.com/couchbase/goxdcr\" revision=\"4c570a31e5a6f3e087e147edf781022352497f64\" groups=\"bsl\"/>",
 "  <project name=\"gsl-lite\" path=\"third_party/gsl-lite\" remote=\"couchbasedeps\" revision=\"e1c381746c2625a76227255f999ae9f14a062208\" upstream=\"refs/tags/v0.38.1\" dest-branch=\"refs/tags/v0.38.1\" groups=\"kv\"/>",
 "  <project name=\"hebrew\" remote=\"couchbase-priv\" revision=\"c57616b187889a5318688f49817ccaceb9c098b9\" groups=\"notdefault,enterprise\"/>",
 "  <project name=\"indexing\" path=\"goproj/src/github.com/couchbase/indexing\" revision=\"7e924978fef8498113ae2d6e2be8ccd27da70d2d\" groups=\"bsl\"/>",
 "  <project name=\"jsonschema\" path=\"godeps/src/github.com/santhosh-tekuri/jsonschema\" remote=\"couchbasedeps\" revision=\"137f44a49015e5060a447c331aa37de6e0f50267\"/>",
 "  <project name=\"kv_engine\" revision=\"cfff435cf5fe6efcf1a18aa5580993fb8a4b2010\" groups=\"kv,bsl\"/>",
 "  <project name=\"libcouchbase\" revision=\"684931e59cd87e0c6292e8142c2b18897be5b10c\"/>",
 "  <project name=\"magma\" remote=\"couchbase-priv\" revision=\"86c2233ab8780e7aa71e0199bb957dcda2cf6cd1\" groups=\"notdefault,enterprise,kv_ee\"/>",
 "  <project name=\"mux\" path=\"godeps/src/github.com/gorilla/mux\" remote=\"couchbasedeps\" revision=\"043ee6597c29786140136a5747b6a886364f5282\"/>",
 "  <project name=\"n1fty\" path=\"goproj/src/github.com/couchbase/n1fty\" revision=\"a1fc533c18e5094ce75262c9e711d7189d256cd2\" groups=\"bsl\"/>",
 "  <project name=\"nitro\" path=\"goproj/src/github.com/couchbase/nitro\" revision=\"b70d849f0207f7cfe7ebf32b2db35b534929e041\" groups=\"bsl\"/>",
 "  <project name=\"ns_server\" revision=\"6954b533143cf0a8f906ef9086a8337ee50004a6\" groups=\"bsl\"/>",
 "  <project name=\"participle\" path=\"godeps/src/github.com/alecthomas/participle\" remote=\"couchbasedeps\" revision=\"d638c6e1953ed899e05a34da3935146790c60e46\"/>",
 "  <project name=\"perks\" path=\"godeps/src/github.com/beorn7/perks\" remote=\"couchbasedeps\" revision=\"37c8de3658fcb183f997c4e13e8337516ab753e6\" upstream=\"refs/tags/v1.0.1\" dest-branch=\"refs/tags/v1.0.1\"/>",
 "  <project name=\"phosphor\" revision=\"c0a034fe407eec4723f2e01db2d72762efdbc276\" groups=\"bsl,kv\"/>",
 "  <project name=\"pkcs8\" path=\"godeps/src/github.com/youmark/pkcs8\" remote=\"couchbasedeps\" revision=\"1be2e3e5546da8a58903ff4adcfab015022538ea\" upstream=\"refs/tags/v1.1\" dest-branch=\"refs/tags/v1.1\"/>",
 "  <project name=\"plasma\" path=\"goproj/src/github.com/couchbase/plasma\" remote=\"couchbase-priv\" revision=\"34f14cf1d8cc543932e60d9780cc13d48fa7ea5c\" groups=\"bsl,notdefault,enterprise\"/>",
 "  <project name=\"platform\" revision=\"a158d359293665b6251973868fdc42c3b642474c\" groups=\"bsl,kv\"/>",
 "  <project name=\"procfs\" path=\"godeps/src/github.com/prometheus/procfs\" remote=\"couchbasedeps\" revision=\"76fc8b844e3a18c31bf689e4fe7efdd5a2f41298\"/>",
 "  <project name=\"product-metadata\" revision=\"1bd027c34f33919f7005ddae0ba032a3120fe776\" groups=\"notdefault,packaging\"/>",
 "  <project name=\"product-texts\" revision=\"ec39f811376df6d18e56c81873fd565093666505\" upstream=\"master\" dest-branch=\"master\"/>",
 "  <project name=\"protobuf\" path=\"godeps/src/github.com/golang/protobuf\" remote=\"couchbasedeps\" revision=\"d04d7b157bb510b1e0c10132224b616ac0e26b17\" upstream=\"refs/tags/v1.4.2\" dest-branch=\"refs/tags/v1.4.2\"/>",
 "  <project name=\"protobuf-go\" path=\"godeps/src/google.golang.org/protobuf\" remote=\"couchbasedeps\" revision=\"32051b4f86e54c2142c7c05362c6e96ae3454a1c\" upstream=\"refs/tags/v1.28.0\" dest-branch=\"refs/tags/v1.28.0\"/>",
 "  <project name=\"query\" path=\"goproj/src/github.com/couchbase/query\" revision=\"5da4cb71df5aa00aad1e01e9a3b9d5be0c4f2769\" groups=\"bsl\"/>",
 "  <project name=\"query-ee\" path=\"goproj/src/github.com/couchbase/query-ee\" remote=\"couchbase-priv\" revision=\"6924a352019351c746fe08a2cf9a1993b54093e8\" groups=\"notdefault,enterprise\"/>",
 "  <project name=\"query-ui\" revision=\"abcc90e091c46ad74a59bb2fe768b6f09864ddbf\" groups=\"bsl\"/>",
 "  <project name=\"regulator\" path=\"goproj/src/github.com/couchbase/regulator\" remote=\"couchbase-priv\" revision=\"4ef404748ecc34fd87bdebc56074ebe99d240464\" groups=\"notdefault,enterprise\"/>",
 "  <project name=\"sigar\" revision=\"2da0c123cfb45ae39e76e730bd960db8812e3f20\" groups=\"kv\"/>",
 "  <project name=\"simdutf\" path=\"third_party/simdutf\" remote=\"couchbasedeps\" revision=\"4a212616ba23c65c7048f9604faccbff5353300f\" upstream=\"refs/tags/v3.2.14\" dest-branch=\"refs/tags/v3.2.14\" groups=\"kv\"/>",
 "  <project name=\"subjson\" revision=\"a619faccb30e43a4bc0708ee11b1b24abb349f18\" groups=\"bsl,kv\"/>",
 "  <project name=\"sys\" path=\"godeps/src/golang.org/x/sys\" remote=\"couchbasedeps\" revision=\"d36c6a25d886e7c9975d5bf247ac24887ba6da37\"/>",
 "  <project name=\"testrunner\" revision=\"b2d76b82c7d75a75ae78f787a76dc8300c427adf\" upstream=\"trinity\" dest-branch=\"trinity\"/>",
 "  <project name=\"tlm\" revision=\"0c610d8e4738567440ffb1f557dfa15bff81b99d\" upstream=\"7.6.2\" dest-branch=\"7.6.2\" groups=\"bsl,kv\">",
 "    <copyfile src=\"Build.sh\" dest=\"Build.sh\"/>",
 "    <copyfile src=\"GNUmakefile\" dest=\"GNUmakefile\"/>",
 "    <copyfile src=\"Makefile\" dest=\"Makefile\"/>",
 "    <copyfile src=\"CMakeLists.txt\" dest=\"CMakeLists.txt\"/>",
 "    <copyfile src=\"dot-clang-format\" dest=\".clang-format\"/>",
 "    <copyfile src=\"dot-clang-tidy\" dest=\".clang-tidy\"/>",
 "    <copyfile src=\"third-party-CMakeLists.txt\" dest=\"third_party/CMakeLists.txt\"/>",
 "  </project>",
 "  <project name=\"udf-api\" path=\"goproj/src/github.com/couchbase/udf-api\" revision=\"b2788ae3d412356a330b36d7f38ad2c66edb5879\"/>",
 "  <project name=\"uuid\" path=\"godeps/src/github.com/google/uuid\" remote=\"couchbasedeps\" revision=\"dec09d789f3dba190787f8b4454c7d3c936fed9e\"/>",
 "  <project name=\"vbmap\" revision=\"6cce93c4af4497d8108c3ed31b84d7139321cc82\"/>",
 "  <project name=\"voltron\" remote=\"couchbase-priv\" revision=\"19881dacfffb6d834a7aaa4a6d1925a904ea387f\" groups=\"notdefault,packaging\"/>",
 "  <project name=\"xxhash\" path=\"goproj/src/github.com/cespare/xxhash\" remote=\"couchbasedeps\" revision=\"e7a6b52374f7e2abfb8abb27249d53a1997b09a7\" upstream=\"refs/tags/v2.1.2\" dest-branch=\"refs/tags/v2.1.2\"/>",
 "</manifest>"]

[error_logger:info,2025-05-15T18:46:45.412Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.237.0>},
              {id,timeout_diag_logger},
              {mfargs,{timeout_diag_logger,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.419Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.238.0>},
              {id,ns_cookie_manager},
              {mfargs,{ns_cookie_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:45.425Z,ns_1@cb.local:chronicle_local<0.239.0>:chronicle_local:init:54]Ensure chronicle is started
[error_logger:info,2025-05-15T18:46:45.486Z,ns_1@cb.local:chronicle_sup<0.244.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.245.0>},
              {id,chronicle_events},
              {mfargs,{chronicle_events,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.487Z,ns_1@cb.local:chronicle_sup<0.244.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.246.0>},
              {id,chronicle_external_events},
              {mfargs,{gen_event,start_link,
                                 [{local,chronicle_external_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.498Z,ns_1@cb.local:chronicle_sup<0.244.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.247.0>},
              {id,chronicle_ets},
              {mfargs,{chronicle_ets,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.503Z,ns_1@cb.local:chronicle_sup<0.244.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.248.0>},
              {id,chronicle_settings},
              {mfargs,{chronicle_settings,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.511Z,ns_1@cb.local:chronicle_agent_sup<0.249.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_agent_sup}
    started: [{pid,<0.250.0>},
              {id,chronicle_snapshot_mgr},
              {mfargs,{chronicle_snapshot_mgr,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.511Z,ns_1@cb.local:chronicle_agent_sup<0.249.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_agent_sup}
    started: [{pid,<0.251.0>},
              {id,chronicle_rsm_events},
              {mfargs,{chronicle_events,start_link,[chronicle_rsm_events]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[chronicle:info,2025-05-15T18:46:45.572Z,ns_1@cb.local:chronicle_agent<0.252.0>:chronicle_storage:open_logs:180]Reading log files [{0,
                    "/opt/couchbase/var/lib/couchbase/config/chronicle/logs/0.log"}]
[chronicle:info,2025-05-15T18:46:45.579Z,ns_1@cb.local:chronicle_agent<0.252.0>:chronicle_storage:open_current_log:232]Error while opening log file "/opt/couchbase/var/lib/couchbase/config/chronicle/logs/0.log": no_header
[chronicle:info,2025-05-15T18:46:45.580Z,ns_1@cb.local:chronicle_agent<0.252.0>:chronicle_storage:create_log:280]Creating log file /opt/couchbase/var/lib/couchbase/config/chronicle/logs/0.log (high seqno = 0)
[chronicle:info,2025-05-15T18:46:45.590Z,ns_1@cb.local:chronicle_agent<0.252.0>:chronicle_agent:maybe_seed_storage:2444]Found empty storage. Seeding it with default metadata:
#{committed_seqno => 0,history_id => <<"no-history">>,peer => nonode@nohost,
  peer_id => <<>>,pending_branch => undefined,state => not_provisioned,
  term => {0,nonode@nohost}}
[error_logger:info,2025-05-15T18:46:45.591Z,ns_1@cb.local:chronicle_agent_sup<0.249.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_agent_sup}
    started: [{pid,<0.252.0>},
              {id,chronicle_agent},
              {mfargs,{chronicle_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.591Z,ns_1@cb.local:chronicle_sup<0.244.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.249.0>},
              {id,chronicle_agent_sup},
              {mfargs,{chronicle_agent_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:45.598Z,ns_1@cb.local:chronicle_sup<0.244.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.254.0>},
              {id,chronicle_secondary_sup},
              {mfargs,{chronicle_secondary_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:45.598Z,ns_1@cb.local:application_controller<0.44.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    application: chronicle
    started_at: 'ns_1@cb.local'

[ns_server:debug,2025-05-15T18:46:45.600Z,ns_1@cb.local:chronicle_local<0.239.0>:chronicle_local:init:58]Chronicle state is: not_provisioned
[ns_server:debug,2025-05-15T18:46:45.601Z,ns_1@cb.local:chronicle_local<0.239.0>:chronicle_local:provision:139]Provision chronicle on this node
[chronicle:debug,2025-05-15T18:46:45.604Z,ns_1@cb.local:chronicle_agent<0.252.0>:chronicle_agent:handle_provision:1199]Provisioning with history <<"5aab03dbecad06b50ffb474da59a00c9">>. Config:
{config,undefined,0,undefined,
        #{'ns_1@cb.local' =>
              #{id => <<"1dd5991463d519e23729dda2c80c376b">>,role => voter}},
        undefined,
        #{chronicle_config_rsm => {rsm_config,chronicle_config_rsm,[]},
          kv => {rsm_config,chronicle_kv,[]}},
        #{},undefined,
        [{<<"5aab03dbecad06b50ffb474da59a00c9">>,0}]}
[error_logger:info,2025-05-15T18:46:45.609Z,ns_1@cb.local:<0.255.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.255.0>,dynamic_supervisor}
    started: [{pid,<0.256.0>},
              {id,chronicle_leader},
              {mfargs,{chronicle_leader,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.613Z,ns_1@cb.local:chronicle_secondary_restartable_sup<0.257.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_secondary_restartable_sup}
    started: [{pid,<0.258.0>},
              {id,chronicle_status},
              {mfargs,{chronicle_status,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.618Z,ns_1@cb.local:chronicle_secondary_restartable_sup<0.257.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_secondary_restartable_sup}
    started: [{pid,<0.259.0>},
              {id,chronicle_failover},
              {mfargs,{chronicle_failover,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.618Z,ns_1@cb.local:<0.255.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.255.0>,dynamic_supervisor}
    started: [{pid,<0.257.0>},
              {id,chronicle_secondary_restartable_sup},
              {mfargs,{chronicle_secondary_restartable_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:45.621Z,ns_1@cb.local:<0.255.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.255.0>,dynamic_supervisor}
    started: [{pid,<0.260.0>},
              {id,chronicle_server},
              {mfargs,{chronicle_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[chronicle:info,2025-05-15T18:46:45.635Z,ns_1@cb.local:chronicle_config_rsm<0.264.0>:chronicle_rsm:get_incarnation:1594]No incarnation file found at '/opt/couchbase/var/lib/couchbase/config/chronicle/rsms/chronicle_config_rsm/incarnation'
[chronicle:debug,2025-05-15T18:46:45.638Z,ns_1@cb.local:chronicle_server<0.260.0>:chronicle_server:handle_register_rsm:361]Registering RSM chronicle_config_rsm with pid <0.264.0>
[error_logger:info,2025-05-15T18:46:45.638Z,ns_1@cb.local:<0.263.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.263.0>,chronicle_single_rsm_sup}
    started: [{pid,<0.264.0>},
              {id,chronicle_config_rsm},
              {mfargs,{chronicle_rsm,start_link,
                                     [chronicle_config_rsm,
                                      <<"1dd5991463d519e23729dda2c80c376b">>,
                                      chronicle_config_rsm,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.638Z,ns_1@cb.local:<0.262.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.262.0>,dynamic_supervisor}
    started: [{pid,<0.263.0>},
              {id,chronicle_config_rsm},
              {mfargs,
                  {chronicle_single_rsm_sup,start_link,
                      [chronicle_config_rsm,
                       <<"1dd5991463d519e23729dda2c80c376b">>,
                       chronicle_config_rsm,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:45.644Z,ns_1@cb.local:<0.266.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.266.0>,chronicle_single_rsm_sup}
    started: [{pid,<0.267.0>},
              {id,'kv-events'},
              {mfargs,{gen_event,start_link,[{local,'kv-events'}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[chronicle:info,2025-05-15T18:46:45.648Z,ns_1@cb.local:kv<0.268.0>:chronicle_rsm:get_incarnation:1594]No incarnation file found at '/opt/couchbase/var/lib/couchbase/config/chronicle/rsms/kv/incarnation'
[chronicle:debug,2025-05-15T18:46:45.657Z,ns_1@cb.local:chronicle_server<0.260.0>:chronicle_server:handle_register_rsm:361]Registering RSM kv with pid <0.268.0>
[error_logger:info,2025-05-15T18:46:45.657Z,ns_1@cb.local:<0.266.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.266.0>,chronicle_single_rsm_sup}
    started: [{pid,<0.268.0>},
              {id,kv},
              {mfargs,{chronicle_rsm,start_link,
                                     [kv,
                                      <<"1dd5991463d519e23729dda2c80c376b">>,
                                      chronicle_kv,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:45.657Z,ns_1@cb.local:<0.262.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.262.0>,dynamic_supervisor}
    started: [{pid,<0.266.0>},
              {id,kv},
              {mfargs,
                  {chronicle_single_rsm_sup,start_link,
                      [kv,<<"1dd5991463d519e23729dda2c80c376b">>,chronicle_kv,
                       []]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:45.657Z,ns_1@cb.local:<0.255.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.255.0>,dynamic_supervisor}
    started: [{pid,<0.261.0>},
              {id,chronicle_rsm_sup},
              {mfargs,{chronicle_rsm_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:info,2025-05-15T18:46:45.659Z,ns_1@cb.local:chronicle_local<0.239.0>:chronicle_upgrade:initialize:70]Setup initial chronicle content [{set,counters,[]},
                                 {set,auto_reprovision_cfg,
                                  [{enabled,true},{max_nodes,1},{count,0}]},
                                 {set,bucket_names,[]},
                                 {set,nodes_wanted,['ns_1@cb.local']},
                                 {set,server_groups,
                                  [[{uuid,<<"0">>},
                                    {name,<<"Group 1">>},
                                    {nodes,['ns_1@cb.local']}]]},
                                 {set,
                                  {node,'ns_1@cb.local',membership},
                                  active},
                                 {set,autocompaction,
                                  [{database_fragmentation_threshold,
                                    {30,undefined}},
                                   {view_fragmentation_threshold,
                                    {30,undefined}},
                                   {magma_fragmentation_percentage,50}]}]
[chronicle:debug,2025-05-15T18:46:45.969Z,ns_1@cb.local:chronicle_leader<0.256.0>:chronicle_leader:handle_state_timeout:608]State timeout when state is: {observer,true,false}
[chronicle:info,2025-05-15T18:46:45.970Z,ns_1@cb.local:<0.270.0>:chronicle_leader:do_election_worker:892]Starting election.
History ID: <<"5aab03dbecad06b50ffb474da59a00c9">>
Log position: {{1,'ns_1@cb.local'},1}
Peers: ['ns_1@cb.local']
Required quorum: {majority,{set,1,16,16,8,80,48,
                                {[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],
                                 []},
                                {{[],[],[],[],[],[],[],[],[],[],[],[],
                                  ['ns_1@cb.local'],
                                  [],[],[]}}}}
[chronicle:info,2025-05-15T18:46:45.970Z,ns_1@cb.local:<0.270.0>:chronicle_leader:do_election_worker:901]I'm the only peer, so I'm the leader.
[chronicle:info,2025-05-15T18:46:45.970Z,ns_1@cb.local:chronicle_leader<0.256.0>:chronicle_leader:handle_election_result:691]Going to become a leader in term {2,'ns_1@cb.local'} (history id <<"5aab03dbecad06b50ffb474da59a00c9">>)
[chronicle:debug,2025-05-15T18:46:45.973Z,ns_1@cb.local:chronicle_agent<0.252.0>:chronicle_agent:handle_establish_term:1529]Accepted term {2,'ns_1@cb.local'} in history <<"5aab03dbecad06b50ffb474da59a00c9">>
[chronicle:debug,2025-05-15T18:46:45.973Z,ns_1@cb.local:chronicle_proposer<0.271.0>:chronicle_proposer:establish_term_init:367]Going to establish term {2,'ns_1@cb.local'} (history id <<"5aab03dbecad06b50ffb474da59a00c9">>).
Quorum peers: ['ns_1@cb.local']
Metadata:
{metadata,'ns_1@cb.local',<<"1dd5991463d519e23729dda2c80c376b">>,
          <<"5aab03dbecad06b50ffb474da59a00c9">>,
          {1,'ns_1@cb.local'},
          {1,'ns_1@cb.local'},
          1,1,
          {log_entry,<<"5aab03dbecad06b50ffb474da59a00c9">>,
                     {1,'ns_1@cb.local'},
                     1,
                     {config,undefined,0,undefined,
                             #{'ns_1@cb.local' =>
                                   #{id =>
                                         <<"1dd5991463d519e23729dda2c80c376b">>,
                                     role => voter}},
                             undefined,
                             #{chronicle_config_rsm =>
                                   {rsm_config,chronicle_config_rsm,[]},
                               kv => {rsm_config,chronicle_kv,[]}},
                             #{},undefined,
                             [{<<"5aab03dbecad06b50ffb474da59a00c9">>,0}]}},
          {log_entry,<<"5aab03dbecad06b50ffb474da59a00c9">>,
                     {1,'ns_1@cb.local'},
                     1,
                     {config,undefined,0,undefined,
                             #{'ns_1@cb.local' =>
                                   #{id =>
                                         <<"1dd5991463d519e23729dda2c80c376b">>,
                                     role => voter}},
                             undefined,
                             #{chronicle_config_rsm =>
                                   {rsm_config,chronicle_config_rsm,[]},
                               kv => {rsm_config,chronicle_kv,[]}},
                             #{},undefined,
                             [{<<"5aab03dbecad06b50ffb474da59a00c9">>,0}]}},
          undefined}
[chronicle:debug,2025-05-15T18:46:45.973Z,ns_1@cb.local:chronicle_proposer<0.271.0>:chronicle_proposer:establish_term_maybe_transition:686]Established term {2,'ns_1@cb.local'} (history id <<"5aab03dbecad06b50ffb474da59a00c9">>) successfully.
Votes: ['ns_1@cb.local']
[chronicle:debug,2025-05-15T18:46:45.973Z,ns_1@cb.local:chronicle_proposer<0.271.0>:chronicle_proposer:handle_state_enter:284]Starting recovery for term {2,'ns_1@cb.local'} in history <<"5aab03dbecad06b50ffb474da59a00c9">>
[chronicle:debug,2025-05-15T18:46:45.975Z,ns_1@cb.local:chronicle_proposer<0.271.0>:chronicle_proposer:handle_state_enter:296]Proposer for term {2,'ns_1@cb.local'} in history <<"5aab03dbecad06b50ffb474da59a00c9">> is ready. Committed seqno: 2
[chronicle:info,2025-05-15T18:46:45.975Z,ns_1@cb.local:chronicle_leader<0.256.0>:chronicle_leader:handle_note_term_status:596]Term {2,'ns_1@cb.local'} established.
[ns_server:info,2025-05-15T18:46:45.998Z,ns_1@cb.local:chronicle_local<0.239.0>:chronicle_upgrade:initialize:72]Chronicle content was initialized. Rev = {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                          3}.
[error_logger:info,2025-05-15T18:46:45.998Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.239.0>},
              {id,chronicle_local},
              {mfargs,{chronicle_local,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:46.000Z,ns_1@cb.local:ns_cluster<0.273.0>:ns_cluster:handle_info:514]Chronicle state is: provisioned
[error_logger:info,2025-05-15T18:46:46.000Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.273.0>},
              {id,ns_cluster},
              {mfargs,{ns_cluster,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:46.004Z,ns_1@cb.local:sigar<0.275.0>:sigar:spawn_sigar:134]Spawning sigar process 'portsigar for ns_1@cb.local'("/opt/couchbase/bin/sigar_port") with babysitter pid: 41 and log file "/opt/couchbase/var/lib/couchbase/logs/sigar_port.log"
[error_logger:info,2025-05-15T18:46:46.005Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.275.0>},
              {id,sigar},
              {mfargs,{sigar,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:46.007Z,ns_1@cb.local:ns_config_sup<0.276.0>:ns_config_sup:init:26]loading static ns_config from "/opt/couchbase/etc/couchbase/config"
[error_logger:info,2025-05-15T18:46:46.008Z,ns_1@cb.local:ns_config_sup<0.276.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.277.0>},
              {id,tombstone_keeper},
              {mfargs,{tombstone_keeper,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:46.008Z,ns_1@cb.local:ns_config_sup<0.276.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.278.0>},
              {id,ns_config_events},
              {mfargs,{gen_event,start_link,[{local,ns_config_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:46.008Z,ns_1@cb.local:ns_config_sup<0.276.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.279.0>},
              {id,ns_config_events_local},
              {mfargs,{gen_event,start_link,[{local,ns_config_events_local}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:46.057Z,ns_1@cb.local:ns_config<0.280.0>:ns_config:load_config:1113]Loading static config from "/opt/couchbase/etc/couchbase/config"
[ns_server:info,2025-05-15T18:46:46.057Z,ns_1@cb.local:ns_config<0.280.0>:ns_config:load_config:1127]Loading dynamic config from "/opt/couchbase/var/lib/couchbase/config/config.dat"
[ns_server:info,2025-05-15T18:46:46.057Z,ns_1@cb.local:ns_config<0.280.0>:ns_config:load_config:1132]No dynamic config file found. Assuming we're brand new node
[ns_server:debug,2025-05-15T18:46:46.060Z,ns_1@cb.local:ns_config<0.280.0>:ns_config:load_config:1135]Here's full dynamic config we loaded:
[[]]
[ns_server:info,2025-05-15T18:46:46.063Z,ns_1@cb.local:ns_config<0.280.0>:ns_config:load_config:1157]Here's full dynamic config we loaded + static & default config:
[{resource_management,
  [{bucket,
    [{resident_ratio,
      [{enabled,false},{couchstore_minimum,1},{magma_minimum,0.2}]},
     {data_size,[{enabled,false},{couchstore_maximum,2},{magma_maximum,16}]}]},
   {index,[]},
   {cores_per_bucket,[{enabled,false},{minimum,0.4}]},
   {disk_usage,[{enabled,false},{maximum,96}]},
   {collections_per_quota,[{enabled,false},{maximum,1}]}]},
 {{node,'ns_1@cb.local',index_dir},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]},
   47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
   98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
 {{node,'ns_1@cb.local',database_dir},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]},
   47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
   98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
 {auto_failover_cfg,
  [{enabled,true},
   {timeout,120},
   {count,0},
   {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
   {failover_server_group,false},
   {max_count,1},
   {failed_over_server_groups,[]},
   {can_abort_rebalance,true}]},
 {retry_rebalance,[{enabled,false},{after_time_period,300},{max_attempts,1}]},
 {{node,'ns_1@cb.local',{project_intact,is_vulnerable}},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   false]},
 {{node,'ns_1@cb.local',ssl_capi_port},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   18092]},
 {{node,'ns_1@cb.local',capi_port},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   8092]},
 {{node,'ns_1@cb.local',backup_grpc_port},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   9124]},
 {{node,'ns_1@cb.local',backup_https_port},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   18097]},
 {{node,'ns_1@cb.local',backup_http_port},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   8097]},
 {{node,'ns_1@cb.local',prometheus_http_port},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   9123]},
 {{node,'ns_1@cb.local',cbas_debug_port},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|-1]},
 {{node,'ns_1@cb.local',cbas_parent_port},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   9122]},
 {{node,'ns_1@cb.local',cbas_metadata_port},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   9121]},
 {{node,'ns_1@cb.local',cbas_replication_port},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   9120]},
 {{node,'ns_1@cb.local',cbas_metadata_callback_port},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   9119]},
 {{node,'ns_1@cb.local',cbas_messaging_port},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   9118]},
 {{node,'ns_1@cb.local',cbas_result_port},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   9117]},
 {{node,'ns_1@cb.local',cbas_data_port},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   9116]},
 {{node,'ns_1@cb.local',cbas_cluster_port},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   9115]},
 {{node,'ns_1@cb.local',cbas_console_port},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   9114]},
 {{node,'ns_1@cb.local',cbas_cc_client_port},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   9113]},
 {{node,'ns_1@cb.local',cbas_cc_cluster_port},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   9112]},
 {{node,'ns_1@cb.local',cbas_cc_http_port},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   9111]},
 {{node,'ns_1@cb.local',cbas_admin_port},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   9110]},
 {{node,'ns_1@cb.local',cbas_ssl_port},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   18095]},
 {{node,'ns_1@cb.local',cbas_http_port},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   8095]},
 {{node,'ns_1@cb.local',eventing_https_port},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   18096]},
 {{node,'ns_1@cb.local',eventing_debug_port},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   9140]},
 {{node,'ns_1@cb.local',eventing_http_port},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   8096]},
 {{node,'ns_1@cb.local',fts_grpc_ssl_port},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   19130]},
 {{node,'ns_1@cb.local',fts_grpc_port},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   9130]},
 {{node,'ns_1@cb.local',fts_ssl_port},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   18094]},
 {{node,'ns_1@cb.local',fts_http_port},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   8094]},
 {{node,'ns_1@cb.local',indexer_https_port},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   19102]},
 {{node,'ns_1@cb.local',indexer_stmaint_port},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   9105]},
 {{node,'ns_1@cb.local',indexer_stcatchup_port},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   9104]},
 {{node,'ns_1@cb.local',indexer_stinit_port},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   9103]},
 {{node,'ns_1@cb.local',indexer_http_port},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   9102]},
 {{node,'ns_1@cb.local',indexer_scan_port},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   9101]},
 {{node,'ns_1@cb.local',indexer_admin_port},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   9100]},
 {{node,'ns_1@cb.local',ssl_query_port},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   18093]},
 {{node,'ns_1@cb.local',query_port},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   8093]},
 {{node,'ns_1@cb.local',projector_ssl_port},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   9999]},
 {{node,'ns_1@cb.local',projector_port},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   9999]},
 {{node,'ns_1@cb.local',memcached_prometheus},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   11280]},
 {{node,'ns_1@cb.local',memcached_dedicated_ssl_port},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   11206]},
 {{node,'ns_1@cb.local',xdcr_rest_port},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   9998]},
 {{node,'ns_1@cb.local',ssl_rest_port},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   18091]},
 {{node,'ns_1@cb.local',rest},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]},
   {port,8091},
   {port_meta,global}]},
 {rest,[{port,8091}]},
 {password_policy,[{min_length,6},{must_present,[]}]},
 {health_monitor_refresh_interval,[]},
 {service_orchestrator_weight,
  [{kv,10000},
   {index,1000},
   {fts,1000},
   {cbas,1000},
   {n1ql,100},
   {eventing,100},
   {backup,10}]},
 {log_redaction_default_cfg,[{redact_level,none}]},
 {replication,[{enabled,true}]},
 {alert_limits,
  [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
 {email_alerts,
  [{recipients,["root@localhost"]},
   {sender,"couchbase@localhost"},
   {enabled,false},
   {email_server,
    [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
   {alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     indexer_low_resident_percentage,ep_clock_cas_drift_threshold_exceeded,
     communication_issue,time_out_of_sync,disk_usage_analyzer_stuck,
     cert_expires_soon,cert_expired,memory_threshold,history_size_warning,
     stuck_rebalance,memcached_connections]},
   {pop_up_alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     indexer_low_resident_percentage,ep_clock_cas_drift_threshold_exceeded,
     communication_issue,time_out_of_sync,disk_usage_analyzer_stuck,
     cert_expires_soon,cert_expired,memory_threshold,history_size_warning,
     stuck_rebalance,memcached_connections]}]},
 {{node,'ns_1@cb.local',event_log},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/event_log"}]},
 {{node,'ns_1@cb.local',ns_log},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
 {{node,'ns_1@cb.local',port_servers},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}]},
 {secure_headers,[]},
 {buckets,[{configs,[]}]},
 {cbas_memory_quota,1549},
 {fts_memory_quota,512},
 {memory_quota,3406},
 {{node,'ns_1@cb.local',memcached_config},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   {[{interfaces,{memcached_config_mgr,get_interfaces,[]}},
     {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
     {connection_idle_time,connection_idle_time},
     {privilege_debug,privilege_debug},
     {breakpad,
      {[{enabled,breakpad_enabled},
        {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
     {deployment_model,{memcached_config_mgr,get_config_profile,[]}},
     {verbosity,verbosity},
     {audit_file,{"~s",[audit_file]}},
     {rbac_file,{"~s",[rbac_file]}},
     {dedupe_nmvb_maps,dedupe_nmvb_maps},
     {tracing_enabled,tracing_enabled},
     {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
     {xattr_enabled,true},
     {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
     {collections_enabled,true},
     {max_connections,max_connections},
     {system_connections,system_connections},
     {num_reader_threads,num_reader_threads},
     {num_writer_threads,num_writer_threads},
     {num_auxio_threads,num_auxio_threads},
     {num_nonio_threads,num_nonio_threads},
     {num_storage_threads,num_storage_threads},
     {logger,
      {[{filename,{"~s/~s",[log_path,log_prefix]}},
        {cyclesize,log_cyclesize}]}},
     {external_auth_service,
      {memcached_config_mgr,get_external_auth_service,[]}},
     {active_external_users_push_interval,
      {memcached_config_mgr,get_external_users_push_interval,[]}},
     {prometheus,{memcached_config_mgr,prometheus_cfg,[]}},
     {sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
     {ssl_sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
     {tcp_keepalive_idle,tcp_keepalive_idle},
     {tcp_keepalive_interval,tcp_keepalive_interval},
     {tcp_keepalive_probes,tcp_keepalive_probes},
     {tcp_user_timeout,tcp_user_timeout},
     {always_collect_trace_info,always_collect_trace_info},
     {connection_limit_mode,connection_limit_mode},
     {free_connection_pool_size,free_connection_pool_size},
     {max_client_connection_details,max_client_connection_details}]}]},
 {{node,'ns_1@cb.local',memcached},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]},
   {port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,11206},
   {ssl_port,11207},
   {admin_user,"@ns_server"},
   {other_users,
    ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing","@cbas",
     "@backup"]},
   {admin_pass,"*****"},
   {engines,
    [{membase,
      [{engine,"/opt/couchbase/lib/memcached/ep.so"},
       {static_config_string,"failpartialwarmup=false"}]},
     {memcached,
      [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
       {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_rotation_period,39003}]},
 {{node,'ns_1@cb.local',memcached_defaults},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]},
   {max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {je_malloc_conf,undefined},
   {tracing_enabled,true},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>},
   {num_auxio_threads,<<"default">>},
   {num_nonio_threads,<<"default">>},
   {num_storage_threads,<<"default">>},
   {tcp_keepalive_idle,360},
   {tcp_keepalive_interval,10},
   {tcp_keepalive_probes,3},
   {tcp_user_timeout,30},
   {always_collect_trace_info,true},
   {connection_limit_mode,<<"disconnect">>},
   {free_connection_pool_size,0},
   {max_client_connection_details,0}]},
 {memcached,[]},
 {{node,'ns_1@cb.local',audit},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}]},
 {audit,
  [{auditd_enabled,false},
   {rotate_interval,86400},
   {rotate_size,20971520},
   {prune_age,0},
   {disabled,[]},
   {enabled,[]},
   {disabled_users,[]},
   {sync,[]},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
 {{node,'ns_1@cb.local',isasl},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]},
   {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
 {remote_clusters,[]},
 {scramsha_fallback_salt,<<199,70,35,121,194,74,234,136,104,77,98,154>>},
 {client_cert_auth,[{state,"disable"},{prefixes,[]}]},
 {rest_creds,null},
 {{metakv,<<"/analytics/settings/config">>},
  <<"{\"analytics.settings.num_replicas\":0}">>},
 {{metakv,<<"/query/settings/config">>},
  <<"{\"cleanupclientattempts\":true,\"cleanuplostattempts\":true,\"cleanupwindow\":\"60s\",\"completed-limit\":4000,\"completed-threshold\":1000,\"loglevel\":\"info\",\"max-parallelism\":1,\"memory-quota\":0,\"n1ql-feat-ctrl\":76,\"numatrs\":1024,\"pipeline-batch\":16,\"pipeline-cap\":512,\"prepared-limit\":16384,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"query.settings.tmp_space_size\":5120,\"scan-cap\":512,\"timeout\":0,\"txtimeout\":\"0ms\",\"use-cbo\":true}">>},
 {{metakv,<<"/eventing/settings/config">>},<<"{\"ram_quota\":256}">>},
 {{metakv,<<"/indexing/settings/config">>},
  <<"{\"indexer.settings.compaction.abort_exceed_interval\":false,\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.enable_page_bloom_filter\":false,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.log_level\":\"info\",\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.num_replica\":0,\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.storage_mode\":\"\"}">>},
 {{couchdb,max_parallel_replica_indexers},2},
 {{couchdb,max_parallel_indexers},4},
 {quorum_nodes,['ns_1@cb.local']},
 {nodes_wanted,[]},
 {{node,'ns_1@cb.local',compaction_daemon},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]},
   {check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}]},
 {set_view_update_daemon,
  [{update_interval,5000},
   {update_min_changes,5000},
   {replica_update_min_changes,5000}]},
 {max_bucket_count,30},
 {index_aware_rebalance_disabled,false},
 {{node,'ns_1@cb.local',saslauthd_enabled},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   true]},
 {{node,'ns_1@cb.local',is_enterprise},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   true]},
 {{node,'ns_1@cb.local',config_version},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   {7,6}]},
 {{node,'ns_1@cb.local',uuid},
  [{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|
   <<"28569ac00b9c1d7c50e39741027d428c">>]}]
[ns_server:debug,2025-05-15T18:46:46.068Z,ns_1@cb.local:tombstone_keeper<0.277.0>:tombstone_keeper:handle_call:49]Refreshed with timestamps []
[error_logger:info,2025-05-15T18:46:46.068Z,ns_1@cb.local:ns_config_sup<0.276.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.280.0>},
              {id,ns_config},
              {mfargs,{ns_config,start_link,
                                 ["/opt/couchbase/etc/couchbase/config",
                                  ns_config_default]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:46.069Z,ns_1@cb.local:ns_config_sup<0.276.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.283.0>},
              {id,ns_config_remote},
              {mfargs,{ns_config_replica,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:46.069Z,ns_1@cb.local:ns_config_sup<0.276.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.284.0>},
              {id,ns_config_log},
              {mfargs,{ns_config_log,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:46.069Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.276.0>},
              {id,ns_config_sup},
              {mfargs,{ns_config_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:46.071Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',n2n_client_cert_auth} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|false]
[error_logger:info,2025-05-15T18:46:46.071Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.286.0>},
              {id,netconfig_updater},
              {mfargs,{netconfig_updater,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:46.071Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',erl_external_listeners} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]},
 {inet,false}]
[ns_server:debug,2025-05-15T18:46:46.071Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',node_encryption} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|false]
[ns_server:debug,2025-05-15T18:46:46.071Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',address_family} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]}|inet]
[error_logger:info,2025-05-15T18:46:46.072Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.289.0>},
              {id,json_rpc_connection_sup},
              {mfargs,{json_rpc_connection_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:46.078Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.292.0>},
              {id,chronicle_compat_events},
              {mfargs,{chronicle_compat_events,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:46.079Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.297.0>},
              {id,remote_monitors},
              {mfargs,{remote_monitors,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:46.080Z,ns_1@cb.local:menelaus_barrier<0.298.0>:one_shot_barrier:barrier_body:52]Barrier menelaus_barrier has started
[error_logger:info,2025-05-15T18:46:46.080Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.298.0>},
              {id,menelaus_barrier},
              {mfargs,{menelaus_sup,barrier_start_link,[]}},
              {restart_type,temporary},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:46.082Z,ns_1@cb.local:<0.301.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,
                                    rest_lhttpc_pool} with delay of 1000
[error_logger:info,2025-05-15T18:46:46.082Z,ns_1@cb.local:<0.302.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.302.0>,suppress_max_restart_intensity}
    started: [{pid,<0.303.0>},
              {id,rest_lhttpc_pool},
              {mfargs,{lhttpc_manager,start_link,
                                      [[{name,rest_lhttpc_pool},
                                        {connection_timeout,120000},
                                        {pool_size,20}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:46.082Z,ns_1@cb.local:<0.300.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.300.0>,suppress_max_restart_intensity}
    started: [{pid,<0.301.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      rest_lhttpc_pool}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,rest_lhttpc_pool},
                       1000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 1,id => rest_lhttpc_pool,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [lhttpc_manager],
                          restart => permanent,shutdown => 1000,
                          start =>
                              {lhttpc_manager,start_link,
                                  [[{name,rest_lhttpc_pool},
                                    {connection_timeout,120000},
                                    {pool_size,20}]]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:46.082Z,ns_1@cb.local:rest_lhttpc_pool_sup<0.299.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,rest_lhttpc_pool_sup}
    started: [{pid,<0.300.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,rest_lhttpc_pool}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 1,id => rest_lhttpc_pool,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [lhttpc_manager],
                         restart => permanent,shutdown => 1000,
                         start =>
                             {lhttpc_manager,start_link,
                                 [[{name,rest_lhttpc_pool},
                                   {connection_timeout,120000},
                                   {pool_size,20}]]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:46.082Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.299.0>},
              {id,rest_lhttpc_pool_sup},
              {mfargs,{rest_lhttpc_pool_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:46.084Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.304.0>},
              {id,memcached_refresh},
              {mfargs,{memcached_refresh,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:46.085Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.305.0>},
              {id,ns_secrets},
              {mfargs,{ns_secrets,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:46.086Z,ns_1@cb.local:ns_ssl_services_sup<0.306.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_ssl_services_sup}
    started: [{pid,<0.307.0>},
              {id,ssl_service_events},
              {mfargs,{gen_event,start_link,[{local,ssl_service_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:46.091Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:maybe_store_ca_certs:832]Considering to store CA certs
[ns_server:debug,2025-05-15T18:46:46.228Z,ns_1@cb.local:<0.312.0>:goport:handle_eof:592]Stream 'stdout' closed
[ns_server:debug,2025-05-15T18:46:46.228Z,ns_1@cb.local:<0.312.0>:goport:handle_eof:592]Stream 'stderr' closed
[ns_server:info,2025-05-15T18:46:46.228Z,ns_1@cb.local:<0.312.0>:goport:handle_process_exit:573]Port exited with status 0.
[ns_server:debug,2025-05-15T18:46:46.240Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_server_cert:generate_cert_and_pkey:155]Generated certificate and private key in 146168 us
[ns_server:debug,2025-05-15T18:46:46.305Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:maybe_store_ca_certs:847]Updating CA file with 1 certificates
[ns_server:info,2025-05-15T18:46:46.311Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:maybe_store_ca_certs:850]CA file updated: 1 cert(s) written
[ns_server:info,2025-05-15T18:46:46.312Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:should_regenerate_certs:899]Should regenerate node_cert because there are no certs on this node
[ns_server:debug,2025-05-15T18:46:46.619Z,ns_1@cb.local:<0.317.0>:goport:handle_eof:592]Stream 'stderr' closed
[ns_server:debug,2025-05-15T18:46:46.620Z,ns_1@cb.local:<0.317.0>:goport:handle_eof:592]Stream 'stdout' closed
[ns_server:info,2025-05-15T18:46:46.620Z,ns_1@cb.local:<0.317.0>:goport:handle_process_exit:573]Port exited with status 0.
[ns_server:info,2025-05-15T18:46:46.622Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:save_certs:968]New node_cert and pkey are written to tmp file
[ns_server:info,2025-05-15T18:46:46.629Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:save_certs_phase2:995]node_cert cert and pkey files updated
[ns_server:debug,2025-05-15T18:46:46.629Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',node_cert} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]},
 {subject,<<"CN=Couchbase Server Node (127.0.0.1)">>},
 {not_after,63985747606},
 {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
 {load_timestamp,63914554006},
 {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQ"...>>},
 {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDOTCCAiGgAwIBAgIIGD/Hv6I/B/UwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ2NDZaFw0y\nNzA4MTcxODQ2NDZaMCwxKjAoBgNVBAMTIUNvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nMTI3LjAuMC4xKTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAOcGnD0i\nK4yErfsC0OEceWFxJLbXv/UYXZqt90WDswgyrkIIRSbthJzY/QEWmjj0Oh0CQmOp\nSoIV712"...>>},
 {pkey_passphrase_settings,[]},
 {certs_epoch,0},
 {type,generated},
 {hostname,"127.0.0.1"}]
[ns_server:info,2025-05-15T18:46:46.640Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:should_regenerate_certs:899]Should regenerate client_cert because there are no certs on this node
[ns_server:debug,2025-05-15T18:46:46.682Z,ns_1@cb.local:<0.323.0>:goport:handle_eof:592]Stream 'stdout' closed
[ns_server:debug,2025-05-15T18:46:46.682Z,ns_1@cb.local:<0.323.0>:goport:handle_eof:592]Stream 'stderr' closed
[ns_server:info,2025-05-15T18:46:46.682Z,ns_1@cb.local:<0.323.0>:goport:handle_process_exit:573]Port exited with status 0.
[ns_server:info,2025-05-15T18:46:46.684Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:save_certs:968]New client_cert and pkey are written to tmp file
[ns_server:info,2025-05-15T18:46:46.688Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:save_certs_phase2:995]client_cert cert and pkey files updated
[ns_server:debug,2025-05-15T18:46:46.688Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',client_cert} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]},
 {subject,<<"CN=Couchbase Internal Client (70858842)">>},
 {not_after,63985747606},
 {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
 {load_timestamp,63914554006},
 {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQ"...>>},
 {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDWTCCAkGgAwIBAgIIGD/Hv7XHejcwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ2NDZaFw0y\nNzA4MTcxODQ2NDZaMC8xLTArBgNVBAMTJENvdWNoYmFzZSBJbnRlcm5hbCBDbGll\nbnQgKDcwODU4ODQyKTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMSK\nvXvBcS/ChHK2Kb84PvDZyFlVdyCnMwkD8aug5L/C9zj5626M2/KO3H+iam8QErOa\nUj7TQ/A"...>>},
 {pkey_passphrase_settings,[]},
 {certs_epoch,0},
 {type,generated},
 {name,"@internal"}]
[ns_server:debug,2025-05-15T18:46:46.707Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:restart_regenerate_client_cert_timer:1447]Time left before client cert regeneration: 70588800000
[ns_server:info,2025-05-15T18:46:46.707Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:handle_info:764]cert_and_pkey changed
[error_logger:info,2025-05-15T18:46:46.707Z,ns_1@cb.local:ns_ssl_services_sup<0.306.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_ssl_services_sup}
    started: [{pid,<0.308.0>},
              {id,ns_ssl_services_setup},
              {mfargs,{ns_ssl_services_setup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:46.721Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:notify_services:1146]Going to notify following services: [capi_ssl_service,cb_dist_tls,
                                     client_cert_event,memcached,
                                     server_cert_event,ssl_service]
[ns_server:info,2025-05-15T18:46:46.722Z,ns_1@cb.local:<0.342.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service client_cert_event
[ns_server:info,2025-05-15T18:46:46.722Z,ns_1@cb.local:<0.344.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service server_cert_event
[ns_server:debug,2025-05-15T18:46:46.722Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Restarting tls distribution protocols (if any)
[ns_server:debug,2025-05-15T18:46:46.724Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Ensure config is going to change listeners. Will be stopped: [], will be started: [], will be restarted: []
[ns_server:info,2025-05-15T18:46:46.724Z,ns_1@cb.local:<0.341.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service cb_dist_tls
[error_logger:info,2025-05-15T18:46:46.726Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {2445260,#Ref<0.2327127311.3650486274.216096>}}}
[ns_server:debug,2025-05-15T18:46:46.727Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:46:46.727Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.2327127311.3650355205.215445>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:46:46.727Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.2327127311.3650355205.215445>,
                                  inet_tcp_dist,<0.346.0>,
                                  #Ref<0.2327127311.3650355205.215448>}
[ns_server:debug,2025-05-15T18:46:46.727Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.2327127311.3650355205.215445>,
                               inet_tcp_dist,<0.346.0>,
                               #Ref<0.2327127311.3650355205.215448>}
[error_logger:info,2025-05-15T18:46:46.727Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.346.0>,shutdown}}
[error_logger:info,2025-05-15T18:46:46.727Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:46:46.727Z,ns_1@cb.local:<0.340.0>:ns_couchdb_api:rpc_couchdb_node:152]RPC to couchdb node failed for restart_capi_ssl_service with {badrpc,nodedown}
Stack: [{ns_couchdb_api,rpc_couchdb_node,4,
                        [{file,"src/ns_couchdb_api.erl"},{line,150}]},
        {ns_ssl_services_setup,do_notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1203}]},
        {ns_ssl_services_setup,notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1179}]},
        {async,'-async_init/4-fun-1-',3,[{file,"src/async.erl"},{line,199}]}]
[ns_server:warn,2025-05-15T18:46:46.727Z,ns_1@cb.local:<0.340.0>:ns_ssl_services_setup:notify_service:1184]Failed to notify service capi_ssl_service: {'EXIT',{error,{badrpc,nodedown}}}
[ns_server:warn,2025-05-15T18:46:46.728Z,ns_1@cb.local:<0.343.0>:ns_ssl_services_setup:notify_service:1184]Failed to notify service memcached: {error,no_proccess}
[ns_server:info,2025-05-15T18:46:46.745Z,ns_1@cb.local:<0.329.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2025-05-15T18:46:46.745Z,ns_1@cb.local:<0.329.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2025-05-15T18:46:46.745Z,ns_1@cb.local:<0.329.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2025-05-15T18:46:46.745Z,ns_1@cb.local:<0.329.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2025-05-15T18:46:46.746Z,ns_1@cb.local:<0.329.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[error_logger:info,2025-05-15T18:46:46.759Z,ns_1@cb.local:kernel_sup<0.49.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,kernel_sup}
    started: [{pid,<0.349.0>},
              {id,timer_server},
              {mfargs,{timer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:46.784Z,ns_1@cb.local:<0.329.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"0.0.0.0"},
 [{keyfile,"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem"},
  {certfile,"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem"},
  {versions,['tlsv1.2','tlsv1.3']},
  {cacerts,[<<48,130,3,12,48,130,1,244,160,3,2,1,2,2,8,24,63,199,191,156,197,
              82,17,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,48,
              32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,101,
              114,118,101,114,32,99,54,52,100,57,49,52,101,48,30,23,13,49,51,
              48,49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,51,
              53,57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,117,
              99,104,98,97,115,101,32,83,101,114,118,101,114,32,99,54,52,100,
              57,49,52,101,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,1,
              5,0,3,130,1,15,0,48,130,1,10,2,130,1,1,0,189,71,7,138,162,116,
              202,173,172,80,169,207,183,229,131,28,103,231,118,113,23,184,
              163,118,132,50,140,197,100,9,14,13,222,41,184,111,89,132,2,102,
              81,20,185,124,217,165,68,88,93,96,211,184,188,36,17,35,176,167,
              109,82,1,80,201,124,250,21,203,22,244,2,187,45,29,22,162,207,
              172,158,177,99,109,81,39,155,109,84,244,123,242,203,49,197,116,
              158,244,151,83,143,242,44,81,204,228,195,44,95,101,252,10,164,
              27,51,148,102,46,28,237,20,182,124,42,212,174,43,184,195,216,
              21,135,233,45,167,9,73,251,27,220,165,132,153,156,0,160,219,
              145,166,59,196,219,128,44,31,123,111,143,183,1,7,155,248,244,
              114,116,107,27,205,0,89,187,228,101,131,201,74,251,79,181,48,
              113,50,90,20,129,109,127,47,137,209,73,173,217,56,197,53,141,3,
              62,155,117,88,76,228,40,82,177,57,208,108,251,183,13,115,126,
              89,27,138,240,86,239,196,39,94,14,94,20,12,94,74,91,82,150,9,
              197,47,35,206,76,210,35,237,68,111,174,228,31,136,249,66,25,
              209,244,127,176,195,100,189,2,3,1,0,1,163,66,48,64,48,14,6,3,
              85,29,15,1,1,255,4,4,3,2,1,134,48,15,6,3,85,29,19,1,1,255,4,5,
              48,3,1,1,255,48,29,6,3,85,29,14,4,22,4,20,188,1,42,182,50,72,
              116,82,198,94,225,242,47,206,206,38,9,39,211,154,48,13,6,9,42,
              134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,7,122,181,155,55,73,
              115,117,88,215,178,67,9,164,79,41,143,213,122,120,183,161,158,
              179,24,159,107,244,185,214,40,78,200,59,208,49,4,2,152,24,193,
              11,28,199,253,134,219,80,153,22,107,94,33,144,139,251,88,92,
              185,69,22,78,200,112,130,3,234,35,142,100,178,218,128,56,66,64,
              147,226,231,103,70,132,16,120,107,97,39,6,241,133,244,226,72,
              227,53,189,39,13,56,28,212,121,112,93,92,197,98,43,69,209,137,
              1,127,228,110,207,204,246,60,244,73,57,87,206,37,77,194,32,24,
              120,98,49,146,58,191,127,2,164,235,7,154,139,200,136,193,5,110,
              232,107,27,181,32,75,241,254,5,68,41,54,188,174,154,212,192,
              249,224,20,3,106,196,82,133,216,183,147,240,201,0,75,88,247,
              105,174,187,93,204,210,55,56,242,96,140,116,69,94,95,19,60,172,
              138,198,232,182,175,206,36,240,180,239,163,144,92,150,142,84,
              114,53,180,171,234,41,126,157,228,150,39,182,108,165,58,62,101,
              129,114,231,144,128,135,156,44,194,78,139,142,141,176,149,8,31,
              25,249,73,108,167,49,3>>]},
  {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,
        10,118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,
        158,232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,
        66,211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,
        250,145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,
        104,159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,
        246,169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,
        110,167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,
        118,190,67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,
        74,8,205,174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,
        221,95,184,110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,
        76,187,66,211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,
        69,254,147,103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,
        202,133,173,72,6,69,167,89,112,174,40,229,171,2,1,2>>},
  {ciphers,[#{cipher => aes_256_gcm,key_exchange => any,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm_8,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_ecdsa,
              mac => aead,prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_rsa,
              mac => aead,prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_128_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf},
            #{cipher => aes_128_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf}]},
  {honor_cipher_order,true},
  {secure_renegotiate,true},
  {client_renegotiation,false},
  {password,"********"}],
 {ssl,true},
 {port,18091}]
[error_logger:info,2025-05-15T18:46:46.786Z,ns_1@cb.local:<0.329.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.329.0>,menelaus_web}
    started: [{pid,<0.350.0>},
              {id,menelaus_web_ipv4},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet},
                        {ssl,true},
                        {name,menelaus_web_ssl},
                        {ssl_opts_fun,#Fun<ns_ssl_services_setup.11.39330155>},
                        {port,18091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:46.788Z,ns_1@cb.local:<0.329.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2025-05-15T18:46:46.788Z,ns_1@cb.local:<0.329.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2025-05-15T18:46:46.788Z,ns_1@cb.local:<0.329.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2025-05-15T18:46:46.788Z,ns_1@cb.local:<0.329.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2025-05-15T18:46:46.789Z,ns_1@cb.local:<0.329.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2025-05-15T18:46:46.791Z,ns_1@cb.local:<0.329.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"::"},
 [{keyfile,"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem"},
  {certfile,"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem"},
  {versions,['tlsv1.2','tlsv1.3']},
  {cacerts,[<<48,130,3,12,48,130,1,244,160,3,2,1,2,2,8,24,63,199,191,156,197,
              82,17,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,48,
              32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,101,
              114,118,101,114,32,99,54,52,100,57,49,52,101,48,30,23,13,49,51,
              48,49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,51,
              53,57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,117,
              99,104,98,97,115,101,32,83,101,114,118,101,114,32,99,54,52,100,
              57,49,52,101,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,1,
              5,0,3,130,1,15,0,48,130,1,10,2,130,1,1,0,189,71,7,138,162,116,
              202,173,172,80,169,207,183,229,131,28,103,231,118,113,23,184,
              163,118,132,50,140,197,100,9,14,13,222,41,184,111,89,132,2,102,
              81,20,185,124,217,165,68,88,93,96,211,184,188,36,17,35,176,167,
              109,82,1,80,201,124,250,21,203,22,244,2,187,45,29,22,162,207,
              172,158,177,99,109,81,39,155,109,84,244,123,242,203,49,197,116,
              158,244,151,83,143,242,44,81,204,228,195,44,95,101,252,10,164,
              27,51,148,102,46,28,237,20,182,124,42,212,174,43,184,195,216,
              21,135,233,45,167,9,73,251,27,220,165,132,153,156,0,160,219,
              145,166,59,196,219,128,44,31,123,111,143,183,1,7,155,248,244,
              114,116,107,27,205,0,89,187,228,101,131,201,74,251,79,181,48,
              113,50,90,20,129,109,127,47,137,209,73,173,217,56,197,53,141,3,
              62,155,117,88,76,228,40,82,177,57,208,108,251,183,13,115,126,
              89,27,138,240,86,239,196,39,94,14,94,20,12,94,74,91,82,150,9,
              197,47,35,206,76,210,35,237,68,111,174,228,31,136,249,66,25,
              209,244,127,176,195,100,189,2,3,1,0,1,163,66,48,64,48,14,6,3,
              85,29,15,1,1,255,4,4,3,2,1,134,48,15,6,3,85,29,19,1,1,255,4,5,
              48,3,1,1,255,48,29,6,3,85,29,14,4,22,4,20,188,1,42,182,50,72,
              116,82,198,94,225,242,47,206,206,38,9,39,211,154,48,13,6,9,42,
              134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,7,122,181,155,55,73,
              115,117,88,215,178,67,9,164,79,41,143,213,122,120,183,161,158,
              179,24,159,107,244,185,214,40,78,200,59,208,49,4,2,152,24,193,
              11,28,199,253,134,219,80,153,22,107,94,33,144,139,251,88,92,
              185,69,22,78,200,112,130,3,234,35,142,100,178,218,128,56,66,64,
              147,226,231,103,70,132,16,120,107,97,39,6,241,133,244,226,72,
              227,53,189,39,13,56,28,212,121,112,93,92,197,98,43,69,209,137,
              1,127,228,110,207,204,246,60,244,73,57,87,206,37,77,194,32,24,
              120,98,49,146,58,191,127,2,164,235,7,154,139,200,136,193,5,110,
              232,107,27,181,32,75,241,254,5,68,41,54,188,174,154,212,192,
              249,224,20,3,106,196,82,133,216,183,147,240,201,0,75,88,247,
              105,174,187,93,204,210,55,56,242,96,140,116,69,94,95,19,60,172,
              138,198,232,182,175,206,36,240,180,239,163,144,92,150,142,84,
              114,53,180,171,234,41,126,157,228,150,39,182,108,165,58,62,101,
              129,114,231,144,128,135,156,44,194,78,139,142,141,176,149,8,31,
              25,249,73,108,167,49,3>>]},
  {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,
        10,118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,
        158,232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,
        66,211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,
        250,145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,
        104,159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,
        246,169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,
        110,167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,
        118,190,67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,
        74,8,205,174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,
        221,95,184,110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,
        76,187,66,211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,
        69,254,147,103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,
        202,133,173,72,6,69,167,89,112,174,40,229,171,2,1,2>>},
  {ciphers,[#{cipher => aes_256_gcm,key_exchange => any,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm_8,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_ecdsa,
              mac => aead,prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_rsa,
              mac => aead,prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_128_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf},
            #{cipher => aes_128_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf}]},
  {honor_cipher_order,true},
  {secure_renegotiate,true},
  {client_renegotiation,false},
  {password,"********"}],
 {ssl,true},
 {port,18091}]
[error_logger:info,2025-05-15T18:46:46.793Z,ns_1@cb.local:<0.329.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.329.0>,menelaus_web}
    started: [{pid,<0.369.0>},
              {id,menelaus_web_ipv6},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet6},
                        {ssl,true},
                        {name,menelaus_web_ssl},
                        {ssl_opts_fun,#Fun<ns_ssl_services_setup.11.39330155>},
                        {port,18091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:46.793Z,ns_1@cb.local:<0.328.0>:restartable:start_child:92]Started child process <0.329.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
[error_logger:info,2025-05-15T18:46:46.793Z,ns_1@cb.local:ns_ssl_services_sup<0.306.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_ssl_services_sup}
    started: [{pid,<0.328.0>},
              {id,ns_rest_ssl_service},
              {mfargs,
                  {restartable,start_link,
                      [{ns_ssl_services_setup,start_link_rest_service,[]},
                       1000]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:46.793Z,ns_1@cb.local:<0.328.0>:restartable:loop:65]Restarting child <0.329.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
  Shutdown policy: 1000
  Caller: {<0.345.0>,#Ref<0.2327127311.3650355205.215529>}
[error_logger:info,2025-05-15T18:46:46.793Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.306.0>},
              {id,ns_ssl_services_sup},
              {mfargs,{ns_ssl_services_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:46.793Z,ns_1@cb.local:<0.328.0>:restartable:shutdown_child:114]Successfully terminated process <0.329.0>
[ns_server:info,2025-05-15T18:46:46.796Z,ns_1@cb.local:<0.388.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2025-05-15T18:46:46.796Z,ns_1@cb.local:<0.388.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2025-05-15T18:46:46.796Z,ns_1@cb.local:<0.388.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2025-05-15T18:46:46.796Z,ns_1@cb.local:<0.388.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2025-05-15T18:46:46.797Z,ns_1@cb.local:<0.388.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2025-05-15T18:46:46.797Z,ns_1@cb.local:<0.388.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"0.0.0.0"},
 [{keyfile,"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem"},
  {certfile,"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem"},
  {versions,['tlsv1.2','tlsv1.3']},
  {cacerts,[<<48,130,3,12,48,130,1,244,160,3,2,1,2,2,8,24,63,199,191,156,197,
              82,17,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,48,
              32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,101,
              114,118,101,114,32,99,54,52,100,57,49,52,101,48,30,23,13,49,51,
              48,49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,51,
              53,57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,117,
              99,104,98,97,115,101,32,83,101,114,118,101,114,32,99,54,52,100,
              57,49,52,101,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,1,
              5,0,3,130,1,15,0,48,130,1,10,2,130,1,1,0,189,71,7,138,162,116,
              202,173,172,80,169,207,183,229,131,28,103,231,118,113,23,184,
              163,118,132,50,140,197,100,9,14,13,222,41,184,111,89,132,2,102,
              81,20,185,124,217,165,68,88,93,96,211,184,188,36,17,35,176,167,
              109,82,1,80,201,124,250,21,203,22,244,2,187,45,29,22,162,207,
              172,158,177,99,109,81,39,155,109,84,244,123,242,203,49,197,116,
              158,244,151,83,143,242,44,81,204,228,195,44,95,101,252,10,164,
              27,51,148,102,46,28,237,20,182,124,42,212,174,43,184,195,216,
              21,135,233,45,167,9,73,251,27,220,165,132,153,156,0,160,219,
              145,166,59,196,219,128,44,31,123,111,143,183,1,7,155,248,244,
              114,116,107,27,205,0,89,187,228,101,131,201,74,251,79,181,48,
              113,50,90,20,129,109,127,47,137,209,73,173,217,56,197,53,141,3,
              62,155,117,88,76,228,40,82,177,57,208,108,251,183,13,115,126,
              89,27,138,240,86,239,196,39,94,14,94,20,12,94,74,91,82,150,9,
              197,47,35,206,76,210,35,237,68,111,174,228,31,136,249,66,25,
              209,244,127,176,195,100,189,2,3,1,0,1,163,66,48,64,48,14,6,3,
              85,29,15,1,1,255,4,4,3,2,1,134,48,15,6,3,85,29,19,1,1,255,4,5,
              48,3,1,1,255,48,29,6,3,85,29,14,4,22,4,20,188,1,42,182,50,72,
              116,82,198,94,225,242,47,206,206,38,9,39,211,154,48,13,6,9,42,
              134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,7,122,181,155,55,73,
              115,117,88,215,178,67,9,164,79,41,143,213,122,120,183,161,158,
              179,24,159,107,244,185,214,40,78,200,59,208,49,4,2,152,24,193,
              11,28,199,253,134,219,80,153,22,107,94,33,144,139,251,88,92,
              185,69,22,78,200,112,130,3,234,35,142,100,178,218,128,56,66,64,
              147,226,231,103,70,132,16,120,107,97,39,6,241,133,244,226,72,
              227,53,189,39,13,56,28,212,121,112,93,92,197,98,43,69,209,137,
              1,127,228,110,207,204,246,60,244,73,57,87,206,37,77,194,32,24,
              120,98,49,146,58,191,127,2,164,235,7,154,139,200,136,193,5,110,
              232,107,27,181,32,75,241,254,5,68,41,54,188,174,154,212,192,
              249,224,20,3,106,196,82,133,216,183,147,240,201,0,75,88,247,
              105,174,187,93,204,210,55,56,242,96,140,116,69,94,95,19,60,172,
              138,198,232,182,175,206,36,240,180,239,163,144,92,150,142,84,
              114,53,180,171,234,41,126,157,228,150,39,182,108,165,58,62,101,
              129,114,231,144,128,135,156,44,194,78,139,142,141,176,149,8,31,
              25,249,73,108,167,49,3>>]},
  {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,
        10,118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,
        158,232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,
        66,211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,
        250,145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,
        104,159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,
        246,169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,
        110,167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,
        118,190,67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,
        74,8,205,174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,
        221,95,184,110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,
        76,187,66,211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,
        69,254,147,103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,
        202,133,173,72,6,69,167,89,112,174,40,229,171,2,1,2>>},
  {ciphers,[#{cipher => aes_256_gcm,key_exchange => any,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm_8,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_ecdsa,
              mac => aead,prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_rsa,
              mac => aead,prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_128_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf},
            #{cipher => aes_128_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf}]},
  {honor_cipher_order,true},
  {secure_renegotiate,true},
  {client_renegotiation,false},
  {password,"********"}],
 {ssl,true},
 {port,18091}]
[error_logger:info,2025-05-15T18:46:46.799Z,ns_1@cb.local:<0.388.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.388.0>,menelaus_web}
    started: [{pid,<0.389.0>},
              {id,menelaus_web_ipv4},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet},
                        {ssl,true},
                        {name,menelaus_web_ssl},
                        {ssl_opts_fun,#Fun<ns_ssl_services_setup.11.39330155>},
                        {port,18091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:46.801Z,ns_1@cb.local:<0.388.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2025-05-15T18:46:46.801Z,ns_1@cb.local:<0.388.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2025-05-15T18:46:46.802Z,ns_1@cb.local:<0.388.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2025-05-15T18:46:46.802Z,ns_1@cb.local:<0.388.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[error_logger:info,2025-05-15T18:46:46.802Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.408.0>},
              {id,ldap_auth_cache},
              {mfargs,{ldap_auth_cache,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:46.802Z,ns_1@cb.local:<0.388.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2025-05-15T18:46:46.803Z,ns_1@cb.local:<0.388.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"::"},
 [{keyfile,"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem"},
  {certfile,"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem"},
  {versions,['tlsv1.2','tlsv1.3']},
  {cacerts,[<<48,130,3,12,48,130,1,244,160,3,2,1,2,2,8,24,63,199,191,156,197,
              82,17,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,48,
              32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,101,
              114,118,101,114,32,99,54,52,100,57,49,52,101,48,30,23,13,49,51,
              48,49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,51,
              53,57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,117,
              99,104,98,97,115,101,32,83,101,114,118,101,114,32,99,54,52,100,
              57,49,52,101,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,1,
              5,0,3,130,1,15,0,48,130,1,10,2,130,1,1,0,189,71,7,138,162,116,
              202,173,172,80,169,207,183,229,131,28,103,231,118,113,23,184,
              163,118,132,50,140,197,100,9,14,13,222,41,184,111,89,132,2,102,
              81,20,185,124,217,165,68,88,93,96,211,184,188,36,17,35,176,167,
              109,82,1,80,201,124,250,21,203,22,244,2,187,45,29,22,162,207,
              172,158,177,99,109,81,39,155,109,84,244,123,242,203,49,197,116,
              158,244,151,83,143,242,44,81,204,228,195,44,95,101,252,10,164,
              27,51,148,102,46,28,237,20,182,124,42,212,174,43,184,195,216,
              21,135,233,45,167,9,73,251,27,220,165,132,153,156,0,160,219,
              145,166,59,196,219,128,44,31,123,111,143,183,1,7,155,248,244,
              114,116,107,27,205,0,89,187,228,101,131,201,74,251,79,181,48,
              113,50,90,20,129,109,127,47,137,209,73,173,217,56,197,53,141,3,
              62,155,117,88,76,228,40,82,177,57,208,108,251,183,13,115,126,
              89,27,138,240,86,239,196,39,94,14,94,20,12,94,74,91,82,150,9,
              197,47,35,206,76,210,35,237,68,111,174,228,31,136,249,66,25,
              209,244,127,176,195,100,189,2,3,1,0,1,163,66,48,64,48,14,6,3,
              85,29,15,1,1,255,4,4,3,2,1,134,48,15,6,3,85,29,19,1,1,255,4,5,
              48,3,1,1,255,48,29,6,3,85,29,14,4,22,4,20,188,1,42,182,50,72,
              116,82,198,94,225,242,47,206,206,38,9,39,211,154,48,13,6,9,42,
              134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,7,122,181,155,55,73,
              115,117,88,215,178,67,9,164,79,41,143,213,122,120,183,161,158,
              179,24,159,107,244,185,214,40,78,200,59,208,49,4,2,152,24,193,
              11,28,199,253,134,219,80,153,22,107,94,33,144,139,251,88,92,
              185,69,22,78,200,112,130,3,234,35,142,100,178,218,128,56,66,64,
              147,226,231,103,70,132,16,120,107,97,39,6,241,133,244,226,72,
              227,53,189,39,13,56,28,212,121,112,93,92,197,98,43,69,209,137,
              1,127,228,110,207,204,246,60,244,73,57,87,206,37,77,194,32,24,
              120,98,49,146,58,191,127,2,164,235,7,154,139,200,136,193,5,110,
              232,107,27,181,32,75,241,254,5,68,41,54,188,174,154,212,192,
              249,224,20,3,106,196,82,133,216,183,147,240,201,0,75,88,247,
              105,174,187,93,204,210,55,56,242,96,140,116,69,94,95,19,60,172,
              138,198,232,182,175,206,36,240,180,239,163,144,92,150,142,84,
              114,53,180,171,234,41,126,157,228,150,39,182,108,165,58,62,101,
              129,114,231,144,128,135,156,44,194,78,139,142,141,176,149,8,31,
              25,249,73,108,167,49,3>>]},
  {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,
        10,118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,
        158,232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,
        66,211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,
        250,145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,
        104,159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,
        246,169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,
        110,167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,
        118,190,67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,
        74,8,205,174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,
        221,95,184,110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,
        76,187,66,211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,
        69,254,147,103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,
        202,133,173,72,6,69,167,89,112,174,40,229,171,2,1,2>>},
  {ciphers,[#{cipher => aes_256_gcm,key_exchange => any,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm_8,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_ecdsa,
              mac => aead,prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_rsa,
              mac => aead,prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_128_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf},
            #{cipher => aes_128_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf}]},
  {honor_cipher_order,true},
  {secure_renegotiate,true},
  {client_renegotiation,false},
  {password,"********"}],
 {ssl,true},
 {port,18091}]
[error_logger:info,2025-05-15T18:46:46.805Z,ns_1@cb.local:<0.388.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.388.0>,menelaus_web}
    started: [{pid,<0.410.0>},
              {id,menelaus_web_ipv6},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet6},
                        {ssl,true},
                        {name,menelaus_web_ssl},
                        {ssl_opts_fun,#Fun<ns_ssl_services_setup.11.39330155>},
                        {port,18091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:46.805Z,ns_1@cb.local:<0.328.0>:restartable:start_child:92]Started child process <0.388.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
[ns_server:info,2025-05-15T18:46:46.805Z,ns_1@cb.local:<0.345.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service ssl_service
[ns_server:debug,2025-05-15T18:46:46.805Z,ns_1@cb.local:cb_saml<0.429.0>:cb_saml:handle_info:316]Settings have changed or this is the first start
[ns_server:debug,2025-05-15T18:46:46.805Z,ns_1@cb.local:cb_saml<0.429.0>:cb_saml:restart_refresh_timer:586]Restarting refresh timer: 54064 ms
[error_logger:info,2025-05-15T18:46:46.805Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.429.0>},
              {id,cb_saml},
              {mfargs,{cb_saml,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:46.805Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:notify_services:1162]Succesfully notified services [ssl_service,server_cert_event,
                               client_cert_event,cb_dist_tls]
[error_logger:info,2025-05-15T18:46:46.807Z,ns_1@cb.local:users_sup<0.431.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_sup}
    started: [{pid,<0.432.0>},
              {id,user_storage_events},
              {mfargs,{gen_event,start_link,[{local,user_storage_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:46.812Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:notify_services:1173]Failed to notify some services. Will retry in 5 sec, [{{error,no_proccess},
                                                       memcached},
                                                      {{'EXIT',
                                                        {error,
                                                         {badrpc,nodedown}}},
                                                       capi_ssl_service}]
[error_logger:info,2025-05-15T18:46:46.812Z,ns_1@cb.local:users_storage_sup<0.433.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_storage_sup}
    started: [{pid,<0.434.0>},
              {id,users_replicator},
              {mfargs,{menelaus_users,start_replicator,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:46.812Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:restart_regenerate_client_cert_timer:1447]Time left before client cert regeneration: 70588800000
[ns_server:debug,2025-05-15T18:46:46.812Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:maybe_store_ca_certs:832]Considering to store CA certs
[ns_server:debug,2025-05-15T18:46:46.815Z,ns_1@cb.local:users_replicator<0.434.0>:replicated_storage:wait_for_startup:47]Start waiting for startup
[ns_server:debug,2025-05-15T18:46:46.819Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:notify_services:1146]Going to notify following services: [capi_ssl_service,memcached]
[error_logger:info,2025-05-15T18:46:46.819Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {2445261,#Ref<0.2327127311.3650486274.216096>}}}
[ns_server:warn,2025-05-15T18:46:46.819Z,ns_1@cb.local:<0.441.0>:ns_ssl_services_setup:notify_service:1184]Failed to notify service memcached: {error,no_proccess}
[ns_server:debug,2025-05-15T18:46:46.819Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:46:46.819Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.2327127311.3650355202.216120>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:46:46.820Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.2327127311.3650355202.216120>,
                                  inet_tcp_dist,<0.442.0>,
                                  #Ref<0.2327127311.3650355202.216123>}
[error_logger:info,2025-05-15T18:46:46.821Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.442.0>,shutdown}}
[ns_server:debug,2025-05-15T18:46:46.821Z,ns_1@cb.local:<0.440.0>:ns_couchdb_api:rpc_couchdb_node:152]RPC to couchdb node failed for restart_capi_ssl_service with {badrpc,nodedown}
Stack: [{ns_couchdb_api,rpc_couchdb_node,4,
                        [{file,"src/ns_couchdb_api.erl"},{line,150}]},
        {ns_ssl_services_setup,do_notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1203}]},
        {ns_ssl_services_setup,notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1179}]},
        {async,'-async_init/4-fun-1-',3,[{file,"src/async.erl"},{line,199}]}]
[ns_server:warn,2025-05-15T18:46:46.821Z,ns_1@cb.local:<0.440.0>:ns_ssl_services_setup:notify_service:1184]Failed to notify service capi_ssl_service: {'EXIT',{error,{badrpc,nodedown}}}
[ns_server:debug,2025-05-15T18:46:46.821Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.2327127311.3650355202.216120>,
                               inet_tcp_dist,<0.442.0>,
                               #Ref<0.2327127311.3650355202.216123>}
[error_logger:info,2025-05-15T18:46:46.821Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:46:46.823Z,ns_1@cb.local:users_storage<0.444.0>:replicated_storage:announce_startup:61]Announce my startup to <0.434.0>
[ns_server:debug,2025-05-15T18:46:46.823Z,ns_1@cb.local:users_storage<0.444.0>:replicated_dets:open:177]Opening file "/opt/couchbase/var/lib/couchbase/config/users.dets"
[ns_server:debug,2025-05-15T18:46:46.823Z,ns_1@cb.local:users_replicator<0.434.0>:replicated_storage:wait_for_startup:50]Received replicated storage registration from <0.444.0>
[error_logger:info,2025-05-15T18:46:46.823Z,ns_1@cb.local:users_storage_sup<0.433.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_storage_sup}
    started: [{pid,<0.444.0>},
              {id,users_storage},
              {mfargs,{menelaus_users,start_storage,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:46.823Z,ns_1@cb.local:users_sup<0.431.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_sup}
    started: [{pid,<0.433.0>},
              {id,users_storage_sup},
              {mfargs,{users_storage_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:info,2025-05-15T18:46:46.825Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:notify_services:1173]Failed to notify some services. Will retry in 5 sec, [{{error,no_proccess},
                                                       memcached},
                                                      {{'EXIT',
                                                        {error,
                                                         {badrpc,nodedown}}},
                                                       capi_ssl_service}]
[ns_server:debug,2025-05-15T18:46:46.825Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:restart_regenerate_client_cert_timer:1447]Time left before client cert regeneration: 70588800000
[ns_server:info,2025-05-15T18:46:46.828Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:validate_pkey:1032]Private key (node_cert) passphrase validation suceeded
[ns_server:info,2025-05-15T18:46:46.829Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:validate_pkey:1032]Private key (client_cert) passphrase validation suceeded
[ns_server:debug,2025-05-15T18:46:46.829Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:notify_services:1146]Going to notify following services: [memcached,capi_ssl_service]
[ns_server:warn,2025-05-15T18:46:46.829Z,ns_1@cb.local:<0.451.0>:ns_ssl_services_setup:notify_service:1184]Failed to notify service memcached: {error,no_proccess}
[error_logger:info,2025-05-15T18:46:46.829Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {2445262,#Ref<0.2327127311.3650486274.216096>}}}
[ns_server:debug,2025-05-15T18:46:46.829Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:46:46.829Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.2327127311.3650355201.216938>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:46:46.829Z,ns_1@cb.local:compiled_roles_cache<0.454.0>:versioned_cache:init:41]Starting versioned cache compiled_roles_cache
[ns_server:debug,2025-05-15T18:46:46.829Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.2327127311.3650355201.216938>,
                                  inet_tcp_dist,<0.453.0>,
                                  #Ref<0.2327127311.3650355201.216941>}
[error_logger:info,2025-05-15T18:46:46.829Z,ns_1@cb.local:users_sup<0.431.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_sup}
    started: [{pid,<0.454.0>},
              {id,compiled_roles_cache},
              {mfargs,{menelaus_roles,start_compiled_roles_cache,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:46.830Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.2327127311.3650355201.216938>,
                               inet_tcp_dist,<0.453.0>,
                               #Ref<0.2327127311.3650355201.216941>}
[error_logger:info,2025-05-15T18:46:46.830Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.453.0>,shutdown}}
[error_logger:info,2025-05-15T18:46:46.830Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:46:46.830Z,ns_1@cb.local:<0.452.0>:ns_couchdb_api:rpc_couchdb_node:152]RPC to couchdb node failed for restart_capi_ssl_service with {badrpc,nodedown}
Stack: [{ns_couchdb_api,rpc_couchdb_node,4,
                        [{file,"src/ns_couchdb_api.erl"},{line,150}]},
        {ns_ssl_services_setup,do_notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1203}]},
        {ns_ssl_services_setup,notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1179}]},
        {async,'-async_init/4-fun-1-',3,[{file,"src/async.erl"},{line,199}]}]
[ns_server:warn,2025-05-15T18:46:46.830Z,ns_1@cb.local:<0.452.0>:ns_ssl_services_setup:notify_service:1184]Failed to notify service capi_ssl_service: {'EXIT',{error,{badrpc,nodedown}}}
[ns_server:info,2025-05-15T18:46:46.832Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:notify_services:1173]Failed to notify some services. Will retry in 5 sec, [{{'EXIT',
                                                        {error,
                                                         {badrpc,nodedown}}},
                                                       capi_ssl_service},
                                                      {{error,no_proccess},
                                                       memcached}]
[error_logger:info,2025-05-15T18:46:46.833Z,ns_1@cb.local:users_sup<0.431.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_sup}
    started: [{pid,<0.458.0>},
              {id,roles_cache},
              {mfargs,{roles_cache,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:46.833Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.431.0>},
              {id,users_sup},
              {mfargs,{users_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:46.834Z,ns_1@cb.local:kernel_safe_sup<0.67.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,kernel_safe_sup}
    started: [{pid,<0.462.0>},
              {id,dets_sup},
              {mfargs,{dets_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:46.834Z,ns_1@cb.local:kernel_safe_sup<0.67.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,kernel_safe_sup}
    started: [{pid,<0.463.0>},
              {id,dets},
              {mfargs,{dets_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,2000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:46.847Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',cbas_dirs} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]},
 "/opt/couchbase/var/lib/couchbase/data"]
[ns_server:debug,2025-05-15T18:46:46.847Z,ns_1@cb.local:<0.468.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,
                                    start_couchdb_node} with delay of 5000
[ns_server:debug,2025-05-15T18:46:46.847Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',eventing_dir} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554006}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2025-05-15T18:46:46.858Z,ns_1@cb.local:users_storage<0.444.0>:replicated_dets:select_from_table:312][dets] Starting select with {users_storage,
                                [{{docv2,'_','_','_'},[],['$_']}],
                                100}
[ns_server:debug,2025-05-15T18:46:46.858Z,ns_1@cb.local:users_storage<0.444.0>:replicated_dets:init_after_ack:170]Loading 0 items, 307 words took 35ms
[error_logger:info,2025-05-15T18:46:46.866Z,ns_1@cb.local:<0.469.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.469.0>,suppress_max_restart_intensity}
    started: [{pid,<0.471.0>},
              {id,start_couchdb_node},
              {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,86400000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:46.866Z,ns_1@cb.local:<0.467.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.467.0>,suppress_max_restart_intensity}
    started: [{pid,<0.468.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      start_couchdb_node}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,start_couchdb_node},
                       5000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 5,id => start_couchdb_node,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [],restart => permanent,
                          shutdown => 86400000,
                          start => {ns_server_nodes_sup,start_couchdb_node,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:46.866Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.467.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,start_couchdb_node}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 5,id => start_couchdb_node,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [],restart => permanent,
                         shutdown => 86400000,
                         start => {ns_server_nodes_sup,start_couchdb_node,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:46.866Z,ns_1@cb.local:wait_link_to_couchdb_node<0.472.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:158]Waiting for ns_couchdb node to start
[error_logger:info,2025-05-15T18:46:46.867Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {2445263,#Ref<0.2327127311.3650486274.216096>}}}
[ns_server:debug,2025-05-15T18:46:46.867Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:46:46.867Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.2327127311.3650355204.215547>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:46:46.867Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.2327127311.3650355204.215547>,
                                  inet_tcp_dist,<0.474.0>,
                                  #Ref<0.2327127311.3650355204.215550>}
[ns_server:debug,2025-05-15T18:46:46.867Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.2327127311.3650355204.215547>,
                               inet_tcp_dist,<0.474.0>,
                               #Ref<0.2327127311.3650355204.215550>}
[error_logger:info,2025-05-15T18:46:46.867Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.474.0>,shutdown}}
[error_logger:info,2025-05-15T18:46:46.867Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:46:46.867Z,ns_1@cb.local:<0.473.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-05-15T18:46:47.068Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {2445264,#Ref<0.2327127311.3650486274.216096>}}}
[ns_server:debug,2025-05-15T18:46:47.068Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:46:47.068Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.2327127311.3650355204.215561>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:46:47.069Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.2327127311.3650355204.215561>,
                                  inet_tcp_dist,<0.476.0>,
                                  #Ref<0.2327127311.3650355204.215564>}
[ns_server:debug,2025-05-15T18:46:47.069Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.2327127311.3650355204.215561>,
                               inet_tcp_dist,<0.476.0>,
                               #Ref<0.2327127311.3650355204.215564>}
[error_logger:info,2025-05-15T18:46:47.069Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.476.0>,shutdown}}
[error_logger:info,2025-05-15T18:46:47.069Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:46:47.069Z,ns_1@cb.local:<0.473.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-05-15T18:46:47.270Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {2445265,#Ref<0.2327127311.3650486274.216096>}}}
[ns_server:debug,2025-05-15T18:46:47.270Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:46:47.271Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.2327127311.3650355203.215759>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:46:47.271Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.2327127311.3650355203.215759>,
                                  inet_tcp_dist,<0.478.0>,
                                  #Ref<0.2327127311.3650355203.215762>}
[error_logger:info,2025-05-15T18:46:47.340Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.478.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2025-05-15T18:46:47.340Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:46:47.340Z,ns_1@cb.local:<0.473.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2025-05-15T18:46:47.340Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.2327127311.3650355203.215759>,
                               inet_tcp_dist,<0.478.0>,
                               #Ref<0.2327127311.3650355203.215762>}
[error_logger:info,2025-05-15T18:46:47.541Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {2445266,#Ref<0.2327127311.3650486274.216096>}}}
[ns_server:debug,2025-05-15T18:46:47.541Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:46:47.542Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.2327127311.3650355203.215772>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:46:47.542Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.2327127311.3650355203.215772>,
                                  inet_tcp_dist,<0.480.0>,
                                  #Ref<0.2327127311.3650355203.215775>}
[ns_server:info,2025-05-15T18:46:47.547Z,ns_1@cb.local:ns_couchdb_port<0.471.0>:ns_port_server:log:226]ns_couchdb<0.471.0>: =ERROR REPORT==== 15-May-2025::18:46:47.339982 ===
ns_couchdb<0.471.0>: ** Connection attempt to/from node 'ns_1@cb.local' rejected. Cookie is not set. **
ns_couchdb<0.471.0>: 

[ns_server:debug,2025-05-15T18:46:47.565Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.2327127311.3650355203.215772>,
                               inet_tcp_dist,<0.480.0>,
                               #Ref<0.2327127311.3650355203.215775>}
[error_logger:info,2025-05-15T18:46:47.565Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.480.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2025-05-15T18:46:47.566Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:46:47.566Z,ns_1@cb.local:<0.473.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-05-15T18:46:47.767Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {2445267,#Ref<0.2327127311.3650486274.216096>}}}
[ns_server:debug,2025-05-15T18:46:47.768Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:46:47.768Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.2327127311.3650355204.215583>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:46:47.768Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.2327127311.3650355204.215583>,
                                  inet_tcp_dist,<0.482.0>,
                                  #Ref<0.2327127311.3650355204.215586>}
[ns_server:debug,2025-05-15T18:46:47.774Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.2327127311.3650355204.215583>,
                               inet_tcp_dist,<0.482.0>,
                               #Ref<0.2327127311.3650355204.215586>}
[error_logger:info,2025-05-15T18:46:47.774Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.482.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2025-05-15T18:46:47.775Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:46:47.775Z,ns_1@cb.local:<0.473.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-05-15T18:46:47.977Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {2445268,#Ref<0.2327127311.3650486274.216096>}}}
[ns_server:debug,2025-05-15T18:46:47.977Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:46:47.978Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.2327127311.3650355204.215596>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:46:47.978Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.2327127311.3650355204.215596>,
                                  inet_tcp_dist,<0.484.0>,
                                  #Ref<0.2327127311.3650355204.215599>}
[ns_server:debug,2025-05-15T18:46:47.987Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.2327127311.3650355204.215596>,
                               inet_tcp_dist,<0.484.0>,
                               #Ref<0.2327127311.3650355204.215599>}
[error_logger:info,2025-05-15T18:46:47.987Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.484.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2025-05-15T18:46:47.988Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:46:47.988Z,ns_1@cb.local:<0.473.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-05-15T18:46:48.189Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {2445269,#Ref<0.2327127311.3650486274.216096>}}}
[ns_server:debug,2025-05-15T18:46:48.190Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:46:48.190Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.2327127311.3650355203.215784>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:46:48.190Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.2327127311.3650355203.215784>,
                                  inet_tcp_dist,<0.486.0>,
                                  #Ref<0.2327127311.3650355202.216168>}
[error_logger:info,2025-05-15T18:46:48.191Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.486.0>,{recv_challenge_failed,{error,closed}}}}
[ns_server:debug,2025-05-15T18:46:48.191Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.2327127311.3650355203.215784>,
                               inet_tcp_dist,<0.486.0>,
                               #Ref<0.2327127311.3650355202.216168>}
[ns_server:debug,2025-05-15T18:46:48.191Z,ns_1@cb.local:<0.473.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-05-15T18:46:48.191Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[error_logger:info,2025-05-15T18:46:48.392Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {2445270,#Ref<0.2327127311.3650486274.216096>}}}
[ns_server:debug,2025-05-15T18:46:48.393Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:46:48.393Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.2327127311.3650355202.216177>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:46:48.393Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.2327127311.3650355202.216177>,
                                  inet_tcp_dist,<0.488.0>,
                                  #Ref<0.2327127311.3650355202.216180>}
[error_logger:info,2025-05-15T18:46:48.396Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.488.0>,{recv_challenge_failed,{error,closed}}}}
[ns_server:debug,2025-05-15T18:46:48.396Z,ns_1@cb.local:<0.473.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2025-05-15T18:46:48.397Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.2327127311.3650355202.216177>,
                               inet_tcp_dist,<0.488.0>,
                               #Ref<0.2327127311.3650355202.216180>}
[error_logger:info,2025-05-15T18:46:48.396Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[error_logger:info,2025-05-15T18:46:48.598Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {2445271,#Ref<0.2327127311.3650486274.216096>}}}
[ns_server:debug,2025-05-15T18:46:48.598Z,ns_1@cb.local:net_kernel<0.229.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:46:48.598Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.2327127311.3650355205.215658>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:46:48.599Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.2327127311.3650355205.215658>,
                                  inet_tcp_dist,<0.490.0>,
                                  #Ref<0.2327127311.3650355205.215661>}
[ns_server:debug,2025-05-15T18:46:48.605Z,ns_1@cb.local:<0.473.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: false
[ns_server:info,2025-05-15T18:46:49.030Z,ns_1@cb.local:ns_couchdb_port<0.471.0>:ns_port_server:log:226]ns_couchdb<0.471.0>: Apache CouchDB v4.5.1-330-g3e5b8f24 (LogLevel=info) is starting.
ns_couchdb<0.471.0>: Apache CouchDB has started. Time to relax.

[error_logger:info,2025-05-15T18:46:49.073Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.472.0>},
              {id,wait_for_couchdb_node},
              {mfargs,{erlang,apply,
                              [#Fun<ns_server_nodes_sup.0.120652322>,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:49.076Z,ns_1@cb.local:<0.496.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,ns_disksup} with delay of 4000
[error_logger:info,2025-05-15T18:46:49.078Z,ns_1@cb.local:<0.497.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.497.0>,suppress_max_restart_intensity}
    started: [{pid,<0.498.0>},
              {id,ns_disksup},
              {mfargs,{ns_disksup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.078Z,ns_1@cb.local:<0.495.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.495.0>,suppress_max_restart_intensity}
    started: [{pid,<0.496.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      ns_disksup}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,ns_disksup},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => ns_disksup,inherited_max_r => 20,
                          inherited_max_t => 10,modules => [],
                          restart => permanent,shutdown => 1000,
                          start => {ns_disksup,start_link,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:49.078Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.495.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,ns_disksup}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => ns_disksup,inherited_max_r => 20,
                         inherited_max_t => 10,modules => [],
                         restart => permanent,shutdown => 1000,
                         start => {ns_disksup,start_link,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:49.080Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.499.0>},
              {id,diag_handler_worker},
              {mfargs,{work_queue,start_link,[diag_handler_worker]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:49.081Z,ns_1@cb.local:ns_server_sup<0.494.0>:dir_size:start_link:33]Starting quick version of dir_size with program name: godu
[error_logger:info,2025-05-15T18:46:49.081Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.500.0>},
              {id,dir_size},
              {mfargs,{dir_size,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.082Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.501.0>},
              {id,request_tracker},
              {mfargs,{request_tracker,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.084Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.502.0>},
              {id,chronicle_kv_log},
              {mfargs,{chronicle_kv_log,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:warn,2025-05-15T18:46:49.087Z,ns_1@cb.local:ns_log<0.504.0>:gossip_replicator:read_logs:244]Couldn't load logs from "/opt/couchbase/var/lib/couchbase/ns_log" (perhaps it's first
                         startup): {error,enoent}
[error_logger:info,2025-05-15T18:46:49.087Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.504.0>},
              {id,ns_log},
              {mfargs,{ns_log,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.087Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.505.0>},
              {id,event_log_events},
              {mfargs,{gen_event,start_link,[{local,event_log_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:warn,2025-05-15T18:46:49.088Z,ns_1@cb.local:event_log_server<0.506.0>:gossip_replicator:read_logs:244]Couldn't load logs from "/opt/couchbase/var/lib/couchbase/event_log" (perhaps it's first
                         startup): {error,enoent}
[error_logger:info,2025-05-15T18:46:49.088Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.506.0>},
              {id,event_log_server},
              {mfargs,{event_log_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.094Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.508.0>},
              {id,initargs_updater},
              {mfargs,{initargs_updater,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.096Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.510.0>},
              {id,timer_lag_recorder},
              {mfargs,{timer_lag_recorder,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:49.096Z,ns_1@cb.local:<0.512.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,
                                    ns_babysitter_log_consumer} with delay of 4000
[error_logger:info,2025-05-15T18:46:49.096Z,ns_1@cb.local:<0.513.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.513.0>,suppress_max_restart_intensity}
    started: [{pid,<0.514.0>},
              {id,ns_babysitter_log_consumer},
              {mfargs,{ns_log,start_link_babysitter_log_consumer,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.096Z,ns_1@cb.local:<0.511.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.511.0>,suppress_max_restart_intensity}
    started: [{pid,<0.512.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      ns_babysitter_log_consumer}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,
                           ns_babysitter_log_consumer},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => ns_babysitter_log_consumer,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [],restart => permanent,shutdown => 1000,
                          start =>
                              {ns_log,start_link_babysitter_log_consumer,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:49.096Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.511.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,
                      ns_babysitter_log_consumer}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => ns_babysitter_log_consumer,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [],restart => permanent,shutdown => 1000,
                         start =>
                             {ns_log,start_link_babysitter_log_consumer,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:49.195Z,ns_1@cb.local:<0.519.0>:goport:handle_eof:592]Stream 'stdout' closed
[ns_server:debug,2025-05-15T18:46:49.196Z,ns_1@cb.local:<0.519.0>:goport:handle_eof:592]Stream 'stderr' closed
[ns_server:info,2025-05-15T18:46:49.196Z,ns_1@cb.local:<0.519.0>:goport:handle_process_exit:573]Port exited with status 0.
[ns_server:debug,2025-05-15T18:46:49.198Z,ns_1@cb.local:prometheus_cfg<0.515.0>:prometheus_cfg:ensure_prometheus_config:932]Updating prometheus config file: /opt/couchbase/var/lib/couchbase/config/prometheus.yml
[ns_server:debug,2025-05-15T18:46:49.202Z,ns_1@cb.local:prometheus_cfg<0.515.0>:prometheus_cfg:ensure_prometheus_config:932]Updating prometheus config file: /opt/couchbase/var/lib/couchbase/config/prometheus_rules.yml
[ns_server:debug,2025-05-15T18:46:49.217Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',prometheus_auth_info} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554009}}]}|
 {"@prometheus",
  {auth,
   [{<<"plain">>,
     {sanitized,<<"Q2PI1LZ3LRw1+eahlGLKWuNQSDehz+cngHMPcRrTMGs=">>}},
    {<<"sha512">>,
     {[{<<"s">>,
        <<"Jffvy8Tr2Sfkw9Or77EPWyC5XrONON1fv8P0ftW9TdaLdXFKO+cU/xzoKI9dM5sOna4o1EQdH/BghYcmTb/Tvg==">>},
       {<<"h">>,
        {sanitized,<<"nEStGaA5ChdwJB6lNCVe1Cp/w5Ch6BwPZKPG4LBt2ts=">>}},
       {<<"i">>,15000}]}},
    {<<"sha256">>,
     {[{<<"s">>,<<"MDz52quNpi8sw747d4hHbca+3A1i4W51vCCcHiSYGUU=">>},
       {<<"h">>,
        {sanitized,<<"lt6CtKSbZX3u2XG/5UgF//eVok7BNZvwDod/UCJgDMg=">>}},
       {<<"i">>,15000}]}},
    {<<"sha1">>,
     {[{<<"s">>,<<"M240+/cxO54Cn4iI9xpa0hLKnEk=">>},
       {<<"h">>,
        {sanitized,<<"vtXUN6LYKdPj0OyNcU05NHqEcBVL+ixnRE9R/0nMCXc=">>}},
       {<<"i">>,15000}]}}]}}]
[ns_server:debug,2025-05-15T18:46:49.219Z,ns_1@cb.local:prometheus_cfg<0.515.0>:prometheus_cfg:apply_config:724]Restarting Prometheus as the start specs have changed
[error_logger:info,2025-05-15T18:46:49.223Z,ns_1@cb.local:ale_dynamic_sup<0.78.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ale_dynamic_sup}
    started: [{pid,<0.522.0>},
              {id,'sink-prometheus'},
              {mfargs,
                  {ale_dynamic_sup,delay_death,
                      [{ale_disk_sink,start_link,
                           ['sink-prometheus',
                            "/opt/couchbase/var/lib/couchbase/logs/prometheus.log",
                            [{rotation,
                                 [{compress,true},
                                  {size,41943040},
                                  {num_files,10},
                                  {buffer_size_max,52428800}]}]]},
                       1000]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:49.284Z,ns_1@cb.local:ns_couchdb_port<0.471.0>:ns_port_server:log:226]ns_couchdb<0.471.0>: 218: Booted. Waiting for shutdown request
ns_couchdb<0.471.0>: 218: Booted. Waiting for shutdown request
ns_couchdb<0.471.0>: working as port

[error_logger:info,2025-05-15T18:46:49.366Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.515.0>},
              {id,prometheus_cfg},
              {mfargs,{prometheus_cfg,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:49.370Z,ns_1@cb.local:memcached_passwords<0.528.0>:memcached_cfg:init:60]Init config writer for memcached_passwords, "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2025-05-15T18:46:49.373Z,ns_1@cb.local:memcached_passwords<0.528.0>:memcached_cfg:write_cfg:156]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2025-05-15T18:46:49.387Z,ns_1@cb.local:memcached_passwords<0.528.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,{auth,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2025-05-15T18:46:49.389Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:handle_call:57]File rename from "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw" is requested
[ns_server:debug,2025-05-15T18:46:49.390Z,ns_1@cb.local:memcached_passwords<0.528.0>:memcached_cfg:rename_and_refresh:178]Successfully renamed "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2025-05-15T18:46:49.390Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:handle_cast:67]Refresh of isasl requested
[ns_server:debug,2025-05-15T18:46:49.390Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:update_refresh_state:137]Refresh of [isasl] skipped. Retry in 1000 ms.
[error_logger:info,2025-05-15T18:46:49.390Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.528.0>},
              {id,memcached_passwords},
              {mfargs,{memcached_passwords,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:49.394Z,ns_1@cb.local:memcached_permissions<0.531.0>:memcached_cfg:init:60]Init config writer for memcached_permissions, "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2025-05-15T18:46:49.397Z,ns_1@cb.local:memcached_permissions<0.531.0>:memcached_cfg:write_cfg:156]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2025-05-15T18:46:49.398Z,ns_1@cb.local:memcached_permissions<0.531.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,{user,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2025-05-15T18:46:49.404Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:handle_call:57]File rename from "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac" is requested
[ns_server:debug,2025-05-15T18:46:49.406Z,ns_1@cb.local:memcached_permissions<0.531.0>:memcached_cfg:rename_and_refresh:178]Successfully renamed "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2025-05-15T18:46:49.406Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:handle_cast:67]Refresh of rbac requested
[error_logger:info,2025-05-15T18:46:49.406Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.531.0>},
              {id,memcached_permissions},
              {mfargs,{memcached_permissions,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.409Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.534.0>},
              {id,ns_email_alert},
              {mfargs,{ns_email_alert,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.412Z,ns_1@cb.local:ns_node_disco_sup<0.535.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_node_disco_sup}
    started: [{pid,<0.536.0>},
              {id,ns_node_disco_events},
              {mfargs,{gen_event,start_link,[{local,ns_node_disco_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:49.412Z,ns_1@cb.local:ns_node_disco<0.537.0>:ns_node_disco:init:111]Initting ns_node_disco with []
[error_logger:info,2025-05-15T18:46:49.413Z,ns_1@cb.local:ns_node_disco_sup<0.535.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_node_disco_sup}
    started: [{pid,<0.537.0>},
              {id,ns_node_disco},
              {mfargs,{ns_node_disco,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:49.413Z,ns_1@cb.local:ns_cookie_manager<0.238.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[user:info,2025-05-15T18:46:49.413Z,ns_1@cb.local:ns_cookie_manager<0.238.0>:ns_cookie_manager:do_cookie_init:78]Initial otp cookie generated: {sanitized,
                                  <<"biSYTaQUFDVndTs/b+IcDmD2L0woktv9naRdv3GCK6A=">>}
[ns_server:debug,2025-05-15T18:46:49.413Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
otp ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554009}}]},
 {cookie,{sanitized,<<"biSYTaQUFDVndTs/b+IcDmD2L0woktv9naRdv3GCK6A=">>}}]
[ns_server:debug,2025-05-15T18:46:49.413Z,ns_1@cb.local:ns_cookie_manager<0.238.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[ns_server:debug,2025-05-15T18:46:49.414Z,ns_1@cb.local:<0.539.0>:ns_node_disco:do_nodes_wanted_updated_fun:210]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"biSYTaQUFDVndTs/b+IcDmD2L0woktv9naRdv3GCK6A=">>}
[ns_server:debug,2025-05-15T18:46:49.414Z,ns_1@cb.local:<0.541.0>:ns_node_disco:do_nodes_wanted_updated_fun:210]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"biSYTaQUFDVndTs/b+IcDmD2L0woktv9naRdv3GCK6A=">>}
[error_logger:info,2025-05-15T18:46:49.415Z,ns_1@cb.local:ns_node_disco_sup<0.535.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_node_disco_sup}
    started: [{pid,<0.542.0>},
              {id,ns_node_disco_log},
              {mfargs,{ns_node_disco_log,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:49.418Z,ns_1@cb.local:<0.539.0>:ns_node_disco:do_nodes_wanted_updated_fun:213]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"biSYTaQUFDVndTs/b+IcDmD2L0woktv9naRdv3GCK6A=">>}
[ns_server:debug,2025-05-15T18:46:49.418Z,ns_1@cb.local:<0.541.0>:ns_node_disco:do_nodes_wanted_updated_fun:213]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"biSYTaQUFDVndTs/b+IcDmD2L0woktv9naRdv3GCK6A=">>}
[error_logger:info,2025-05-15T18:46:49.423Z,ns_1@cb.local:ns_config_rep_sup<0.543.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_rep_sup}
    started: [{pid,<0.544.0>},
              {id,ns_config_rep_merger},
              {mfargs,{ns_config_rep,start_link_merger,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:49.423Z,ns_1@cb.local:ns_config_rep<0.545.0>:ns_config_rep:init:80]init pulling
[ns_server:debug,2025-05-15T18:46:49.423Z,ns_1@cb.local:ns_config_rep<0.545.0>:ns_config_rep:init:82]init pushing
[error_logger:info,2025-05-15T18:46:49.424Z,ns_1@cb.local:ns_config_rep_sup<0.543.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_rep_sup}
    started: [{pid,<0.545.0>},
              {id,ns_config_rep},
              {mfargs,{ns_config_rep,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.424Z,ns_1@cb.local:ns_node_disco_sup<0.535.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_node_disco_sup}
    started: [{pid,<0.543.0>},
              {id,ns_config_rep_sup},
              {mfargs,{ns_config_rep_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:49.424Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.535.0>},
              {id,ns_node_disco_sup},
              {mfargs,{ns_node_disco_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:49.425Z,ns_1@cb.local:tombstone_keeper<0.277.0>:tombstone_keeper:handle_call:49]Refreshed with timestamps []
[error_logger:info,2025-05-15T18:46:49.425Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.555.0>},
              {id,tombstone_agent},
              {mfargs,{tombstone_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.427Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.557.0>},
              {id,vbucket_map_mirror},
              {mfargs,{vbucket_map_mirror,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.429Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.559.0>},
              {id,capi_url_cache},
              {mfargs,{capi_url_cache,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.435Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.561.0>},
              {id,bucket_info_cache},
              {mfargs,{bucket_info_cache,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.435Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.564.0>},
              {id,ns_tick_event},
              {mfargs,{gen_event,start_link,[{local,ns_tick_event}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.435Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.565.0>},
              {id,buckets_events},
              {mfargs,{gen_event,start_link,[{local,buckets_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.435Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.566.0>},
              {id,ns_stats_event},
              {mfargs,{gen_event,start_link,[{local,ns_stats_event}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.438Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.567.0>},
              {id,samples_loader_tasks},
              {mfargs,{samples_loader_tasks,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.442Z,ns_1@cb.local:ns_heart_sup<0.568.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_heart_sup}
    started: [{pid,<0.569.0>},
              {id,ns_heart},
              {mfargs,{ns_heart,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.442Z,ns_1@cb.local:ns_heart_sup<0.568.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_heart_sup}
    started: [{pid,<0.571.0>},
              {id,ns_heart_slow_updater},
              {mfargs,{ns_heart,start_link_slow_updater,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.442Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.568.0>},
              {id,ns_heart_sup},
              {mfargs,{ns_heart_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:49.445Z,ns_1@cb.local:ns_doctor_sup<0.573.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_doctor_sup}
    started: [{pid,<0.574.0>},
              {id,ns_doctor_events},
              {mfargs,{gen_event,start_link,[{local,ns_doctor_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.457Z,ns_1@cb.local:ns_doctor_sup<0.573.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_doctor_sup}
    started: [{pid,<0.577.0>},
              {id,ns_doctor},
              {mfargs,{ns_doctor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:49.457Z,ns_1@cb.local:<0.572.0>:restartable:start_child:92]Started child process <0.573.0>
  MFA: {ns_doctor_sup,start_link,[]}
[error_logger:info,2025-05-15T18:46:49.457Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.572.0>},
              {id,ns_doctor_sup},
              {mfargs,{restartable,start_link,
                                   [{ns_doctor_sup,start_link,[]},infinity]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:49.457Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.580.0>},
              {id,master_activity_events},
              {mfargs,{gen_event,start_link,[{local,master_activity_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.463Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.581.0>},
              {id,xdcr_ckpt_store},
              {mfargs,{simple_store,start_link,[xdcr_ckpt_data]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.465Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.582.0>},
              {id,metakv_worker},
              {mfargs,{work_queue,start_link,[metakv_worker]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.465Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.584.0>},
              {id,index_events},
              {mfargs,{gen_event,start_link,[{local,index_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.465Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.585.0>},
              {id,index_settings_manager},
              {mfargs,{index_settings_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.466Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.589.0>},
              {id,query_settings_manager},
              {mfargs,{query_settings_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.466Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.591.0>},
              {id,eventing_settings_manager},
              {mfargs,{eventing_settings_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.466Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.593.0>},
              {id,analytics_settings_manager},
              {mfargs,{analytics_settings_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.466Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.595.0>},
              {id,audit_events},
              {mfargs,{gen_event,start_link,[{local,audit_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:49.466Z,ns_1@cb.local:<0.597.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,
                                    encryption_service} with delay of 1000
[ns_server:debug,2025-05-15T18:46:49.467Z,ns_1@cb.local:encryption_service<0.599.0>:encryption_service:recover:199]Config marker /opt/couchbase/var/lib/couchbase/config/sm_load_config_marker doesn't exist
[error_logger:info,2025-05-15T18:46:49.467Z,ns_1@cb.local:<0.598.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.598.0>,suppress_max_restart_intensity}
    started: [{pid,<0.599.0>},
              {id,encryption_service},
              {mfargs,{encryption_service,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.467Z,ns_1@cb.local:<0.596.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.596.0>,suppress_max_restart_intensity}
    started: [{pid,<0.597.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      encryption_service}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,encryption_service},
                       1000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 1,id => encryption_service,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [encryption_service],
                          restart => permanent,shutdown => 5000,
                          start => {encryption_service,start_link,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:49.467Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.596.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,encryption_service}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 1,id => encryption_service,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [encryption_service],
                         restart => permanent,shutdown => 5000,
                         start => {encryption_service,start_link,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:49.479Z,ns_1@cb.local:menelaus_sup<0.601.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.607.0>},
              {id,menelaus_ui_auth},
              {mfargs,{menelaus_ui_auth,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.479Z,ns_1@cb.local:menelaus_sup<0.601.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.609.0>},
              {id,scram_sha},
              {mfargs,{scram_sha,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.486Z,ns_1@cb.local:menelaus_sup<0.601.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.610.0>},
              {id,menelaus_local_auth},
              {mfargs,{menelaus_local_auth,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.495Z,ns_1@cb.local:menelaus_sup<0.601.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.612.0>},
              {id,menelaus_web_cache},
              {mfargs,{menelaus_web_cache,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.505Z,ns_1@cb.local:menelaus_sup<0.601.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.620.0>},
              {id,menelaus_stats_gatherer},
              {mfargs,{menelaus_stats_gatherer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.505Z,ns_1@cb.local:menelaus_sup<0.601.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.621.0>},
              {id,json_rpc_events},
              {mfargs,{gen_event,start_link,[{local,json_rpc_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.524Z,ns_1@cb.local:menelaus_web_sup<0.622.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_web_sup}
    started: [{pid,<0.624.0>},
              {id,menelaus_event},
              {mfargs,{menelaus_event,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:49.525Z,ns_1@cb.local:<0.626.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2025-05-15T18:46:49.526Z,ns_1@cb.local:<0.626.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2025-05-15T18:46:49.527Z,ns_1@cb.local:<0.626.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2025-05-15T18:46:49.528Z,ns_1@cb.local:<0.626.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2025-05-15T18:46:49.528Z,ns_1@cb.local:<0.626.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2025-05-15T18:46:49.528Z,ns_1@cb.local:<0.626.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"0.0.0.0"},{port,8091}]
[error_logger:info,2025-05-15T18:46:49.528Z,ns_1@cb.local:<0.626.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.626.0>,menelaus_web}
    started: [{pid,<0.627.0>},
              {id,menelaus_web_ipv4},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet},
                        {name,menelaus_web},
                        {port,8091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.528Z,ns_1@cb.local:inet_gethost_native_sup<0.644.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,inet_gethost_native_sup}
    started: [{pid,<0.645.0>},{mfa,{inet_gethost_native,init,[[]]}}]

[error_logger:info,2025-05-15T18:46:49.529Z,ns_1@cb.local:kernel_safe_sup<0.67.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,kernel_safe_sup}
    started: [{pid,<0.644.0>},
              {id,inet_gethost_native_sup},
              {mfargs,{inet_gethost_native,start_link,[]}},
              {restart_type,temporary},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:49.529Z,ns_1@cb.local:<0.626.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2025-05-15T18:46:49.529Z,ns_1@cb.local:<0.626.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2025-05-15T18:46:49.529Z,ns_1@cb.local:<0.626.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2025-05-15T18:46:49.530Z,ns_1@cb.local:<0.626.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2025-05-15T18:46:49.530Z,ns_1@cb.local:<0.626.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2025-05-15T18:46:49.530Z,ns_1@cb.local:<0.626.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"::"},{port,8091}]
[error_logger:info,2025-05-15T18:46:49.530Z,ns_1@cb.local:<0.626.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.626.0>,menelaus_web}
    started: [{pid,<0.646.0>},
              {id,menelaus_web_ipv6},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet6},
                        {name,menelaus_web},
                        {port,8091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:49.530Z,ns_1@cb.local:<0.625.0>:restartable:start_child:92]Started child process <0.626.0>
  MFA: {menelaus_web,start_link,[]}
[error_logger:info,2025-05-15T18:46:49.530Z,ns_1@cb.local:menelaus_web_sup<0.622.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_web_sup}
    started: [{pid,<0.625.0>},
              {id,menelaus_web},
              {mfargs,{restartable,start_link,
                                   [{menelaus_web,start_link,[]},infinity]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[user:info,2025-05-15T18:46:49.530Z,ns_1@cb.local:menelaus_sup<0.601.0>:menelaus_web_sup:start_link:38]Couchbase Server has started on web port 8091 on node 'ns_1@cb.local'. Version: "7.6.2-3721-enterprise".
[error_logger:info,2025-05-15T18:46:49.530Z,ns_1@cb.local:menelaus_sup<0.601.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.622.0>},
              {id,menelaus_web_sup},
              {mfargs,{menelaus_web_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:49.531Z,ns_1@cb.local:menelaus_sup<0.601.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.663.0>},
              {id,menelaus_web_alerts_srv},
              {mfargs,{menelaus_web_alerts_srv,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.534Z,ns_1@cb.local:menelaus_sup<0.601.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.665.0>},
              {id,guardrail_monitor},
              {mfargs,{guardrail_monitor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.537Z,ns_1@cb.local:menelaus_sup<0.601.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.666.0>},
              {id,menelaus_cbauth},
              {mfargs,{menelaus_cbauth,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.537Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.601.0>},
              {id,menelaus},
              {mfargs,{menelaus_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:49.537Z,ns_1@cb.local:<0.673.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,ns_ports_setup} with delay of 4000
[error_logger:info,2025-05-15T18:46:49.537Z,ns_1@cb.local:<0.674.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.674.0>,suppress_max_restart_intensity}
    started: [{pid,<0.675.0>},
              {id,ns_ports_setup},
              {mfargs,{ns_ports_setup,start,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.537Z,ns_1@cb.local:<0.672.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.672.0>,suppress_max_restart_intensity}
    started: [{pid,<0.673.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      ns_ports_setup}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,ns_ports_setup},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => ns_ports_setup,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [],restart => permanent,
                          shutdown => brutal_kill,
                          start => {ns_ports_setup,start,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:49.537Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.672.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,ns_ports_setup}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => ns_ports_setup,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [],restart => permanent,
                         shutdown => brutal_kill,
                         start => {ns_ports_setup,start,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:49.539Z,ns_1@cb.local:service_agent_sup<0.678.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_agent_sup}
    started: [{pid,<0.679.0>},
              {id,service_agent_children_sup},
              {mfargs,{supervisor,start_link,
                                  [{local,service_agent_children_sup},
                                   service_agent_sup,child]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:49.540Z,ns_1@cb.local:service_agent_sup<0.678.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_agent_sup}
    started: [{pid,<0.680.0>},
              {id,service_agent_worker},
              {mfargs,{erlang,apply,[#Fun<service_agent_sup.0.125430937>,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.540Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.678.0>},
              {id,service_agent_sup},
              {mfargs,{service_agent_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:49.540Z,ns_1@cb.local:ns_ports_setup<0.675.0>:ns_ports_manager:set_dynamic_children:48]Setting children [memcached,saslauthd_port,goxdcr]
[error_logger:info,2025-05-15T18:46:49.544Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.682.0>},
              {id,ns_memcached_sockets_pool},
              {mfargs,{ns_memcached_sockets_pool,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:49.547Z,ns_1@cb.local:memcached_auth_server<0.683.0>:memcached_auth_server:reconnect:239]Skipping creation of 'Auth provider' connection because external users are disabled
[error_logger:info,2025-05-15T18:46:49.547Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.683.0>},
              {id,memcached_auth_server},
              {mfargs,{memcached_auth_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:49.547Z,ns_1@cb.local:<0.686.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,ns_audit_cfg} with delay of 4000
[ns_server:debug,2025-05-15T18:46:49.548Z,ns_1@cb.local:ns_audit_cfg<0.688.0>:ns_audit_cfg:write_audit_json:238]Writing new content to "/opt/couchbase/var/lib/couchbase/config/audit.json", Params [{descriptors_path,
                                                                                      "/opt/couchbase/etc/security"},
                                                                                     {version,
                                                                                      2},
                                                                                     {uuid,
                                                                                      "33985209"},
                                                                                     {event_states,
                                                                                      {[]}},
                                                                                     {filtering_enabled,
                                                                                      true},
                                                                                     {disabled_userids,
                                                                                      []},
                                                                                     {auditd_enabled,
                                                                                      false},
                                                                                     {log_path,
                                                                                      "/opt/couchbase/var/lib/couchbase/logs"},
                                                                                     {prune_age,
                                                                                      0},
                                                                                     {rotate_interval,
                                                                                      86400},
                                                                                     {rotate_size,
                                                                                      20971520},
                                                                                     {sync,
                                                                                      []}]
[ns_server:debug,2025-05-15T18:46:49.551Z,ns_1@cb.local:ns_audit_cfg<0.688.0>:ns_audit_cfg:notify_memcached:152]Instruct memcached to reload audit config
[error_logger:info,2025-05-15T18:46:49.551Z,ns_1@cb.local:<0.687.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.687.0>,suppress_max_restart_intensity}
    started: [{pid,<0.688.0>},
              {id,ns_audit_cfg},
              {mfargs,{ns_audit_cfg,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.551Z,ns_1@cb.local:<0.685.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.685.0>,suppress_max_restart_intensity}
    started: [{pid,<0.686.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      ns_audit_cfg}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,ns_audit_cfg},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => ns_audit_cfg,inherited_max_r => 20,
                          inherited_max_t => 10,modules => [],
                          restart => permanent,shutdown => 1000,
                          start => {ns_audit_cfg,start_link,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:49.551Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.685.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,ns_audit_cfg}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => ns_audit_cfg,inherited_max_r => 20,
                         inherited_max_t => 10,modules => [],
                         restart => permanent,shutdown => 1000,
                         start => {ns_audit_cfg,start_link,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:49.551Z,ns_1@cb.local:<0.691.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,ns_audit} with delay of 4000
[ns_server:warn,2025-05-15T18:46:49.552Z,ns_1@cb.local:<0.694.0>:ns_memcached:connect:1460]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}, retrying.
[error_logger:info,2025-05-15T18:46:49.556Z,ns_1@cb.local:<0.692.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.692.0>,suppress_max_restart_intensity}
    started: [{pid,<0.696.0>},
              {id,ns_audit},
              {mfargs,{ns_audit,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.557Z,ns_1@cb.local:<0.690.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.690.0>,suppress_max_restart_intensity}
    started: [{pid,<0.691.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      ns_audit}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,ns_audit},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => ns_audit,inherited_max_r => 20,
                          inherited_max_t => 10,modules => [],
                          restart => permanent,shutdown => 1000,
                          start => {ns_audit,start_link,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:49.557Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.690.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,ns_audit}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => ns_audit,inherited_max_r => 20,
                         inherited_max_t => 10,modules => [],
                         restart => permanent,shutdown => 1000,
                         start => {ns_audit,start_link,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:49.557Z,ns_1@cb.local:<0.699.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,
                                    memcached_config_mgr} with delay of 4000
[ns_server:debug,2025-05-15T18:46:49.557Z,ns_1@cb.local:memcached_config_mgr<0.701.0>:memcached_config_mgr:memcached_port_pid:154]waiting for completion of initial ns_ports_setup round
[error_logger:info,2025-05-15T18:46:49.557Z,ns_1@cb.local:<0.700.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.700.0>,suppress_max_restart_intensity}
    started: [{pid,<0.701.0>},
              {id,memcached_config_mgr},
              {mfargs,{memcached_config_mgr,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.557Z,ns_1@cb.local:<0.698.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.698.0>,suppress_max_restart_intensity}
    started: [{pid,<0.699.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      memcached_config_mgr}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,memcached_config_mgr},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => memcached_config_mgr,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [],restart => permanent,shutdown => 1000,
                          start => {memcached_config_mgr,start_link,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:49.557Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.698.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,memcached_config_mgr}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => memcached_config_mgr,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [],restart => permanent,shutdown => 1000,
                         start => {memcached_config_mgr,start_link,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:info,2025-05-15T18:46:49.565Z,ns_1@cb.local:<0.702.0>:ns_memcached_log_rotator:init:36]Starting log rotator on "/opt/couchbase/var/lib/couchbase/logs"/"memcached.log"* with an initial period of 39003ms
[error_logger:info,2025-05-15T18:46:49.565Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.702.0>},
              {id,ns_memcached_log_rotator},
              {mfargs,{ns_memcached_log_rotator,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.570Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.703.0>},
              {id,testconditions_store},
              {mfargs,{simple_store,start_link,[testconditions]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:49.575Z,ns_1@cb.local:<0.704.0>:memcached_config_mgr:memcached_port_pid:154]waiting for completion of initial ns_ports_setup round
[error_logger:info,2025-05-15T18:46:49.575Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.704.0>},
              {id,terse_cluster_info_uploader},
              {mfargs,{terse_cluster_info_uploader,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.598Z,ns_1@cb.local:ns_bucket_worker_sup<0.707.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_bucket_worker_sup}
    started: [{pid,<0.708.0>},
              {id,ns_bucket_sup},
              {mfargs,{ns_bucket_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:49.609Z,ns_1@cb.local:ns_bucket_worker_sup<0.707.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_bucket_worker_sup}
    started: [{pid,<0.709.0>},
              {id,ns_bucket_worker},
              {mfargs,{ns_bucket_worker,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.610Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.707.0>},
              {id,ns_bucket_worker_sup},
              {mfargs,{ns_bucket_worker_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:warn,2025-05-15T18:46:49.611Z,ns_1@cb.local:<0.715.0>:ns_memcached:connect:1460]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}, retrying.
[error_logger:info,2025-05-15T18:46:49.616Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.712.0>},
              {id,ns_server_stats},
              {mfargs,{ns_server_stats,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:49.627Z,ns_1@cb.local:<0.714.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ale_stats_events,<0.712.0>} exited with reason {noproc,
                                                                                {gen_statem,
                                                                                 call,
                                                                                 [mb_master,
                                                                                  master_node,
                                                                                  infinity]}}
[error_logger:info,2025-05-15T18:46:49.635Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.719.0>},
              {id,{stats_reader,"@system"}},
              {mfargs,{stats_reader,start_link,["@system"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.635Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.721.0>},
              {id,{stats_reader,"@system-processes"}},
              {mfargs,{stats_reader,start_link,["@system-processes"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.635Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.723.0>},
              {id,{stats_reader,"@query"}},
              {mfargs,{stats_reader,start_link,["@query"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.635Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.725.0>},
              {id,{stats_reader,"@global"}},
              {mfargs,{stats_reader,start_link,["@global"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.638Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.729.0>},
              {id,goxdcr_status_keeper},
              {mfargs,{goxdcr_status_keeper,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.641Z,ns_1@cb.local:services_stats_sup<0.730.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,services_stats_sup}
    started: [{pid,<0.731.0>},
              {id,service_stats_children_sup},
              {mfargs,{supervisor,start_link,
                                  [{local,service_stats_children_sup},
                                   services_stats_sup,child]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:49.643Z,ns_1@cb.local:service_status_keeper_sup<0.732.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_status_keeper_sup}
    started: [{pid,<0.733.0>},
              {id,service_status_keeper_worker},
              {mfargs,{work_queue,start_link,[service_status_keeper_worker]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.650Z,ns_1@cb.local:service_status_keeper_sup<0.732.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_status_keeper_sup}
    started: [{pid,<0.736.0>},
              {id,service_status_keeper_index},
              {mfargs,{service_index,start_keeper,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:49.652Z,ns_1@cb.local:ns_heart<0.569.0>:goxdcr_rest:get_from_goxdcr:139]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2025-05-15T18:46:49.652Z,ns_1@cb.local:goxdcr_status_keeper<0.729.0>:goxdcr_rest:get_from_goxdcr:139]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2025-05-15T18:46:49.653Z,ns_1@cb.local:goxdcr_status_keeper<0.729.0>:goxdcr_rest:get_from_goxdcr:139]Goxdcr is temporary not available. Return empty list.
[error_logger:info,2025-05-15T18:46:49.653Z,ns_1@cb.local:service_status_keeper_sup<0.732.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_status_keeper_sup}
    started: [{pid,<0.740.0>},
              {id,service_status_keeper_fts},
              {mfargs,{service_fts,start_keeper,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:49.656Z,ns_1@cb.local:ns_heart<0.569.0>:cluster_logs_collection_task:maybe_build_cluster_logs_task:40]Ignoring exception trying to read cluster_logs_collection_task_status table: error:badarg
[error_logger:info,2025-05-15T18:46:49.658Z,ns_1@cb.local:service_status_keeper_sup<0.732.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_status_keeper_sup}
    started: [{pid,<0.743.0>},
              {id,service_status_keeper_eventing},
              {mfargs,{service_eventing,start_keeper,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.658Z,ns_1@cb.local:services_stats_sup<0.730.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,services_stats_sup}
    started: [{pid,<0.732.0>},
              {id,service_status_keeper_sup},
              {mfargs,{service_status_keeper_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:49.659Z,ns_1@cb.local:services_stats_sup<0.730.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,services_stats_sup}
    started: [{pid,<0.746.0>},
              {id,service_stats_worker},
              {mfargs,{erlang,apply,[#Fun<services_stats_sup.0.35188242>,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.659Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.730.0>},
              {id,services_stats_sup},
              {mfargs,{services_stats_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:49.660Z,ns_1@cb.local:<0.750.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,
                                    compaction_daemon} with delay of 4000
[ns_server:debug,2025-05-15T18:46:49.664Z,ns_1@cb.local:<0.758.0>:new_concurrency_throttle:init:109]init concurrent throttle process, pid: <0.758.0>, type: kv_throttle# of available token: 1
[ns_server:debug,2025-05-15T18:46:49.666Z,ns_1@cb.local:compaction_daemon<0.754.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_kv. Rescheduling compaction.
[error_logger:info,2025-05-15T18:46:49.666Z,ns_1@cb.local:<0.751.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.751.0>,suppress_max_restart_intensity}
    started: [{pid,<0.754.0>},
              {id,compaction_daemon},
              {mfargs,{compaction_daemon,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,86400000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:49.666Z,ns_1@cb.local:compaction_daemon<0.754.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:46:49.667Z,ns_1@cb.local:compaction_daemon<0.754.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2025-05-15T18:46:49.667Z,ns_1@cb.local:compaction_daemon<0.754.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:46:49.667Z,ns_1@cb.local:compaction_daemon<0.754.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2025-05-15T18:46:49.667Z,ns_1@cb.local:compaction_daemon<0.754.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_master too soon. Next run will be in 3600s
[error_logger:info,2025-05-15T18:46:49.667Z,ns_1@cb.local:<0.749.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.749.0>,suppress_max_restart_intensity}
    started: [{pid,<0.750.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      compaction_daemon}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,compaction_daemon},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => compaction_daemon,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [compaction_daemon],
                          restart => permanent,shutdown => 86400000,
                          start => {compaction_daemon,start_link,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:49.667Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.749.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,compaction_daemon}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => compaction_daemon,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [compaction_daemon],
                         restart => permanent,shutdown => 86400000,
                         start => {compaction_daemon,start_link,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:49.671Z,ns_1@cb.local:cluster_logs_sup<0.759.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,cluster_logs_sup}
    started: [{pid,<0.760.0>},
              {id,ets_holder},
              {mfargs,{cluster_logs_collection_task,start_link_ets_holder,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.671Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.759.0>},
              {id,cluster_logs_sup},
              {mfargs,{cluster_logs_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:49.672Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.761.0>},
              {id,collections},
              {mfargs,{collections,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.672Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.763.0>},
              {id,leader_events},
              {mfargs,{gen_event,start_link,[{local,leader_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.679Z,ns_1@cb.local:leader_leases_sup<0.774.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_leases_sup}
    started: [{pid,<0.779.0>},
              {id,leader_activities},
              {mfargs,{leader_activities,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,10000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:49.679Z,ns_1@cb.local:ns_heart_slow_status_updater<0.571.0>:goxdcr_rest:get_from_goxdcr:139]Goxdcr is temporary not available. Return empty list.
[error_logger:info,2025-05-15T18:46:49.681Z,ns_1@cb.local:leader_leases_sup<0.774.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_leases_sup}
    started: [{pid,<0.782.0>},
              {id,leader_lease_agent},
              {mfargs,{leader_lease_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.681Z,ns_1@cb.local:leader_services_sup<0.765.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_services_sup}
    started: [{pid,<0.774.0>},
              {id,leader_leases_sup},
              {mfargs,{leader_leases_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:49.685Z,ns_1@cb.local:leader_registry_sup<0.787.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_registry_sup}
    started: [{pid,<0.788.0>},
              {id,leader_registry},
              {mfargs,{leader_registry,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:49.685Z,ns_1@cb.local:leader_registry_sup<0.787.0>:mb_master:check_master_takeover_needed:183]Sending master node question to the following nodes: []
[ns_server:debug,2025-05-15T18:46:49.685Z,ns_1@cb.local:leader_registry_sup<0.787.0>:mb_master:check_master_takeover_needed:187]Got replies: []
[ns_server:debug,2025-05-15T18:46:49.685Z,ns_1@cb.local:leader_registry_sup<0.787.0>:mb_master:check_master_takeover_needed:193]Was unable to discover master, not going to force mastership takeover
[ns_server:debug,2025-05-15T18:46:49.685Z,ns_1@cb.local:mb_master<0.790.0>:mb_master:init:86]Heartbeat interval is 2000
[user:info,2025-05-15T18:46:49.685Z,ns_1@cb.local:mb_master<0.790.0>:mb_master:init:91]I'm the only node, so I'm the master.
[ns_server:debug,2025-05-15T18:46:49.685Z,ns_1@cb.local:leader_registry<0.788.0>:leader_registry:handle_new_leader:275]New leader is 'ns_1@cb.local'. Invalidating name cache.
[ns_server:debug,2025-05-15T18:46:49.699Z,ns_1@cb.local:mb_master<0.790.0>:master_activity_events:submit_cast:75]Failed to send master activity event {became_master,'ns_1@cb.local'}: {error,
                                                                       badarg}
[error_logger:info,2025-05-15T18:46:49.701Z,ns_1@cb.local:mb_master_sup<0.792.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.794.0>},
              {id,leader_lease_acquirer},
              {mfargs,{leader_lease_acquirer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,10000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:49.702Z,ns_1@cb.local:leader_quorum_nodes_manager<0.796.0>:leader_quorum_nodes_manager:pull_config:99]Attempting to pull config from nodes:
[]
[error_logger:info,2025-05-15T18:46:49.702Z,ns_1@cb.local:mb_master_sup<0.792.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.796.0>},
              {id,leader_quorum_nodes_manager},
              {mfargs,{leader_quorum_nodes_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:49.703Z,ns_1@cb.local:leader_quorum_nodes_manager<0.796.0>:leader_quorum_nodes_manager:pull_config:104]Pulled config successfully.
[ns_server:info,2025-05-15T18:46:49.705Z,ns_1@cb.local:mb_master_sup<0.792.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,ns_tick},
                                         ns_tick,[],[]]): started as <0.801.0> on 'ns_1@cb.local'

[error_logger:info,2025-05-15T18:46:49.705Z,ns_1@cb.local:mb_master_sup<0.792.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.801.0>},
              {id,ns_tick},
              {mfargs,{ns_tick,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,10},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:49.706Z,ns_1@cb.local:ns_ports_setup<0.675.0>:ns_ports_setup:set_children:65]Monitor ns_child_ports_sup <16971.133.0>
[ns_server:debug,2025-05-15T18:46:49.706Z,ns_1@cb.local:memcached_config_mgr<0.701.0>:memcached_config_mgr:memcached_port_pid:156]ns_ports_setup seems to be ready
[ns_server:debug,2025-05-15T18:46:49.706Z,ns_1@cb.local:<0.704.0>:memcached_config_mgr:memcached_port_pid:156]ns_ports_setup seems to be ready
[ns_server:debug,2025-05-15T18:46:49.707Z,ns_1@cb.local:leader_lease_agent<0.782.0>:leader_lease_agent:do_handle_acquire_lease:140]Granting lease to {lease_holder,<<"12235e8cca1e7367c554bc33cea65983">>,
                                'ns_1@cb.local'} for 15000ms
[ns_server:debug,2025-05-15T18:46:49.707Z,ns_1@cb.local:<0.704.0>:memcached_config_mgr:find_port_pid_loop:164]Found memcached port <16971.139.0>
[ns_server:debug,2025-05-15T18:46:49.707Z,ns_1@cb.local:memcached_config_mgr<0.701.0>:memcached_config_mgr:find_port_pid_loop:164]Found memcached port <16971.139.0>
[ns_server:debug,2025-05-15T18:46:49.709Z,ns_1@cb.local:<0.808.0>:chronicle_master:do_init:115]Starting with SelfRef = #Ref<0.2327127311.3650355204.215787>
[ns_server:info,2025-05-15T18:46:49.709Z,ns_1@cb.local:mb_master_sup<0.792.0>:misc:start_singleton:913]start_singleton(gen_server2, start_link, [{via,leader_registry,
                                           chronicle_master},
                                          chronicle_master,[],[]]): started as <0.808.0> on 'ns_1@cb.local'

[error_logger:info,2025-05-15T18:46:49.709Z,ns_1@cb.local:mb_master_sup<0.792.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.808.0>},
              {id,chronicle_master},
              {mfargs,{chronicle_master,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:49.710Z,ns_1@cb.local:memcached_config_mgr<0.701.0>:memcached_config_mgr:init:93]wrote memcached config to /opt/couchbase/var/lib/couchbase/config/memcached.json. Will activate memcached port server
[ns_server:debug,2025-05-15T18:46:49.713Z,ns_1@cb.local:memcached_config_mgr<0.701.0>:memcached_config_mgr:init:97]activated memcached port server
[ns_server:info,2025-05-15T18:46:49.714Z,ns_1@cb.local:<0.800.0>:leader_lease_acquire_worker:handle_fresh_lease_acquired:296]Acquired lease from node 'ns_1@cb.local' (lease uuid: <<"12235e8cca1e7367c554bc33cea65983">>)
[ns_server:info,2025-05-15T18:46:49.714Z,ns_1@cb.local:memcached_config_mgr<0.701.0>:memcached_config_mgr:push_tls_config:233]Pushing TLS config to memcached:
{[{<<"private key">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem">>},
  {<<"certificate chain">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem">>},
  {<<"CA file">>,<<"/opt/couchbase/var/lib/couchbase/config/certs/ca.pem">>},
  {<<"minimum version">>,<<"TLS 1.2">>},
  {<<"cipher list">>,
   {[{<<"TLS 1.2">>,<<"HIGH">>},
     {<<"TLS 1.3">>,
      <<"TLS_AES_256_GCM_SHA384:TLS_AES_128_GCM_SHA256:TLS_CHACHA20_POLY1305_SHA256">>}]}},
  {<<"cipher order">>,true},
  {<<"client cert auth">>,<<"disabled">>}]}
[ns_server:debug,2025-05-15T18:46:49.715Z,ns_1@cb.local:<0.704.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":11,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true,\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\"]}}">>
[ns_server:warn,2025-05-15T18:46:49.715Z,ns_1@cb.local:<0.812.0>:ns_memcached:connect:1460]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}, retrying.
[ns_server:warn,2025-05-15T18:46:49.715Z,ns_1@cb.local:<0.813.0>:ns_memcached:connect:1460]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}, retrying.
[error_logger:info,2025-05-15T18:46:49.716Z,ns_1@cb.local:ns_orchestrator_sup<0.814.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.815.0>},
              {id,compat_mode_events},
              {mfargs,{gen_event,start_link,[{local,compat_mode_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:49.718Z,ns_1@cb.local:compat_mode_manager<0.816.0>:cluster_compat_mode:do_upgrades:205]Initiating rbac upgrade due to version change from [7,1] to [7,6] (target version: [7,
                                                                                    6])
[ns_server:info,2025-05-15T18:46:49.718Z,ns_1@cb.local:compat_mode_manager<0.816.0>:menelaus_users:upgrade:1057]Upgrading users database to [7,6]
[ns_server:debug,2025-05-15T18:46:49.718Z,ns_1@cb.local:ns_config_rep<0.545.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([rbac_upgrade]..)
[ns_server:debug,2025-05-15T18:46:49.718Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
rbac_upgrade ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554009}}]}|
 started]
[ns_server:debug,2025-05-15T18:46:49.718Z,ns_1@cb.local:ns_config_rep<0.545.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 2 us
[ns_server:debug,2025-05-15T18:46:49.718Z,ns_1@cb.local:ns_config_rep<0.545.0>:ns_config_rep:handle_call:149]Got full synchronization request from 'ns_1@cb.local'
[ns_server:debug,2025-05-15T18:46:49.718Z,ns_1@cb.local:ns_config_rep<0.545.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 2 us
[ns_server:debug,2025-05-15T18:46:49.718Z,ns_1@cb.local:users_storage<0.444.0>:replicated_storage:handle_call:151]Received sync_to_me with timeout = 60000, nodes = ['ns_1@cb.local']
[ns_server:debug,2025-05-15T18:46:49.718Z,ns_1@cb.local:users_replicator<0.434.0>:doc_replicator:loop:100]Received sync_to_me with timeout = 60000, nodes = ['ns_1@cb.local']
[ns_server:debug,2025-05-15T18:46:49.718Z,ns_1@cb.local:users_storage<0.444.0>:replicated_storage:handle_call:146]Received sync_token from {<0.832.0>,
                          [alias|#Ref<0.2327127311.3650420741.215917>]}
[ns_server:debug,2025-05-15T18:46:49.718Z,ns_1@cb.local:users_replicator<0.434.0>:doc_replicator:loop:95]Received sync_token from {<0.832.0>,
                          [alias|#Ref<0.2327127311.3650420741.215917>]}
[ns_server:debug,2025-05-15T18:46:49.718Z,ns_1@cb.local:<0.829.0>:replicated_storage:handle_call:157]sync_to_me reply: ok
[ns_server:debug,2025-05-15T18:46:49.718Z,ns_1@cb.local:users_storage<0.444.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,'_','_','_'},[],['$_']}],
                               100}
[ns_server:info,2025-05-15T18:46:49.718Z,ns_1@cb.local:compat_mode_manager<0.816.0>:menelaus_users:upgrade:1071]Users database was upgraded to [7,6]
[ns_server:debug,2025-05-15T18:46:49.718Z,ns_1@cb.local:users_storage<0.444.0>:replicated_storage:handle_call:151]Received sync_to_me with timeout = 60000, nodes = ['ns_1@cb.local']
[ns_server:debug,2025-05-15T18:46:49.718Z,ns_1@cb.local:users_replicator<0.434.0>:doc_replicator:loop:100]Received sync_to_me with timeout = 60000, nodes = ['ns_1@cb.local']
[ns_server:debug,2025-05-15T18:46:49.719Z,ns_1@cb.local:users_storage<0.444.0>:replicated_storage:handle_call:146]Received sync_token from {<0.836.0>,
                          [alias|#Ref<0.2327127311.3650420741.215936>]}
[ns_server:debug,2025-05-15T18:46:49.719Z,ns_1@cb.local:users_replicator<0.434.0>:doc_replicator:loop:95]Received sync_token from {<0.836.0>,
                          [alias|#Ref<0.2327127311.3650420741.215936>]}
[ns_server:debug,2025-05-15T18:46:49.719Z,ns_1@cb.local:<0.833.0>:replicated_storage:handle_call:157]sync_to_me reply: ok
[ns_server:info,2025-05-15T18:46:49.719Z,ns_1@cb.local:compat_mode_manager<0.816.0>:menelaus_users:upgrade:1073]Users database upgrade was delivered to ['ns_1@cb.local']
[ns_server:info,2025-05-15T18:46:49.719Z,ns_1@cb.local:kv<0.268.0>:chronicle_upgrade:upgrade_loop:114]Upgrading chronicle from [7,1]. Final version = [7,6]
[ns_server:info,2025-05-15T18:46:49.719Z,ns_1@cb.local:kv<0.268.0>:chronicle_upgrade:upgrade_loop:114]Upgrading chronicle from [7,2]. Final version = [7,6]
[ns_server:debug,2025-05-15T18:46:49.747Z,ns_1@cb.local:memcached_permissions<0.531.0>:memcached_permissions:producer:512]Skipping update during users upgrade
[ns_server:debug,2025-05-15T18:46:49.747Z,ns_1@cb.local:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: cluster_compat_version, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                           5})
[7,6]
[ns_server:debug,2025-05-15T18:46:49.747Z,ns_1@cb.local:ns_cookie_manager<0.238.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[ns_server:debug,2025-05-15T18:46:49.747Z,ns_1@cb.local:tombstone_keeper<0.277.0>:tombstone_keeper:handle_call:49]Refreshed with timestamps []
[ns_server:debug,2025-05-15T18:46:49.747Z,ns_1@cb.local:roles_cache<0.458.0>:active_cache:clean:181]Clearing the roles_cache cache
[ns_server:debug,2025-05-15T18:46:49.748Z,ns_1@cb.local:compiled_roles_cache<0.454.0>:versioned_cache:handle_info:89]Flushing cache compiled_roles_cache due to version change from undefined to {[7,
                                                                              6],
                                                                             {0,
                                                                              249091395},
                                                                             {0,
                                                                              249091395},
                                                                             false,
                                                                             []}
[ns_server:debug,2025-05-15T18:46:49.748Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:maybe_store_ca_certs:832]Considering to store CA certs
[ns_server:debug,2025-05-15T18:46:49.748Z,ns_1@cb.local:<0.837.0>:ns_node_disco:do_nodes_wanted_updated_fun:210]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"biSYTaQUFDVndTs/b+IcDmD2L0woktv9naRdv3GCK6A=">>}
[ns_server:debug,2025-05-15T18:46:49.749Z,ns_1@cb.local:memcached_passwords<0.528.0>:memcached_cfg:write_cfg:156]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2025-05-15T18:46:49.749Z,ns_1@cb.local:<0.837.0>:ns_node_disco:do_nodes_wanted_updated_fun:213]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"biSYTaQUFDVndTs/b+IcDmD2L0woktv9naRdv3GCK6A=">>}
[ns_server:info,2025-05-15T18:46:49.750Z,ns_1@cb.local:ns_config<0.280.0>:ns_config:do_upgrade_config:733]Upgrading config by changes:
[{set,cluster_compat_version,[7,1]}]

[ns_server:info,2025-05-15T18:46:49.750Z,ns_1@cb.local:ns_config<0.280.0>:ns_online_config_upgrader:do_upgrade_config:59]Performing online config upgrade to [7,2]
[ns_server:info,2025-05-15T18:46:49.750Z,ns_1@cb.local:ns_config<0.280.0>:ns_config:do_upgrade_config:733]Upgrading config by changes:
[{set,cluster_compat_version,[7,2]},
 {set,auto_failover_cfg,
      [{enabled,true},
       {timeout,120},
       {count,0},
       {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
       {failover_server_group,false},
       {max_count,1},
       {failed_over_server_groups,[]},
       {can_abort_rebalance,true},
       {failover_preserve_durability_majority,false}]}]

[ns_server:debug,2025-05-15T18:46:49.758Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:notify_services:1146]Going to notify following services: [capi_ssl_service,memcached]
[ns_server:info,2025-05-15T18:46:49.758Z,ns_1@cb.local:<0.846.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service memcached
[ns_server:info,2025-05-15T18:46:49.769Z,ns_1@cb.local:ns_config<0.280.0>:ns_online_config_upgrader:do_upgrade_config:59]Performing online config upgrade to [7,6]
[ns_server:debug,2025-05-15T18:46:49.791Z,ns_1@cb.local:memcached_passwords<0.528.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,{auth,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2025-05-15T18:46:49.794Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:handle_call:57]File rename from "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw" is requested
[ns_server:debug,2025-05-15T18:46:49.794Z,ns_1@cb.local:memcached_passwords<0.528.0>:memcached_cfg:rename_and_refresh:178]Successfully renamed "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2025-05-15T18:46:49.794Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:handle_cast:67]Refresh of isasl requested
[ns_server:info,2025-05-15T18:46:49.792Z,ns_1@cb.local:ns_config<0.280.0>:ns_config:do_upgrade_config:733]Upgrading config by changes:
[{set,cluster_compat_version,[7,6]},
 {set,audit_decriptors,
      [{8243,
        [{name,<<"mutate document">>},
         {description,<<"Document was mutated via the REST API">>},
         {enabled,true},
         {module,ns_server}]},
       {8255,
        [{name,<<"read document">>},
         {description,<<"Document was read via the REST API">>},
         {enabled,false},
         {module,ns_server}]},
       {8257,
        [{name,<<"alert email sent">>},
         {description,<<"An alert email was successfully sent">>},
         {enabled,true},
         {module,ns_server}]},
       {8265,
        [{name,<<"RBAC information retrieved">>},
         {description,<<"RBAC information was retrieved">>},
         {enabled,true},
         {module,ns_server}]},
       {16384,
        [{name,<<"remote cluster ref creation">>},
         {description,<<"created remote cluster ref">>},
         {enabled,true},
         {module,xdcr}]},
       {16385,
        [{name,<<"remote cluster ref update">>},
         {description,<<"updated remote cluster ref">>},
         {enabled,true},
         {module,xdcr}]},
       {16386,
        [{name,<<"remote cluster ref deletion">>},
         {description,<<"deleted remote cluster ref">>},
         {enabled,true},
         {module,xdcr}]},
       {16387,
        [{name,<<"replication creation">>},
         {description,<<"created replication">>},
         {enabled,true},
         {module,xdcr}]},
       {16388,
        [{name,<<"replication pause">>},
         {description,<<"paused replication">>},
         {enabled,true},
         {module,xdcr}]},
       {16389,
        [{name,<<"replication resume">>},
         {description,<<"resumed replication">>},
         {enabled,true},
         {module,xdcr}]},
       {16390,
        [{name,<<"replication cancellation">>},
         {description,<<"canceled replication">>},
         {enabled,true},
         {module,xdcr}]},
       {16391,
        [{name,<<"default replication settings update">>},
         {description,<<"updated default replication settings">>},
         {enabled,true},
         {module,xdcr}]},
       {16392,
        [{name,<<"individual replication settings update">>},
         {description,<<"updated individual replication settings">>},
         {enabled,true},
         {module,xdcr}]},
       {16393,
        [{name,<<"bucket settings update">>},
         {description,<<"updated bucket settings">>},
         {enabled,true},
         {module,xdcr}]},
       {16394,
        [{name,<<"authorization failure while adding remote cluster ref">>},
         {description,<<"failed to add remote cluster ref because of authorization failure">>},
         {enabled,true},
         {module,xdcr}]},
       {16395,
        [{name,<<"authorization failure while updating remote cluster ref">>},
         {description,<<"failed to update remote cluster ref because of authorization failure">>},
         {enabled,true},
         {module,xdcr}]},
       {16396,
        [{name,<<"access denied">>},
         {description,<<"access denied">>},
         {enabled,true},
         {module,xdcr}]},
       {20480,
        [{name,<<"opened DCP connection">>},
         {description,<<"opened DCP connection">>},
         {enabled,true},
         {module,memcached}]},
       {20482,
        [{name,<<"external memcached bucket flush">>},
         {description,<<"External user flushed the content of a memcached bucket">>},
         {enabled,true},
         {module,memcached}]},
       {20483,
        [{name,<<"invalid packet">>},
         {description,<<"Rejected an invalid packet">>},
         {enabled,true},
         {module,memcached}]},
       {20485,
        [{name,<<"authentication succeeded">>},
         {description,<<"Authentication to the cluster succeeded">>},
         {enabled,false},
         {module,memcached}]},
       {20488,
        [{name,<<"document read">>},
         {description,<<"Document was read">>},
         {enabled,false},
         {module,memcached}]},
       {20489,
        [{name,<<"document locked">>},
         {description,<<"Document was locked">>},
         {enabled,false},
         {module,memcached}]},
       {20490,
        [{name,<<"document modify">>},
         {description,<<"Document was modified">>},
         {enabled,false},
         {module,memcached}]},
       {20491,
        [{name,<<"document delete">>},
         {description,<<"Document was deleted">>},
         {enabled,false},
         {module,memcached}]},
       {20492,
        [{name,<<"select bucket">>},
         {description,<<"The specified bucket was selected">>},
         {enabled,true},
         {module,memcached}]},
       {20493,
        [{name,<<"session terminated">>},
         {description,<<"Session to the cluster has terminated">>},
         {enabled,false},
         {module,memcached}]},
       {20494,
        [{name,<<"tenant rate limited">>},
         {description,<<"The given tenant was rate limited">>},
         {enabled,true},
         {module,memcached}]},
       {24576,
        [{name,<<"Delete index">>},
         {description,<<"FTS index was deleted">>},
         {enabled,true},
         {module,fts}]},
       {24577,
        [{name,<<"Create/Update index">>},
         {description,<<"FTS index was created/Updated">>},
         {enabled,true},
         {module,fts}]},
       {24579,
        [{name,<<"Control index">>},
         {description,<<"FTS index control command was issued">>},
         {enabled,true},
         {module,fts}]},
       {24582,
        [{name,<<"GC run">>},
         {description,<<"GC run was triggered">>},
         {enabled,true},
         {module,fts}]},
       {24583,
        [{name,<<"CPU profile">>},
         {description,<<"CPU profiling was started">>},
         {enabled,true},
         {module,fts}]},
       {24584,
        [{name,<<"Memory profile">>},
         {description,<<"Memory profiling was started">>},
         {enabled,true},
         {module,fts}]},
       {28672,
        [{name,<<"SELECT statement">>},
         {description,<<"A N1QL SELECT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28673,
        [{name,<<"EXPLAIN statement">>},
         {description,<<"A N1QL EXPLAIN statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28674,
        [{name,<<"PREPARE statement">>},
         {description,<<"A N1QL PREPARE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28675,
        [{name,<<"INFER statement">>},
         {description,<<"A N1QL INFER statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28676,
        [{name,<<"INSERT statement">>},
         {description,<<"A N1QL INSERT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28677,
        [{name,<<"UPSERT statement">>},
         {description,<<"A N1QL UPSERT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28678,
        [{name,<<"DELETE statement">>},
         {description,<<"A N1QL DELETE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28679,
        [{name,<<"UPDATE statement">>},
         {description,<<"A N1QL UPDATE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28680,
        [{name,<<"MERGE statement">>},
         {description,<<"A N1QL MERGE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28681,
        [{name,<<"CREATE INDEX statement">>},
         {description,<<"A N1QL CREATE INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28682,
        [{name,<<"DROP INDEX statement">>},
         {description,<<"A N1QL DROP INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28683,
        [{name,<<"ALTER INDEX statement">>},
         {description,<<"A N1QL ALTER INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28684,
        [{name,<<"BUILD INDEX statement">>},
         {description,<<"A N1QL BUILD INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28685,
        [{name,<<"GRANT ROLE statement">>},
         {description,<<"A N1QL GRANT ROLE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28686,
        [{name,<<"REVOKE ROLE statement">>},
         {description,<<"A N1QL REVOKE ROLE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28687,
        [{name,<<"UNRECOGNIZED statement">>},
         {description,<<"An unrecognized statement was received by the N1QL query engine">>},
         {enabled,false},
         {module,n1ql}]},
       {28688,
        [{name,<<"CREATE PRIMARY INDEX statement">>},
         {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28689,
        [{name,<<"/admin/stats API request">>},
         {description,<<"An HTTP request was made to the API at /admin/stats.">>},
         {enabled,false},
         {module,n1ql}]},
       {28690,
        [{name,<<"/admin/vitals API request">>},
         {description,<<"An HTTP request was made to the API at /admin/vitals.">>},
         {enabled,false},
         {module,n1ql}]},
       {28691,
        [{name,<<"/admin/prepareds API request">>},
         {description,<<"An HTTP request was made to the API at /admin/prepareds.">>},
         {enabled,false},
         {module,n1ql}]},
       {28692,
        [{name,<<"/admin/active_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/active_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28693,
        [{name,<<"/admin/indexes/prepareds API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
         {enabled,false},
         {module,n1ql}]},
       {28694,
        [{name,<<"/admin/indexes/active_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28695,
        [{name,<<"/admin/indexes/completed_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28697,
        [{name,<<"/admin/ping API request">>},
         {description,<<"An HTTP request was made to the API at /admin/ping.">>},
         {enabled,false},
         {module,n1ql}]},
       {28698,
        [{name,<<"/admin/config API request">>},
         {description,<<"An HTTP request was made to the API at /admin/config.">>},
         {enabled,false},
         {module,n1ql}]},
       {28699,
        [{name,<<"/admin/ssl_cert API request">>},
         {description,<<"An HTTP request was made to the API at /admin/ssl_cert.">>},
         {enabled,false},
         {module,n1ql}]},
       {28700,
        [{name,<<"/admin/settings API request">>},
         {description,<<"An HTTP request was made to the API at /admin/settings.">>},
         {enabled,false},
         {module,n1ql}]},
       {28701,
        [{name,<<"/admin/clusters API request">>},
         {description,<<"An HTTP request was made to the API at /admin/clusters.">>},
         {enabled,false},
         {module,n1ql}]},
       {28702,
        [{name,<<"/admin/completed_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/completed_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28704,
        [{name,<<"/admin/functions API request">>},
         {description,<<"An HTTP request was made to the API at /admin/functions.">>},
         {enabled,false},
         {module,n1ql}]},
       {28705,
        [{name,<<"/admin/indexes/functions API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/functions.">>},
         {enabled,false},
         {module,n1ql}]},
       {28706,
        [{name,<<"CREATE FUNCTION statement">>},
         {description,<<"A N1QL CREATE FUNCTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28707,
        [{name,<<"DROP FUNCTION statement">>},
         {description,<<"A N1QL DROP FUNCTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28708,
        [{name,<<"EXECUTE FUNCTION statement">>},
         {description,<<"A N1QL EXECUTE FUNCTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28709,
        [{name,<<"/admin/tasks API request">>},
         {description,<<"An HTTP request was made to the API at /admin/tasks.">>},
         {enabled,false},
         {module,n1ql}]},
       {28710,
        [{name,<<"/admin/indexes/tasks API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/tasks.">>},
         {enabled,false},
         {module,n1ql}]},
       {28711,
        [{name,<<"/admin/dictionary_cache API request">>},
         {description,<<"An HTTP request was made to the API at /admin/dictionary_cache.">>},
         {enabled,false},
         {module,n1ql}]},
       {28712,
        [{name,<<"/admin/indexes/dictionary_cache API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/dictionary_cache.">>},
         {enabled,false},
         {module,n1ql}]},
       {28713,
        [{name,<<"CREATE SCOPE statement">>},
         {description,<<"A N1QL CREATE SCOPE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28714,
        [{name,<<"DROP SCOPE statement">>},
         {description,<<"A N1QL DROP SCOPE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28715,
        [{name,<<"CREATE COLLECTION statement">>},
         {description,<<"A N1QL CREATE COLLECTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28716,
        [{name,<<"DROP COLLECTION statement">>},
         {description,<<"A N1QL DROP COLLECTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28717,
        [{name,<<"FLUSH COLLECTION statement">>},
         {description,<<"A N1QL FLUSH COLLECTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28718,
        [{name,<<"UPDATE STATISTICS statement">>},
         {description,<<"A N1QL UPDATE STATISTICS statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28719,
        [{name,<<"ADVISE statement">>},
         {description,<<"A N1QL ADVISE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28720,
        [{name,<<"START TRANSACTION statement">>},
         {description,<<"A N1QL START TRANSACTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28721,
        [{name,<<"COMMIT TRANSACTION statement">>},
         {description,<<"A N1QL COMMIT TRANSACTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28722,
        [{name,<<"ROLLBACK TRANSACTION statement">>},
         {description,<<"A N1QL ROLLBACK TRANSACTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28723,
        [{name,<<"ROLLBACK TRANSACTION TO SAVEPOINT statement">>},
         {description,<<"A N1QL ROLLBACK TRANSACTION TO SAVEPOINT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28724,
        [{name,<<"SET TRANSACTION ISOLATION statement">>},
         {description,<<"A N1QL SET TRANSACTION ISOLATION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28725,
        [{name,<<"SAVEPOINT statement">>},
         {description,<<"A N1QL SAVEPOINT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28726,
        [{name,<<"/admin/transactions API request">>},
         {description,<<"An HTTP request was made to the API at /admin/transactions.">>},
         {enabled,false},
         {module,n1ql}]},
       {28727,
        [{name,<<"/admin/indexes/transactions API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/transactions.">>},
         {enabled,false},
         {module,n1ql}]},
       {28728,
        [{name,<<"N1QL backup / restore API request">>},
         {description,<<"An HTTP request was made to archive or restore N1QL metadata">>},
         {enabled,false},
         {module,n1ql}]},
       {28729,
        [{name,<<"/admin/shutdown API request">>},
         {description,<<"An HTTP request was made to initiate graceful shutdown">>},
         {enabled,false},
         {module,n1ql}]},
       {28730,
        [{name,<<"/admin/gc API request">>},
         {description,<<"An HTTP request was made to run garbage collection">>},
         {enabled,false},
         {module,n1ql}]},
       {28731,
        [{name,<<"/admin/ffdc API request">>},
         {description,<<"An HTTP request was made to run an FFDC collection">>},
         {enabled,false},
         {module,n1ql}]},
       {28732,
        [{name,<<"/admin/log/ API request">>},
         {description,<<"An HTTP request was made to access diagnostic logs">>},
         {enabled,false},
         {module,n1ql}]},
       {28733,
        [{name,<<"/admin/sequences_cache API request">>},
         {description,<<"An HTTP request was made to access sequences">>},
         {enabled,false},
         {module,n1ql}]},
       {28734,
        [{name,<<"CREATE SEQUENCE statement">>},
         {description,<<"A N1QL CREATE SEQUENCE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28735,
        [{name,<<"ALTER SEQUENCE statement">>},
         {description,<<"A N1QL ALTER SEQUENCE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28736,
        [{name,<<"DROP SEQUENCE statement">>},
         {description,<<"A N1QL DROP SEQUENCE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28737,
        [{name,<<"Migration abort">>},
         {description,<<"Migration was aborted">>},
         {enabled,false},
         {module,n1ql}]},
       {32768,
        [{name,<<"Create Function">>},
         {description,<<"Request to create or update eventing function definition">>},
         {enabled,true},
         {module,eventing}]},
       {32769,
        [{name,<<"Delete Function">>},
         {description,<<"Request to delete eventing function definition">>},
         {enabled,true},
         {module,eventing}]},
       {32770,
        [{name,<<"Fetch Functions">>},
         {description,<<"Request to fetch eventing function definition">>},
         {enabled,false},
         {module,eventing}]},
       {32771,
        [{name,<<"List Deployed">>},
         {description,<<"Request to fetch eventing deployed functions list">>},
         {enabled,false},
         {module,eventing}]},
       {32772,
        [{name,<<"Fetch Drafts">>},
         {description,<<"Request to fetch eventing function draft definitions">>},
         {enabled,false},
         {module,eventing}]},
       {32773,
        [{name,<<"Delete Drafts">>},
         {description,<<"Request to delete eventing function draft definitions">>},
         {enabled,true},
         {module,eventing}]},
       {32774,
        [{name,<<"Save Draft">>},
         {description,<<"Request to save a draft definition">>},
         {enabled,true},
         {module,eventing}]},
       {32775,
        [{name,<<"Start Debug">>},
         {description,<<"Request to start eventing function debugger">>},
         {enabled,true},
         {module,eventing}]},
       {32776,
        [{name,<<"Stop Debug">>},
         {description,<<"Request to stop eventing function debugger">>},
         {enabled,true},
         {module,eventing}]},
       {32777,
        [{name,<<"Start Tracing">>},
         {description,<<"Request to start tracing eventing function execution">>},
         {enabled,true},
         {module,eventing}]},
       {32778,
        [{name,<<"Stop Tracing">>},
         {description,<<"Request to stop tracing eventing function execution">>},
         {enabled,true},
         {module,eventing}]},
       {32779,
        [{name,<<"Set Settings">>},
         {description,<<"Request to save settings for an eventing function">>},
         {enabled,true},
         {module,eventing}]},
       {32780,
        [{name,<<"Fetch Config">>},
         {description,<<"Request to fetch eventing config">>},
         {enabled,false},
         {module,eventing}]},
       {32781,
        [{name,<<"Save Config">>},
         {description,<<"Request to save eventing config">>},
         {enabled,true},
         {module,eventing}]},
       {32783,
        [{name,<<"Get Settings">>},
         {description,<<"Request to fetch eventing function settings">>},
         {enabled,false},
         {module,eventing}]},
       {32784,
        [{name,<<"Import Functions">>},
         {description,<<"Request to import one or more eventing functions">>},
         {enabled,true},
         {module,eventing}]},
       {32785,
        [{name,<<"Export Functions">>},
         {description,<<"Request to export all eventing functions">>},
         {enabled,false},
         {module,eventing}]},
       {32786,
        [{name,<<"List Running">>},
         {description,<<"Request to fetch eventing running function list">>},
         {enabled,false},
         {module,eventing}]},
       {32789,
        [{name,<<"Deploy Function">>},
         {description,<<"Request to deploy eventing function">>},
         {enabled,true},
         {module,eventing}]},
       {32790,
        [{name,<<"Undeploy Function">>},
         {description,<<"Request to undeploy eventing function">>},
         {enabled,true},
         {module,eventing}]},
       {32791,
        [{name,<<"Pause Function">>},
         {description,<<"Request to pause eventing function">>},
         {enabled,true},
         {module,eventing}]},
       {32792,
        [{name,<<"Resume Function">>},
         {description,<<"Request to resume eventing function">>},
         {enabled,true},
         {module,eventing}]},
       {32793,
        [{name,<<"Backup Functions">>},
         {description,<<"Request to backup one or more eventing functions">>},
         {enabled,false},
         {module,eventing}]},
       {32794,
        [{name,<<"Restore Functions">>},
         {description,<<"Request to restore one or more eventing functions from a backup">>},
         {enabled,true},
         {module,eventing}]},
       {32795,
        [{name,<<"List Function">>},
         {description,<<"Request to fetch eventing functions">>},
         {enabled,false},
         {module,eventing}]},
       {32796,
        [{name,<<"Function Status">>},
         {description,<<"Request to fetch eventing function status">>},
         {enabled,false},
         {module,eventing}]},
       {32797,
        [{name,<<"Clear Stats">>},
         {description,<<"Request to reset eventing function stats">>},
         {enabled,true},
         {module,eventing}]},
       {32798,
        [{name,<<"Fetch Stats">>},
         {description,<<"Request to fetch eventing function stats">>},
         {enabled,false},
         {module,eventing}]},
       {32799,
        [{name,<<"Eventing Cluster Stats">>},
         {description,<<"Request to fetch eventing cluster stats">>},
         {enabled,false},
         {module,eventing}]},
       {32801,
        [{name,<<"Eventing System Event">>},
         {description,<<"Request to execute eventing node related functions">>},
         {enabled,false},
         {module,eventing}]},
       {32802,
        [{name,<<"Get User Info">>},
         {description,<<"Request to get real_userid eventing permissions">>},
         {enabled,false},
         {module,eventing}]},
       {36865,
        [{name,<<"Service configuration change">>},
         {description,<<"A successful service configuration change was made.">>},
         {enabled,true},
         {module,analytics}]},
       {36866,
        [{name,<<"Node configuration change">>},
         {description,<<"A successful node configuration change was made.">>},
         {enabled,true},
         {module,analytics}]},
       {36867,
        [{name,<<"SELECT statement">>},
         {description,<<"A N1QL SELECT statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36868,
        [{name,<<"CREATE DATAVERSE statement">>},
         {description,<<"A N1QL CREATE DATAVERSE statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36869,
        [{name,<<"DROP DATAVERSE statement">>},
         {description,<<"A N1QL DROP DATAVERSE statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36870,
        [{name,<<"CREATE DATASET statement">>},
         {description,<<"A N1QL CREATE DATASET statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36871,
        [{name,<<"DROP DATASET statement">>},
         {description,<<"A N1QL DROP DATASET statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36872,
        [{name,<<"CREATE INDEX statement">>},
         {description,<<"A N1QL CREATE INDEX statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36873,
        [{name,<<"DROP INDEX statement">>},
         {description,<<"A N1QL DROP INDEX statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36877,
        [{name,<<"CONNECT LINK statement">>},
         {description,<<"A N1QL CONNECT LINK statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36878,
        [{name,<<"DISCONNECT LINK statement">>},
         {description,<<"A N1QL DISCONNECT LINK statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36879,
        [{name,<<"UNRECOGNIZED statement">>},
         {description,<<"An UNRECOGNIZED N1QL statement was encountered">>},
         {enabled,false},
         {module,analytics}]},
       {36880,
        [{name,<<"ALTER COLLECTION statement">>},
         {description,<<"A N1QL ALTER COLLECTION statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {40960,
        [{name,<<"Create Design Doc">>},
         {description,<<"Design Doc is Created">>},
         {enabled,true},
         {module,view_engine}]},
       {40961,
        [{name,<<"Delete Design Doc">>},
         {description,<<"Design Doc is Deleted">>},
         {enabled,true},
         {module,view_engine}]},
       {40962,
        [{name,<<"Query DDoc Meta Data">>},
         {description,<<"Design Doc Meta Data Query Request">>},
         {enabled,true},
         {module,view_engine}]},
       {40963,
        [{name,<<"View Query">>},
         {description,<<"View Query Request">>},
         {enabled,false},
         {module,view_engine}]},
       {40964,
        [{name,<<"Update Design Doc">>},
         {description,<<"Design Doc is Updated">>},
         {enabled,true},
         {module,view_engine}]},
       {40966,
        [{name,<<"Access denied">>},
         {description,<<"Access denied to the REST API due to invalid permissions or credentials">>},
         {enabled,true},
         {module,view_engine}]},
       {45056,
        [{name,<<"Modify configuration">>},
         {description,<<"Backup service configuration was modified">>},
         {enabled,true},
         {module,backup}]},
       {45057,
        [{name,<<"Fetch configuration">>},
         {description,<<"Backup service configuration was retrieved">>},
         {enabled,false},
         {module,backup}]},
       {45058,
        [{name,<<"Add plan">>},
         {description,<<"A new backup plan was added">>},
         {enabled,true},
         {module,backup}]},
       {45059,
        [{name,<<"Modify plan">>},
         {description,<<"Existing backup plan was modified">>},
         {enabled,true},
         {module,backup}]},
       {45060,
        [{name,<<"Delete plan">>},
         {description,<<"A backup plan was removed">>},
         {enabled,true},
         {module,backup}]},
       {45061,
        [{name,<<"Fetch plan">>},
         {description,<<"One or more backup plans where fetched">>},
         {enabled,false},
         {module,backup}]},
       {45062,
        [{name,<<"Add repository">>},
         {description,<<"A new active backup repository was added">>},
         {enabled,true},
         {module,backup}]},
       {45063,
        [{name,<<"Archive repository">>},
         {description,<<"An active repository was archived">>},
         {enabled,true},
         {module,backup}]},
       {45064,
        [{name,<<"Pause repository">>},
         {description,<<"An active repository was paused">>},
         {enabled,true},
         {module,backup}]},
       {45065,
        [{name,<<"Resume repository">>},
         {description,<<"An active repository was resumed">>},
         {enabled,true},
         {module,backup}]},
       {45066,
        [{name,<<"Fetch repository">>},
         {description,<<"A repository was fetched">>},
         {enabled,false},
         {module,backup}]},
       {45067,
        [{name,<<"Restore repository">>},
         {description,<<"The repository data was restored">>},
         {enabled,true},
         {module,backup}]},
       {45068,
        [{name,<<"Backup repository">>},
         {description,<<"A manual backup was triggered on an active repository">>},
         {enabled,true},
         {module,backup}]},
       {45069,
        [{name,<<"Merge repository">>},
         {description,<<"A manual merge was triggered on an active repository">>},
         {enabled,true},
         {module,backup}]},
       {45070,
        [{name,<<"Info repository">>},
         {description,<<"Information about the structure and contents of the backup repository was fetched.">>},
         {enabled,false},
         {module,backup}]},
       {45071,
        [{name,<<"Examine repository">>},
         {description,<<"A document was retrieved from the repository backups">>},
         {enabled,true},
         {module,backup}]},
       {45072,
        [{name,<<"Delete repository">>},
         {description,<<"A repository was deleted">>},
         {enabled,true},
         {module,backup}]},
       {45073,
        [{name,<<"Delete backup">>},
         {description,<<"An active repository backup was deleted">>},
         {enabled,true},
         {module,backup}]},
       {45074,
        [{name,<<"Access denied">>},
         {description,<<"A user has been denied access to the REST API due to invalid permissions or credentials">>},
         {enabled,true},
         {module,backup}]}]},
 {delete,rbac_upgrade},
 {set,auto_failover_cfg,
      [{disable_max_count,false},
       {enabled,true},
       {timeout,120},
       {count,0},
       {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
       {failover_server_group,false},
       {max_count,1},
       {failed_over_server_groups,[]},
       {can_abort_rebalance,true},
       {failover_preserve_durability_majority,false}]},
 {delete,memory_alert_email},
 {delete,memory_alert_popup},
 {delete,popup_alerts_auto_failover_upgrade_70_fixed},
 {set,{metakv,<<"/indexing/settings/config">>},
      <<"{\"indexer.settings.compaction.abort_exceed_interval\":false,\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.enable_page_bloom_filter\":false,\"indexer.settings.enable_shard_affinity\":false,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.log_level\":\"info\",\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.num_replica\":0,\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.rebalance.blob_storage_bucket\":\"\",\"indexer.settings.rebalance.blob_storage_prefix\":\"\",\"indexer.settings.rebalance.blob_storage_region\":\"\",\"indexer.settings.rebalance.blob_storage_scheme\":\"\",\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.thresholds.mem_high\":70,\"indexer.settings.thresholds.mem_low\":50,\"indexer.settings.thresholds.units_high\":60,\"indexer.settings.thresholds.units_low\":40}">>},
 {set,{metakv,<<"/query/settings/config">>},
      <<"{\"cleanupclientattempts\":true,\"cleanuplostattempts\":true,\"cleanupwindow\":\"60s\",\"completed-limit\":4000,\"completed-max-plan-size\":262144,\"completed-threshold\":1000,\"loglevel\":\"info\",\"max-parallelism\":1,\"memory-quota\":0,\"n1ql-feat-ctrl\":76,\"node-quota\":0,\"node-quota-val-percent\":67,\"num-cpus\":0,\"numatrs\":1024,\"pipeline-batch\":16,\"pipeline-cap\":512,\"prepared-limit\":16384,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"query.settings.tmp_space_size\":5120,\"scan-cap\":512,\"timeout\":0,\"txtimeout\":\"0ms\",\"use-cbo\":true,\"use-replica\":\"unset\"}">>},
 {set,{metakv,<<"/analytics/settings/config">>},
      <<"{\"analytics.settings.blob_storage_bucket\":\"\",\"analytics.settings.blob_storage_prefix\":\"\",\"analytics.settings.blob_storage_region\":\"\",\"analytics.settings.blob_storage_scheme\":\"\",\"analytics.settings.num_replicas\":0}">>},
 {delete,mb33750_workaround_enabled},
 {delete,cert_and_pkey},
 {set,resource_management,
      [{bucket,[{resident_ratio,[{enabled,false},
                                 {couchstore_minimum,1},
                                 {magma_minimum,0.2}]},
                {data_size,[{enabled,false},
                            {couchstore_maximum,2},
                            {magma_maximum,16}]}]},
       {index,[]},
       {cores_per_bucket,[{enabled,false},{minimum,0.4}]},
       {disk_usage,[{enabled,false},{maximum,96}]},
       {collections_per_quota,[{enabled,false},{maximum,1}]}]}]

[ns_server:debug,2025-05-15T18:46:49.799Z,ns_1@cb.local:ns_cookie_manager<0.238.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[ns_server:debug,2025-05-15T18:46:49.800Z,ns_1@cb.local:prometheus_cfg<0.515.0>:prometheus_cfg:maybe_apply_new_settings:704]Settings didn't change, ignoring update
[ns_server:debug,2025-05-15T18:46:49.801Z,ns_1@cb.local:prometheus_cfg<0.515.0>:prometheus_cfg:maybe_apply_new_settings:704]Settings didn't change, ignoring update
[ns_server:debug,2025-05-15T18:46:49.801Z,ns_1@cb.local:<0.850.0>:ns_node_disco:do_nodes_wanted_updated_fun:210]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"biSYTaQUFDVndTs/b+IcDmD2L0woktv9naRdv3GCK6A=">>}
[ns_server:debug,2025-05-15T18:46:49.801Z,ns_1@cb.local:<0.850.0>:ns_node_disco:do_nodes_wanted_updated_fun:213]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"biSYTaQUFDVndTs/b+IcDmD2L0woktv9naRdv3GCK6A=">>}
[ns_server:debug,2025-05-15T18:46:49.802Z,ns_1@cb.local:roles_cache<0.458.0>:active_cache:clean:181]Clearing the roles_cache cache
[ns_server:debug,2025-05-15T18:46:49.803Z,ns_1@cb.local:tombstone_keeper<0.277.0>:tombstone_keeper:handle_call:49]Refreshed with timestamps []
[ns_server:debug,2025-05-15T18:46:49.805Z,ns_1@cb.local:memcached_permissions<0.531.0>:memcached_cfg:write_cfg:156]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2025-05-15T18:46:49.799Z,ns_1@cb.local:ns_config_rep<0.545.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([audit_decriptors,auto_failover_cfg,
                               cluster_compat_version,rbac_upgrade,
                               resource_management,
                               {metakv,<<"/analytics/settings/config">>},
                               {metakv,<<"/indexing/settings/config">>},
                               {metakv,<<"/query/settings/config">>},
                               {node,'ns_1@cb.local',prometheus_auth_info}]..)
[ns_server:debug,2025-05-15T18:46:49.806Z,ns_1@cb.local:memcached_permissions<0.531.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,{user,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2025-05-15T18:46:49.806Z,ns_1@cb.local:memcached_passwords<0.528.0>:memcached_cfg:write_cfg:156]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2025-05-15T18:46:49.806Z,ns_1@cb.local:ns_config_rep<0.545.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 3 us
[ns_server:debug,2025-05-15T18:46:49.806Z,ns_1@cb.local:ns_config_rep<0.545.0>:ns_config_rep:handle_call:149]Got full synchronization request from 'ns_1@cb.local'
[ns_server:debug,2025-05-15T18:46:49.807Z,ns_1@cb.local:ns_config_rep<0.545.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 4 us
[user:warn,2025-05-15T18:46:49.807Z,ns_1@cb.local:compat_mode_manager<0.816.0>:compat_mode_manager:handle_consider_switching_compat_mode:43]Changed cluster compat mode from [7,1] to [7,6]
[error_logger:info,2025-05-15T18:46:49.807Z,ns_1@cb.local:ns_orchestrator_sup<0.814.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.816.0>},
              {id,compat_mode_manager},
              {mfargs,{compat_mode_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:49.807Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:handle_call:57]File rename from "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac" is requested
[ns_server:debug,2025-05-15T18:46:49.808Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
audit_decriptors ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554009}}]},
 {8243,
  [{name,<<"mutate document">>},
   {description,<<"Document was mutated via the REST API">>},
   {enabled,true},
   {module,ns_server}]},
 {8255,
  [{name,<<"read document">>},
   {description,<<"Document was read via the REST API">>},
   {enabled,false},
   {module,ns_server}]},
 {8257,
  [{name,<<"alert email sent">>},
   {description,<<"An alert email was successfully sent">>},
   {enabled,true},
   {module,ns_server}]},
 {8265,
  [{name,<<"RBAC information retrieved">>},
   {description,<<"RBAC information was retrieved">>},
   {enabled,true},
   {module,ns_server}]},
 {16384,
  [{name,<<"remote cluster ref creation">>},
   {description,<<"created remote cluster ref">>},
   {enabled,true},
   {module,xdcr}]},
 {16385,
  [{name,<<"remote cluster ref update">>},
   {description,<<"updated remote cluster ref">>},
   {enabled,true},
   {module,xdcr}]},
 {16386,
  [{name,<<"remote cluster ref deletion">>},
   {description,<<"deleted remote cluster ref">>},
   {enabled,true},
   {module,xdcr}]},
 {16387,
  [{name,<<"replication creation">>},
   {description,<<"created replication">>},
   {enabled,true},
   {module,xdcr}]},
 {16388,
  [{name,<<"replication pause">>},
   {description,<<"paused replication">>},
   {enabled,true},
   {module,xdcr}]},
 {16389,
  [{name,<<"replication resume">>},
   {description,<<"resumed replication">>},
   {enabled,true},
   {module,xdcr}]},
 {16390,
  [{name,<<"replication cancellation">>},
   {description,<<"canceled replication">>},
   {enabled,true},
   {module,xdcr}]},
 {16391,
  [{name,<<"default replication settings update">>},
   {description,<<"updated default replication settings">>},
   {enabled,true},
   {module,xdcr}]},
 {16392,
  [{name,<<"individual replication settings update">>},
   {description,<<"updated individual replication settings">>},
   {enabled,true},
   {module,xdcr}]},
 {16393,
  [{name,<<"bucket settings update">>},
   {description,<<"updated bucket settings">>},
   {enabled,true},
   {module,xdcr}]},
 {16394,
  [{name,<<"authorization failure while adding remote cluster ref">>},
   {description,<<"failed to add remote cluster ref because of authorization failure">>},
   {enabled,true},
   {module,xdcr}]},
 {16395,
  [{name,<<"authorization failure while updating remote cluster ref">>},
   {description,<<"failed to update remote cluster ref because of authorization failure">>},
   {enabled,true},
   {module,xdcr}]},
 {16396,
  [{name,<<"access denied">>},
   {description,<<"access denied">>},
   {enabled,true},
   {module,xdcr}]},
 {20480,
  [{name,<<"opened DCP connection">>},
   {description,<<"opened DCP connection">>},
   {enabled,true},
   {module,memcached}]},
 {20482,
  [{name,<<"external memcached bucket flush">>},
   {description,<<"External user flushed the content of a memcached bucket">>},
   {enabled,true},
   {module,memcached}]},
 {20483,
  [{name,<<"invalid packet">>},
   {description,<<"Rejected an invalid packet">>},
   {enabled,true},
   {module,memcached}]},
 {20485,
  [{name,<<"authentication succeeded">>},
   {description,<<"Authentication to the cluster succeeded">>},
   {enabled,false},
   {module,memcached}]},
 {20488,
  [{name,<<"document read">>},
   {description,<<"Document was read">>},
   {enabled,false},
   {module,memcached}]},
 {20489,
  [{name,<<"document locked">>},
   {description,<<"Document was locked">>},
   {enabled,false},
   {module,memcached}]},
 {20490,
  [{name,<<"document modify">>},
   {description,<<"Document was modified">>},
   {enabled,false},
   {module,memcached}]},
 {20491,
  [{name,<<"document delete">>},
   {description,<<"Document was deleted">>},
   {enabled,false},
   {module,memcached}]},
 {20492,
  [{name,<<"select bucket">>},
   {description,<<"The specified bucket was selected">>},
   {enabled,true},
   {module,memcached}]},
 {20493,
  [{name,<<"session terminated">>},
   {description,<<"Session to the cluster has terminated">>},
   {enabled,false},
   {module,memcached}]},
 {20494,
  [{name,<<"tenant rate limited">>},
   {description,<<"The given tenant was rate limited">>},
   {enabled,true},
   {module,memcached}]},
 {24576,
  [{name,<<"Delete index">>},
   {description,<<"FTS index was deleted">>},
   {enabled,true},
   {module,fts}]},
 {24577,
  [{name,<<"Create/Update index">>},
   {description,<<"FTS index was created/Updated">>},
   {enabled,true},
   {module,fts}]},
 {24579,
  [{name,<<"Control index">>},
   {description,<<"FTS index control command was issued">>},
   {enabled,true},
   {module,fts}]},
 {24582,
  [{name,<<"GC run">>},
   {description,<<"GC run was triggered">>},
   {enabled,true},
   {module,fts}]},
 {24583,
  [{name,<<"CPU profile">>},
   {description,<<"CPU profiling was started">>},
   {enabled,true},
   {module,fts}]},
 {24584,
  [{name,<<"Memory profile">>},
   {description,<<"Memory profiling was started">>},
   {enabled,true},
   {module,fts}]},
 {28672,
  [{name,<<"SELECT statement">>},
   {description,<<"A N1QL SELECT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28673,
  [{name,<<"EXPLAIN statement">>},
   {description,<<"A N1QL EXPLAIN statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28674,
  [{name,<<"PREPARE statement">>},
   {description,<<"A N1QL PREPARE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28675,
  [{name,<<"INFER statement">>},
   {description,<<"A N1QL INFER statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28676,
  [{name,<<"INSERT statement">>},
   {description,<<"A N1QL INSERT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28677,
  [{name,<<"UPSERT statement">>},
   {description,<<"A N1QL UPSERT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28678,
  [{name,<<"DELETE statement">>},
   {description,<<"A N1QL DELETE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28679,
  [{name,<<"UPDATE statement">>},
   {description,<<"A N1QL UPDATE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28680,
  [{name,<<"MERGE statement">>},
   {description,<<"A N1QL MERGE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28681,
  [{name,<<"CREATE INDEX statement">>},
   {description,<<"A N1QL CREATE INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28682,
  [{name,<<"DROP INDEX statement">>},
   {description,<<"A N1QL DROP INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28683,
  [{name,<<"ALTER INDEX statement">>},
   {description,<<"A N1QL ALTER INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28684,
  [{name,<<"BUILD INDEX statement">>},
   {description,<<"A N1QL BUILD INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28685,
  [{name,<<"GRANT ROLE statement">>},
   {description,<<"A N1QL GRANT ROLE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28686,
  [{name,<<"REVOKE ROLE statement">>},
   {description,<<"A N1QL REVOKE ROLE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28687,
  [{name,<<"UNRECOGNIZED statement">>},
   {description,<<"An unrecognized statement was received by the N1QL query engine">>},
   {enabled,false},
   {module,n1ql}]},
 {28688,
  [{name,<<"CREATE PRIMARY INDEX statement">>},
   {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28689,
  [{name,<<"/admin/stats API request">>},
   {description,<<"An HTTP request was made to the API at /admin/stats.">>},
   {enabled,false},
   {module,n1ql}]},
 {28690,
  [{name,<<"/admin/vitals API request">>},
   {description,<<"An HTTP request was made to the API at /admin/vitals.">>},
   {enabled,false},
   {module,n1ql}]},
 {28691,
  [{name,<<"/admin/prepareds API request">>},
   {description,<<"An HTTP request was made to the API at /admin/prepareds.">>},
   {enabled,false},
   {module,n1ql}]},
 {28692,
  [{name,<<"/admin/active_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/active_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28693,
  [{name,<<"/admin/indexes/prepareds API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
   {enabled,false},
   {module,n1ql}]},
 {28694,
  [{name,<<"/admin/indexes/active_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28695,
  [{name,<<"/admin/indexes/completed_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28697,
  [{name,<<"/admin/ping API request">>},
   {description,<<"An HTTP request was made to the API at /admin/ping.">>},
   {enabled,false},
   {module,n1ql}]},
 {28698,
  [{name,<<"/admin/config API request">>},
   {description,<<"An HTTP request was made to the API at /admin/config.">>},
   {enabled,false},
   {module,n1ql}]},
 {28699,
  [{name,<<"/admin/ssl_cert API request">>},
   {description,<<"An HTTP request was made to the API at /admin/ssl_cert.">>},
   {enabled,false},
   {module,n1ql}]},
 {28700,
  [{name,<<"/admin/settings API request">>},
   {description,<<"An HTTP request was made to the API at /admin/settings.">>},
   {enabled,false},
   {module,n1ql}]},
 {28701,
  [{name,<<"/admin/clusters API request">>},
   {description,<<"An HTTP request was made to the API at /admin/clusters.">>},
   {enabled,false},
   {module,n1ql}]},
 {28702,
  [{name,<<"/admin/completed_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/completed_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28704,
  [{name,<<"/admin/functions API request">>},
   {description,<<"An HTTP request was made to the API at /admin/functions.">>},
   {enabled,false},
   {module,n1ql}]},
 {28705,
  [{name,<<"/admin/indexes/functions API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/functions.">>},
   {enabled,false},
   {module,n1ql}]},
 {28706,
  [{name,<<"CREATE FUNCTION statement">>},
   {description,<<"A N1QL CREATE FUNCTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28707,
  [{name,<<"DROP FUNCTION statement">>},
   {description,<<"A N1QL DROP FUNCTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28708,
  [{name,<<"EXECUTE FUNCTION statement">>},
   {description,<<"A N1QL EXECUTE FUNCTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28709,
  [{name,<<"/admin/tasks API request">>},
   {description,<<"An HTTP request was made to the API at /admin/tasks.">>},
   {enabled,false},
   {module,n1ql}]},
 {28710,
  [{name,<<"/admin/indexes/tasks API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/tasks.">>},
   {enabled,false},
   {module,n1ql}]},
 {28711,
  [{name,<<"/admin/dictionary_cache API request">>},
   {description,<<"An HTTP request was made to the API at /admin/dictionary_cache.">>},
   {enabled,false},
   {module,n1ql}]},
 {28712,
  [{name,<<"/admin/indexes/dictionary_cache API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/dictionary_cache.">>},
   {enabled,false},
   {module,n1ql}]},
 {28713,
  [{name,<<"CREATE SCOPE statement">>},
   {description,<<"A N1QL CREATE SCOPE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28714,
  [{name,<<"DROP SCOPE statement">>},
   {description,<<"A N1QL DROP SCOPE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28715,
  [{name,<<"CREATE COLLECTION statement">>},
   {description,<<"A N1QL CREATE COLLECTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28716,
  [{name,<<"DROP COLLECTION statement">>},
   {description,<<"A N1QL DROP COLLECTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28717,
  [{name,<<"FLUSH COLLECTION statement">>},
   {description,<<"A N1QL FLUSH COLLECTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28718,
  [{name,<<"UPDATE STATISTICS statement">>},
   {description,<<"A N1QL UPDATE STATISTICS statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28719,
  [{name,<<"ADVISE statement">>},
   {description,<<"A N1QL ADVISE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28720,
  [{name,<<"START TRANSACTION statement">>},
   {description,<<"A N1QL START TRANSACTION statement was execu"...>>},
   {enabled,false},
   {module,n1ql}]},
 {28721,
  [{name,<<"COMMIT TRANSACTION statement">>},
   {description,<<"A N1QL COMMIT TRANSACTION statement was "...>>},
   {enabled,false},
   {module,n1ql}]},
 {28722,
  [{name,<<"ROLLBACK TRANSACTION statement">>},
   {description,<<"A N1QL ROLLBACK TRANSACTION statemen"...>>},
   {enabled,false},
   {module,n1ql}]},
 {28723,
  [{name,<<"ROLLBACK TRANSACTION TO SAVEPOINT st"...>>},
   {description,<<"A N1QL ROLLBACK TRANSACTION TO S"...>>},
   {enabled,false},
   {module,n1ql}]},
 {28724,
  [{name,<<"SET TRANSACTION ISOLATION statem"...>>},
   {description,<<"A N1QL SET TRANSACTION ISOLA"...>>},
   {enabled,false},
   {module,n1ql}]},
 {28725,
  [{name,<<"SAVEPOINT statement">>},
   {description,<<"A N1QL SAVEPOINT stateme"...>>},
   {enabled,false},
   {module,n1ql}]},
 {28726,
  [{name,<<"/admin/transactions API "...>>},
   {description,<<"An HTTP request was "...>>},
   {enabled,false},
   {module,n1ql}]},
 {28727,
  [{name,<<"/admin/indexes/trans"...>>},
   {description,<<"An HTTP request "...>>},
   {enabled,false},
   {module,n1ql}]},
 {28728,
  [{name,<<"N1QL backup / re"...>>},
   {description,<<"An HTTP requ"...>>},
   {enabled,false},
   {module,n1ql}]},
 {28729,
  [{name,<<"/admin/shutd"...>>},
   {description,<<"An HTTP "...>>},
   {enabled,false},
   {module,n1ql}]},
 {28730,
  [{name,<<"/admin/g"...>>},
   {description,<<"An H"...>>},
   {enabled,false},
   {module,...}]},
 {28731,[{name,<<"/adm"...>>},{description,<<...>>},{enabled,...},{...}]},
 {28732,[{name,<<...>>},{description,...},{...}|...]},
 {28733,[{name,...},{...}|...]},
 {28734,[{...}|...]},
 {28735,[...]},
 {28736,...},
 {...}|...]
[ns_server:debug,2025-05-15T18:46:49.809Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
cluster_compat_version ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{3,63914554009}}]},7,6]
[ns_server:debug,2025-05-15T18:46:49.809Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
rbac_upgrade ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554009}}]}|
 '_deleted']
[ns_server:debug,2025-05-15T18:46:49.809Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
auto_failover_cfg ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554009}}]},
 {disable_max_count,false},
 {enabled,true},
 {timeout,120},
 {count,0},
 {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
 {failover_server_group,false},
 {max_count,1},
 {failed_over_server_groups,[]},
 {can_abort_rebalance,true},
 {failover_preserve_durability_majority,false}]
[ns_server:debug,2025-05-15T18:46:49.809Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
resource_management ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554009}}]},
 {bucket,[{resident_ratio,[{enabled,false},
                           {couchstore_minimum,1},
                           {magma_minimum,0.2}]},
          {data_size,[{enabled,false},
                      {couchstore_maximum,2},
                      {magma_maximum,16}]}]},
 {index,[]},
 {cores_per_bucket,[{enabled,false},{minimum,0.4}]},
 {disk_usage,[{enabled,false},{maximum,96}]},
 {collections_per_quota,[{enabled,false},{maximum,1}]}]
[ns_server:debug,2025-05-15T18:46:49.809Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/analytics/settings/config">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554009}}]}|
 <<"{\"analytics.settings.blob_storage_bucket\":\"\",\"analytics.settings.blob_storage_prefix\":\"\",\"analytics.settings.blob_storage_region\":\"\",\"analytics.settings.blob_storage_scheme\":\"\",\"analytics.settings.num_replicas\":0}">>]
[ns_server:debug,2025-05-15T18:46:49.809Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554009}}]}|
 <<"{\"indexer.settings.compaction.abort_exceed_interval\":false,\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.enable_page_bloom_filter\":false,\"indexer.settings.enable_"...>>]
[ns_server:debug,2025-05-15T18:46:49.809Z,ns_1@cb.local:memcached_permissions<0.531.0>:memcached_cfg:rename_and_refresh:178]Successfully renamed "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2025-05-15T18:46:49.809Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:handle_cast:67]Refresh of rbac requested
[ns_server:debug,2025-05-15T18:46:49.810Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/query/settings/config">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554009}}]}|
 <<"{\"cleanupclientattempts\":true,\"cleanuplostattempts\":true,\"cleanupwindow\":\"60s\",\"completed-limit\":4000,\"completed-max-plan-size\":262144,\"completed-threshold\":1000,\"loglevel\":\"info\",\"max-parallelism\":1,\"memory-quota\":0,\"n1ql-feat-ctrl\":76,\"node-quota\":0,\"node-quota-val-percent\":67,\"num-cpus\":0,\"numatrs\":1024,\"pipeline-batch\":16,\"pipeline-cap\":512,\"prepared-limit\":16384,\"query.settings.cu"...>>]
[ns_server:debug,2025-05-15T18:46:49.810Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',prometheus_auth_info} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554009}}]}|
 {"@prometheus",
  {auth,
   [{<<"hash">>,
     {[{<<"hashes">>,
        {sanitized,<<"ocCjmLXobw413zJaq2v5hbk6F9JaoNKMHAEhwOXAF/k=">>}},
       {<<"algorithm">>,<<"argon2id">>},
       {<<"salt">>,<<"FlBs6+3flsH1DlD+dLEwjg==">>},
       {<<"parallelism">>,1},
       {<<"time">>,3},
       {<<"memory">>,524288}]}},
    {<<"scram-sha-512">>,
     {[{<<"salt">>,
        <<"F/Zqupd2IseMgxcuAf9fe4gpX0DBCFm1/fWg3iuaAuQdnTMBGYBMeqXKzW13cDp+Q9KGShsd3l23qMAKTVwiVA==">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"fNgWati8g2faSgrPjZUuj7XSR1xhSOtSh9nzY5vi8O4=">>}}]}},
    {<<"scram-sha-256">>,
     {[{<<"salt">>,<<"OKVHe2p1Uu/nuWHnr4hDmY1YmTEi+LTh2QSQmLrl6JU=">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"wJZdBxOuuYIylriwbbRV9JdvCNKKT1kZtZR+FOCJUQg=">>}}]}},
    {<<"scram-sha-1">>,
     {[{<<"salt">>,<<"FmlE6xOIeEZ8k+MADCxX6iijEW8=">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"gfRT+N7lHhRndbdyfaO+d/dOfRbaCQl3VgGgatuBZck=">>}}]}}]}}]
[ns_server:info,2025-05-15T18:46:49.812Z,ns_1@cb.local:<0.845.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service capi_ssl_service
[ns_server:info,2025-05-15T18:46:49.813Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:notify_services:1162]Succesfully notified services [memcached,capi_ssl_service]
[error_logger:info,2025-05-15T18:46:49.813Z,ns_1@cb.local:ns_orchestrator_child_sup<0.865.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.866.0>},
              {id,ns_janitor_server},
              {mfargs,{ns_janitor_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:49.814Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:restart_regenerate_client_cert_timer:1447]Time left before client cert regeneration: 70588797000
[ns_server:debug,2025-05-15T18:46:49.814Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:maybe_store_ca_certs:832]Considering to store CA certs
[ns_server:debug,2025-05-15T18:46:49.816Z,ns_1@cb.local:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:restart_regenerate_client_cert_timer:1447]Time left before client cert regeneration: 70588797000
[ns_server:info,2025-05-15T18:46:49.828Z,ns_1@cb.local:ns_orchestrator_child_sup<0.865.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          auto_reprovision},
                                         auto_reprovision,[],[]]): started as <0.869.0> on 'ns_1@cb.local'

[error_logger:info,2025-05-15T18:46:49.828Z,ns_1@cb.local:ns_orchestrator_child_sup<0.865.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.869.0>},
              {id,auto_reprovision},
              {mfargs,{auto_reprovision,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:49.828Z,ns_1@cb.local:memcached_passwords<0.528.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,{auth,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2025-05-15T18:46:49.830Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:handle_call:57]File rename from "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw" is requested
[ns_server:info,2025-05-15T18:46:49.830Z,ns_1@cb.local:ns_orchestrator_child_sup<0.865.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,auto_rebalance},
                                         auto_rebalance,[],[]]): started as <0.870.0> on 'ns_1@cb.local'

[error_logger:info,2025-05-15T18:46:49.830Z,ns_1@cb.local:ns_orchestrator_child_sup<0.865.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.870.0>},
              {id,auto_rebalance},
              {mfargs,{auto_rebalance,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:46:49.830Z,ns_1@cb.local:ns_orchestrator_child_sup<0.865.0>:misc:start_singleton:913]start_singleton(gen_statem, start_link, [{via,leader_registry,ns_orchestrator},
                                         ns_orchestrator,[],[]]): started as <0.871.0> on 'ns_1@cb.local'

[error_logger:info,2025-05-15T18:46:49.830Z,ns_1@cb.local:ns_orchestrator_child_sup<0.865.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.871.0>},
              {id,ns_orchestrator},
              {mfargs,{ns_orchestrator,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.830Z,ns_1@cb.local:ns_orchestrator_sup<0.814.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.865.0>},
              {id,ns_orchestrator_child_sup},
              {mfargs,{ns_orchestrator_child_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:49.830Z,ns_1@cb.local:<0.873.0>:auto_failover:init:223]init auto_failover.
[ns_server:debug,2025-05-15T18:46:49.831Z,ns_1@cb.local:memcached_passwords<0.528.0>:memcached_cfg:rename_and_refresh:178]Successfully renamed "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2025-05-15T18:46:49.831Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:handle_cast:67]Refresh of isasl requested
[user:info,2025-05-15T18:46:49.836Z,ns_1@cb.local:<0.873.0>:auto_failover:enable_auto_failover:460]Enabled auto-failover with timeout 120 and max count 1
[ns_server:debug,2025-05-15T18:46:49.836Z,ns_1@cb.local:<0.873.0>:auto_failover:update_and_save_auto_failover_state:479]No change in timeout 120
[ns_server:debug,2025-05-15T18:46:49.836Z,ns_1@cb.local:<0.873.0>:auto_failover:update_and_save_auto_failover_state:487]No change in max count 1
[ns_server:debug,2025-05-15T18:46:49.838Z,ns_1@cb.local:<0.873.0>:auto_failover:init_logic_state:249]Using auto-failover logic state {state,[],
                                    [{service_state,kv,nil,false},
                                     {service_state,n1ql,nil,false},
                                     {service_state,index,nil,false},
                                     {service_state,fts,nil,false},
                                     {service_state,cbas,nil,false},
                                     {service_state,eventing,nil,false},
                                     {service_state,backup,nil,false}],
                                    118}
[ns_server:debug,2025-05-15T18:46:49.838Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
auto_failover_cfg ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{3,63914554009}}]},
 {enabled,true},
 {timeout,120},
 {count,0},
 {max_count,1},
 {disable_max_count,false},
 {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
 {failover_server_group,false},
 {failed_over_server_groups,[]},
 {can_abort_rebalance,true},
 {failover_preserve_durability_majority,false}]
[ns_server:info,2025-05-15T18:46:49.838Z,ns_1@cb.local:ns_orchestrator_sup<0.814.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,auto_failover},
                                         auto_failover,[],[]]): started as <0.873.0> on 'ns_1@cb.local'

[ns_server:debug,2025-05-15T18:46:49.838Z,ns_1@cb.local:ns_config_rep<0.545.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([auto_failover_cfg]..)
[error_logger:info,2025-05-15T18:46:49.838Z,ns_1@cb.local:ns_orchestrator_sup<0.814.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.873.0>},
              {id,auto_failover},
              {mfargs,{auto_failover,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.838Z,ns_1@cb.local:mb_master_sup<0.792.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.814.0>},
              {id,ns_orchestrator_sup},
              {mfargs,{ns_orchestrator_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:info,2025-05-15T18:46:49.839Z,ns_1@cb.local:mb_master_sup<0.792.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          tombstone_purger},
                                         tombstone_purger,[],[]]): started as <0.885.0> on 'ns_1@cb.local'

[error_logger:info,2025-05-15T18:46:49.839Z,ns_1@cb.local:mb_master_sup<0.792.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.885.0>},
              {id,tombstone_purger},
              {mfargs,{tombstone_purger,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.840Z,ns_1@cb.local:mb_master_sup<0.792.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.886.0>},
              {id,global_tasks},
              {mfargs,{global_tasks,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.842Z,ns_1@cb.local:mb_master_sup<0.792.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.887.0>},
              {id,guardrail_enforcer},
              {mfargs,{guardrail_enforcer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:49.842Z,ns_1@cb.local:guardrail_enforcer<0.887.0>:guardrail_enforcer:maybe_notify_services:137]Changed Statuses: #{}
[ns_server:debug,2025-05-15T18:46:49.844Z,ns_1@cb.local:<0.889.0>:license_reporting:init:60]Starting license_reporting server
[ns_server:info,2025-05-15T18:46:49.844Z,ns_1@cb.local:mb_master_sup<0.792.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          license_reporting},
                                         license_reporting,[],[]]): started as <0.889.0> on 'ns_1@cb.local'

[error_logger:info,2025-05-15T18:46:49.844Z,ns_1@cb.local:mb_master_sup<0.792.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.889.0>},
              {id,license_reporting},
              {mfargs,{license_reporting,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.844Z,ns_1@cb.local:leader_registry_sup<0.787.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_registry_sup}
    started: [{pid,<0.790.0>},
              {id,mb_master},
              {mfargs,{mb_master,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:49.844Z,ns_1@cb.local:leader_services_sup<0.765.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_services_sup}
    started: [{pid,<0.787.0>},
              {id,leader_registry_sup},
              {mfargs,{leader_registry_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:49.844Z,ns_1@cb.local:<0.764.0>:restartable:start_child:92]Started child process <0.765.0>
  MFA: {leader_services_sup,start_link,[]}
[error_logger:info,2025-05-15T18:46:49.844Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.764.0>},
              {id,leader_services_sup},
              {mfargs,{restartable,start_link,
                                   [{leader_services_sup,start_link,[]},
                                    infinity]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:49.846Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.891.0>},
              {id,ns_tick_agent},
              {mfargs,{ns_tick_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.846Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.893.0>},
              {id,master_activity_events_ingress},
              {mfargs,{gen_event,start_link,
                                 [{local,master_activity_events_ingress}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.846Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.894.0>},
              {id,master_activity_events_timestamper},
              {mfargs,{master_activity_events,start_link_timestamper,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.847Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.895.0>},
              {id,master_activity_events_pids_watcher},
              {mfargs,{master_activity_events_pids_watcher,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.849Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.896.0>},
              {id,master_activity_events_keeper},
              {mfargs,{master_activity_events_keeper,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.851Z,ns_1@cb.local:health_monitor_sup<0.898.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.899.0>},
              {id,ns_server_monitor},
              {mfargs,{ns_server_monitor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.852Z,ns_1@cb.local:health_monitor_sup<0.898.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.901.0>},
              {id,service_monitor_children_sup},
              {mfargs,{supervisor,start_link,
                                  [{local,service_monitor_children_sup},
                                   health_monitor_sup,child]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:49.852Z,ns_1@cb.local:health_monitor_sup<0.898.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.902.0>},
              {id,service_monitor_worker},
              {mfargs,{erlang,apply,[#Fun<health_monitor_sup.0.30770475>,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.855Z,ns_1@cb.local:health_monitor_sup<0.898.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.908.0>},
              {id,node_monitor},
              {mfargs,{node_monitor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.856Z,ns_1@cb.local:health_monitor_sup<0.898.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.915.0>},
              {id,node_status_analyzer},
              {mfargs,{node_status_analyzer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.856Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.898.0>},
              {id,health_monitor_sup},
              {mfargs,{health_monitor_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:49.859Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.918.0>},
              {id,rebalance_agent},
              {mfargs,{rebalance_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.860Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.919.0>},
              {id,kv_hibernation_agent},
              {mfargs,{kv_hibernation_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.862Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.920.0>},
              {id,ns_rebalance_report_manager},
              {mfargs,{ns_rebalance_report_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:49.863Z,ns_1@cb.local:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: tasks, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,6})
[]
[ns_server:debug,2025-05-15T18:46:49.864Z,ns_1@cb.local:cb_creds_rotation<0.922.0>:cb_creds_rotation:start_rotate_timer:214]Starting creds rotation timer (1800000ms)
[error_logger:info,2025-05-15T18:46:49.864Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.922.0>},
              {id,creds_rotation},
              {mfargs,{cb_creds_rotation,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.864Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.494.0>},
              {id,ns_server_sup},
              {mfargs,{ns_server_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:49.864Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:one_shot_barrier:notify:21]Notifying on barrier menelaus_barrier
[error_logger:error,2025-05-15T18:46:49.864Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {local,ns_server_sup}
    errorContext: child_terminated
    reason: {noproc,{gen_statem,call,[mb_master,master_node,infinity]}}
    offender: [{pid,<0.712.0>},
               {id,ns_server_stats},
               {mfargs,{ns_server_stats,start_link,[]}},
               {restart_type,permanent},
               {significant,false},
               {shutdown,1000},
               {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:49.864Z,ns_1@cb.local:menelaus_barrier<0.298.0>:one_shot_barrier:barrier_body:56]Barrier menelaus_barrier got notification from <0.291.0>
[ns_server:debug,2025-05-15T18:46:49.864Z,ns_1@cb.local:ns_server_nodes_sup<0.291.0>:one_shot_barrier:notify:26]Successfuly notified on barrier menelaus_barrier
[error_logger:info,2025-05-15T18:46:49.864Z,ns_1@cb.local:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.924.0>},
              {id,ns_server_stats},
              {mfargs,{ns_server_stats,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:46:49.864Z,ns_1@cb.local:<0.290.0>:restartable:start_child:92]Started child process <0.291.0>
  MFA: {ns_server_nodes_sup,start_link,[]}
[error_logger:info,2025-05-15T18:46:49.864Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.290.0>},
              {id,ns_server_nodes_sup},
              {mfargs,{restartable,start_link,
                                   [{ns_server_nodes_sup,start_link,[]},
                                    infinity]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:46:49.866Z,ns_1@cb.local:compiled_roles_cache<0.454.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@",admin}
[error_logger:info,2025-05-15T18:46:49.868Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.928.0>},
              {id,remote_api},
              {mfargs,{remote_api,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.873Z,ns_1@cb.local:ns_server_cluster_sup<0.235.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.929.0>},
              {id,ns_gc_runner},
              {mfargs,{ns_gc_runner,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:46:49.873Z,ns_1@cb.local:root_sup<0.214.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,root_sup}
    started: [{pid,<0.235.0>},
              {id,ns_server_cluster_sup},
              {mfargs,{ns_server_cluster_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:46:49.874Z,ns_1@cb.local:application_controller<0.44.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    application: ns_server
    started_at: 'ns_1@cb.local'

[ns_server:debug,2025-05-15T18:46:49.874Z,ns_1@cb.local:<0.9.0>:child_erlang:child_loop:147]118: Entered child_loop
[ns_server:debug,2025-05-15T18:46:49.874Z,ns_1@cb.local:<0.9.0>:child_erlang:child_loop:147]118: Entered child_loop
[ns_server:debug,2025-05-15T18:46:49.876Z,ns_1@cb.local:json_rpc_connection-goxdcr-cbauth<0.932.0>:json_rpc_connection:init:71]Observed revrpc connection: label "goxdcr-cbauth", handling process <0.932.0>
[ns_server:debug,2025-05-15T18:46:49.876Z,ns_1@cb.local:json_rpc_connection-saslauthd-saslauthd-port<0.931.0>:json_rpc_connection:init:71]Observed revrpc connection: label "saslauthd-saslauthd-port", handling process <0.931.0>
[ns_server:debug,2025-05-15T18:46:49.876Z,ns_1@cb.local:menelaus_cbauth<0.666.0>:menelaus_cbauth:handle_cast:203]Observed json rpc process {"goxdcr-cbauth",[{internal,true}],<0.932.0>} started
[ns_server:debug,2025-05-15T18:46:49.881Z,ns_1@cb.local:compiled_roles_cache<0.454.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@goxdcr",admin}
[ns_server:debug,2025-05-15T18:46:49.882Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/query/sequences_cache/revision">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554009}}]}|
 <<"0">>]
[ns_server:debug,2025-05-15T18:46:49.882Z,ns_1@cb.local:ns_config_rep<0.545.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([{metakv,<<"/query/sequences_cache/revision">>}]..)
[ns_server:debug,2025-05-15T18:46:50.014Z,ns_1@cb.local:compiled_roles_cache<0.454.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@ns_server",admin}
[ns_server:debug,2025-05-15T18:46:50.411Z,ns_1@cb.local:<0.954.0>:memcached_refresh:do_refresh:153]Successfully connected to memcached, Trying to refresh [rbac,isasl]
[ns_server:debug,2025-05-15T18:46:50.413Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:update_refresh_state:116]Refresh of [rbac,isasl] succeeded
[ns_server:debug,2025-05-15T18:46:50.722Z,ns_1@cb.local:<0.704.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":18,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true,\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[ns_server:info,2025-05-15T18:46:50.724Z,ns_1@cb.local:memcached_config_mgr<0.701.0>:memcached_config_mgr:push_tls_config:237]Successfully pushed TLS config to memcached
[ns_server:info,2025-05-15T18:46:50.725Z,ns_1@cb.local:memcached_config_mgr<0.701.0>:memcached_config_mgr:push_tls_config:233]Pushing TLS config to memcached:
{[{<<"private key">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem">>},
  {<<"certificate chain">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem">>},
  {<<"CA file">>,<<"/opt/couchbase/var/lib/couchbase/config/certs/ca.pem">>},
  {<<"minimum version">>,<<"TLS 1.2">>},
  {<<"cipher list">>,
   {[{<<"TLS 1.2">>,<<"HIGH">>},
     {<<"TLS 1.3">>,
      <<"TLS_AES_256_GCM_SHA384:TLS_AES_128_GCM_SHA256:TLS_CHACHA20_POLY1305_SHA256">>}]}},
  {<<"cipher order">>,true},
  {<<"client cert auth">>,<<"disabled">>}]}
[ns_server:info,2025-05-15T18:46:50.726Z,ns_1@cb.local:memcached_config_mgr<0.701.0>:memcached_config_mgr:push_tls_config:237]Successfully pushed TLS config to memcached
[ns_server:debug,2025-05-15T18:46:50.840Z,ns_1@cb.local:<0.873.0>:auto_failover_logic:log_master_activity:130]Transitioned node {'ns_1@cb.local',<<"28569ac00b9c1d7c50e39741027d428c">>} state new -> up
[ns_server:debug,2025-05-15T18:46:50.878Z,ns_1@cb.local:ns_gc_runner<0.929.0>:ns_gc_runner:handle_info:125]GC populating new pid list of size=591, prevMaxGcDuration=0 us
[ns_server:debug,2025-05-15T18:46:59.019Z,ns_1@cb.local:compiled_roles_cache<0.454.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@prometheus",stats_reader}
[ns_server:debug,2025-05-15T18:47:19.668Z,ns_1@cb.local:compaction_daemon<0.754.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2025-05-15T18:47:19.669Z,ns_1@cb.local:compaction_daemon<0.754.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:47:19.669Z,ns_1@cb.local:compaction_daemon<0.754.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2025-05-15T18:47:19.669Z,ns_1@cb.local:compaction_daemon<0.754.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:47:21.710Z,ns_1@cb.local:compiled_roles_cache<0.454.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"<ud>jaba_admin</ud>",admin}
[ns_server:debug,2025-05-15T18:47:21.734Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
cbas_memory_quota ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554041}}]}|1024]
[ns_server:debug,2025-05-15T18:47:21.734Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
memory_quota ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554041}}]}|3072]
[ns_server:debug,2025-05-15T18:47:21.734Z,ns_1@cb.local:ns_config_rep<0.545.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([cbas_memory_quota,memory_quota]..)
[ns_server:debug,2025-05-15T18:47:21.738Z,ns_1@cb.local:ns_audit<0.696.0>:ns_audit:handle_call:168]Audit cluster_settings: [{local,{[{ip,<<"172.19.0.4">>},{port,8091}]}},
                         {remote,{[{ip,<<"172.19.0.4">>},{port,42770}]}},
                         {real_userid,{[{domain,builtin},
                                        {user,<<"<ud>jaba_admin</ud>">>}]}},
                         {timestamp,<<"2025-05-15T18:47:21.738Z">>},
                         {cluster_name,<<>>},
                         {quotas,{[{kv,3072},
                                   {n1ql,0},
                                   {index,512},
                                   {fts,512},
                                   {cbas,1024},
                                   {eventing,256}]}}]
[ns_server:debug,2025-05-15T18:47:21.738Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
settings ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554041}}]},
 {stats,[{send_stats,true}]}]
[ns_server:debug,2025-05-15T18:47:21.739Z,ns_1@cb.local:ns_config_rep<0.545.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([settings]..)
[ns_server:debug,2025-05-15T18:47:21.764Z,ns_1@cb.local:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@cb.local',services}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                    8})
[index,kv,n1ql]
[ns_server:debug,2025-05-15T18:47:21.764Z,ns_1@cb.local:ns_audit<0.696.0>:ns_audit:handle_call:168]Audit setup_node_services: [{local,{[{ip,<<"172.19.0.4">>},{port,8091}]}},
                            {remote,{[{ip,<<"172.19.0.4">>},{port,42770}]}},
                            {real_userid,
                                {[{domain,builtin},
                                  {user,<<"<ud>jaba_admin</ud>">>}]}},
                            {timestamp,<<"2025-05-15T18:47:21.764Z">>},
                            {services,[index,kv,n1ql]},
                            {node,'ns_1@cb.local'}]
[ns_server:debug,2025-05-15T18:47:21.764Z,ns_1@cb.local:prometheus_cfg<0.515.0>:prometheus_cfg:maybe_apply_new_settings:707]New settings received: [{enabled,true},
                        {retention_size,1024},
                        {retention_time,365},
                        {wal_compression,false},
                        {storage_path,"./stats_data"},
                        {config_file,"prometheus.yml"},
                        {log_file_name,"prometheus.log"},
                        {prometheus_auth_enabled,true},
                        {prometheus_auth_filename,
                            "/opt/couchbase/var/lib/couchbase/config/prometheus_auth"},
                        {log_level,"debug"},
                        {max_block_duration,25},
                        {scrape_interval,10},
                        {scrape_timeout,10},
                        {snapshot_timeout_msecs,30000},
                        {query_request_timeout,5000},
                        {quit_request_timeout,5000},
                        {delete_series_request_timeout,30000},
                        {clean_tombstones_request_timeout,30000},
                        {query_derived_request_timeout,500},
                        {decimation_enabled,false},
                        {truncation_enabled,false},
                        {gomaxprocs,-1},
                        {clean_tombstones_enabled,false},
                        {decimation_defs,
                            [{low,14400,skip},
                             {medium,14400,20},
                             {large,57600,60},
                             {large,31536000,600}]},
                        {pruning_interval,60000},
                        {truncate_max_age,259200},
                        {decimation_match_patterns,["{job=\"general\"}"]},
                        {truncation_match_patterns,
                            ["{job=~\".*_high_cardinality\"}"]},
                        {token_file,"prometheus_token"},
                        {query_max_samples,200000},
                        {intervals_calculation_period,600000},
                        {cbcollect_stats_dump_max_size,1073741824},
                        {cbcollect_stats_min_period,14},
                        {average_sample_size,3},
                        {services,
                            [{ns_server,
                                 [{high_cardinality_enabled,true},
                                  {high_cardinality_scrape_interval,60},
                                  {high_cardinality_scrape_timeout,-1}]},
                             {index,
                                 [{high_cardinality_enabled,true},
                                  {high_cardinality_scrape_interval,-1},
                                  {high_cardinality_scrape_timeout,-1}]},
                             {fts,
                                 [{high_cardinality_enabled,true},
                                  {high_cardinality_scrape_interval,-1},
                                  {high_cardinality_scrape_timeout,-1}]},
                             {kv,[{high_cardinality_enabled,true},
                                  {high_cardinality_scrape_interval,-1},
                                  {high_cardinality_scrape_timeout,-1}]},
                             {cbas,
                                 [{high_cardinality_enabled,true},
                                  {high_cardinality_scrape_interval,-1},
                                  {high_cardinality_scrape_timeout,-1}]},
                             {eventing,
                                 [{high_cardinality_enabled,true},
                                  {high_cardinality_scrape_interval,-1},
                                  {high_cardinality_scrape_timeout,-1}]}]},
                        {external_prometheus_services,
                            [{index,
                                 [{high_cardinality_enabled,true},
                                  {derived_stats_enabled,true}]},
                             {fts,
                                 [{high_cardinality_enabled,true},
                                  {derived_stats_enabled,true}]},
                             {kv,[{high_cardinality_enabled,true},
                                  {derived_stats_enabled,true}]},
                             {cbas,
                                 [{high_cardinality_enabled,true},
                                  {derived_stats_enabled,true}]},
                             {eventing,
                                 [{high_cardinality_enabled,true},
                                  {derived_stats_enabled,true}]},
                             {ns_server,
                                 [{high_cardinality_enabled,true},
                                  {derived_stats_enabled,true}]}]},
                        {prometheus_metrics_enabled,false},
                        {prometheus_metrics_scrape_interval,60},
                        {listen_addr_type,loopback},
                        {log_queries,false},
                        {derived_metrics_filter,all},
                        {derived_metrics_interval,-1},
                        {rules_config_file,"prometheus_rules.yml"},
                        {loopback_delta,600},
                        {listen_port,9123},
                        {addr,"127.0.0.1:9123"},
                        {prometheus_creds,{"ns_server","********"}},
                        {targets,
                            [{ns_server,"127.0.0.1:8091"},
                             {xdcr,"127.0.0.1:9998"},
                             {index,"127.0.0.1:9102"},
                             {kv,"127.0.0.1:11280"},
                             {n1ql,"127.0.0.1:8093"}]},
                        {afamily,inet},
                        {dynamic_scrape_intervals,[]}]
Added: [{targets,[{ns_server,"127.0.0.1:8091"},
                  {xdcr,"127.0.0.1:9998"},
                  {index,"127.0.0.1:9102"},
                  {kv,"127.0.0.1:11280"},
                  {n1ql,"127.0.0.1:8093"}]}], Deleted: [{targets,
                                                         [{ns_server,
                                                           "127.0.0.1:8091"},
                                                          {xdcr,
                                                           "127.0.0.1:9998"},
                                                          {kv,
                                                           "127.0.0.1:11280"}]}]
[ns_server:debug,2025-05-15T18:47:21.764Z,ns_1@cb.local:<0.704.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":22,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true,\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[ns_server:debug,2025-05-15T18:47:21.765Z,ns_1@cb.local:prometheus_cfg<0.515.0>:prometheus_cfg:ensure_prometheus_config:932]Updating prometheus config file: /opt/couchbase/var/lib/couchbase/config/prometheus.yml
[ns_server:debug,2025-05-15T18:47:21.767Z,ns_1@cb.local:prometheus_cfg<0.515.0>:prometheus_cfg:ensure_prometheus_config:932]Updating prometheus config file: /opt/couchbase/var/lib/couchbase/config/prometheus_rules.yml
[ns_server:debug,2025-05-15T18:47:21.768Z,ns_1@cb.local:ns_config_rep<0.545.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([{metakv,<<"/indexing/settings/config">>}]..)
[ns_server:debug,2025-05-15T18:47:21.768Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554041}}]}|
 <<"{\"indexer.settings.compaction.abort_exceed_interval\":false,\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.enable_page_bloom_filter\":false,\"indexer.settings.enable_"...>>]
[ns_server:debug,2025-05-15T18:47:21.768Z,ns_1@cb.local:ns_audit<0.696.0>:ns_audit:handle_call:168]Audit modify_index_storage_mode: [{local,
                                      {[{ip,<<"172.19.0.4">>},{port,8091}]}},
                                  {remote,
                                      {[{ip,<<"172.19.0.4">>},{port,42770}]}},
                                  {real_userid,
                                      {[{domain,builtin},
                                        {user,<<"<ud>jaba_admin</ud>">>}]}},
                                  {timestamp,<<"2025-05-15T18:47:21.768Z">>},
                                  {storageMode,<<"plasma">>}]
[ns_server:debug,2025-05-15T18:47:21.771Z,ns_1@cb.local:prometheus_cfg<0.515.0>:prometheus_cfg:try_config_reload:814]Started prometheus reload process: <0.2073.0>
[ns_server:debug,2025-05-15T18:47:21.781Z,ns_1@cb.local:<0.2073.0>:prometheus_cfg:do_prometheus_reload:822]Config successfully reloaded in 9 msecs
[ns_server:debug,2025-05-15T18:47:21.786Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
rest ->
[{port,8091}]
[ns_server:debug,2025-05-15T18:47:21.786Z,ns_1@cb.local:ns_config_rep<0.545.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([rest]..)
[ns_server:debug,2025-05-15T18:47:21.787Z,ns_1@cb.local:prometheus_cfg<0.515.0>:prometheus_cfg:maybe_apply_new_settings:704]Settings didn't change, ignoring update
[ns_server:debug,2025-05-15T18:47:21.787Z,ns_1@cb.local:<0.704.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":24,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true,\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[ns_server:debug,2025-05-15T18:47:21.797Z,ns_1@cb.local:menelaus_ui_auth<0.607.0>:token_server:handle_cast:360]Purge tokens []
[ns_server:debug,2025-05-15T18:47:21.798Z,ns_1@cb.local:compiled_roles_cache<0.454.0>:versioned_cache:handle_info:89]Flushing cache compiled_roles_cache due to version change from {[7,6],
                                                                {0,249091395},
                                                                {0,249091395},
                                                                false,[]} to {[7,
                                                                               6],
                                                                              {0,
                                                                               249091395},
                                                                              {0,
                                                                               249091395},
                                                                              true,
                                                                              []}
[ns_server:debug,2025-05-15T18:47:21.798Z,ns_1@cb.local:ns_config_rep<0.545.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([rest_creds]..)
[ns_server:debug,2025-05-15T18:47:21.798Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
rest_creds ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554041}}]}|
 {"<ud>jaba_admin</ud>",
  {auth,
   [{<<"hash">>,
     {[{<<"hashes">>,
        {sanitized,<<"3I14lVC6WTo2HkE1Ma2OkSoP1R5UwTP501YfMTKM7ak=">>}},
       {<<"algorithm">>,<<"argon2id">>},
       {<<"salt">>,<<"5CFoS4MqJh4VclPVzeJxBw==">>},
       {<<"parallelism">>,1},
       {<<"time">>,3},
       {<<"memory">>,524288}]}},
    {<<"scram-sha-512">>,
     {[{<<"salt">>,
        <<"pulx/Jw0I8WxLIxMcCD8CqAxiVNfHNaHWpBI6wjnArfbB+WyxXwBuk5SnOGA7W5LgTZZWGevGaY3ih1tGH4jeg==">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"+8aFWL6Fwfs8HKehUbj7ZkLi5LW9/xRhbkbg/BwGA9w=">>}}]}},
    {<<"scram-sha-256">>,
     {[{<<"salt">>,<<"6mOH6zwaMRxGOfTsFTCuTxfMVctxE7w9qyktmzrJtVg=">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"t9IO+WS5kijXIacgFn1gB5R0bN61IAKLCL/JWibrJSA=">>}}]}},
    {<<"scram-sha-1">>,
     {[{<<"salt">>,<<"pEmVjw1zOFSsjZc/8lZKrEstBkA=">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"tJt6Wglz80zHvML1mQcTdrXmQggip4HdkH+YfamhrEc=">>}}]}}]}}]
[ns_server:debug,2025-05-15T18:47:21.798Z,ns_1@cb.local:ns_audit<0.696.0>:ns_audit:handle_call:168]Audit password_change: [{local,{[{ip,<<"172.19.0.4">>},{port,8091}]}},
                        {remote,{[{ip,<<"172.19.0.4">>},{port,42770}]}},
                        {real_userid,{[{domain,builtin},
                                       {user,<<"<ud>jaba_admin</ud>">>}]}},
                        {timestamp,<<"2025-05-15T18:47:21.798Z">>},
                        {identity,{[{domain,builtin},
                                    {user,<<"<ud>jaba_admin</ud>">>}]}}]
[ns_server:debug,2025-05-15T18:47:21.798Z,ns_1@cb.local:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
uuid ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554041}}]}|
 <<"61933a8028482692b0278a91060e0d90">>]
[ns_server:debug,2025-05-15T18:47:21.798Z,ns_1@cb.local:memcached_passwords<0.528.0>:memcached_cfg:write_cfg:156]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2025-05-15T18:47:21.798Z,ns_1@cb.local:memcached_permissions<0.531.0>:memcached_cfg:write_cfg:156]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2025-05-15T18:47:21.798Z,ns_1@cb.local:ns_config_rep<0.545.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([uuid]..)
[ns_server:debug,2025-05-15T18:47:21.799Z,ns_1@cb.local:memcached_permissions<0.531.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,{user,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2025-05-15T18:47:21.800Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:handle_call:57]File rename from "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac" is requested
[error_logger:info,2025-05-15T18:47:21.800Z,ns_1@cb.local:service_stats_children_sup<0.731.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_stats_children_sup}
    started: [{pid,<0.2105.0>},
              {id,{service_index,stats_reader,"@index"}},
              {mfargs,{stats_reader,start_link,["@index"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:21.801Z,ns_1@cb.local:memcached_permissions<0.531.0>:memcached_cfg:rename_and_refresh:178]Successfully renamed "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2025-05-15T18:47:21.801Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:handle_cast:67]Refresh of rbac requested
[ns_server:debug,2025-05-15T18:47:21.803Z,ns_1@cb.local:<0.2107.0>:memcached_refresh:do_refresh:153]Successfully connected to memcached, Trying to refresh [rbac]
[ns_server:debug,2025-05-15T18:47:21.804Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:update_refresh_state:116]Refresh of [rbac] succeeded
[ns_server:debug,2025-05-15T18:47:21.811Z,ns_1@cb.local:menelaus_cbauth<0.666.0>:menelaus_cbauth:handle_cast:203]Observed json rpc process {"goxdcr-cbauth",[{internal,true}],<0.932.0>} needs_update
[error_logger:info,2025-05-15T18:47:21.811Z,ns_1@cb.local:service_agent_children_sup<0.679.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_agent_children_sup}
    started: [{pid,<0.2109.0>},
              {id,{service_agent,index}},
              {mfargs,{service_agent,start_link,[index]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:21.812Z,ns_1@cb.local:memcached_passwords<0.528.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,{auth,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[error_logger:info,2025-05-15T18:47:21.812Z,ns_1@cb.local:service_agent_children_sup<0.679.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_agent_children_sup}
    started: [{pid,<0.2113.0>},
              {id,{service_agent,n1ql}},
              {mfargs,{service_agent,start_link,[n1ql]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:21.812Z,ns_1@cb.local:menelaus_cbauth<0.666.0>:menelaus_cbauth:handle_cast:203]Observed json rpc process {"goxdcr-cbauth",[{internal,true}],<0.932.0>} needs_update
[ns_server:debug,2025-05-15T18:47:21.813Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:handle_call:57]File rename from "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw" is requested
[ns_server:debug,2025-05-15T18:47:21.814Z,ns_1@cb.local:memcached_passwords<0.528.0>:memcached_cfg:rename_and_refresh:178]Successfully renamed "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2025-05-15T18:47:21.814Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:handle_cast:67]Refresh of isasl requested
[error_logger:info,2025-05-15T18:47:21.815Z,ns_1@cb.local:service_monitor_children_sup<0.901.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_monitor_children_sup}
    started: [{pid,<0.2118.0>},
              {id,{kv,dcp_traffic_monitor}},
              {mfargs,{dcp_traffic_monitor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:21.816Z,ns_1@cb.local:<0.2117.0>:memcached_refresh:do_refresh:153]Successfully connected to memcached, Trying to refresh [isasl]
[ns_server:debug,2025-05-15T18:47:21.817Z,ns_1@cb.local:memcached_refresh<0.304.0>:memcached_refresh:update_refresh_state:116]Refresh of [isasl] succeeded
[error_logger:info,2025-05-15T18:47:21.818Z,ns_1@cb.local:service_monitor_children_sup<0.901.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_monitor_children_sup}
    started: [{pid,<0.2120.0>},
              {id,{kv,kv_stats_monitor}},
              {mfargs,{kv_stats_monitor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:21.820Z,ns_1@cb.local:service_monitor_children_sup<0.901.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_monitor_children_sup}
    started: [{pid,<0.2124.0>},
              {id,{kv,kv_monitor}},
              {mfargs,{kv_monitor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:21.839Z,ns_1@cb.local:ns_ports_setup<0.675.0>:ns_ports_manager:set_dynamic_children:48]Setting children [memcached,saslauthd_port,goxdcr,index,n1ql,projector]
[ns_server:debug,2025-05-15T18:47:21.839Z,ns_1@cb.local:compiled_roles_cache<0.454.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@goxdcr",admin}
[ns_server:debug,2025-05-15T18:47:22.001Z,ns_1@cb.local:compiled_roles_cache<0.454.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"<ud>jaba_admin</ud>",admin}
[ns_server:debug,2025-05-15T18:47:22.012Z,ns_1@cb.local:compiled_roles_cache<0.454.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@",admin}
[ns_server:debug,2025-05-15T18:47:22.012Z,ns_1@cb.local:json_rpc_connection-index-cbauth<0.2143.0>:json_rpc_connection:init:71]Observed revrpc connection: label "index-cbauth", handling process <0.2143.0>
[ns_server:debug,2025-05-15T18:47:22.012Z,ns_1@cb.local:menelaus_cbauth<0.666.0>:menelaus_cbauth:handle_cast:203]Observed json rpc process {"index-cbauth",[{internal,true}],<0.2143.0>} started
[ns_server:debug,2025-05-15T18:47:22.013Z,ns_1@cb.local:compiled_roles_cache<0.454.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@index",admin}
[cluster:debug,2025-05-15T18:47:22.014Z,ns_1@cb.local:ns_cluster<0.273.0>:ns_cluster:handle_call:415]handling add_node(https, "db2.lan", 18091, undefined, ..)
[ns_server:info,2025-05-15T18:47:22.014Z,ns_1@cb.local:ns_cluster<0.273.0>:ns_cluster:check_add_possible:781]Checking if host 'db2.lan' is allowed to join the cluster (allowed hosts: [<<"*">>])
[ns_server:debug,2025-05-15T18:47:22.018Z,ns_1@cb.local:ns_cluster<0.273.0>:ns_cluster:check_host_port_connectivity:656]Successfully checked TCP connectivity to "db2.lan":18091
[cluster:info,2025-05-15T18:47:22.019Z,ns_1@cb.local:ns_cluster<0.273.0>:ns_cluster:do_change_address:715]Change of address to "172.19.0.4" is requested.
[ns_server:debug,2025-05-15T18:47:22.019Z,ns_1@cb.local:ns_node_disco<0.537.0>:ns_node_disco:maybe_monitor_rename_txn:201]Monitor node renaming transaction. Pid = <0.2150.0>, MRef = #Ref<0.2327127311.3650355202.217522>
[ns_server:debug,2025-05-15T18:47:22.019Z,ns_1@cb.local:remote_monitors<0.297.0>:remote_monitors:maybe_monitor_rename_txn:159]Monitor node renaming transaction. Pid = <0.2150.0>, MRef = #Ref<0.2327127311.3650355202.217528>
[ns_server:debug,2025-05-15T18:47:22.020Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Closing listener {external,inet_tcp_dist}
[ns_server:debug,2025-05-15T18:47:22.020Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Full list of processes expected to stop: [<0.231.0>]
[ns_server:debug,2025-05-15T18:47:22.020Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Down from <0.231.0>
[error_logger:info,2025-05-15T18:47:22.020Z,ns_1@cb.local:net_kernel<0.229.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,959,nodedown,'ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:47:22.020Z,ns_1@cb.local:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.2327127311.3650355205.215658>,
                               inet_tcp_dist,<0.490.0>,
                               #Ref<0.2327127311.3650355205.215661>}
[chronicle:info,2025-05-15T18:47:22.020Z,nonode@nohost:chronicle_proposer<0.271.0>:chronicle_proposer:handle_nodedown:1135]Peer 'ns_1@cb.local' went down: [{nodedown_reason,net_kernel_terminated}]
[ns_server:debug,2025-05-15T18:47:22.021Z,nonode@nohost:cb_dist<0.227.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.2327127311.3650355203.215446>,
                               inet_tcp_dist,<0.233.0>,
                               #Ref<0.2327127311.3650355203.215449>}
[user:warn,2025-05-15T18:47:22.021Z,nonode@nohost:ns_node_disco<0.537.0>:ns_node_disco:handle_info:169]Node nonode@nohost saw that node 'ns_1@cb.local' went down. Details: [{nodedown_reason,
                                                                       net_kernel_terminated}]
[ns_server:debug,2025-05-15T18:47:22.021Z,nonode@nohost:<0.514.0>:misc:delaying_crash:1810]Delaying crash exit:{{nodedown,'babysitter_of_ns_1@cb.local'},
                     {gen_server,call,
                         [{ns_babysitter_log,'babysitter_of_ns_1@cb.local'},
                          consume,infinity]}} by 1000ms
Stacktrace: [{gen_server,call,3,[{file,"gen_server.erl"},{line,385}]},
             {ns_log,babysitter_log_consumption_loop,0,
                     [{file,"src/ns_log.erl"},{line,66}]},
             {misc,delaying_crash,2,[{file,"src/misc.erl"},{line,1808}]},
             {proc_lib,init_p,3,[{file,"proc_lib.erl"},{line,225}]}]
[error_logger:error,2025-05-15T18:47:22.022Z,nonode@nohost:cb_dist<0.227.0>:ale_error_logger_handler:do_log:101]cb_dist: terminating with reason: shutdown
[ns_server:debug,2025-05-15T18:47:22.033Z,nonode@nohost:<0.2152.0>:dist_manager:teardown:315]Got nodedown msg {nodedown,'ns_1@cb.local',
                           [{nodedown_reason,net_kernel_terminated}]} after terminating net kernel
[ns_server:info,2025-05-15T18:47:22.033Z,nonode@nohost:<0.2150.0>:dist_manager:do_adjust_address:357]Adjusted IP to "172.19.0.4"
[ns_server:info,2025-05-15T18:47:22.033Z,nonode@nohost:<0.2150.0>:dist_manager:bringup:246]Attempting to bring up net_kernel with name 'ns_1@172.19.0.4'
[error_logger:error,2025-05-15T18:47:22.033Z,nonode@nohost:ns_ports_setup<0.675.0>:ale_error_logger_handler:do_log:101]
=========================CRASH REPORT=========================
  crasher:
    initial call: ns_ports_setup:setup_body_tramp/0
    pid: <0.675.0>
    registered_name: ns_ports_setup
    exception exit: {{nodedown,'babysitter_of_ns_1@cb.local'},
                     {gen_server,call,
                         [{ns_ports_manager,'babysitter_of_ns_1@cb.local'},
                          {set_dynamic_children,
                              #Fun<ns_ports_manager.0.117370397>},
                          infinity]}}
      in function  gen_server:call/3 (gen_server.erl, line 385)
      in call from ns_ports_setup:set_children/2 (src/ns_ports_setup.erl, line 61)
      in call from ns_ports_setup:set_children_and_loop/3 (src/ns_ports_setup.erl, line 77)
    ancestors: [<0.674.0>,<0.673.0>,<0.672.0>,ns_server_sup,
                  ns_server_nodes_sup,<0.290.0>,ns_server_cluster_sup,
                  root_sup,<0.154.0>]
    message_queue_len: 0
    messages: []
    links: [<0.676.0>,<0.677.0>,<0.674.0>]
    dictionary: [{'ns_ports_setup-cbft-available',"/opt/couchbase/bin/cbft"},
                  {'ns_ports_setup-eventing-producer-available',
                      "/opt/couchbase/bin/eventing-producer"},
                  {'ns_ports_setup-projector-available',
                      "/opt/couchbase/bin/projector"},
                  {'ns_ports_setup-cbas-available',"/opt/couchbase/bin/cbas"},
                  {'ns_ports_setup-cbq-engine-available',
                      "/opt/couchbase/bin/cbq-engine"},
                  {'ns_ports_setup-backup-available',
                      "/opt/couchbase/bin/backup"},
                  {'ns_ports_setup-indexer-available',
                      "/opt/couchbase/bin/indexer"},
                  {'ns_ports_setup-saslauthd-port-available',
                      "/opt/couchbase/bin/saslauthd-port"},
                  {'ns_ports_setup-goxdcr-available',
                      "/opt/couchbase/bin/goxdcr"}]
    trap_exit: false
    status: running
    heap_size: 46422
    stack_size: 28
    reductions: 35072
  neighbours:

[ns_server:debug,2025-05-15T18:47:22.034Z,nonode@nohost:<0.677.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {user_storage_events,<0.675.0>} exited with reason {{nodedown,
                                                                                    'babysitter_of_ns_1@cb.local'},
                                                                                   {gen_server,
                                                                                    call,
                                                                                    [{ns_ports_manager,
                                                                                      'babysitter_of_ns_1@cb.local'},
                                                                                     {set_dynamic_children,
                                                                                      #Fun<ns_ports_manager.0.117370397>},
                                                                                     infinity]}}
[ns_server:debug,2025-05-15T18:47:22.034Z,nonode@nohost:<0.676.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.675.0>} exited with reason {{nodedown,
                                                                                               'babysitter_of_ns_1@cb.local'},
                                                                                              {gen_server,
                                                                                               call,
                                                                                               [{ns_ports_manager,
                                                                                                 'babysitter_of_ns_1@cb.local'},
                                                                                                {set_dynamic_children,
                                                                                                 #Fun<ns_ports_manager.0.117370397>},
                                                                                                infinity]}}
[error_logger:error,2025-05-15T18:47:22.034Z,nonode@nohost:<0.674.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {<0.674.0>,suppress_max_restart_intensity}
    errorContext: child_terminated
    reason: {{nodedown,'babysitter_of_ns_1@cb.local'},
             {gen_server,call,
                 [{ns_ports_manager,'babysitter_of_ns_1@cb.local'},
                  {set_dynamic_children,#Fun<ns_ports_manager.0.117370397>},
                  infinity]}}
    offender: [{pid,<0.675.0>},
               {id,ns_ports_setup},
               {mfargs,{ns_ports_setup,start,[]}},
               {restart_type,permanent},
               {significant,false},
               {shutdown,brutal_kill},
               {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:22.034Z,nonode@nohost:<0.802.0>:remote_monitors:handle_down:151]Caller of remote monitor <0.675.0> died with {{nodedown,
                                               'babysitter_of_ns_1@cb.local'},
                                              {gen_server,call,
                                               [{ns_ports_manager,
                                                 'babysitter_of_ns_1@cb.local'},
                                                {set_dynamic_children,
                                                 #Fun<ns_ports_manager.0.117370397>},
                                                infinity]}}. Exiting
[error_logger:info,2025-05-15T18:47:22.034Z,nonode@nohost:<0.674.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.674.0>,suppress_max_restart_intensity}
    started: [{pid,<0.2158.0>},
              {id,ns_ports_setup},
              {mfargs,{ns_ports_setup,start,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:22.034Z,nonode@nohost:ssl_dist_admin_sup<0.2156.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_admin_sup}
    started: [{pid,<0.2157.0>},
              {id,ssl_pem_cache_dist},
              {mfargs,{ssl_pem_cache,start_link_dist,[[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:22.034Z,nonode@nohost:ssl_dist_admin_sup<0.2156.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_admin_sup}
    started: [{pid,<0.2160.0>},
              {id,ssl_dist_manager},
              {mfargs,{ssl_manager,start_link_dist,[[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:22.034Z,nonode@nohost:ssl_dist_sup<0.2155.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_sup}
    started: [{pid,<0.2156.0>},
              {id,ssl_dist_admin_sup},
              {mfargs,{ssl_dist_admin_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:22.035Z,nonode@nohost:tls_dist_sup<0.2162.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_sup}
    started: [{pid,<0.2163.0>},
              {id,dist_tls_connection_sup},
              {mfargs,{tls_connection_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:22.035Z,nonode@nohost:tls_dist_server_sup<0.2164.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_server_sup}
    started: [{pid,<0.2165.0>},
              {id,dist_ssl_listen_tracker_sup},
              {mfargs,{ssl_listen_tracker_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:22.035Z,nonode@nohost:tls_dist_server_sup<0.2164.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_server_sup}
    started: [{pid,<0.2166.0>},
              {id,dist_tls_server_session_ticket},
              {mfargs,{tls_server_session_ticket_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:22.035Z,nonode@nohost:tls_dist_server_sup<0.2164.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_server_sup}
    started: [{pid,<0.2167.0>},
              {id,dist_ssl_upgrade_server_session_cache_sup},
              {mfargs,
                  {ssl_upgrade_server_session_cache_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:22.035Z,nonode@nohost:tls_dist_sup<0.2162.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_sup}
    started: [{pid,<0.2164.0>},
              {id,tls_dist_server_sup},
              {mfargs,{tls_dist_server_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:22.035Z,nonode@nohost:ssl_dist_sup<0.2155.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_sup}
    started: [{pid,<0.2162.0>},
              {id,tls_dist_sup},
              {mfargs,{tls_dist_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:22.035Z,nonode@nohost:net_sup<0.2154.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.2155.0>},
              {id,ssl_dist_sup},
              {mfargs,{ssl_dist_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:47:22.035Z,nonode@nohost:cb_dist<0.2168.0>:cb_dist:info_msg:1098]cb_dist: Starting cb_dist with config [{config_vsn,2}]
[ns_server:debug,2025-05-15T18:47:22.036Z,nonode@nohost:ns_ports_setup<0.2158.0>:misc:delaying_crash:1810]Delaying crash error:distribution_not_started by 1000ms
Stacktrace: [{auth,set_cookie,2,[{file,"auth.erl"},{line,143}]},
             {ns_server,get_babysitter_node,0,
                        [{file,"src/ns_server.erl"},{line,330}]},
             {ns_ports_setup,set_children,2,
                             [{file,"src/ns_ports_setup.erl"},{line,61}]},
             {ns_ports_setup,set_children_and_loop,3,
                             [{file,"src/ns_ports_setup.erl"},{line,77}]},
             {misc,delaying_crash,2,[{file,"src/misc.erl"},{line,1808}]},
             {proc_lib,init_p_do_apply,3,[{file,"proc_lib.erl"},{line,240}]}]
[error_logger:info,2025-05-15T18:47:22.037Z,nonode@nohost:net_sup<0.2154.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.2168.0>},
              {id,cb_dist},
              {mfargs,{cb_dist,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:22.037Z,nonode@nohost:net_sup<0.2154.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.2169.0>},
              {id,auth},
              {mfargs,{auth,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,2000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:22.037Z,nonode@nohost:cb_dist<0.2168.0>:cb_dist:info_msg:1098]cb_dist: Initial protos: [{external,inet_tcp_dist}], required protos: [{external,
                                                                        inet_tcp_dist}]
[ns_server:debug,2025-05-15T18:47:22.037Z,nonode@nohost:cb_dist<0.2168.0>:cb_dist:info_msg:1098]cb_dist: Starting {external,inet_tcp_dist} listener on 21100...
[ns_server:debug,2025-05-15T18:47:22.037Z,nonode@nohost:cb_dist<0.2168.0>:cb_dist:info_msg:1098]cb_dist: Started listener: inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:22.037Z,ns_1@172.19.0.4:cb_dist<0.2168.0>:cb_dist:info_msg:1098]cb_dist: Updated server ssl_dist_opts (keep_secrets) - false
[ns_server:debug,2025-05-15T18:47:22.037Z,ns_1@172.19.0.4:<0.446.0>:doc_replicator:nodeup_monitoring_loop:145]got nodeup event. Considering ddocs replication
[chronicle:info,2025-05-15T18:47:22.037Z,ns_1@172.19.0.4:chronicle_proposer<0.271.0>:chronicle_proposer:handle_nodeup:1093]Peer 'ns_1@172.19.0.4' came up
[user:info,2025-05-15T18:47:22.038Z,ns_1@172.19.0.4:ns_node_disco<0.537.0>:ns_node_disco:handle_info:163]Node 'ns_1@172.19.0.4' saw that node 'ns_1@172.19.0.4' came up. Tags: []
[ns_server:debug,2025-05-15T18:47:22.038Z,ns_1@172.19.0.4:cb_dist<0.2168.0>:cb_dist:info_msg:1098]cb_dist: Started acceptor inet_tcp_dist: <0.2171.0>
[chronicle:info,2025-05-15T18:47:22.038Z,ns_1@172.19.0.4:chronicle_proposer<0.271.0>:chronicle_proposer:handle_nodeup:1127]Peer 'ns_1@172.19.0.4' is not in peers: ['ns_1@cb.local']
[chronicle:debug,2025-05-15T18:47:22.038Z,ns_1@172.19.0.4:chronicle_leader<0.256.0>:chronicle_leader:handle_note_term_status:603]Ignoring stale term status {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                            {2,'ns_1@cb.local'},
                            finished}: {error,{not_a_leader,follower}}
[ns_server:debug,2025-05-15T18:47:22.038Z,ns_1@172.19.0.4:cb_dist<0.2168.0>:cb_dist:info_msg:1098]cb_dist: Ensure config is going to change listeners. Will be stopped: [], will be started: [], will be restarted: []
[chronicle:info,2025-05-15T18:47:22.038Z,ns_1@172.19.0.4:chronicle_proposer<0.271.0>:chronicle_proposer:handle_stop:1269]Proposer for term {2,'ns_1@cb.local'} in history <<"5aab03dbecad06b50ffb474da59a00c9">> is terminating.
[chronicle:debug,2025-05-15T18:47:22.038Z,ns_1@172.19.0.4:chronicle_agent<0.252.0>:chronicle_agent:handle_local_mark_committed:1997]Marked seqno 8 committed
[ns_server:debug,2025-05-15T18:47:22.038Z,ns_1@172.19.0.4:net_kernel<0.2170.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2025-05-15T18:47:22.038Z,ns_1@172.19.0.4:net_sup<0.2154.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.2170.0>},
              {id,net_kernel},
              {mfargs,{net_kernel,start_link,
                                  [#{clean_halt => false,
                                     name => 'ns_1@172.19.0.4',
                                     name_domain => longnames,
                                     net_tickintensity => 4,
                                     net_ticktime => 60,
                                     supervisor => net_sup_dynamic}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,2000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:22.038Z,ns_1@172.19.0.4:cb_dist<0.2168.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.2327127311.3650355204.217326>,
                               inet_tcp_dist,undefined,undefined}
[chronicle:info,2025-05-15T18:47:22.038Z,ns_1@172.19.0.4:chronicle_server<0.260.0>:chronicle_proposer:stop:136]Proposer <0.271.0> stopped: {shutdown,stop}
[error_logger:info,2025-05-15T18:47:22.038Z,ns_1@172.19.0.4:kernel_sup<0.49.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,kernel_sup}
    started: [{pid,<0.2154.0>},
              {id,net_sup_dynamic},
              {mfargs,
                  {erl_distribution,start_link,
                      [#{clean_halt => false,name => 'ns_1@172.19.0.4',
                         name_domain => longnames,net_tickintensity => 4,
                         net_ticktime => 60,supervisor => net_sup_dynamic}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,60000},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:47:22.038Z,ns_1@172.19.0.4:cb_dist<0.2168.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.2327127311.3650355204.217326>,
                                  inet_tcp_dist,<0.2175.0>,
                                  #Ref<0.2327127311.3650355204.217328>}
[ns_server:debug,2025-05-15T18:47:22.038Z,ns_1@172.19.0.4:<0.2150.0>:dist_manager:configure_net_kernel:302]Set net_kernel vebosity to 10 -> 0
[ns_server:debug,2025-05-15T18:47:22.038Z,ns_1@172.19.0.4:cb_dist<0.2168.0>:cb_dist:info_msg:1098]cb_dist: Accepted new connection from <0.2171.0> DistCtrl #Port<0.81>: {con,
                                                                        #Ref<0.2327127311.3650355205.216815>,
                                                                        inet_tcp_dist,
                                                                        undefined,
                                                                        undefined}
[ns_server:debug,2025-05-15T18:47:22.038Z,ns_1@172.19.0.4:net_kernel<0.2170.0>:cb_dist:info_msg:1098]cb_dist: Accepting connection from <0.2171.0> using module inet_tcp_dist
[ns_server:info,2025-05-15T18:47:22.038Z,ns_1@172.19.0.4:<0.2150.0>:dist_manager:save_node:160]saving node name '"ns_1@172.19.0.4"' to "/opt/couchbase/var/lib/couchbase/couchbase-server.node"
[ns_server:debug,2025-05-15T18:47:22.038Z,ns_1@172.19.0.4:cb_dist<0.2168.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.2327127311.3650355205.216815>,
                                  inet_tcp_dist,<0.2177.0>,
                                  #Ref<0.2327127311.3650355207.215644>}
[error_logger:error,2025-05-15T18:47:22.038Z,ns_1@172.19.0.4:net_kernel<0.2170.0>:ale_error_logger_handler:do_log:101]
=========================ERROR REPORT=========================

** Cannot get connection id for node 'ns_1@172.19.0.4'

[error_logger:info,2025-05-15T18:47:22.038Z,ns_1@172.19.0.4:net_kernel<0.2170.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.2177.0>,shutdown}}
[ns_server:debug,2025-05-15T18:47:22.038Z,ns_1@172.19.0.4:cb_dist<0.2168.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.2327127311.3650355205.216815>,
                               inet_tcp_dist,<0.2177.0>,
                               #Ref<0.2327127311.3650355207.215644>}
[ns_server:debug,2025-05-15T18:47:22.039Z,ns_1@172.19.0.4:cb_dist<0.2168.0>:cb_dist:info_msg:1098]cb_dist: Accepted new connection from <0.2171.0> DistCtrl #Port<0.82>: {con,
                                                                        #Ref<0.2327127311.3650355205.216821>,
                                                                        inet_tcp_dist,
                                                                        undefined,
                                                                        undefined}
[ns_server:debug,2025-05-15T18:47:22.040Z,ns_1@172.19.0.4:net_kernel<0.2170.0>:cb_dist:info_msg:1098]cb_dist: Accepting connection from <0.2171.0> using module inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:22.040Z,ns_1@172.19.0.4:cb_dist<0.2168.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.2327127311.3650355205.216821>,
                                  inet_tcp_dist,<0.2179.0>,
                                  #Ref<0.2327127311.3650355201.220485>}
[ns_server:debug,2025-05-15T18:47:22.040Z,ns_1@172.19.0.4:cb_dist<0.2168.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.2327127311.3650355205.216821>,
                               inet_tcp_dist,<0.2179.0>,
                               #Ref<0.2327127311.3650355201.220485>}
[error_logger:info,2025-05-15T18:47:22.040Z,ns_1@172.19.0.4:net_kernel<0.2170.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.2179.0>,{recv_challenge_reply_failed,{error,closed}}}}
[ns_server:debug,2025-05-15T18:47:22.041Z,ns_1@172.19.0.4:<0.2150.0>:dist_manager:bringup:269]Attempted to save node name to disk: ok
[ns_server:debug,2025-05-15T18:47:22.041Z,ns_1@172.19.0.4:<0.2150.0>:dist_manager:wait_for_node:276]Waiting for connection to node 'babysitter_of_ns_1@cb.local' to be established
[error_logger:info,2025-05-15T18:47:22.041Z,ns_1@172.19.0.4:net_kernel<0.2170.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{connect,normal,'babysitter_of_ns_1@cb.local'}}
[ns_server:debug,2025-05-15T18:47:22.041Z,ns_1@172.19.0.4:net_kernel<0.2170.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'babysitter_of_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:22.041Z,ns_1@172.19.0.4:cb_dist<0.2168.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.2327127311.3650355205.216830>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:47:22.042Z,ns_1@172.19.0.4:cb_dist<0.2168.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.2327127311.3650355205.216830>,
                                  inet_tcp_dist,<0.2181.0>,
                                  #Ref<0.2327127311.3650355201.220490>}
[ns_server:debug,2025-05-15T18:47:22.043Z,ns_1@172.19.0.4:<0.2150.0>:dist_manager:wait_for_node:288]Observed node 'babysitter_of_ns_1@cb.local' to come up
[ns_server:info,2025-05-15T18:47:22.043Z,ns_1@172.19.0.4:<0.2150.0>:dist_manager:do_adjust_address:361]Re-setting cookie {{sanitized,<<"biSYTaQUFDVndTs/b+IcDmD2L0woktv9naRdv3GCK6A=">>},
                   'ns_1@172.19.0.4'}
[ns_server:info,2025-05-15T18:47:22.046Z,ns_1@172.19.0.4:<0.2150.0>:dist_manager:save_address_config:147]Deleting irrelevant ip file "/opt/couchbase/var/lib/couchbase/ip_start": {error,
                                                                          enoent}
[ns_server:info,2025-05-15T18:47:22.046Z,ns_1@172.19.0.4:<0.2150.0>:dist_manager:save_address_config:148]saving ip config to "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2025-05-15T18:47:22.047Z,ns_1@172.19.0.4:<0.2150.0>:dist_manager:save_address_config:151]Persisted the address successfully
[ns_server:debug,2025-05-15T18:47:22.048Z,ns_1@172.19.0.4:<0.2150.0>:dist_manager:rename_node_in_configs:442]Renaming node from 'ns_1@cb.local' to 'ns_1@172.19.0.4' in config
[ns_server:debug,2025-05-15T18:47:22.048Z,ns_1@172.19.0.4:chronicle_local<0.239.0>:chronicle_local:handle_rename:177]Handle renaming from 'ns_1@cb.local' to 'ns_1@172.19.0.4'
[chronicle:debug,2025-05-15T18:47:22.048Z,ns_1@172.19.0.4:chronicle_agent<0.252.0>:chronicle_agent:handle_reprovision:1140]Reprovisioning peer with config:
{log_entry,<<"5aab03dbecad06b50ffb474da59a00c9">>,
           {3,'ns_1@172.19.0.4'},
           9,
           {config,undefined,0,undefined,
                   #{'ns_1@172.19.0.4' =>
                         #{id => <<"1dd5991463d519e23729dda2c80c376b">>,
                           role => voter}},
                   undefined,
                   #{chronicle_config_rsm =>
                         {rsm_config,chronicle_config_rsm,[]},
                     kv => {rsm_config,chronicle_kv,[]}},
                   #{},undefined,
                   [{<<"5aab03dbecad06b50ffb474da59a00c9">>,0}]}}
[chronicle:info,2025-05-15T18:47:22.048Z,ns_1@172.19.0.4:chronicle_leader<0.256.0>:chronicle_leader:handle_reprovisioned:498]System reprovisioned.
[chronicle:info,2025-05-15T18:47:22.048Z,ns_1@172.19.0.4:<0.2184.0>:chronicle_leader:do_election_worker:892]Starting election.
History ID: <<"5aab03dbecad06b50ffb474da59a00c9">>
Log position: {{3,'ns_1@172.19.0.4'},9}
Peers: ['ns_1@172.19.0.4']
Required quorum: {majority,{set,1,16,16,8,80,48,
                                {[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],
                                 []},
                                {{[],[],[],[],
                                  ['ns_1@172.19.0.4'],
                                  [],[],[],[],[],[],[],[],[],[],[]}}}}
[chronicle:info,2025-05-15T18:47:22.049Z,ns_1@172.19.0.4:<0.2184.0>:chronicle_leader:do_election_worker:901]I'm the only peer, so I'm the leader.
[chronicle:info,2025-05-15T18:47:22.049Z,ns_1@172.19.0.4:chronicle_leader<0.256.0>:chronicle_leader:handle_election_result:691]Going to become a leader in term {4,'ns_1@172.19.0.4'} (history id <<"5aab03dbecad06b50ffb474da59a00c9">>)
[chronicle:debug,2025-05-15T18:47:22.049Z,ns_1@172.19.0.4:chronicle_agent<0.252.0>:chronicle_agent:handle_establish_term:1529]Accepted term {4,'ns_1@172.19.0.4'} in history <<"5aab03dbecad06b50ffb474da59a00c9">>
[chronicle:debug,2025-05-15T18:47:22.049Z,ns_1@172.19.0.4:chronicle_proposer<0.2185.0>:chronicle_proposer:establish_term_init:367]Going to establish term {4,'ns_1@172.19.0.4'} (history id <<"5aab03dbecad06b50ffb474da59a00c9">>).
Quorum peers: ['ns_1@172.19.0.4']
Metadata:
{metadata,'ns_1@172.19.0.4',<<"1dd5991463d519e23729dda2c80c376b">>,
          <<"5aab03dbecad06b50ffb474da59a00c9">>,
          {3,'ns_1@172.19.0.4'},
          {3,'ns_1@172.19.0.4'},
          9,9,
          {log_entry,<<"5aab03dbecad06b50ffb474da59a00c9">>,
                     {3,'ns_1@172.19.0.4'},
                     9,
                     {config,undefined,0,undefined,
                             #{'ns_1@172.19.0.4' =>
                                   #{id =>
                                         <<"1dd5991463d519e23729dda2c80c376b">>,
                                     role => voter}},
                             undefined,
                             #{chronicle_config_rsm =>
                                   {rsm_config,chronicle_config_rsm,[]},
                               kv => {rsm_config,chronicle_kv,[]}},
                             #{},undefined,
                             [{<<"5aab03dbecad06b50ffb474da59a00c9">>,0}]}},
          {log_entry,<<"5aab03dbecad06b50ffb474da59a00c9">>,
                     {3,'ns_1@172.19.0.4'},
                     9,
                     {config,undefined,0,undefined,
                             #{'ns_1@172.19.0.4' =>
                                   #{id =>
                                         <<"1dd5991463d519e23729dda2c80c376b">>,
                                     role => voter}},
                             undefined,
                             #{chronicle_config_rsm =>
                                   {rsm_config,chronicle_config_rsm,[]},
                               kv => {rsm_config,chronicle_kv,[]}},
                             #{},undefined,
                             [{<<"5aab03dbecad06b50ffb474da59a00c9">>,0}]}},
          undefined}
[chronicle:debug,2025-05-15T18:47:22.050Z,ns_1@172.19.0.4:chronicle_proposer<0.2185.0>:chronicle_proposer:establish_term_maybe_transition:686]Established term {4,'ns_1@172.19.0.4'} (history id <<"5aab03dbecad06b50ffb474da59a00c9">>) successfully.
Votes: ['ns_1@172.19.0.4']
[chronicle:debug,2025-05-15T18:47:22.050Z,ns_1@172.19.0.4:chronicle_proposer<0.2185.0>:chronicle_proposer:handle_state_enter:284]Starting recovery for term {4,'ns_1@172.19.0.4'} in history <<"5aab03dbecad06b50ffb474da59a00c9">>
[chronicle:debug,2025-05-15T18:47:22.050Z,ns_1@172.19.0.4:chronicle_proposer<0.2185.0>:chronicle_proposer:handle_state_enter:296]Proposer for term {4,'ns_1@172.19.0.4'} in history <<"5aab03dbecad06b50ffb474da59a00c9">> is ready. Committed seqno: 10
[chronicle:info,2025-05-15T18:47:22.050Z,ns_1@172.19.0.4:chronicle_leader<0.256.0>:chronicle_leader:handle_note_term_status:596]Term {4,'ns_1@172.19.0.4'} established.
[ns_server:debug,2025-05-15T18:47:22.073Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:handle_info:42]delete (key: {node,'ns_1@cb.local',services}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                    11})
[ns_server:debug,2025-05-15T18:47:22.073Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@172.19.0.4',services}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                      11})
[index,kv,n1ql]
[ns_server:debug,2025-05-15T18:47:22.073Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:handle_info:42]delete (key: {node,'ns_1@cb.local',membership}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                      11})
[ns_server:debug,2025-05-15T18:47:22.073Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@172.19.0.4',membership}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                        11})
active
[ns_server:debug,2025-05-15T18:47:22.073Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: server_groups, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,11})
[[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@172.19.0.4']}]]
[ns_server:debug,2025-05-15T18:47:22.073Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',prometheus_auth_info} -> {node,
                                                                   'ns_1@172.19.0.4',
                                                                   prometheus_auth_info}:
  {"@prometheus",
   {auth,[{<<"hash">>,
           {[{<<"hashes">>,
              {sanitized,<<"ocCjmLXobw413zJaq2v5hbk6F9JaoNKMHAEhwOXAF/k=">>}},
             {<<"algorithm">>,<<"argon2id">>},
             {<<"salt">>,<<"FlBs6+3flsH1DlD+dLEwjg==">>},
             {<<"parallelism">>,1},
             {<<"time">>,3},
             {<<"memory">>,524288}]}},
          {<<"scram-sha-512">>,
           {[{<<"salt">>,
              <<"F/Zqupd2IseMgxcuAf9fe4gpX0DBCFm1/fWg3iuaAuQdnTMBGYBMeqXKzW13cDp+Q9KGShsd3l23qMAKTVwiVA==">>},
             {<<"iterations">>,15000},
             {<<"hashes">>,
              {sanitized,<<"fNgWati8g2faSgrPjZUuj7XSR1xhSOtSh9nzY5vi8O4=">>}}]}},
          {<<"scram-sha-256">>,
           {[{<<"salt">>,<<"OKVHe2p1Uu/nuWHnr4hDmY1YmTEi+LTh2QSQmLrl6JU=">>},
             {<<"iterations">>,15000},
             {<<"hashes">>,
              {sanitized,<<"wJZdBxOuuYIylriwbbRV9JdvCNKKT1kZtZR+FOCJUQg=">>}}]}},
          {<<"scram-sha-1">>,
           {[{<<"salt">>,<<"FmlE6xOIeEZ8k+MADCxX6iijEW8=">>},
             {<<"iterations">>,15000},
             {<<"hashes">>,
              {sanitized,<<"gfRT+N7lHhRndbdyfaO+d/dOfRbaCQl3VgGgatuBZck=">>}}]}}]}} ->
  {"@prometheus",
   {auth,[{<<"hash">>,
           {[{<<"hashes">>,
              {sanitized,<<"ocCjmLXobw413zJaq2v5hbk6F9JaoNKMHAEhwOXAF/k=">>}},
             {<<"algorithm">>,<<"argon2id">>},
             {<<"salt">>,<<"FlBs6+3flsH1DlD+dLEwjg==">>},
             {<<"parallelism">>,1},
             {<<"time">>,3},
             {<<"memory">>,524288}]}},
          {<<"scram-sha-512">>,
           {[{<<"salt">>,
              <<"F/Zqupd2IseMgxcuAf9fe4gpX0DBCFm1/fWg3iuaAuQdnTMBGYBMeqXKzW13cDp+Q9KGShsd3l23qMAKTVwiVA==">>},
             {<<"iterations">>,15000},
             {<<"hashes">>,
              {sanitized,<<"fNgWati8g2faSgrPjZUuj7XSR1xhSOtSh9nzY5vi8O4=">>}}]}},
          {<<"scram-sha-256">>,
           {[{<<"salt">>,<<"OKVHe2p1Uu/nuWHnr4hDmY1YmTEi+LTh2QSQmLrl6JU=">>},
             {<<"iterations">>,15000},
             {<<"hashes">>,
              {sanitized,<<"wJZdBxOuuYIylriwbbRV9JdvCNKKT1kZtZR+FOCJUQg=">>}}]}},
          {<<"scram-sha-1">>,
           {[{<<"salt">>,<<"FmlE6xOIeEZ8k+MADCxX6iijEW8=">>},
             {<<"iterations">>,15000},
             {<<"hashes">>,
              {sanitized,<<"gfRT+N7lHhRndbdyfaO+d/dOfRbaCQl3VgGgatuBZck=">>}}]}}]}}
[ns_server:debug,2025-05-15T18:47:22.073Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: nodes_wanted, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,11})
['ns_1@172.19.0.4']
[ns_server:debug,2025-05-15T18:47:22.073Z,ns_1@172.19.0.4:mb_master<0.790.0>:mb_master:update_peers:543]List of peers has changed from ['ns_1@cb.local'] to ['ns_1@172.19.0.4']
[ns_server:debug,2025-05-15T18:47:22.075Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',eventing_dir} -> {node,
                                                           'ns_1@172.19.0.4',
                                                           eventing_dir}:
  "/opt/couchbase/var/lib/couchbase/data" ->
  "/opt/couchbase/var/lib/couchbase/data"
[ns_server:debug,2025-05-15T18:47:22.075Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_dirs} -> {node,
                                                        'ns_1@172.19.0.4',
                                                        cbas_dirs}:
  ["/opt/couchbase/var/lib/couchbase/data"] ->
  ["/opt/couchbase/var/lib/couchbase/data"]
[ns_server:debug,2025-05-15T18:47:22.075Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',client_cert} -> {node,
                                                          'ns_1@172.19.0.4',
                                                          client_cert}:
  [{subject,<<"CN=Couchbase Internal Client (70858842)">>},
   {not_after,63985747606},
   {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
   {load_timestamp,63914554006},
   {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQCuy0dFqLPrJ6xY21RJ5ttVPR78ssxxXSe9JdTj/IsUczkwyxfZfwK\npBszlGYuHO0Utnwq1K4ruMPYFYfpLacJSfsb3KWEmZwAoNuRpjvE24AsH3tvj7cB\nB5v49HJ0axvNAFm75GWDyUr7T7UwcTJaFIFtfy+J0Umt2TjFNY0DPpt1WEzkKFKx\nOdBs+7cNc35ZG4rwVu/EJ14OXhQMXkpbUpYJxS8jzkzSI+1Eb67kH4j5QhnR9H+w\nw2S9AgMBAAGjQjBAMA4GA1UdDwEB/wQEAwIBhjAPBgNVHRMBAf8EBTADAQH/MB0G\nA1UdDgQWBBS8ASq2Mkh0UsZe4fIvzs4mCSfTmjANBgkqhkiG9w0BAQsFAAOCAQEA\nB3q1mzdJc3VY17JDCaRPKY/Veni3oZ6zGJ9r9LnWKE7IO9AxBAKYGMELHMf9httQ\nmRZrXiGQi/tYXLlFFk7IcIID6iOOZLLagDhCQJPi52dGhBB4a2EnBvGF9OJI4zW9\nJw04HNR5cF1cxWIrRdGJAX/kbs/M9jz0STlXziVNwiAYeGIxkjq/fwKk6weai8iI\nwQVu6GsbtSBL8f4FRCk2vK6a1MD54BQDasRShdi3k/DJAEtY92muu13M0jc48mCM\ndEVeXxM8rIrG6LavziTwtO+jkFyWjlRyNbSr6il+neSWJ7ZspTo+ZYFy55CAh5ws\nwk6Ljo2wlQgfGflJbKcxAw==\n-----END CERTIFICATE-----\n">>},
   {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDWTCCAkGgAwIBAgIIGD/Hv7XHejcwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ2NDZaFw0y\nNzA4MTcxODQ2NDZaMC8xLTArBgNVBAMTJENvdWNoYmFzZSBJbnRlcm5hbCBDbGll\nbnQgKDcwODU4ODQyKTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMSK\nvXvBcS/ChHK2Kb84PvDZyFlVdyCnMwkD8aug5L/C9zj5626M2/KO3H+iam8QErOa\nUj7TQ/AEl0o2GZmp+BCHRIVyepCtL3XAlmVirkKWPm4aTAFz73FUNSBZowawvPbx\nCs6M9yMRlEFNOvyAuABjUeWb9F6cwouclY443ocubDOBQdSowkOJnEKcYiVuU+3z\nKG+UKvLQ//yxIpbhDQnfOxSfj5X/iGIbZUsvzhZcBe5yqR0xPJHe42lmK0R1uLny\nYrcdKC+5YKGwJC9CW1K65HupPzfbRl2z2v6VL/BcMNl1bbj33VaTQpTAxuw3w72l\nnnJjCMi99j206o0QxdkCAwEAAaOBgzCBgDAOBgNVHQ8BAf8EBAMCBaAwEwYDVR0l\nBAwwCgYIKwYBBQUHAwIwDAYDVR0TAQH/BAIwADAfBgNVHSMEGDAWgBS8ASq2Mkh0\nUsZe4fIvzs4mCSfTmjAqBgNVHREEIzAhgR9pbnRlcm5hbEBpbnRlcm5hbC5jb3Vj\naGJhc2UuY29tMA0GCSqGSIb3DQEBCwUAA4IBAQCJjgUwMzywVqyyIj9UHO+Vy25y\nwoGQcww/QnRwVqwcnTS5bAC2KlQi4kzgdBkrWsOy0zM+QR6/B3FeiLEkq1PSDTDT\nu/jvj/W4nqu4nILGus6jmCUdemStiEJwpz6NtTqwcFGJfgj2OGXBj4FzUxuDvsF4\ngUp6F+UVjPyHkNBolGN/rNG3oaXTf1vwUW5mjZSytXgNp6iTJSm6u0Gmbr771DH/\n1Ag0LlKAQwYoSI2FuaLL8cAePV+Y2qdbryfak7wLcldVgpBxwKZQj1YOXyCiylC8\nD8xCWsKrBtLL13zDKpzNlZEudRRk4fj/u8zDdlxXnXaKvo1FUoC432WbKy/0\n-----END CERTIFICATE-----\n">>},
   {pkey_passphrase_settings,[]},
   {certs_epoch,0},
   {type,generated},
   {name,"@internal"}] ->
  [{subject,<<"CN=Couchbase Internal Client (70858842)">>},
   {not_after,63985747606},
   {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
   {load_timestamp,63914554006},
   {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQCuy0dFqLPrJ6xY21RJ5ttVPR78ssxxXSe9JdTj/IsUczkwyxfZfwK\npBszlGYuHO0Utnwq1K4ruMPYFYfpLacJSfsb3KWEmZwAoNuRpjvE24AsH3tvj7cB\nB5v49HJ0axvNAFm75GWDyUr7T7UwcTJaFIFtfy+J0Umt2TjFNY0DPpt1WEzkKFKx\nOdBs+7cNc35ZG4rwVu/EJ14OXhQMXkpbUpYJxS8jzkzSI+1Eb67kH4j5QhnR9H+w\nw2S9AgMBAAGjQjBAMA4GA1UdDwEB/wQEAwIBhjAPBgNVHRMBAf8EBTADAQH/MB0G\nA1UdDgQWBBS8ASq2Mkh0UsZe4fIvzs4mCSfTmjANBgkqhkiG9w0BAQsFAAOCAQEA\nB3q1mzdJc3VY17JDCaRPKY/Veni3oZ6zGJ9r9LnWKE7IO9AxBAKYGMELHMf9httQ\nmRZrXiGQi/tYXLlFFk7IcIID6iOOZLLagDhCQJPi52dGhBB4a2EnBvGF9OJI4zW9\nJw04HNR5cF1cxWIrRdGJAX/kbs/M9jz0STlXziVNwiAYeGIxkjq/fwKk6weai8iI\nwQVu6GsbtSBL8f4FRCk2vK6a1MD54BQDasRShdi3k/DJAEtY92muu13M0jc48mCM\ndEVeXxM8rIrG6LavziTwtO+jkFyWjlRyNbSr6il+neSWJ7ZspTo+ZYFy55CAh5ws\nwk6Ljo2wlQgfGflJbKcxAw==\n-----END CERTIFICATE-----\n">>},
   {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDWTCCAkGgAwIBAgIIGD/Hv7XHejcwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ2NDZaFw0y\nNzA4MTcxODQ2NDZaMC8xLTArBgNVBAMTJENvdWNoYmFzZSBJbnRlcm5hbCBDbGll\nbnQgKDcwODU4ODQyKTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMSK\nvXvBcS/ChHK2Kb84PvDZyFlVdyCnMwkD8aug5L/C9zj5626M2/KO3H+iam8QErOa\nUj7TQ/AEl0o2GZmp+BCHRIVyepCtL3XAlmVirkKWPm4aTAFz73FUNSBZowawvPbx\nCs6M9yMRlEFNOvyAuABjUeWb9F6cwouclY443ocubDOBQdSowkOJnEKcYiVuU+3z\nKG+UKvLQ//yxIpbhDQnfOxSfj5X/iGIbZUsvzhZcBe5yqR0xPJHe42lmK0R1uLny\nYrcdKC+5YKGwJC9CW1K65HupPzfbRl2z2v6VL/BcMNl1bbj33VaTQpTAxuw3w72l\nnnJjCMi99j206o0QxdkCAwEAAaOBgzCBgDAOBgNVHQ8BAf8EBAMCBaAwEwYDVR0l\nBAwwCgYIKwYBBQUHAwIwDAYDVR0TAQH/BAIwADAfBgNVHSMEGDAWgBS8ASq2Mkh0\nUsZe4fIvzs4mCSfTmjAqBgNVHREEIzAhgR9pbnRlcm5hbEBpbnRlcm5hbC5jb3Vj\naGJhc2UuY29tMA0GCSqGSIb3DQEBCwUAA4IBAQCJjgUwMzywVqyyIj9UHO+Vy25y\nwoGQcww/QnRwVqwcnTS5bAC2KlQi4kzgdBkrWsOy0zM+QR6/B3FeiLEkq1PSDTDT\nu/jvj/W4nqu4nILGus6jmCUdemStiEJwpz6NtTqwcFGJfgj2OGXBj4FzUxuDvsF4\ngUp6F+UVjPyHkNBolGN/rNG3oaXTf1vwUW5mjZSytXgNp6iTJSm6u0Gmbr771DH/\n1Ag0LlKAQwYoSI2FuaLL8cAePV+Y2qdbryfak7wLcldVgpBxwKZQj1YOXyCiylC8\nD8xCWsKrBtLL13zDKpzNlZEudRRk4fj/u8zDdlxXnXaKvo1FUoC432WbKy/0\n-----END CERTIFICATE-----\n">>},
   {pkey_passphrase_settings,[]},
   {certs_epoch,0},
   {type,generated},
   {name,"@internal"}]
[ns_server:debug,2025-05-15T18:47:22.076Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',node_cert} -> {node,
                                                        'ns_1@172.19.0.4',
                                                        node_cert}:
  [{subject,<<"CN=Couchbase Server Node (127.0.0.1)">>},
   {not_after,63985747606},
   {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
   {load_timestamp,63914554006},
   {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQCuy0dFqLPrJ6xY21RJ5ttVPR78ssxxXSe9JdTj/IsUczkwyxfZfwK\npBszlGYuHO0Utnwq1K4ruMPYFYfpLacJSfsb3KWEmZwAoNuRpjvE24AsH3tvj7cB\nB5v49HJ0axvNAFm75GWDyUr7T7UwcTJaFIFtfy+J0Umt2TjFNY0DPpt1WEzkKFKx\nOdBs+7cNc35ZG4rwVu/EJ14OXhQMXkpbUpYJxS8jzkzSI+1Eb67kH4j5QhnR9H+w\nw2S9AgMBAAGjQjBAMA4GA1UdDwEB/wQEAwIBhjAPBgNVHRMBAf8EBTADAQH/MB0G\nA1UdDgQWBBS8ASq2Mkh0UsZe4fIvzs4mCSfTmjANBgkqhkiG9w0BAQsFAAOCAQEA\nB3q1mzdJc3VY17JDCaRPKY/Veni3oZ6zGJ9r9LnWKE7IO9AxBAKYGMELHMf9httQ\nmRZrXiGQi/tYXLlFFk7IcIID6iOOZLLagDhCQJPi52dGhBB4a2EnBvGF9OJI4zW9\nJw04HNR5cF1cxWIrRdGJAX/kbs/M9jz0STlXziVNwiAYeGIxkjq/fwKk6weai8iI\nwQVu6GsbtSBL8f4FRCk2vK6a1MD54BQDasRShdi3k/DJAEtY92muu13M0jc48mCM\ndEVeXxM8rIrG6LavziTwtO+jkFyWjlRyNbSr6il+neSWJ7ZspTo+ZYFy55CAh5ws\nwk6Ljo2wlQgfGflJbKcxAw==\n-----END CERTIFICATE-----\n">>},
   {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDOTCCAiGgAwIBAgIIGD/Hv6I/B/UwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ2NDZaFw0y\nNzA4MTcxODQ2NDZaMCwxKjAoBgNVBAMTIUNvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nMTI3LjAuMC4xKTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAOcGnD0i\nK4yErfsC0OEceWFxJLbXv/UYXZqt90WDswgyrkIIRSbthJzY/QEWmjj0Oh0CQmOp\nSoIV712ZEhM0O/ag5SXfJ57AnBrrmHwwdjn3zAgduhYRDBRJ7OeCFZTSt4OtBhtO\nNMS9cD6TP1VKoU7LptAIGSwuW4i6bCOfUvh/htfi8ifOdsSCghMPlmG8BFkEVIqb\ne/Q8kbzBnslCfYHVrgEleX5c/IQABhcVI7eM9SsqRuewdh3/cJLX/coxFZmYldR6\n79b5YGr0bdLW4UNEPf02azM9uCz8gB5o51bvPfnsSLimfswMkn3XfMVBsG5jPA07\nGY81WBaiqRa6Q5cCAwEAAaNnMGUwDgYDVR0PAQH/BAQDAgWgMBMGA1UdJQQMMAoG\nCCsGAQUFBwMBMAwGA1UdEwEB/wQCMAAwHwYDVR0jBBgwFoAUvAEqtjJIdFLGXuHy\nL87OJgkn05owDwYDVR0RBAgwBocEfwAAATANBgkqhkiG9w0BAQsFAAOCAQEAO6un\n698NEmWKB+Eew8BIHmh/eXPE1nxBV2AHfGgYizBBR9Y1wNy0NWSc8O+PQNRwKk9x\ntQNjq33W/VF0wssWmD0Ln88YWP9dTwdAoNViWWA3KJxdoWxJrQKCsb5bBh1x3POP\nGpA8hronIjY1qrkjNaebwhFqhd+wqq9tlxdy9UGdbXko+cMR+lwrQ4ygtkafWfyc\nK7vui3eK06IGTpeiehQXD+OfL6jiXqRdGqnyVgHVlZJkGN2vqCMDjsiAtL3+nUaC\n/bxnDIovDSVzDjtq5rgvn6IKJ8W18eC9PbufhRghwwgWVFdu9ZfQBbKTVKL4r0sK\n9alPaSY+XVZrqm6ujA==\n-----END CERTIFICATE-----\n">>},
   {pkey_passphrase_settings,[]},
   {certs_epoch,0},
   {type,generated},
   {hostname,"127.0.0.1"}] ->
  [{subject,<<"CN=Couchbase Server Node (127.0.0.1)">>},
   {not_after,63985747606},
   {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
   {load_timestamp,63914554006},
   {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQCuy0dFqLPrJ6xY21RJ5ttVPR78ssxxXSe9JdTj/IsUczkwyxfZfwK\npBszlGYuHO0Utnwq1K4ruMPYFYfpLacJSfsb3KWEmZwAoNuRpjvE24AsH3tvj7cB\nB5v49HJ0axvNAFm75GWDyUr7T7UwcTJaFIFtfy+J0Umt2TjFNY0DPpt1WEzkKFKx\nOdBs+7cNc35ZG4rwVu/EJ14OXhQMXkpbUpYJxS8jzkzSI+1Eb67kH4j5QhnR9H+w\nw2S9AgMBAAGjQjBAMA4GA1UdDwEB/wQEAwIBhjAPBgNVHRMBAf8EBTADAQH/MB0G\nA1UdDgQWBBS8ASq2Mkh0UsZe4fIvzs4mCSfTmjANBgkqhkiG9w0BAQsFAAOCAQEA\nB3q1mzdJc3VY17JDCaRPKY/Veni3oZ6zGJ9r9LnWKE7IO9AxBAKYGMELHMf9httQ\nmRZrXiGQi/tYXLlFFk7IcIID6iOOZLLagDhCQJPi52dGhBB4a2EnBvGF9OJI4zW9\nJw04HNR5cF1cxWIrRdGJAX/kbs/M9jz0STlXziVNwiAYeGIxkjq/fwKk6weai8iI\nwQVu6GsbtSBL8f4FRCk2vK6a1MD54BQDasRShdi3k/DJAEtY92muu13M0jc48mCM\ndEVeXxM8rIrG6LavziTwtO+jkFyWjlRyNbSr6il+neSWJ7ZspTo+ZYFy55CAh5ws\nwk6Ljo2wlQgfGflJbKcxAw==\n-----END CERTIFICATE-----\n">>},
   {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDOTCCAiGgAwIBAgIIGD/Hv6I/B/UwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ2NDZaFw0y\nNzA4MTcxODQ2NDZaMCwxKjAoBgNVBAMTIUNvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nMTI3LjAuMC4xKTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAOcGnD0i\nK4yErfsC0OEceWFxJLbXv/UYXZqt90WDswgyrkIIRSbthJzY/QEWmjj0Oh0CQmOp\nSoIV712ZEhM0O/ag5SXfJ57AnBrrmHwwdjn3zAgduhYRDBRJ7OeCFZTSt4OtBhtO\nNMS9cD6TP1VKoU7LptAIGSwuW4i6bCOfUvh/htfi8ifOdsSCghMPlmG8BFkEVIqb\ne/Q8kbzBnslCfYHVrgEleX5c/IQABhcVI7eM9SsqRuewdh3/cJLX/coxFZmYldR6\n79b5YGr0bdLW4UNEPf02azM9uCz8gB5o51bvPfnsSLimfswMkn3XfMVBsG5jPA07\nGY81WBaiqRa6Q5cCAwEAAaNnMGUwDgYDVR0PAQH/BAQDAgWgMBMGA1UdJQQMMAoG\nCCsGAQUFBwMBMAwGA1UdEwEB/wQCMAAwHwYDVR0jBBgwFoAUvAEqtjJIdFLGXuHy\nL87OJgkn05owDwYDVR0RBAgwBocEfwAAATANBgkqhkiG9w0BAQsFAAOCAQEAO6un\n698NEmWKB+Eew8BIHmh/eXPE1nxBV2AHfGgYizBBR9Y1wNy0NWSc8O+PQNRwKk9x\ntQNjq33W/VF0wssWmD0Ln88YWP9dTwdAoNViWWA3KJxdoWxJrQKCsb5bBh1x3POP\nGpA8hronIjY1qrkjNaebwhFqhd+wqq9tlxdy9UGdbXko+cMR+lwrQ4ygtkafWfyc\nK7vui3eK06IGTpeiehQXD+OfL6jiXqRdGqnyVgHVlZJkGN2vqCMDjsiAtL3+nUaC\n/bxnDIovDSVzDjtq5rgvn6IKJ8W18eC9PbufhRghwwgWVFdu9ZfQBbKTVKL4r0sK\n9alPaSY+XVZrqm6ujA==\n-----END CERTIFICATE-----\n">>},
   {pkey_passphrase_settings,[]},
   {certs_epoch,0},
   {type,generated},
   {hostname,"127.0.0.1"}]
[ns_server:debug,2025-05-15T18:47:22.077Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',n2n_client_cert_auth} -> {node,
                                                                   'ns_1@172.19.0.4',
                                                                   n2n_client_cert_auth}:
  false ->
  false
[ns_server:debug,2025-05-15T18:47:22.077Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',erl_external_listeners} -> {node,
                                                                     'ns_1@172.19.0.4',
                                                                     erl_external_listeners}:
  [{inet,false}] ->
  [{inet,false}]
[ns_server:debug,2025-05-15T18:47:22.077Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',node_encryption} -> {node,
                                                              'ns_1@172.19.0.4',
                                                              node_encryption}:
  false ->
  false
[ns_server:debug,2025-05-15T18:47:22.077Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',address_family} -> {node,
                                                             'ns_1@172.19.0.4',
                                                             address_family}:
  inet ->
  inet
[ns_server:debug,2025-05-15T18:47:22.077Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf quorum_nodes -> quorum_nodes:
  ['ns_1@cb.local'] ->
  ['ns_1@172.19.0.4']
[ns_server:debug,2025-05-15T18:47:22.077Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',audit} -> {node,'ns_1@172.19.0.4',
                                                    audit}:
  [] ->
  []
[ns_server:debug,2025-05-15T18:47:22.077Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',backup_grpc_port} -> {node,
                                                               'ns_1@172.19.0.4',
                                                               backup_grpc_port}:
  9124 ->
  9124
[ns_server:debug,2025-05-15T18:47:22.077Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',backup_http_port} -> {node,
                                                               'ns_1@172.19.0.4',
                                                               backup_http_port}:
  8097 ->
  8097
[ns_server:debug,2025-05-15T18:47:22.077Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',backup_https_port} -> {node,
                                                                'ns_1@172.19.0.4',
                                                                backup_https_port}:
  18097 ->
  18097
[ns_server:debug,2025-05-15T18:47:22.077Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',capi_port} -> {node,
                                                        'ns_1@172.19.0.4',
                                                        capi_port}:
  8092 ->
  8092
[ns_server:debug,2025-05-15T18:47:22.077Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_admin_port} -> {node,
                                                              'ns_1@172.19.0.4',
                                                              cbas_admin_port}:
  9110 ->
  9110
[ns_server:debug,2025-05-15T18:47:22.077Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_cc_client_port} -> {node,
                                                                  'ns_1@172.19.0.4',
                                                                  cbas_cc_client_port}:
  9113 ->
  9113
[ns_server:debug,2025-05-15T18:47:22.077Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_cc_cluster_port} -> {node,
                                                                   'ns_1@172.19.0.4',
                                                                   cbas_cc_cluster_port}:
  9112 ->
  9112
[ns_server:debug,2025-05-15T18:47:22.077Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_cc_http_port} -> {node,
                                                                'ns_1@172.19.0.4',
                                                                cbas_cc_http_port}:
  9111 ->
  9111
[ns_server:debug,2025-05-15T18:47:22.077Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_cluster_port} -> {node,
                                                                'ns_1@172.19.0.4',
                                                                cbas_cluster_port}:
  9115 ->
  9115
[ns_server:debug,2025-05-15T18:47:22.077Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_console_port} -> {node,
                                                                'ns_1@172.19.0.4',
                                                                cbas_console_port}:
  9114 ->
  9114
[ns_server:debug,2025-05-15T18:47:22.078Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_data_port} -> {node,
                                                             'ns_1@172.19.0.4',
                                                             cbas_data_port}:
  9116 ->
  9116
[ns_server:debug,2025-05-15T18:47:22.078Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_debug_port} -> {node,
                                                              'ns_1@172.19.0.4',
                                                              cbas_debug_port}:
  -1 ->
  -1
[ns_server:debug,2025-05-15T18:47:22.078Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_http_port} -> {node,
                                                             'ns_1@172.19.0.4',
                                                             cbas_http_port}:
  8095 ->
  8095
[ns_server:debug,2025-05-15T18:47:22.078Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_messaging_port} -> {node,
                                                                  'ns_1@172.19.0.4',
                                                                  cbas_messaging_port}:
  9118 ->
  9118
[ns_server:debug,2025-05-15T18:47:22.078Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_metadata_callback_port} -> {node,
                                                                          'ns_1@172.19.0.4',
                                                                          cbas_metadata_callback_port}:
  9119 ->
  9119
[ns_server:debug,2025-05-15T18:47:22.078Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_metadata_port} -> {node,
                                                                 'ns_1@172.19.0.4',
                                                                 cbas_metadata_port}:
  9121 ->
  9121
[ns_server:debug,2025-05-15T18:47:22.078Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_parent_port} -> {node,
                                                               'ns_1@172.19.0.4',
                                                               cbas_parent_port}:
  9122 ->
  9122
[ns_server:debug,2025-05-15T18:47:22.078Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_replication_port} -> {node,
                                                                    'ns_1@172.19.0.4',
                                                                    cbas_replication_port}:
  9120 ->
  9120
[ns_server:debug,2025-05-15T18:47:22.078Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_result_port} -> {node,
                                                               'ns_1@172.19.0.4',
                                                               cbas_result_port}:
  9117 ->
  9117
[ns_server:debug,2025-05-15T18:47:22.078Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',cbas_ssl_port} -> {node,
                                                            'ns_1@172.19.0.4',
                                                            cbas_ssl_port}:
  18095 ->
  18095
[ns_server:debug,2025-05-15T18:47:22.078Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',compaction_daemon} -> {node,
                                                                'ns_1@172.19.0.4',
                                                                compaction_daemon}:
  [{check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}] ->
  [{check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}]
[ns_server:debug,2025-05-15T18:47:22.078Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',config_version} -> {node,
                                                             'ns_1@172.19.0.4',
                                                             config_version}:
  {7,6} ->
  {7,6}
[ns_server:debug,2025-05-15T18:47:22.079Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',database_dir} -> {node,
                                                           'ns_1@172.19.0.4',
                                                           database_dir}:
  "/opt/couchbase/var/lib/couchbase/data" ->
  "/opt/couchbase/var/lib/couchbase/data"
[ns_server:debug,2025-05-15T18:47:22.079Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',event_log} -> {node,
                                                        'ns_1@172.19.0.4',
                                                        event_log}:
  [{filename,"/opt/couchbase/var/lib/couchbase/event_log"}] ->
  [{filename,"/opt/couchbase/var/lib/couchbase/event_log"}]
[ns_server:debug,2025-05-15T18:47:22.079Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',eventing_debug_port} -> {node,
                                                                  'ns_1@172.19.0.4',
                                                                  eventing_debug_port}:
  9140 ->
  9140
[ns_server:debug,2025-05-15T18:47:22.079Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',eventing_http_port} -> {node,
                                                                 'ns_1@172.19.0.4',
                                                                 eventing_http_port}:
  8096 ->
  8096
[ns_server:debug,2025-05-15T18:47:22.080Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',eventing_https_port} -> {node,
                                                                  'ns_1@172.19.0.4',
                                                                  eventing_https_port}:
  18096 ->
  18096
[ns_server:debug,2025-05-15T18:47:22.080Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',fts_grpc_port} -> {node,
                                                            'ns_1@172.19.0.4',
                                                            fts_grpc_port}:
  9130 ->
  9130
[ns_server:debug,2025-05-15T18:47:22.080Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',fts_grpc_ssl_port} -> {node,
                                                                'ns_1@172.19.0.4',
                                                                fts_grpc_ssl_port}:
  19130 ->
  19130
[ns_server:debug,2025-05-15T18:47:22.080Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',fts_http_port} -> {node,
                                                            'ns_1@172.19.0.4',
                                                            fts_http_port}:
  8094 ->
  8094
[ns_server:debug,2025-05-15T18:47:22.080Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',fts_ssl_port} -> {node,
                                                           'ns_1@172.19.0.4',
                                                           fts_ssl_port}:
  18094 ->
  18094
[ns_server:debug,2025-05-15T18:47:22.081Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',index_dir} -> {node,
                                                        'ns_1@172.19.0.4',
                                                        index_dir}:
  "/opt/couchbase/var/lib/couchbase/data" ->
  "/opt/couchbase/var/lib/couchbase/data"
[ns_server:debug,2025-05-15T18:47:22.081Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',indexer_admin_port} -> {node,
                                                                 'ns_1@172.19.0.4',
                                                                 indexer_admin_port}:
  9100 ->
  9100
[ns_server:debug,2025-05-15T18:47:22.081Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',indexer_http_port} -> {node,
                                                                'ns_1@172.19.0.4',
                                                                indexer_http_port}:
  9102 ->
  9102
[ns_server:debug,2025-05-15T18:47:22.081Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',indexer_https_port} -> {node,
                                                                 'ns_1@172.19.0.4',
                                                                 indexer_https_port}:
  19102 ->
  19102
[ns_server:debug,2025-05-15T18:47:22.081Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',indexer_scan_port} -> {node,
                                                                'ns_1@172.19.0.4',
                                                                indexer_scan_port}:
  9101 ->
  9101
[ns_server:debug,2025-05-15T18:47:22.081Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',indexer_stcatchup_port} -> {node,
                                                                     'ns_1@172.19.0.4',
                                                                     indexer_stcatchup_port}:
  9104 ->
  9104
[ns_server:debug,2025-05-15T18:47:22.081Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',indexer_stinit_port} -> {node,
                                                                  'ns_1@172.19.0.4',
                                                                  indexer_stinit_port}:
  9103 ->
  9103
[ns_server:debug,2025-05-15T18:47:22.081Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',indexer_stmaint_port} -> {node,
                                                                   'ns_1@172.19.0.4',
                                                                   indexer_stmaint_port}:
  9105 ->
  9105
[ns_server:debug,2025-05-15T18:47:22.081Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',is_enterprise} -> {node,
                                                            'ns_1@172.19.0.4',
                                                            is_enterprise}:
  true ->
  true
[ns_server:debug,2025-05-15T18:47:22.081Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',isasl} -> {node,'ns_1@172.19.0.4',
                                                    isasl}:
  [{path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}] ->
  [{path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]
[ns_server:debug,2025-05-15T18:47:22.082Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',memcached} -> {node,
                                                        'ns_1@172.19.0.4',
                                                        memcached}:
  [{port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,11206},
   {ssl_port,11207},
   {admin_user,"@ns_server"},
   {other_users,["@cbq-engine","@projector","@goxdcr","@index","@fts",
                 "@eventing","@cbas","@backup"]},
   {admin_pass,"*****"},
   {engines,[{membase,[{engine,"/opt/couchbase/lib/memcached/ep.so"},
                       {static_config_string,"failpartialwarmup=false"}]},
             {memcached,[{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
                         {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_rotation_period,39003}] ->
  [{port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,11206},
   {ssl_port,11207},
   {admin_user,"@ns_server"},
   {other_users,["@cbq-engine","@projector","@goxdcr","@index","@fts",
                 "@eventing","@cbas","@backup"]},
   {admin_pass,"*****"},
   {engines,[{membase,[{engine,"/opt/couchbase/lib/memcached/ep.so"},
                       {static_config_string,"failpartialwarmup=false"}]},
             {memcached,[{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
                         {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_rotation_period,39003}]
[ns_server:debug,2025-05-15T18:47:22.082Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',memcached_config} -> {node,
                                                               'ns_1@172.19.0.4',
                                                               memcached_config}:
  {[{interfaces,{memcached_config_mgr,get_interfaces,[]}},
    {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
    {connection_idle_time,connection_idle_time},
    {privilege_debug,privilege_debug},
    {breakpad,
        {[{enabled,breakpad_enabled},
          {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
    {deployment_model,{memcached_config_mgr,get_config_profile,[]}},
    {verbosity,verbosity},
    {audit_file,{"~s",[audit_file]}},
    {rbac_file,{"~s",[rbac_file]}},
    {dedupe_nmvb_maps,dedupe_nmvb_maps},
    {tracing_enabled,tracing_enabled},
    {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
    {xattr_enabled,true},
    {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
    {collections_enabled,true},
    {max_connections,max_connections},
    {system_connections,system_connections},
    {num_reader_threads,num_reader_threads},
    {num_writer_threads,num_writer_threads},
    {num_auxio_threads,num_auxio_threads},
    {num_nonio_threads,num_nonio_threads},
    {num_storage_threads,num_storage_threads},
    {logger,
        {[{filename,{"~s/~s",[log_path,log_prefix]}},
          {cyclesize,log_cyclesize}]}},
    {external_auth_service,
        {memcached_config_mgr,get_external_auth_service,[]}},
    {active_external_users_push_interval,
        {memcached_config_mgr,get_external_users_push_interval,[]}},
    {prometheus,{memcached_config_mgr,prometheus_cfg,[]}},
    {sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
    {ssl_sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
    {tcp_keepalive_idle,tcp_keepalive_idle},
    {tcp_keepalive_interval,tcp_keepalive_interval},
    {tcp_keepalive_probes,tcp_keepalive_probes},
    {tcp_user_timeout,tcp_user_timeout},
    {always_collect_trace_info,always_collect_trace_info},
    {connection_limit_mode,connection_limit_mode},
    {free_connection_pool_size,free_connection_pool_size},
    {max_client_connection_details,max_client_connection_details}]} ->
  {[{interfaces,{memcached_config_mgr,get_interfaces,[]}},
    {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
    {connection_idle_time,connection_idle_time},
    {privilege_debug,privilege_debug},
    {breakpad,
        {[{enabled,breakpad_enabled},
          {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
    {deployment_model,{memcached_config_mgr,get_config_profile,[]}},
    {verbosity,verbosity},
    {audit_file,{"~s",[audit_file]}},
    {rbac_file,{"~s",[rbac_file]}},
    {dedupe_nmvb_maps,dedupe_nmvb_maps},
    {tracing_enabled,tracing_enabled},
    {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
    {xattr_enabled,true},
    {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
    {collections_enabled,true},
    {max_connections,max_connections},
    {system_connections,system_connections},
    {num_reader_threads,num_reader_threads},
    {num_writer_threads,num_writer_threads},
    {num_auxio_threads,num_auxio_threads},
    {num_nonio_threads,num_nonio_threads},
    {num_storage_threads,num_storage_threads},
    {logger,
        {[{filename,{"~s/~s",[log_path,log_prefix]}},
          {cyclesize,log_cyclesize}]}},
    {external_auth_service,
        {memcached_config_mgr,get_external_auth_service,[]}},
    {active_external_users_push_interval,
        {memcached_config_mgr,get_external_users_push_interval,[]}},
    {prometheus,{memcached_config_mgr,prometheus_cfg,[]}},
    {sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
    {ssl_sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
    {tcp_keepalive_idle,tcp_keepalive_idle},
    {tcp_keepalive_interval,tcp_keepalive_interval},
    {tcp_keepalive_probes,tcp_keepalive_probes},
    {tcp_user_timeout,tcp_user_timeout},
    {always_collect_trace_info,always_collect_trace_info},
    {connection_limit_mode,connection_limit_mode},
    {free_connection_pool_size,free_connection_pool_size},
    {max_client_connection_details,max_client_connection_details}]}
[ns_server:debug,2025-05-15T18:47:22.083Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',memcached_dedicated_ssl_port} -> {node,
                                                                           'ns_1@172.19.0.4',
                                                                           memcached_dedicated_ssl_port}:
  11206 ->
  11206
[ns_server:debug,2025-05-15T18:47:22.083Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',memcached_defaults} -> {node,
                                                                 'ns_1@172.19.0.4',
                                                                 memcached_defaults}:
  [{max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {je_malloc_conf,undefined},
   {tracing_enabled,true},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>},
   {num_auxio_threads,<<"default">>},
   {num_nonio_threads,<<"default">>},
   {num_storage_threads,<<"default">>},
   {tcp_keepalive_idle,360},
   {tcp_keepalive_interval,10},
   {tcp_keepalive_probes,3},
   {tcp_user_timeout,30},
   {always_collect_trace_info,true},
   {connection_limit_mode,<<"disconnect">>},
   {free_connection_pool_size,0},
   {max_client_connection_details,0}] ->
  [{max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {je_malloc_conf,undefined},
   {tracing_enabled,true},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>},
   {num_auxio_threads,<<"default">>},
   {num_nonio_threads,<<"default">>},
   {num_storage_threads,<<"default">>},
   {tcp_keepalive_idle,360},
   {tcp_keepalive_interval,10},
   {tcp_keepalive_probes,3},
   {tcp_user_timeout,30},
   {always_collect_trace_info,true},
   {connection_limit_mode,<<"disconnect">>},
   {free_connection_pool_size,0},
   {max_client_connection_details,0}]
[ns_server:debug,2025-05-15T18:47:22.083Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',memcached_prometheus} -> {node,
                                                                   'ns_1@172.19.0.4',
                                                                   memcached_prometheus}:
  11280 ->
  11280
[ns_server:debug,2025-05-15T18:47:22.083Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',ns_log} -> {node,'ns_1@172.19.0.4',
                                                     ns_log}:
  [{filename,"/opt/couchbase/var/lib/couchbase/ns_log"}] ->
  [{filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]
[ns_server:debug,2025-05-15T18:47:22.083Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',port_servers} -> {node,
                                                           'ns_1@172.19.0.4',
                                                           port_servers}:
  [] ->
  []
[ns_server:debug,2025-05-15T18:47:22.083Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',projector_port} -> {node,
                                                             'ns_1@172.19.0.4',
                                                             projector_port}:
  9999 ->
  9999
[ns_server:debug,2025-05-15T18:47:22.084Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',projector_ssl_port} -> {node,
                                                                 'ns_1@172.19.0.4',
                                                                 projector_ssl_port}:
  9999 ->
  9999
[ns_server:debug,2025-05-15T18:47:22.084Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',prometheus_http_port} -> {node,
                                                                   'ns_1@172.19.0.4',
                                                                   prometheus_http_port}:
  9123 ->
  9123
[ns_server:debug,2025-05-15T18:47:22.084Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',query_port} -> {node,
                                                         'ns_1@172.19.0.4',
                                                         query_port}:
  8093 ->
  8093
[ns_server:debug,2025-05-15T18:47:22.084Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',rest} -> {node,'ns_1@172.19.0.4',
                                                   rest}:
  [{port,8091},{port_meta,global}] ->
  [{port,8091},{port_meta,global}]
[ns_server:debug,2025-05-15T18:47:22.084Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',saslauthd_enabled} -> {node,
                                                                'ns_1@172.19.0.4',
                                                                saslauthd_enabled}:
  true ->
  true
[ns_server:debug,2025-05-15T18:47:22.084Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',ssl_capi_port} -> {node,
                                                            'ns_1@172.19.0.4',
                                                            ssl_capi_port}:
  18092 ->
  18092
[ns_server:debug,2025-05-15T18:47:22.084Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',ssl_query_port} -> {node,
                                                             'ns_1@172.19.0.4',
                                                             ssl_query_port}:
  18093 ->
  18093
[ns_server:debug,2025-05-15T18:47:22.084Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',ssl_rest_port} -> {node,
                                                            'ns_1@172.19.0.4',
                                                            ssl_rest_port}:
  18091 ->
  18091
[ns_server:debug,2025-05-15T18:47:22.084Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',uuid} -> {node,'ns_1@172.19.0.4',
                                                   uuid}:
  <<"28569ac00b9c1d7c50e39741027d428c">> ->
  <<"28569ac00b9c1d7c50e39741027d428c">>
[ns_server:debug,2025-05-15T18:47:22.084Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',xdcr_rest_port} -> {node,
                                                             'ns_1@172.19.0.4',
                                                             xdcr_rest_port}:
  9998 ->
  9998
[ns_server:debug,2025-05-15T18:47:22.084Z,ns_1@172.19.0.4:ns_config<0.280.0>:dist_manager:rename_config_kv:451]renaming node conf {node,'ns_1@cb.local',{project_intact,is_vulnerable}} -> {node,
                                                                             'ns_1@172.19.0.4',
                                                                             {project_intact,
                                                                              is_vulnerable}}:
  false ->
  false
[ns_server:debug,2025-05-15T18:47:22.085Z,ns_1@172.19.0.4:<0.2150.0>:dist_manager:wait_for_node:276]Waiting for connection to node 'couchdb_ns_1@cb.local' to be established
[error_logger:info,2025-05-15T18:47:22.085Z,ns_1@172.19.0.4:net_kernel<0.2170.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:warn,2025-05-15T18:47:22.085Z,ns_1@172.19.0.4:leader_quorum_nodes_manager<0.796.0>:leader_quorum_nodes_manager:handle_quorum_nodes_updated:155]Somebody else updated the quorum nodes when we are the master node.
Our quorum nodes: ['ns_1@cb.local']
Their quorum nodes: ['ns_1@172.19.0.4']
[ns_server:debug,2025-05-15T18:47:22.085Z,ns_1@172.19.0.4:<0.799.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.796.0>} exited with reason {shutdown,
                                                                                {quorum_nodes_update_conflict,
                                                                                 ['ns_1@cb.local'],
                                                                                 ['ns_1@172.19.0.4']}}
[ns_server:debug,2025-05-15T18:47:22.085Z,ns_1@172.19.0.4:leader_activities<0.779.0>:leader_activities:handle_internal_process_down:440]Process {quorum_nodes_manager,<0.796.0>} terminated with reason {shutdown,
                                                                 {quorum_nodes_update_conflict,
                                                                  ['ns_1@cb.local'],
                                                                  ['ns_1@172.19.0.4']}}
[error_logger:error,2025-05-15T18:47:22.085Z,ns_1@172.19.0.4:mb_master_sup<0.792.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {local,mb_master_sup}
    errorContext: child_terminated
    reason: {shutdown,
                {quorum_nodes_update_conflict,
                    ['ns_1@cb.local'],
                    ['ns_1@172.19.0.4']}}
    offender: [{pid,<0.796.0>},
               {id,leader_quorum_nodes_manager},
               {mfargs,{leader_quorum_nodes_manager,start_link,[]}},
               {restart_type,permanent},
               {significant,false},
               {shutdown,1000},
               {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:22.085Z,ns_1@172.19.0.4:leader_quorum_nodes_manager<0.2188.0>:leader_quorum_nodes_manager:pull_config:99]Attempting to pull config from nodes:
[]
[ns_server:debug,2025-05-15T18:47:22.085Z,ns_1@172.19.0.4:prometheus_cfg<0.515.0>:prometheus_cfg:maybe_apply_new_settings:704]Settings didn't change, ignoring update
[ns_server:debug,2025-05-15T18:47:22.085Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',{project_intact,is_vulnerable}} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|false]
[ns_server:debug,2025-05-15T18:47:22.085Z,ns_1@172.19.0.4:net_kernel<0.2170.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:22.085Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',xdcr_rest_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9998]
[ns_server:debug,2025-05-15T18:47:22.085Z,ns_1@172.19.0.4:cb_dist<0.2168.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.2327127311.3650355203.217208>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:47:22.085Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',uuid} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
 <<"28569ac00b9c1d7c50e39741027d428c">>]
[ns_server:debug,2025-05-15T18:47:22.085Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',ssl_rest_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|18091]
[ns_server:debug,2025-05-15T18:47:22.085Z,ns_1@172.19.0.4:cb_dist<0.2168.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.2327127311.3650355203.217208>,
                                  inet_tcp_dist,<0.2189.0>,
                                  #Ref<0.2327127311.3650355201.220577>}
[ns_server:debug,2025-05-15T18:47:22.085Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',ssl_query_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|18093]
[ns_server:debug,2025-05-15T18:47:22.085Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',ssl_capi_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|18092]
[ns_server:debug,2025-05-15T18:47:22.085Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',saslauthd_enabled} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|true]
[ns_server:debug,2025-05-15T18:47:22.086Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',rest} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
 {port,8091},
 {port_meta,global}]
[ns_server:debug,2025-05-15T18:47:22.086Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',query_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|8093]
[ns_server:debug,2025-05-15T18:47:22.086Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',prometheus_http_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9123]
[ns_server:debug,2025-05-15T18:47:22.085Z,ns_1@172.19.0.4:<0.704.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":30,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true,\"hostname\":\"172.19.0.4\",\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[ns_server:debug,2025-05-15T18:47:22.086Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',projector_ssl_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9999]
[ns_server:debug,2025-05-15T18:47:22.086Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',projector_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9999]
[ns_server:debug,2025-05-15T18:47:22.086Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',port_servers} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}]
[ns_server:debug,2025-05-15T18:47:22.086Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',ns_log} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
 {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]
[ns_server:debug,2025-05-15T18:47:22.086Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',memcached_prometheus} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|11280]
[ns_server:debug,2025-05-15T18:47:22.086Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',memcached_defaults} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
 {max_connections,65000},
 {system_connections,5000},
 {connection_idle_time,0},
 {verbosity,0},
 {privilege_debug,false},
 {breakpad_enabled,true},
 {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
 {dedupe_nmvb_maps,false},
 {je_malloc_conf,undefined},
 {tracing_enabled,true},
 {datatype_snappy,true},
 {num_reader_threads,<<"default">>},
 {num_writer_threads,<<"default">>},
 {num_auxio_threads,<<"default">>},
 {num_nonio_threads,<<"default">>},
 {num_storage_threads,<<"default">>},
 {tcp_keepalive_idle,360},
 {tcp_keepalive_interval,10},
 {tcp_keepalive_probes,3},
 {tcp_user_timeout,30},
 {always_collect_trace_info,true},
 {connection_limit_mode,<<"disconnect">>},
 {free_connection_pool_size,0},
 {max_client_connection_details,0}]
[ns_server:debug,2025-05-15T18:47:22.086Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',memcached_dedicated_ssl_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|11206]
[error_logger:info,2025-05-15T18:47:22.085Z,ns_1@172.19.0.4:mb_master_sup<0.792.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.2188.0>},
              {id,leader_quorum_nodes_manager},
              {mfargs,{leader_quorum_nodes_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:22.086Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',memcached_config} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|
 {[{interfaces,{memcached_config_mgr,get_interfaces,[]}},
   {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
   {connection_idle_time,connection_idle_time},
   {privilege_debug,privilege_debug},
   {breakpad,
    {[{enabled,breakpad_enabled},
      {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
   {deployment_model,{memcached_config_mgr,get_config_profile,[]}},
   {verbosity,verbosity},
   {audit_file,{"~s",[audit_file]}},
   {rbac_file,{"~s",[rbac_file]}},
   {dedupe_nmvb_maps,dedupe_nmvb_maps},
   {tracing_enabled,tracing_enabled},
   {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
   {xattr_enabled,true},
   {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
   {collections_enabled,true},
   {max_connections,max_connections},
   {system_connections,system_connections},
   {num_reader_threads,num_reader_threads},
   {num_writer_threads,num_writer_threads},
   {num_auxio_threads,num_auxio_threads},
   {num_nonio_threads,num_nonio_threads},
   {num_storage_threads,num_storage_threads},
   {logger,
    {[{filename,{"~s/~s",[log_path,log_prefix]}},{cyclesize,log_cyclesize}]}},
   {external_auth_service,{memcached_config_mgr,get_external_auth_service,[]}},
   {active_external_users_push_interval,
    {memcached_config_mgr,get_external_users_push_interval,[]}},
   {prometheus,{memcached_config_mgr,prometheus_cfg,[]}},
   {sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
   {ssl_sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
   {tcp_keepalive_idle,tcp_keepalive_idle},
   {tcp_keepalive_interval,tcp_keepalive_interval},
   {tcp_keepalive_probes,tcp_keepalive_probes},
   {tcp_user_timeout,tcp_user_timeout},
   {always_collect_trace_info,always_collect_trace_info},
   {connection_limit_mode,connection_limit_mode},
   {free_connection_pool_size,free_connection_pool_size},
   {max_client_connection_details,max_client_connection_details}]}]
[ns_server:info,2025-05-15T18:47:22.086Z,ns_1@172.19.0.4:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:handle_info:764]cert_and_pkey changed
[ns_server:debug,2025-05-15T18:47:22.087Z,ns_1@172.19.0.4:prometheus_cfg<0.515.0>:prometheus_cfg:maybe_apply_new_settings:704]Settings didn't change, ignoring update
[ns_server:debug,2025-05-15T18:47:22.086Z,ns_1@172.19.0.4:ns_config_rep<0.545.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([quorum_nodes,
                               {node,'ns_1@172.19.0.4',address_family},
                               {node,'ns_1@172.19.0.4',audit},
                               {node,'ns_1@172.19.0.4',backup_grpc_port},
                               {node,'ns_1@172.19.0.4',backup_http_port},
                               {node,'ns_1@172.19.0.4',backup_https_port},
                               {node,'ns_1@172.19.0.4',capi_port},
                               {node,'ns_1@172.19.0.4',cbas_admin_port},
                               {node,'ns_1@172.19.0.4',cbas_cc_client_port},
                               {node,'ns_1@172.19.0.4',cbas_cc_cluster_port},
                               {node,'ns_1@172.19.0.4',cbas_cc_http_port},
                               {node,'ns_1@172.19.0.4',cbas_cluster_port},
                               {node,'ns_1@172.19.0.4',cbas_console_port},
                               {node,'ns_1@172.19.0.4',cbas_data_port},
                               {node,'ns_1@172.19.0.4',cbas_debug_port},
                               {node,'ns_1@172.19.0.4',cbas_dirs},
                               {node,'ns_1@172.19.0.4',cbas_http_port},
                               {node,'ns_1@172.19.0.4',cbas_messaging_port},
                               {node,'ns_1@172.19.0.4',
                                     cbas_metadata_callback_port},
                               {node,'ns_1@172.19.0.4',cbas_metadata_port},
                               {node,'ns_1@172.19.0.4',cbas_parent_port},
                               {node,'ns_1@172.19.0.4',cbas_replication_port},
                               {node,'ns_1@172.19.0.4',cbas_result_port},
                               {node,'ns_1@172.19.0.4',cbas_ssl_port},
                               {node,'ns_1@172.19.0.4',client_cert},
                               {node,'ns_1@172.19.0.4',compaction_daemon},
                               {node,'ns_1@172.19.0.4',config_version},
                               {node,'ns_1@172.19.0.4',database_dir},
                               {node,'ns_1@172.19.0.4',erl_external_listeners},
                               {node,'ns_1@172.19.0.4',event_log},
                               {node,'ns_1@172.19.0.4',eventing_debug_port},
                               {node,'ns_1@172.19.0.4',eventing_dir},
                               {node,'ns_1@172.19.0.4',eventing_http_port},
                               {node,'ns_1@172.19.0.4',eventing_https_port},
                               {node,'ns_1@172.19.0.4',fts_grpc_port},
                               {node,'ns_1@172.19.0.4',fts_grpc_ssl_port},
                               {node,'ns_1@172.19.0.4',fts_http_port},
                               {node,'ns_1@172.19.0.4',fts_ssl_port},
                               {node,'ns_1@172.19.0.4',index_dir},
                               {node,'ns_1@172.19.0.4',indexer_admin_port},
                               {node,'ns_1@172.19.0.4',indexer_http_port},
                               {node,'ns_1@172.19.0.4',indexer_https_port},
                               {node,'ns_1@172.19.0.4',indexer_scan_port},
                               {node,'ns_1@172.19.0.4',indexer_stcatchup_port},
                               {node,'ns_1@172.19.0.4',indexer_stinit_port},
                               {node,'ns_1@172.19.0.4',indexer_stmaint_port},
                               {node,'ns_1@172.19.0.4',is_enterprise},
                               {node,'ns_1@172.19.0.4',isasl},
                               {node,'ns_1@172.19.0.4',memcached},
                               {node,'ns_1@172.19.0.4',memcached_config},
                               {node,'ns_1@172.19.0.4',
                                     memcached_dedicated_ssl_port},
                               {node,'ns_1@172.19.0.4',memcached_defaults},
                               {node,'ns_1@172.19.0.4',memcached_prometheus},
                               {node,'ns_1@172.19.0.4',n2n_client_cert_auth},
                               {node,'ns_1@172.19.0.4',node_cert},
                               {node,'ns_1@172.19.0.4',node_encryption},
                               {node,'ns_1@172.19.0.4',ns_log},
                               {node,'ns_1@172.19.0.4',port_servers},
                               {node,'ns_1@172.19.0.4',projector_port},
                               {node,'ns_1@172.19.0.4',projector_ssl_port},
                               {node,'ns_1@172.19.0.4',prometheus_auth_info},
                               {node,'ns_1@172.19.0.4',prometheus_http_port},
                               {node,'ns_1@172.19.0.4',query_port}]..)
[ns_server:debug,2025-05-15T18:47:22.087Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',memcached} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
 {port,11210},
 {dedicated_port,11209},
 {dedicated_ssl_port,11206},
 {ssl_port,11207},
 {admin_user,"@ns_server"},
 {other_users,["@cbq-engine","@projector","@goxdcr","@index","@fts",
               "@eventing","@cbas","@backup"]},
 {admin_pass,"*****"},
 {engines,[{membase,[{engine,"/opt/couchbase/lib/memcached/ep.so"},
                     {static_config_string,"failpartialwarmup=false"}]},
           {memcached,[{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
                       {static_config_string,"vb0=true"}]}]},
 {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
 {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
 {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
 {log_prefix,"memcached.log"},
 {log_generations,20},
 {log_cyclesize,10485760},
 {log_rotation_period,39003}]
[ns_server:debug,2025-05-15T18:47:22.087Z,ns_1@172.19.0.4:leader_quorum_nodes_manager<0.2188.0>:leader_quorum_nodes_manager:pull_config:104]Pulled config successfully.
[ns_server:debug,2025-05-15T18:47:22.087Z,ns_1@172.19.0.4:prometheus_cfg<0.515.0>:prometheus_cfg:maybe_apply_new_settings:704]Settings didn't change, ignoring update
[ns_server:debug,2025-05-15T18:47:22.087Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',isasl} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
 {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]
[ns_server:debug,2025-05-15T18:47:22.088Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',is_enterprise} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|true]
[ns_server:debug,2025-05-15T18:47:22.088Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',indexer_stmaint_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9105]
[ns_server:debug,2025-05-15T18:47:22.088Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',indexer_stinit_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9103]
[ns_server:debug,2025-05-15T18:47:22.088Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',indexer_stcatchup_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9104]
[ns_server:debug,2025-05-15T18:47:22.088Z,ns_1@172.19.0.4:prometheus_cfg<0.515.0>:prometheus_cfg:maybe_apply_new_settings:704]Settings didn't change, ignoring update
[ns_server:debug,2025-05-15T18:47:22.088Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',indexer_scan_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9101]
[ns_server:debug,2025-05-15T18:47:22.088Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',indexer_https_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|19102]
[ns_server:debug,2025-05-15T18:47:22.088Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',indexer_http_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9102]
[ns_server:debug,2025-05-15T18:47:22.088Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',indexer_admin_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9100]
[ns_server:debug,2025-05-15T18:47:22.088Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',index_dir} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2025-05-15T18:47:22.088Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',fts_ssl_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|18094]
[ns_server:debug,2025-05-15T18:47:22.088Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',fts_http_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|8094]
[ns_server:debug,2025-05-15T18:47:22.088Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',fts_grpc_ssl_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|19130]
[ns_server:debug,2025-05-15T18:47:22.088Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',fts_grpc_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9130]
[ns_server:debug,2025-05-15T18:47:22.088Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',eventing_https_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|18096]
[ns_server:debug,2025-05-15T18:47:22.088Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',eventing_http_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|8096]
[ns_server:debug,2025-05-15T18:47:22.088Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',eventing_debug_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9140]
[ns_server:debug,2025-05-15T18:47:22.088Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',event_log} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
 {filename,"/opt/couchbase/var/lib/couchbase/event_log"}]
[ns_server:debug,2025-05-15T18:47:22.088Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',database_dir} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2025-05-15T18:47:22.088Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',config_version} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|{7,6}]
[ns_server:debug,2025-05-15T18:47:22.089Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',compaction_daemon} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
 {check_interval,30},
 {min_db_file_size,131072},
 {min_view_file_size,20971520}]
[ns_server:debug,2025-05-15T18:47:22.089Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_ssl_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|18095]
[ns_server:debug,2025-05-15T18:47:22.089Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_result_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9117]
[ns_server:debug,2025-05-15T18:47:22.089Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_replication_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9120]
[ns_server:debug,2025-05-15T18:47:22.089Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_parent_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9122]
[ns_server:debug,2025-05-15T18:47:22.089Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_metadata_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9121]
[ns_server:debug,2025-05-15T18:47:22.089Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_metadata_callback_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9119]
[ns_server:debug,2025-05-15T18:47:22.089Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_messaging_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9118]
[ns_server:debug,2025-05-15T18:47:22.089Z,ns_1@172.19.0.4:<0.704.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":30,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true,\"hostname\":\"172.19.0.4\",\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[ns_server:debug,2025-05-15T18:47:22.089Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_http_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|8095]
[ns_server:debug,2025-05-15T18:47:22.089Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_debug_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|-1]
[ns_server:debug,2025-05-15T18:47:22.089Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_data_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9116]
[ns_server:debug,2025-05-15T18:47:22.089Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_console_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9114]
[ns_server:debug,2025-05-15T18:47:22.089Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_cluster_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9115]
[ns_server:debug,2025-05-15T18:47:22.089Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_cc_http_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9111]
[ns_server:debug,2025-05-15T18:47:22.089Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_cc_cluster_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9112]
[ns_server:debug,2025-05-15T18:47:22.089Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_cc_client_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9113]
[ns_server:debug,2025-05-15T18:47:22.089Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_admin_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9110]
[ns_server:debug,2025-05-15T18:47:22.089Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',capi_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|8092]
[ns_server:debug,2025-05-15T18:47:22.089Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',backup_https_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|18097]
[ns_server:debug,2025-05-15T18:47:22.089Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',backup_http_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|8097]
[ns_server:debug,2025-05-15T18:47:22.090Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',backup_grpc_port} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|9124]
[ns_server:debug,2025-05-15T18:47:22.090Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',audit} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}]
[ns_server:debug,2025-05-15T18:47:22.090Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
quorum_nodes ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554042}}]},
 'ns_1@172.19.0.4']
[ns_server:debug,2025-05-15T18:47:22.090Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',address_family} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|inet]
[ns_server:debug,2025-05-15T18:47:22.090Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',node_encryption} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|false]
[ns_server:debug,2025-05-15T18:47:22.090Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',erl_external_listeners} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
 {inet,false}]
[ns_server:debug,2025-05-15T18:47:22.091Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',n2n_client_cert_auth} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]}|false]
[ns_server:debug,2025-05-15T18:47:22.091Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',node_cert} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
 {subject,<<"CN=Couchbase Server Node (127.0.0.1)">>},
 {not_after,63985747606},
 {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
 {load_timestamp,63914554006},
 {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQ"...>>},
 {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDOTCCAiGgAwIBAgIIGD/Hv6I/B/UwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ2NDZaFw0y\nNzA4MTcxODQ2NDZaMCwxKjAoBgNVBAMTIUNvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nMTI3LjAuMC4xKTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAOcGnD0i\nK4yErfsC0OEceWFxJLbXv/UYXZqt90WDswgyrkIIRSbthJzY/QEWmjj0Oh0CQmOp\nSoIV712"...>>},
 {pkey_passphrase_settings,[]},
 {certs_epoch,0},
 {type,generated},
 {hostname,"127.0.0.1"}]
[ns_server:debug,2025-05-15T18:47:22.091Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',client_cert} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
 {subject,<<"CN=Couchbase Internal Client (70858842)">>},
 {not_after,63985747606},
 {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
 {load_timestamp,63914554006},
 {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQ"...>>},
 {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDWTCCAkGgAwIBAgIIGD/Hv7XHejcwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ2NDZaFw0y\nNzA4MTcxODQ2NDZaMC8xLTArBgNVBAMTJENvdWNoYmFzZSBJbnRlcm5hbCBDbGll\nbnQgKDcwODU4ODQyKTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMSK\nvXvBcS/ChHK2Kb84PvDZyFlVdyCnMwkD8aug5L/C9zj5626M2/KO3H+iam8QErOa\nUj7TQ/A"...>>},
 {pkey_passphrase_settings,[]},
 {certs_epoch,0},
 {type,generated},
 {name,"@internal"}]
[ns_server:debug,2025-05-15T18:47:22.091Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',cbas_dirs} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
 "/opt/couchbase/var/lib/couchbase/data"]
[ns_server:debug,2025-05-15T18:47:22.091Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',eventing_dir} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554042}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2025-05-15T18:47:22.091Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',prometheus_auth_info} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{3,63914554042}}]}|
 {"@prometheus",
  {auth,
   [{<<"hash">>,
     {[{<<"hashes">>,
        {sanitized,<<"ocCjmLXobw413zJaq2v5hbk6F9JaoNKMHAEhwOXAF/k=">>}},
       {<<"algorithm">>,<<"argon2id">>},
       {<<"salt">>,<<"FlBs6+3flsH1DlD+dLEwjg==">>},
       {<<"parallelism">>,1},
       {<<"time">>,3},
       {<<"memory">>,524288}]}},
    {<<"scram-sha-512">>,
     {[{<<"salt">>,
        <<"F/Zqupd2IseMgxcuAf9fe4gpX0DBCFm1/fWg3iuaAuQdnTMBGYBMeqXKzW13cDp+Q9KGShsd3l23qMAKTVwiVA==">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"fNgWati8g2faSgrPjZUuj7XSR1xhSOtSh9nzY5vi8O4=">>}}]}},
    {<<"scram-sha-256">>,
     {[{<<"salt">>,<<"OKVHe2p1Uu/nuWHnr4hDmY1YmTEi+LTh2QSQmLrl6JU=">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"wJZdBxOuuYIylriwbbRV9JdvCNKKT1kZtZR+FOCJUQg=">>}}]}},
    {<<"scram-sha-1">>,
     {[{<<"salt">>,<<"FmlE6xOIeEZ8k+MADCxX6iijEW8=">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"gfRT+N7lHhRndbdyfaO+d/dOfRbaCQl3VgGgatuBZck=">>}}]}}]}}]
[ns_server:debug,2025-05-15T18:47:22.092Z,ns_1@172.19.0.4:<0.2150.0>:dist_manager:wait_for_node:288]Observed node 'couchdb_ns_1@cb.local' to come up
[ns_server:info,2025-05-15T18:47:22.093Z,ns_1@172.19.0.4:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:should_regenerate_certs:894]Should regenerate node_cert because CA or name in the certificate has changed
[ns_server:debug,2025-05-15T18:47:22.098Z,ns_1@172.19.0.4:ns_config_rep<0.545.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 44 us
[ns_server:debug,2025-05-15T18:47:22.098Z,ns_1@172.19.0.4:<0.2150.0>:dist_manager:complete_rename:411]Node 'ns_1@cb.local' has been renamed to 'ns_1@172.19.0.4'.
[ns_server:debug,2025-05-15T18:47:22.216Z,ns_1@172.19.0.4:<0.2205.0>:goport:handle_eof:592]Stream 'stdout' closed
[ns_server:debug,2025-05-15T18:47:22.217Z,ns_1@172.19.0.4:<0.2205.0>:goport:handle_eof:592]Stream 'stderr' closed
[ns_server:info,2025-05-15T18:47:22.217Z,ns_1@172.19.0.4:<0.2205.0>:goport:handle_process_exit:573]Port exited with status 0.
[ns_server:info,2025-05-15T18:47:22.225Z,ns_1@172.19.0.4:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:save_certs:968]New node_cert and pkey are written to tmp file
[ns_server:info,2025-05-15T18:47:22.248Z,ns_1@172.19.0.4:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:save_certs_phase2:995]node_cert cert and pkey files updated
[ns_server:debug,2025-05-15T18:47:22.249Z,ns_1@172.19.0.4:ns_config_rep<0.545.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([{node,'ns_1@172.19.0.4',node_cert}]..)
[ns_server:debug,2025-05-15T18:47:22.249Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@172.19.0.4',node_cert} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{3,63914554042}}]},
 {subject,<<"CN=Couchbase Server Node (172.19.0.4)">>},
 {not_after,63985747642},
 {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
 {load_timestamp,63914554042},
 {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQ"...>>},
 {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDOjCCAiKgAwIBAgIIGD/Hx/dYFPowDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjJaFw0y\nNzA4MTcxODQ3MjJaMC0xKzApBgNVBAMTIkNvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nMTcyLjE5LjAuNCkwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQD6Fik5\nZTKKG0O216KNVrKyjS2SrdB4IermP3HNcmWnhkrwz0Ppm9expYhmviu+eAmFSeOK\n3LRZtjU"...>>},
 {pkey_passphrase_settings,[]},
 {certs_epoch,0},
 {type,generated},
 {hostname,"172.19.0.4"}]
[ns_server:debug,2025-05-15T18:47:22.252Z,ns_1@172.19.0.4:json_rpc_connection-cbq-engine-cbauth<0.2220.0>:json_rpc_connection:init:71]Observed revrpc connection: label "cbq-engine-cbauth", handling process <0.2220.0>
[ns_server:debug,2025-05-15T18:47:22.252Z,ns_1@172.19.0.4:menelaus_cbauth<0.666.0>:menelaus_cbauth:handle_cast:203]Observed json rpc process {"cbq-engine-cbauth",[{internal,true}],<0.2220.0>} started
[ns_server:debug,2025-05-15T18:47:22.254Z,ns_1@172.19.0.4:compiled_roles_cache<0.454.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@cbq-engine",admin}
[ns_server:debug,2025-05-15T18:47:22.259Z,ns_1@172.19.0.4:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:notify_services:1146]Going to notify following services: [capi_ssl_service,cb_dist_tls,memcached,
                                     server_cert_event,ssl_service]
[ns_server:info,2025-05-15T18:47:22.259Z,ns_1@172.19.0.4:<0.2243.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service memcached
[ns_server:debug,2025-05-15T18:47:22.259Z,ns_1@172.19.0.4:cb_dist<0.2168.0>:cb_dist:info_msg:1098]cb_dist: Restarting tls distribution protocols (if any)
[ns_server:info,2025-05-15T18:47:22.259Z,ns_1@172.19.0.4:<0.2244.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service server_cert_event
[ns_server:debug,2025-05-15T18:47:22.259Z,ns_1@172.19.0.4:<0.328.0>:restartable:loop:65]Restarting child <0.388.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
  Shutdown policy: 1000
  Caller: {<0.2245.0>,#Ref<0.2327127311.3650355205.216880>}
[ns_server:debug,2025-05-15T18:47:22.260Z,ns_1@172.19.0.4:<0.328.0>:restartable:shutdown_child:114]Successfully terminated process <0.388.0>
[ns_server:info,2025-05-15T18:47:22.260Z,ns_1@172.19.0.4:memcached_config_mgr<0.701.0>:memcached_config_mgr:push_tls_config:233]Pushing TLS config to memcached:
{[{<<"private key">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem">>},
  {<<"certificate chain">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem">>},
  {<<"CA file">>,<<"/opt/couchbase/var/lib/couchbase/config/certs/ca.pem">>},
  {<<"minimum version">>,<<"TLS 1.2">>},
  {<<"cipher list">>,
   {[{<<"TLS 1.2">>,<<"HIGH">>},
     {<<"TLS 1.3">>,
      <<"TLS_AES_256_GCM_SHA384:TLS_AES_128_GCM_SHA256:TLS_CHACHA20_POLY1305_SHA256">>}]}},
  {<<"cipher order">>,true},
  {<<"client cert auth">>,<<"disabled">>}]}
[ns_server:debug,2025-05-15T18:47:22.260Z,ns_1@172.19.0.4:<0.2240.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.2038.0>} exited with reason normal
[ns_server:debug,2025-05-15T18:47:22.261Z,ns_1@172.19.0.4:cb_dist<0.2168.0>:cb_dist:info_msg:1098]cb_dist: Ensure config is going to change listeners. Will be stopped: [], will be started: [], will be restarted: []
[ns_server:info,2025-05-15T18:47:22.262Z,ns_1@172.19.0.4:<0.2242.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service cb_dist_tls
[ns_server:info,2025-05-15T18:47:22.262Z,ns_1@172.19.0.4:<0.2246.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2025-05-15T18:47:22.263Z,ns_1@172.19.0.4:<0.2246.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2025-05-15T18:47:22.264Z,ns_1@172.19.0.4:<0.2246.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2025-05-15T18:47:22.264Z,ns_1@172.19.0.4:<0.2246.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2025-05-15T18:47:22.264Z,ns_1@172.19.0.4:memcached_config_mgr<0.701.0>:memcached_config_mgr:push_tls_config:237]Successfully pushed TLS config to memcached
[ns_server:info,2025-05-15T18:47:22.265Z,ns_1@172.19.0.4:<0.2246.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2025-05-15T18:47:22.265Z,ns_1@172.19.0.4:<0.2246.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"0.0.0.0"},
 [{keyfile,"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem"},
  {certfile,"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem"},
  {versions,['tlsv1.2','tlsv1.3']},
  {cacerts,[<<48,130,3,12,48,130,1,244,160,3,2,1,2,2,8,24,63,199,191,156,197,
              82,17,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,48,
              32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,101,
              114,118,101,114,32,99,54,52,100,57,49,52,101,48,30,23,13,49,51,
              48,49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,51,
              53,57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,117,
              99,104,98,97,115,101,32,83,101,114,118,101,114,32,99,54,52,100,
              57,49,52,101,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,1,
              5,0,3,130,1,15,0,48,130,1,10,2,130,1,1,0,189,71,7,138,162,116,
              202,173,172,80,169,207,183,229,131,28,103,231,118,113,23,184,
              163,118,132,50,140,197,100,9,14,13,222,41,184,111,89,132,2,102,
              81,20,185,124,217,165,68,88,93,96,211,184,188,36,17,35,176,167,
              109,82,1,80,201,124,250,21,203,22,244,2,187,45,29,22,162,207,
              172,158,177,99,109,81,39,155,109,84,244,123,242,203,49,197,116,
              158,244,151,83,143,242,44,81,204,228,195,44,95,101,252,10,164,
              27,51,148,102,46,28,237,20,182,124,42,212,174,43,184,195,216,
              21,135,233,45,167,9,73,251,27,220,165,132,153,156,0,160,219,
              145,166,59,196,219,128,44,31,123,111,143,183,1,7,155,248,244,
              114,116,107,27,205,0,89,187,228,101,131,201,74,251,79,181,48,
              113,50,90,20,129,109,127,47,137,209,73,173,217,56,197,53,141,3,
              62,155,117,88,76,228,40,82,177,57,208,108,251,183,13,115,126,
              89,27,138,240,86,239,196,39,94,14,94,20,12,94,74,91,82,150,9,
              197,47,35,206,76,210,35,237,68,111,174,228,31,136,249,66,25,
              209,244,127,176,195,100,189,2,3,1,0,1,163,66,48,64,48,14,6,3,
              85,29,15,1,1,255,4,4,3,2,1,134,48,15,6,3,85,29,19,1,1,255,4,5,
              48,3,1,1,255,48,29,6,3,85,29,14,4,22,4,20,188,1,42,182,50,72,
              116,82,198,94,225,242,47,206,206,38,9,39,211,154,48,13,6,9,42,
              134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,7,122,181,155,55,73,
              115,117,88,215,178,67,9,164,79,41,143,213,122,120,183,161,158,
              179,24,159,107,244,185,214,40,78,200,59,208,49,4,2,152,24,193,
              11,28,199,253,134,219,80,153,22,107,94,33,144,139,251,88,92,
              185,69,22,78,200,112,130,3,234,35,142,100,178,218,128,56,66,64,
              147,226,231,103,70,132,16,120,107,97,39,6,241,133,244,226,72,
              227,53,189,39,13,56,28,212,121,112,93,92,197,98,43,69,209,137,
              1,127,228,110,207,204,246,60,244,73,57,87,206,37,77,194,32,24,
              120,98,49,146,58,191,127,2,164,235,7,154,139,200,136,193,5,110,
              232,107,27,181,32,75,241,254,5,68,41,54,188,174,154,212,192,
              249,224,20,3,106,196,82,133,216,183,147,240,201,0,75,88,247,
              105,174,187,93,204,210,55,56,242,96,140,116,69,94,95,19,60,172,
              138,198,232,182,175,206,36,240,180,239,163,144,92,150,142,84,
              114,53,180,171,234,41,126,157,228,150,39,182,108,165,58,62,101,
              129,114,231,144,128,135,156,44,194,78,139,142,141,176,149,8,31,
              25,249,73,108,167,49,3>>]},
  {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,
        10,118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,
        158,232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,
        66,211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,
        250,145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,
        104,159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,
        246,169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,
        110,167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,
        118,190,67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,
        74,8,205,174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,
        221,95,184,110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,
        76,187,66,211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,
        69,254,147,103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,
        202,133,173,72,6,69,167,89,112,174,40,229,171,2,1,2>>},
  {ciphers,[#{cipher => aes_256_gcm,key_exchange => any,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm_8,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_ecdsa,
              mac => aead,prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_rsa,
              mac => aead,prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_128_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf},
            #{cipher => aes_128_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf}]},
  {honor_cipher_order,true},
  {secure_renegotiate,true},
  {client_renegotiation,false},
  {password,"********"}],
 {ssl,true},
 {port,18091}]
[error_logger:info,2025-05-15T18:47:22.267Z,ns_1@172.19.0.4:<0.2246.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2246.0>,menelaus_web}
    started: [{pid,<0.2250.0>},
              {id,menelaus_web_ipv4},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet},
                        {ssl,true},
                        {name,menelaus_web_ssl},
                        {ssl_opts_fun,#Fun<ns_ssl_services_setup.11.39330155>},
                        {port,18091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:47:22.268Z,ns_1@172.19.0.4:<0.2246.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2025-05-15T18:47:22.268Z,ns_1@172.19.0.4:<0.2246.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2025-05-15T18:47:22.269Z,ns_1@172.19.0.4:<0.2246.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2025-05-15T18:47:22.269Z,ns_1@172.19.0.4:<0.2246.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2025-05-15T18:47:22.269Z,ns_1@172.19.0.4:<0.2246.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2025-05-15T18:47:22.271Z,ns_1@172.19.0.4:<0.2241.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service capi_ssl_service
[ns_server:info,2025-05-15T18:47:22.270Z,ns_1@172.19.0.4:<0.2246.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"::"},
 [{keyfile,"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem"},
  {certfile,"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem"},
  {versions,['tlsv1.2','tlsv1.3']},
  {cacerts,[<<48,130,3,12,48,130,1,244,160,3,2,1,2,2,8,24,63,199,191,156,197,
              82,17,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,48,
              32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,101,
              114,118,101,114,32,99,54,52,100,57,49,52,101,48,30,23,13,49,51,
              48,49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,51,
              53,57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,117,
              99,104,98,97,115,101,32,83,101,114,118,101,114,32,99,54,52,100,
              57,49,52,101,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,1,
              5,0,3,130,1,15,0,48,130,1,10,2,130,1,1,0,189,71,7,138,162,116,
              202,173,172,80,169,207,183,229,131,28,103,231,118,113,23,184,
              163,118,132,50,140,197,100,9,14,13,222,41,184,111,89,132,2,102,
              81,20,185,124,217,165,68,88,93,96,211,184,188,36,17,35,176,167,
              109,82,1,80,201,124,250,21,203,22,244,2,187,45,29,22,162,207,
              172,158,177,99,109,81,39,155,109,84,244,123,242,203,49,197,116,
              158,244,151,83,143,242,44,81,204,228,195,44,95,101,252,10,164,
              27,51,148,102,46,28,237,20,182,124,42,212,174,43,184,195,216,
              21,135,233,45,167,9,73,251,27,220,165,132,153,156,0,160,219,
              145,166,59,196,219,128,44,31,123,111,143,183,1,7,155,248,244,
              114,116,107,27,205,0,89,187,228,101,131,201,74,251,79,181,48,
              113,50,90,20,129,109,127,47,137,209,73,173,217,56,197,53,141,3,
              62,155,117,88,76,228,40,82,177,57,208,108,251,183,13,115,126,
              89,27,138,240,86,239,196,39,94,14,94,20,12,94,74,91,82,150,9,
              197,47,35,206,76,210,35,237,68,111,174,228,31,136,249,66,25,
              209,244,127,176,195,100,189,2,3,1,0,1,163,66,48,64,48,14,6,3,
              85,29,15,1,1,255,4,4,3,2,1,134,48,15,6,3,85,29,19,1,1,255,4,5,
              48,3,1,1,255,48,29,6,3,85,29,14,4,22,4,20,188,1,42,182,50,72,
              116,82,198,94,225,242,47,206,206,38,9,39,211,154,48,13,6,9,42,
              134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,7,122,181,155,55,73,
              115,117,88,215,178,67,9,164,79,41,143,213,122,120,183,161,158,
              179,24,159,107,244,185,214,40,78,200,59,208,49,4,2,152,24,193,
              11,28,199,253,134,219,80,153,22,107,94,33,144,139,251,88,92,
              185,69,22,78,200,112,130,3,234,35,142,100,178,218,128,56,66,64,
              147,226,231,103,70,132,16,120,107,97,39,6,241,133,244,226,72,
              227,53,189,39,13,56,28,212,121,112,93,92,197,98,43,69,209,137,
              1,127,228,110,207,204,246,60,244,73,57,87,206,37,77,194,32,24,
              120,98,49,146,58,191,127,2,164,235,7,154,139,200,136,193,5,110,
              232,107,27,181,32,75,241,254,5,68,41,54,188,174,154,212,192,
              249,224,20,3,106,196,82,133,216,183,147,240,201,0,75,88,247,
              105,174,187,93,204,210,55,56,242,96,140,116,69,94,95,19,60,172,
              138,198,232,182,175,206,36,240,180,239,163,144,92,150,142,84,
              114,53,180,171,234,41,126,157,228,150,39,182,108,165,58,62,101,
              129,114,231,144,128,135,156,44,194,78,139,142,141,176,149,8,31,
              25,249,73,108,167,49,3>>]},
  {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,
        10,118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,
        158,232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,
        66,211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,
        250,145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,
        104,159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,
        246,169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,
        110,167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,
        118,190,67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,
        74,8,205,174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,
        221,95,184,110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,
        76,187,66,211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,
        69,254,147,103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,
        202,133,173,72,6,69,167,89,112,174,40,229,171,2,1,2>>},
  {ciphers,[#{cipher => aes_256_gcm,key_exchange => any,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm_8,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_ecdsa,
              mac => aead,prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_rsa,
              mac => aead,prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_128_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf},
            #{cipher => aes_128_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf}]},
  {honor_cipher_order,true},
  {secure_renegotiate,true},
  {client_renegotiation,false},
  {password,"********"}],
 {ssl,true},
 {port,18091}]
[error_logger:info,2025-05-15T18:47:22.271Z,ns_1@172.19.0.4:<0.2246.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.2246.0>,menelaus_web}
    started: [{pid,<0.2271.0>},
              {id,menelaus_web_ipv6},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet6},
                        {ssl,true},
                        {name,menelaus_web_ssl},
                        {ssl_opts_fun,#Fun<ns_ssl_services_setup.11.39330155>},
                        {port,18091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:22.272Z,ns_1@172.19.0.4:<0.328.0>:restartable:start_child:92]Started child process <0.2246.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
[ns_server:info,2025-05-15T18:47:22.272Z,ns_1@172.19.0.4:<0.2245.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service ssl_service
[ns_server:info,2025-05-15T18:47:22.272Z,ns_1@172.19.0.4:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:notify_services:1162]Succesfully notified services [ssl_service,server_cert_event,memcached,
                               cb_dist_tls,capi_ssl_service]
[ns_server:debug,2025-05-15T18:47:22.273Z,ns_1@172.19.0.4:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:restart_regenerate_client_cert_timer:1447]Time left before client cert regeneration: 70588764000
[ns_server:info,2025-05-15T18:47:22.273Z,ns_1@172.19.0.4:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:validate_pkey:1032]Private key (node_cert) passphrase validation suceeded
[ns_server:debug,2025-05-15T18:47:22.273Z,ns_1@172.19.0.4:cb_dist<0.2168.0>:cb_dist:info_msg:1098]cb_dist: Ensure config is going to change listeners. Will be stopped: [], will be started: [], will be restarted: []
[ns_server:debug,2025-05-15T18:47:22.273Z,ns_1@172.19.0.4:<0.572.0>:restartable:loop:65]Restarting child <0.573.0>
  MFA: {ns_doctor_sup,start_link,[]}
  Shutdown policy: infinity
  Caller: {<0.2150.0>,#Ref<0.2327127311.3650355208.215970>}
[ns_server:debug,2025-05-15T18:47:22.274Z,ns_1@172.19.0.4:<0.578.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.577.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:22.274Z,ns_1@172.19.0.4:<0.572.0>:restartable:shutdown_child:114]Successfully terminated process <0.573.0>
[error_logger:info,2025-05-15T18:47:22.274Z,ns_1@172.19.0.4:ns_doctor_sup<0.2290.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_doctor_sup}
    started: [{pid,<0.2291.0>},
              {id,ns_doctor_events},
              {mfargs,{gen_event,start_link,[{local,ns_doctor_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:22.274Z,ns_1@172.19.0.4:ns_doctor_sup<0.2290.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_doctor_sup}
    started: [{pid,<0.2292.0>},
              {id,ns_doctor},
              {mfargs,{ns_doctor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:22.274Z,ns_1@172.19.0.4:<0.572.0>:restartable:start_child:92]Started child process <0.2290.0>
  MFA: {ns_doctor_sup,start_link,[]}
[ns_server:debug,2025-05-15T18:47:22.274Z,ns_1@172.19.0.4:<0.764.0>:restartable:loop:65]Restarting child <0.765.0>
  MFA: {leader_services_sup,start_link,[]}
  Shutdown policy: infinity
  Caller: {<0.2150.0>,#Ref<0.2327127311.3650355206.215917>}
[ns_server:info,2025-05-15T18:47:22.274Z,ns_1@172.19.0.4:mb_master<0.790.0>:mb_master:terminate:247]Synchronously shutting down child mb_master_sup
[ns_server:debug,2025-05-15T18:47:22.274Z,ns_1@172.19.0.4:<0.890.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.889.0>} exited with reason shutdown
[ns_server:info,2025-05-15T18:47:22.274Z,ns_1@172.19.0.4:leader_registry<0.788.0>:leader_registry:handle_down:286]Process <0.889.0> registered as 'license_reporting' terminated.
[ns_server:debug,2025-05-15T18:47:22.274Z,ns_1@172.19.0.4:<0.888.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.887.0>} exited with reason shutdown
[ns_server:info,2025-05-15T18:47:22.274Z,ns_1@172.19.0.4:leader_registry<0.788.0>:leader_registry:handle_down:286]Process <0.885.0> registered as 'tombstone_purger' terminated.
[ns_server:info,2025-05-15T18:47:22.274Z,ns_1@172.19.0.4:leader_registry<0.788.0>:leader_registry:handle_down:286]Process <0.873.0> registered as 'auto_failover' terminated.
[ns_server:debug,2025-05-15T18:47:22.274Z,ns_1@172.19.0.4:<0.875.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.873.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:22.274Z,ns_1@172.19.0.4:<0.874.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {compat_mode_events,<0.873.0>} exited with reason shutdown
[ns_server:info,2025-05-15T18:47:22.274Z,ns_1@172.19.0.4:leader_registry<0.788.0>:leader_registry:handle_down:286]Process <0.871.0> registered as 'ns_orchestrator' terminated.
[ns_server:info,2025-05-15T18:47:22.274Z,ns_1@172.19.0.4:leader_registry<0.788.0>:leader_registry:handle_down:286]Process <0.870.0> registered as 'auto_rebalance' terminated.
[ns_server:info,2025-05-15T18:47:22.274Z,ns_1@172.19.0.4:leader_registry<0.788.0>:leader_registry:handle_down:286]Process <0.869.0> registered as 'auto_reprovision' terminated.
[ns_server:info,2025-05-15T18:47:22.275Z,ns_1@172.19.0.4:leader_registry<0.788.0>:leader_registry:handle_down:286]Process <0.808.0> registered as 'chronicle_master' terminated.
[ns_server:debug,2025-05-15T18:47:22.275Z,ns_1@172.19.0.4:<0.809.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {'kv-events',<0.808.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:22.275Z,ns_1@172.19.0.4:leader_activities<0.779.0>:leader_activities:handle_internal_process_down:440]Process {quorum_nodes_manager,<0.2188.0>} terminated with reason shutdown
[ns_server:debug,2025-05-15T18:47:22.275Z,ns_1@172.19.0.4:<0.2199.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.2188.0>} exited with reason shutdown
[ns_server:info,2025-05-15T18:47:22.275Z,ns_1@172.19.0.4:leader_registry<0.788.0>:leader_registry:handle_down:286]Process <0.801.0> registered as 'ns_tick' terminated.
[ns_server:debug,2025-05-15T18:47:22.275Z,ns_1@172.19.0.4:leader_activities<0.779.0>:leader_activities:handle_internal_process_down:440]Process {acquirer,<0.794.0>} terminated with reason shutdown
[ns_server:debug,2025-05-15T18:47:22.275Z,ns_1@172.19.0.4:<0.795.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_node_disco_events,<0.794.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:22.275Z,ns_1@172.19.0.4:leader_registry<0.788.0>:leader_registry:handle_new_leader:275]New leader is undefined. Invalidating name cache.
[ns_server:debug,2025-05-15T18:47:22.275Z,ns_1@172.19.0.4:<0.791.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.790.0>} exited with reason shutdown
[ns_server:debug,2025-05-15T18:47:22.275Z,ns_1@172.19.0.4:<0.789.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {leader_events,<0.788.0>} exited with reason shutdown
[ns_server:warn,2025-05-15T18:47:22.275Z,ns_1@172.19.0.4:leader_lease_agent<0.782.0>:leader_lease_agent:handle_terminate:302]Terminating with reason shutdown when we own an active lease:
{lease,
    {lease_holder,<<"12235e8cca1e7367c554bc33cea65983">>,'ns_1@cb.local'},
    -576460710694908522,-576460695694908522,
    {timer,#Ref<0.2327127311.3650355204.217071>,
        {lease_expired,
            {lease_holder,<<"12235e8cca1e7367c554bc33cea65983">>,
                'ns_1@cb.local'}}},
    active}
Persisting updated lease.
[ns_server:debug,2025-05-15T18:47:22.279Z,ns_1@172.19.0.4:leader_activities<0.779.0>:leader_activities:handle_internal_process_down:440]Process {agent,<0.782.0>} terminated with reason shutdown
[ns_server:debug,2025-05-15T18:47:22.279Z,ns_1@172.19.0.4:<0.764.0>:restartable:shutdown_child:114]Successfully terminated process <0.765.0>
[error_logger:info,2025-05-15T18:47:22.280Z,ns_1@172.19.0.4:leader_leases_sup<0.2306.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_leases_sup}
    started: [{pid,<0.2307.0>},
              {id,leader_activities},
              {mfargs,{leader_activities,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,10000},
              {child_type,worker}]

[ns_server:warn,2025-05-15T18:47:22.280Z,ns_1@172.19.0.4:leader_lease_agent<0.2308.0>:leader_lease_agent:maybe_recover_persisted_lease:393]Found persisted lease [{node,'ns_1@cb.local'},
                       {uuid,<<"12235e8cca1e7367c554bc33cea65983">>},
                       {time_left,14473},
                       {status,active}]
[error_logger:info,2025-05-15T18:47:22.281Z,ns_1@172.19.0.4:leader_leases_sup<0.2306.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_leases_sup}
    started: [{pid,<0.2308.0>},
              {id,leader_lease_agent},
              {mfargs,{leader_lease_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:22.281Z,ns_1@172.19.0.4:leader_services_sup<0.2305.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_services_sup}
    started: [{pid,<0.2306.0>},
              {id,leader_leases_sup},
              {mfargs,{leader_leases_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:22.281Z,ns_1@172.19.0.4:leader_registry_sup<0.2310.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_registry_sup}
    started: [{pid,<0.2311.0>},
              {id,leader_registry},
              {mfargs,{leader_registry,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:22.281Z,ns_1@172.19.0.4:leader_registry_sup<0.2310.0>:mb_master:check_master_takeover_needed:183]Sending master node question to the following nodes: []
[ns_server:debug,2025-05-15T18:47:22.281Z,ns_1@172.19.0.4:leader_registry_sup<0.2310.0>:mb_master:check_master_takeover_needed:187]Got replies: []
[ns_server:debug,2025-05-15T18:47:22.281Z,ns_1@172.19.0.4:leader_registry_sup<0.2310.0>:mb_master:check_master_takeover_needed:193]Was unable to discover master, not going to force mastership takeover
[ns_server:debug,2025-05-15T18:47:22.281Z,ns_1@172.19.0.4:mb_master<0.2313.0>:mb_master:init:86]Heartbeat interval is 2000
[user:info,2025-05-15T18:47:22.281Z,ns_1@172.19.0.4:mb_master<0.2313.0>:mb_master:init:91]I'm the only node, so I'm the master.
[ns_server:info,2025-05-15T18:47:22.281Z,ns_1@172.19.0.4:ns_log<0.504.0>:ns_log:is_duplicate_log:156]suppressing duplicate log mb_master:0([<<"I'm the only node, so I'm the master.">>]) because it's been seen 1 times in the past 32.595934 secs (last seen 32.595934 secs ago
[ns_server:debug,2025-05-15T18:47:22.281Z,ns_1@172.19.0.4:leader_registry<0.2311.0>:leader_registry:handle_new_leader:275]New leader is 'ns_1@172.19.0.4'. Invalidating name cache.
[error_logger:info,2025-05-15T18:47:22.281Z,ns_1@172.19.0.4:mb_master_sup<0.2315.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.2316.0>},
              {id,leader_lease_acquirer},
              {mfargs,{leader_lease_acquirer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,10000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:22.281Z,ns_1@172.19.0.4:leader_quorum_nodes_manager<0.2319.0>:leader_quorum_nodes_manager:pull_config:99]Attempting to pull config from nodes:
[]
[error_logger:info,2025-05-15T18:47:22.281Z,ns_1@172.19.0.4:mb_master_sup<0.2315.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.2319.0>},
              {id,leader_quorum_nodes_manager},
              {mfargs,{leader_quorum_nodes_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:22.281Z,ns_1@172.19.0.4:leader_quorum_nodes_manager<0.2319.0>:leader_quorum_nodes_manager:pull_config:104]Pulled config successfully.
[ns_server:info,2025-05-15T18:47:22.282Z,ns_1@172.19.0.4:mb_master_sup<0.2315.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,ns_tick},
                                         ns_tick,[],[]]): started as <0.2323.0> on 'ns_1@172.19.0.4'

[error_logger:info,2025-05-15T18:47:22.282Z,ns_1@172.19.0.4:mb_master_sup<0.2315.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.2323.0>},
              {id,ns_tick},
              {mfargs,{ns_tick,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,10},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:22.282Z,ns_1@172.19.0.4:<0.2326.0>:chronicle_master:do_init:115]Starting with SelfRef = #Ref<0.2327127311.3650355203.217418>
[ns_server:warn,2025-05-15T18:47:22.282Z,ns_1@172.19.0.4:<0.2318.0>:leader_lease_acquire_worker:handle_lease_already_acquired:226]Failed to acquire lease from 'ns_1@172.19.0.4' because its already taken by {'ns_1@cb.local',
                                                                             <<"12235e8cca1e7367c554bc33cea65983">>} (valid for 14471ms)
[ns_server:info,2025-05-15T18:47:22.282Z,ns_1@172.19.0.4:mb_master_sup<0.2315.0>:misc:start_singleton:913]start_singleton(gen_server2, start_link, [{via,leader_registry,
                                           chronicle_master},
                                          chronicle_master,[],[]]): started as <0.2326.0> on 'ns_1@172.19.0.4'

[error_logger:info,2025-05-15T18:47:22.282Z,ns_1@172.19.0.4:mb_master_sup<0.2315.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.2326.0>},
              {id,chronicle_master},
              {mfargs,{chronicle_master,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:22.282Z,ns_1@172.19.0.4:ns_orchestrator_sup<0.2328.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.2329.0>},
              {id,compat_mode_events},
              {mfargs,{gen_event,start_link,[{local,compat_mode_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:22.282Z,ns_1@172.19.0.4:ns_orchestrator_sup<0.2328.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.2330.0>},
              {id,compat_mode_manager},
              {mfargs,{compat_mode_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:22.282Z,ns_1@172.19.0.4:ns_orchestrator_child_sup<0.2331.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.2332.0>},
              {id,ns_janitor_server},
              {mfargs,{ns_janitor_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:47:22.282Z,ns_1@172.19.0.4:ns_orchestrator_child_sup<0.2331.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          auto_reprovision},
                                         auto_reprovision,[],[]]): started as <0.2333.0> on 'ns_1@172.19.0.4'

[error_logger:info,2025-05-15T18:47:22.283Z,ns_1@172.19.0.4:ns_orchestrator_child_sup<0.2331.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.2333.0>},
              {id,auto_reprovision},
              {mfargs,{auto_reprovision,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:47:22.283Z,ns_1@172.19.0.4:ns_orchestrator_child_sup<0.2331.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,auto_rebalance},
                                         auto_rebalance,[],[]]): started as <0.2334.0> on 'ns_1@172.19.0.4'

[error_logger:info,2025-05-15T18:47:22.283Z,ns_1@172.19.0.4:ns_orchestrator_child_sup<0.2331.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.2334.0>},
              {id,auto_rebalance},
              {mfargs,{auto_rebalance,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-05-15T18:47:22.283Z,ns_1@172.19.0.4:ns_orchestrator_child_sup<0.2331.0>:misc:start_singleton:913]start_singleton(gen_statem, start_link, [{via,leader_registry,ns_orchestrator},
                                         ns_orchestrator,[],[]]): started as <0.2338.0> on 'ns_1@172.19.0.4'

[error_logger:info,2025-05-15T18:47:22.283Z,ns_1@172.19.0.4:ns_orchestrator_child_sup<0.2331.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.2338.0>},
              {id,ns_orchestrator},
              {mfargs,{ns_orchestrator,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:22.283Z,ns_1@172.19.0.4:ns_orchestrator_sup<0.2328.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.2331.0>},
              {id,ns_orchestrator_child_sup},
              {mfargs,{ns_orchestrator_child_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:47:22.283Z,ns_1@172.19.0.4:<0.2340.0>:auto_failover:init:223]init auto_failover.
[user:info,2025-05-15T18:47:22.283Z,ns_1@172.19.0.4:<0.2340.0>:auto_failover:enable_auto_failover:460]Enabled auto-failover with timeout 120 and max count 1
[ns_server:debug,2025-05-15T18:47:22.283Z,ns_1@172.19.0.4:<0.2340.0>:auto_failover:update_and_save_auto_failover_state:479]No change in timeout 120
[ns_server:debug,2025-05-15T18:47:22.283Z,ns_1@172.19.0.4:<0.2340.0>:auto_failover:update_and_save_auto_failover_state:487]No change in max count 1
[ns_server:info,2025-05-15T18:47:22.283Z,ns_1@172.19.0.4:ns_log<0.504.0>:ns_log:is_duplicate_log:156]suppressing duplicate log auto_failover:0([<<"Enabled auto-failover with timeout 120 and max count 1">>]) because it's been seen 1 times in the past 32.44753 secs (last seen 32.44753 secs ago
[ns_server:debug,2025-05-15T18:47:22.284Z,ns_1@172.19.0.4:cleanup_process<0.2339.0>:service_janitor:init_topology_aware_service:82]Doing initial topology change for service `index'
[ns_server:debug,2025-05-15T18:47:22.284Z,ns_1@172.19.0.4:<0.2340.0>:auto_failover:init_logic_state:249]Using auto-failover logic state {state,[],
                                    [{service_state,kv,nil,false},
                                     {service_state,n1ql,nil,false},
                                     {service_state,index,nil,false},
                                     {service_state,fts,nil,false},
                                     {service_state,cbas,nil,false},
                                     {service_state,eventing,nil,false},
                                     {service_state,backup,nil,false}],
                                    118}
[ns_server:info,2025-05-15T18:47:22.284Z,ns_1@172.19.0.4:ns_orchestrator_sup<0.2328.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,auto_failover},
                                         auto_failover,[],[]]): started as <0.2340.0> on 'ns_1@172.19.0.4'

[error_logger:info,2025-05-15T18:47:22.284Z,ns_1@172.19.0.4:ns_orchestrator_sup<0.2328.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.2340.0>},
              {id,auto_failover},
              {mfargs,{auto_failover,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:22.284Z,ns_1@172.19.0.4:mb_master_sup<0.2315.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.2328.0>},
              {id,ns_orchestrator_sup},
              {mfargs,{ns_orchestrator_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:info,2025-05-15T18:47:22.284Z,ns_1@172.19.0.4:mb_master_sup<0.2315.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          tombstone_purger},
                                         tombstone_purger,[],[]]): started as <0.2344.0> on 'ns_1@172.19.0.4'

[error_logger:info,2025-05-15T18:47:22.284Z,ns_1@172.19.0.4:mb_master_sup<0.2315.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.2344.0>},
              {id,tombstone_purger},
              {mfargs,{tombstone_purger,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:22.284Z,ns_1@172.19.0.4:mb_master_sup<0.2315.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.2346.0>},
              {id,global_tasks},
              {mfargs,{global_tasks,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:22.284Z,ns_1@172.19.0.4:guardrail_enforcer<0.2347.0>:guardrail_enforcer:maybe_notify_services:137]Changed Statuses: #{}
[error_logger:info,2025-05-15T18:47:22.284Z,ns_1@172.19.0.4:mb_master_sup<0.2315.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.2347.0>},
              {id,guardrail_enforcer},
              {mfargs,{guardrail_enforcer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:22.284Z,ns_1@172.19.0.4:<0.2349.0>:license_reporting:init:60]Starting license_reporting server
[ns_server:info,2025-05-15T18:47:22.284Z,ns_1@172.19.0.4:mb_master_sup<0.2315.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          license_reporting},
                                         license_reporting,[],[]]): started as <0.2349.0> on 'ns_1@172.19.0.4'

[error_logger:info,2025-05-15T18:47:22.284Z,ns_1@172.19.0.4:mb_master_sup<0.2315.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.2349.0>},
              {id,license_reporting},
              {mfargs,{license_reporting,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:22.285Z,ns_1@172.19.0.4:leader_registry_sup<0.2310.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_registry_sup}
    started: [{pid,<0.2313.0>},
              {id,mb_master},
              {mfargs,{mb_master,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:22.285Z,ns_1@172.19.0.4:leader_services_sup<0.2305.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_services_sup}
    started: [{pid,<0.2310.0>},
              {id,leader_registry_sup},
              {mfargs,{leader_registry_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:47:22.285Z,ns_1@172.19.0.4:<0.764.0>:restartable:start_child:92]Started child process <0.2305.0>
  MFA: {leader_services_sup,start_link,[]}
[ns_server:debug,2025-05-15T18:47:22.286Z,ns_1@172.19.0.4:remote_monitors<0.297.0>:remote_monitors:handle_info:86]Node renaming transaction ended. MRef = #Ref<0.2327127311.3650355202.217528>
[ns_server:debug,2025-05-15T18:47:22.286Z,ns_1@172.19.0.4:<0.704.0>:terse_cluster_info_uploader:handle_info:64]Got DOWN with reason: unpaused from memcached port server: <16971.139.0>. Shutting down
[ns_server:debug,2025-05-15T18:47:22.286Z,ns_1@172.19.0.4:<0.705.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {bucket_info_cache_invalidations,<0.704.0>} exited with reason {shutdown,
                                                                                               {memcached_port_server_down,
                                                                                                <16971.139.0>,
                                                                                                unpaused}}
[ns_server:debug,2025-05-15T18:47:22.287Z,ns_1@172.19.0.4:memcached_config_mgr<0.701.0>:memcached_config_mgr:handle_info:198]Got DOWN with reason: unpaused from memcached port server: <16971.139.0>. Shutting down
[ns_server:debug,2025-05-15T18:47:22.287Z,ns_1@172.19.0.4:wait_link_to_couchdb_node<0.472.0>:ns_server_nodes_sup:wait_link_to_couchdb_node_loop:202]Link to couchdb node was unpaused.
[ns_server:debug,2025-05-15T18:47:22.287Z,ns_1@172.19.0.4:ns_node_disco<0.537.0>:ns_node_disco:handle_info:160]Node renaming transaction ended. MRef = #Ref<0.2327127311.3650355202.217522>
[ns_server:debug,2025-05-15T18:47:22.287Z,ns_1@172.19.0.4:ns_cookie_manager<0.238.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[ns_server:debug,2025-05-15T18:47:22.287Z,ns_1@172.19.0.4:wait_link_to_couchdb_node<0.472.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:158]Waiting for ns_couchdb node to start
[ns_server:debug,2025-05-15T18:47:22.288Z,ns_1@172.19.0.4:<0.807.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.701.0>} exited with reason {shutdown,
                                                                                {memcached_port_server_down,
                                                                                 <16971.139.0>,
                                                                                 unpaused}}
[cluster:debug,2025-05-15T18:47:22.288Z,ns_1@172.19.0.4:ns_cluster<0.273.0>:ns_cluster:maybe_rename:751]Renamed node from 'ns_1@cb.local' to 'ns_1@172.19.0.4'.
[error_logger:error,2025-05-15T18:47:22.288Z,ns_1@172.19.0.4:<0.700.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {<0.700.0>,suppress_max_restart_intensity}
    errorContext: child_terminated
    reason: {shutdown,{memcached_port_server_down,<16971.139.0>,unpaused}}
    offender: [{pid,<0.701.0>},
               {id,memcached_config_mgr},
               {mfargs,{memcached_config_mgr,start_link,[]}},
               {restart_type,permanent},
               {significant,false},
               {shutdown,1000},
               {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:22.288Z,ns_1@172.19.0.4:<0.2352.0>:ns_node_disco:do_nodes_wanted_updated_fun:210]ns_node_disco: nodes_wanted updated: ['ns_1@172.19.0.4'], with cookie: {sanitized,
                                                                        <<"biSYTaQUFDVndTs/b+IcDmD2L0woktv9naRdv3GCK6A=">>}
[ns_server:debug,2025-05-15T18:47:22.288Z,ns_1@172.19.0.4:memcached_config_mgr<0.2357.0>:memcached_config_mgr:memcached_port_pid:154]waiting for completion of initial ns_ports_setup round
[error_logger:info,2025-05-15T18:47:22.288Z,ns_1@172.19.0.4:<0.700.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.700.0>,suppress_max_restart_intensity}
    started: [{pid,<0.2357.0>},
              {id,memcached_config_mgr},
              {mfargs,{memcached_config_mgr,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[cluster:info,2025-05-15T18:47:22.288Z,ns_1@172.19.0.4:ns_cluster<0.273.0>:ns_cluster:do_change_address:720]Renamed node. New name is 'ns_1@172.19.0.4'.
[error_logger:error,2025-05-15T18:47:22.288Z,ns_1@172.19.0.4:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {local,ns_server_sup}
    errorContext: child_terminated
    reason: {shutdown,{memcached_port_server_down,<16971.139.0>,unpaused}}
    offender: [{pid,<0.704.0>},
               {id,terse_cluster_info_uploader},
               {mfargs,{terse_cluster_info_uploader,start_link,[]}},
               {restart_type,permanent},
               {significant,false},
               {shutdown,1000},
               {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:22.288Z,ns_1@172.19.0.4:<0.2359.0>:memcached_config_mgr:memcached_port_pid:154]waiting for completion of initial ns_ports_setup round
[ns_server:debug,2025-05-15T18:47:22.288Z,ns_1@172.19.0.4:<0.2352.0>:ns_node_disco:do_nodes_wanted_updated_fun:213]ns_node_disco: nodes_wanted pong: ['ns_1@172.19.0.4'], with cookie: {sanitized,
                                                                     <<"biSYTaQUFDVndTs/b+IcDmD2L0woktv9naRdv3GCK6A=">>}
[error_logger:info,2025-05-15T18:47:22.288Z,ns_1@172.19.0.4:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2359.0>},
              {id,terse_cluster_info_uploader},
              {mfargs,{terse_cluster_info_uploader,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:22.289Z,ns_1@172.19.0.4:ns_node_disco_events<0.536.0>:ns_config_rep:handle_node_disco_event:513]Detected new nodes (['ns_1@172.19.0.4']).  Moving config around.
[ns_server:info,2025-05-15T18:47:22.289Z,ns_1@172.19.0.4:ns_node_disco_events<0.536.0>:ns_node_disco_log:handle_event:40]ns_node_disco_log: nodes changed: ['ns_1@172.19.0.4']
[ns_server:debug,2025-05-15T18:47:22.291Z,ns_1@172.19.0.4:service_manager-index<0.2361.0>:service_agent:wait_for_agents:74]Waiting for the service agents for service index to come up on nodes:
['ns_1@172.19.0.4']
[ns_server:debug,2025-05-15T18:47:22.291Z,ns_1@172.19.0.4:service_manager-index<0.2361.0>:service_agent:wait_for_agents_loop:92]All service agents are ready for index
[ns_server:debug,2025-05-15T18:47:22.297Z,ns_1@172.19.0.4:json_rpc_connection-projector-cbauth<0.2375.0>:json_rpc_connection:init:71]Observed revrpc connection: label "projector-cbauth", handling process <0.2375.0>
[ns_server:debug,2025-05-15T18:47:22.297Z,ns_1@172.19.0.4:menelaus_cbauth<0.666.0>:menelaus_cbauth:handle_cast:203]Observed json rpc process {"projector-cbauth",[{internal,true}],<0.2375.0>} started
[ns_server:debug,2025-05-15T18:47:22.298Z,ns_1@172.19.0.4:compiled_roles_cache<0.454.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@projector",admin}
[ns_server:debug,2025-05-15T18:47:22.309Z,ns_1@172.19.0.4:ns_config_rep<0.545.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([{metakv,<<"/query/functions_cache/counter">>}]..)
[ns_server:debug,2025-05-15T18:47:22.310Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/query/functions_cache/counter">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554042}}]}|
 <<"[172.19.0.4:8091]0">>]
[ns_server:debug,2025-05-15T18:47:22.491Z,ns_1@172.19.0.4:<0.2373.0>:goport:handle_eof:592]Stream 'stdout' closed
[ns_server:debug,2025-05-15T18:47:22.491Z,ns_1@172.19.0.4:<0.2373.0>:goport:handle_eof:592]Stream 'stderr' closed
[ns_server:info,2025-05-15T18:47:22.491Z,ns_1@172.19.0.4:<0.2373.0>:goport:handle_process_exit:573]Port exited with status 0.
[ns_server:debug,2025-05-15T18:47:22.580Z,ns_1@172.19.0.4:<0.2403.0>:goport:handle_eof:592]Stream 'stderr' closed
[ns_server:debug,2025-05-15T18:47:22.580Z,ns_1@172.19.0.4:<0.2403.0>:goport:handle_eof:592]Stream 'stdout' closed
[ns_server:info,2025-05-15T18:47:22.580Z,ns_1@172.19.0.4:<0.2403.0>:goport:handle_process_exit:573]Port exited with status 0.
[cluster:debug,2025-05-15T18:47:22.581Z,ns_1@172.19.0.4:ns_cluster<0.273.0>:ns_cluster:post_json_to_joinee:920]Posting the following to [https,"db2.lan",18091,"/engageCluster2"]:
{{[{<<"requestedTargetNodeHostname">>,<<"db2.lan">>},
   {<<"requestedServices">>,[index,kv,n1ql]},
   {<<"isDeveloperPreview">>,false},
   {availableStorage,{[{hdd,[{[{path,<<"/">>},
                               {sizeKBytes,1056557396},
                               {usagePercent,2}]},
                             {[{path,<<"/dev">>},
                               {sizeKBytes,65536},
                               {usagePercent,0}]},
                             {[{path,<<"/dev/shm">>},
                               {sizeKBytes,65536},
                               {usagePercent,0}]},
                             {[{path,<<"/etc/resolv.conf">>},
                               {sizeKBytes,1056557396},
                               {usagePercent,2}]},
                             {[{path,<<"/etc/hostname">>},
                               {sizeKBytes,1056557396},
                               {usagePercent,2}]},
                             {[{path,<<"/etc/hosts">>},
                               {sizeKBytes,1056557396},
                               {usagePercent,2}]},
                             {[{path,<<"/opt/couchbase/var">>},
                               {sizeKBytes,482797652},
                               {usagePercent,92}]},
                             {[{path,<<"/proc/kcore">>},
                               {sizeKBytes,65536},
                               {usagePercent,0}]},
                             {[{path,<<"/proc/keys">>},
                               {sizeKBytes,65536},
                               {usagePercent,0}]},
                             {[{path,<<"/proc/timer_list">>},
                               {sizeKBytes,65536},
                               {usagePercent,0}]},
                             {[{path,<<"/proc/scsi">>},
                               {sizeKBytes,4012680},
                               {usagePercent,0}]},
                             {[{path,<<"/sys/firmware">>},
                               {sizeKBytes,4012680},
                               {usagePercent,0}]}]}]}},
   {storageTotals,{[{ram,{[{total,8217968640},
                           {quotaTotal,3221225472},
                           {quotaUsed,0},
                           {used,4251119616},
                           {usedByData,0},
                           {quotaUsedPerNode,0},
                           {quotaTotalPerNode,3221225472}]}},
                    {hdd,{[{total,494384795648},
                           {quotaTotal,494384795648},
                           {used,454834011996},
                           {usedByData,0},
                           {free,39550783652}]}}]}},
   {storage,{[{ssd,[]},
              {hdd,[{[{path,<<"/opt/couchbase/var/lib/couchbase/data">>},
                      {index_path,<<"/opt/couchbase/var/lib/couchbase/data">>},
                      {cbas_dirs,[<<"/opt/couchbase/var/lib/couchbase/data">>]},
                      {eventing_path,<<"/opt/couchbase/var/lib/couchbase/data">>},
                      {java_home,<<>>},
                      {quotaMb,none},
                      {state,ok}]}]}]}},
   {clusterMembership,active},
   {recoveryType,none},
   {status,<<"healthy">>},
   {otpNode,'ns_1@172.19.0.4'},
   {thisNode,true},
   {hostname,<<"172.19.0.4:8091">>},
   {nodeUUID,<<"28569ac00b9c1d7c50e39741027d428c">>},
   {clusterCompatibility,458758},
   {version,<<"7.6.2-3721-enterprise">>},
   {os,<<"aarch64-unknown-linux-gnu">>},
   {cpuCount,10},
   {ports,{[{direct,11210},
            {httpsMgmt,18091},
            {httpsCAPI,18092},
            {distTCP,21100},
            {distTLS,21150}]}},
   {services,[index,kv,n1ql]},
   {nodeEncryption,false},
   {nodeEncryptionClientCertVerification,false},
   {addressFamilyOnly,false},
   {configuredHostname,<<"172.19.0.4:8091">>},
   {addressFamily,inet},
   {externalListeners,[{[{afamily,inet},{nodeEncryption,false}]}]},
   {serverGroup,<<"Group 1">>},
   {couchApiBase,<<"http://172.19.0.4:8092/">>},
   {couchApiBaseHTTPS,<<"https://172.19.0.4:18092/">>},
   {nodeHash,10838665},
   {systemStats,{[{cpu_utilization_rate,3.89883035089008},
                  {cpu_stolen_rate,0},
                  {swap_total,1073737728},
                  {swap_used,3956736},
                  {mem_total,8217968640},
                  {mem_free,4805804032},
                  {mem_limit,8217968640},
                  {cpu_cores_available,10},
                  {allocstall,17}]}},
   {interestingStats,{[]}},
   {uptime,<<"41">>},
   {memoryTotal,8217968640},
   {memoryFree,4805804032},
   {mcdMemoryReserved,6269},
   {mcdMemoryAllocated,6269},
   {memoryQuota,3072},
   {queryMemoryQuota,0},
   {indexMemoryQuota,512},
   {ftsMemoryQuota,512},
   {cbasMemoryQuota,1024},
   {eventingMemoryQuota,256},
   {<<"autogeneratedCA">>,
    <<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQCuy0dFqLPrJ6xY21RJ5ttVPR78ssxxXSe9JdTj/IsUczkwyxfZfwK\npBszlGYuHO0Utnwq1K4ruMPYFYfpLacJSfsb3KWEmZwAoNuRpjvE24AsH3tvj7cB\nB5v49HJ0axvNAFm75GWDyUr7T7UwcTJaFIFtfy+J0Umt2TjFNY0DPpt1WEzkKFKx\nOdBs+7cNc35ZG4rwVu/EJ14OXhQMXkpbUpYJxS8jzkzSI+1Eb67kH4j5QhnR9H+w\nw2S9AgMBAAGjQjBAMA4GA1UdDwEB/wQEAwIBhjAPBgNVHRMBAf8EBTADAQH/MB0G\nA1UdDgQWBBS8ASq2Mkh0UsZe4fIvzs4mCSfTmjANBgkqhkiG9w0BAQsFAAOCAQEA\nB3q1mzdJc3VY17JDCaRPKY/Veni3oZ6zGJ9r9LnWKE7IO9AxBAKYGMELHMf9httQ\nmRZrXiGQi/tYXLlFFk7IcIID6iOOZLLagDhCQJPi52dGhBB4a2EnBvGF9OJI4zW9\nJw04HNR5cF1cxWIrRdGJAX/kbs/M9jz0STlXziVNwiAYeGIxkjq/fwKk6weai8iI\nwQVu6GsbtSBL8f4FRCk2vK6a1MD54BQDasRShdi3k/DJAEtY92muu13M0jc48mCM\ndEVeXxM8rIrG6LavziTwtO+jkFyWjlRyNbSr6il+neSWJ7ZspTo+ZYFy55CAh5ws\nwk6Ljo2wlQgfGflJbKcxAw==\n-----END CERTIFICATE-----\n">>},
   {<<"autogeneratedClientCert">>,
    <<"-----BEGIN CERTIFICATE-----\nMIIDWjCCAkKgAwIBAgIIGD/HyA7ClpEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjJaFw0y\nNzA4MTcxODQ3MjJaMDAxLjAsBgNVBAMTJUNvdWNoYmFzZSBJbnRlcm5hbCBDbGll\nbnQgKDExNjU1MDk1MikwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDg\nAaBAyJnrm54fDG0xlsEQH9E51WTLpkwtleiz980c498XKRPsgRWnjY920KTUIH2/\ndQcryFyvGTggbCNUUsAqs6qWzDX6iZW6uhcXUh9ujNASr1VNBMLPY60ejukP9Jbq\nxlTfAYvoidlCAZ43nBN17IbZjObU9wEL5AJVX+DLfxX9K0uTI37qWIX6lODZMnek\nj8dGQcmpjyB0ktQaFPLCfzIyXDoA0XE4zsxXW+qX4Xji5cwFwgUgJ3E1IYamZJZv\nF3WkWl7ai8xuos3gTqe41d4rvGemY5Pxao60ClgbgFPaimvOFT7Is2h4TzVQI8f7\nonIen4hIAiwE18Iz4WRJAgMBAAGjgYMwgYAwDgYDVR0PAQH/BAQDAgWgMBMGA1Ud\nJQQMMAoGCCsGAQUFBwMCMAwGA1UdEwEB/wQCMAAwHwYDVR0jBBgwFoAUvAEqtjJI\ndFLGXuHyL87OJgkn05owKgYDVR0RBCMwIYEfaW50ZXJuYWxAaW50ZXJuYWwuY291\nY2hiYXNlLmNvbTANBgkqhkiG9w0BAQsFAAOCAQEAjrpupGzgLn2L0Zf3xDsp7paD\nNHbJDPJeJAeG3u4nAur7tcZqAQbUgzkIPe9IFvz1dfdS9CRfyUYLzjcGK7M6Nzyt\nhgAwmdTOjAxTtcjVstxSdyhSxLORZnyMQcFpzJH+weMhVe5khi/T3nDWyR9yqwW0\nw9iReXBJhh/QyE+EBLMYJ58gXG5qQOHiOVw0ePQmQtoQhkjEM2jWVJegD+boNmOK\nKoCXNSxWZB2dC1MOQxtvA5ggysGIVOYH1ASMyXyZszSCRjKfGDzTYyJSfnBACijg\nWXt8NyZIaucN/aXR9H4CzrC077tJz9hihz0Ld4uhaIbVddLOY13TGq/V/0H6cg==\n-----END CERTIFICATE-----\n">>},
   {<<"autogeneratedClientKey">>,<<"********">>},
   {<<"autogeneratedCert">>,
    <<"-----BEGIN CERTIFICATE-----\nMIIDOjCCAiKgAwIBAgIIGD/HyAMALgMwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjJaFw0y\nNzA4MTcxODQ3MjJaMCoxKDAmBgNVBAMTH0NvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nZGIyLmxhbikwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC63r7EIhIk\nOyMwE9mw7bzVJfstC9FfdZvk0bP8vxZkuhNt/bVcND8p00ihMP8FCbfsa5d8VgWC\nDoao/+R+8KpeXDZeXWTzOfGPz6ySJpDMUbPsj/pymSkYBvwsl0iBlPrBYDVlHe6C\nIFfKfnfHOsS9itHFANxppJ+spLeJP7Na1p/rOGFELp9Y0pYnDvAccES6DCwgzMKr\n4lk/1l1L4jJkyHzK0hQ5UutHe7jGBhOxRoIyTCPO5wf6miZJNlKxA/1zwCJuLaft\ny4N6DDBMIs9R/dS02i1IrhKLDvC/7OnK/BkXi4HKLKY/wE3YNBmgMNXEwxUMeqZk\n2IUILCZn4W9fAgMBAAGjajBoMA4GA1UdDwEB/wQEAwIFoDATBgNVHSUEDDAKBggr\nBgEFBQcDATAMBgNVHRMBAf8EAjAAMB8GA1UdIwQYMBaAFLwBKrYySHRSxl7h8i/O\nziYJJ9OaMBIGA1UdEQQLMAmCB2RiMi5sYW4wDQYJKoZIhvcNAQELBQADggEBADkD\n1qqpKjJY4cyeoPzdD5mW7I12+dWN/oW0yUQ4heVH9TZzgThchsucMQvFg3vpZ9j5\nSbxsH9h23M26Cv8PxOjpu8lmlYAtENDDGnagAiwDJuMtSeBkKw36xzLrB4jj8UZ+\n8k+5CLVUzncPt9J9o7Lgm+wgavlprqSMJLUGfAc+O0qGzN+mQk1UeM2a8++Ty97Q\neks+wPVQCPbprWQU7k7F8mSepoAWHglkhfkBjBXgHGvGMIMGDv68l7yfNBJ9yRoJ\nO5cAQgoNiHZDzZ9clPCCGeE+JQnxNQCMOIxq7xhnTrYgBeU9hWu0hBcrHR6ZwO+m\n6MNtblxGbkNee9HO7LY=\n-----END CERTIFICATE-----\n">>},
   {<<"autogeneratedKey">>,<<"********">>}]}}
[ns_server:debug,2025-05-15T18:47:22.802Z,ns_1@172.19.0.4:cb_dist<0.2168.0>:cb_dist:info_msg:1098]cb_dist: Accepted new connection from <0.2171.0> DistCtrl #Port<0.112>: {con,
                                                                         #Ref<0.2327127311.3650355201.220791>,
                                                                         inet_tcp_dist,
                                                                         undefined,
                                                                         undefined}
[ns_server:debug,2025-05-15T18:47:22.803Z,ns_1@172.19.0.4:net_kernel<0.2170.0>:cb_dist:info_msg:1098]cb_dist: Accepting connection from <0.2171.0> using module inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:22.803Z,ns_1@172.19.0.4:cb_dist<0.2168.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.2327127311.3650355201.220791>,
                                  inet_tcp_dist,<0.2409.0>,
                                  #Ref<0.2327127311.3650355207.215827>}
[ns_server:debug,2025-05-15T18:47:22.803Z,ns_1@172.19.0.4:cb_dist<0.2168.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.2327127311.3650355201.220791>,
                               inet_tcp_dist,<0.2409.0>,
                               #Ref<0.2327127311.3650355207.215827>}
[error_logger:info,2025-05-15T18:47:22.803Z,ns_1@172.19.0.4:net_kernel<0.2170.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.2409.0>,shutdown}}
[cluster:debug,2025-05-15T18:47:22.881Z,ns_1@172.19.0.4:ns_cluster<0.273.0>:ns_cluster:post_json_to_joinee:927]Reply from [https,"db2.lan",18091,"/engageCluster2"]:
{ok,{[{<<"availableStorage">>,
       {[{<<"hdd">>,
          [{[{<<"path">>,<<"/">>},
             {<<"sizeKBytes">>,1056557396},
             {<<"usagePercent">>,2}]},
           {[{<<"path">>,<<"/dev">>},
             {<<"sizeKBytes">>,65536},
             {<<"usagePercent">>,0}]},
           {[{<<"path">>,<<"/dev/shm">>},
             {<<"sizeKBytes">>,65536},
             {<<"usagePercent">>,0}]},
           {[{<<"path">>,<<"/etc/resolv.conf">>},
             {<<"sizeKBytes">>,1056557396},
             {<<"usagePercent">>,2}]},
           {[{<<"path">>,<<"/etc/hostname">>},
             {<<"sizeKBytes">>,1056557396},
             {<<"usagePercent">>,2}]},
           {[{<<"path">>,<<"/etc/hosts">>},
             {<<"sizeKBytes">>,1056557396},
             {<<"usagePercent">>,2}]},
           {[{<<"path">>,<<"/opt/couchbase/var">>},
             {<<"sizeKBytes">>,482797652},
             {<<"usagePercent">>,92}]},
           {[{<<"path">>,<<"/proc/kcore">>},
             {<<"sizeKBytes">>,65536},
             {<<"usagePercent">>,0}]},
           {[{<<"path">>,<<"/proc/keys">>},
             {<<"sizeKBytes">>,65536},
             {<<"usagePercent">>,0}]},
           {[{<<"path">>,<<"/proc/timer_list">>},
             {<<"sizeKBytes">>,65536},
             {<<"usagePercent">>,0}]},
           {[{<<"path">>,<<"/proc/scsi">>},
             {<<"sizeKBytes">>,4012680},
             {<<"usagePercent">>,0}]},
           {[{<<"path">>,<<"/sys/firmware">>},
             {<<"sizeKBytes">>,4012680},
             {<<"usagePercent">>,0}]}]}]}},
      {<<"storageTotals">>,
       {[{<<"ram">>,
          {[{<<"total">>,8217968640},
            {<<"quotaTotal">>,3571449856},
            {<<"quotaUsed">>,0},
            {<<"used">>,4306186240},
            {<<"usedByData">>,0},
            {<<"quotaUsedPerNode">>,0},
            {<<"quotaTotalPerNode">>,3571449856}]}},
         {<<"hdd">>,
          {[{<<"total">>,494384795648},
            {<<"quotaTotal">>,494384795648},
            {<<"used">>,454834011996},
            {<<"usedByData">>,0},
            {<<"free">>,39550783652}]}}]}},
      {<<"storage">>,
       {[{<<"ssd">>,[]},
         {<<"hdd">>,
          [{[{<<"path">>,<<"/opt/couchbase/var/lib/couchbase/data">>},
             {<<"index_path">>,<<"/opt/couchbase/var/lib/couchbase/data">>},
             {<<"cbas_dirs">>,[<<"/opt/couchbase/var/lib/couchbase/data">>]},
             {<<"eventing_path">>,<<"/opt/couchbase/var/lib/couchbase/data">>},
             {<<"java_home">>,<<>>},
             {<<"quotaMb">>,<<"none">>},
             {<<"state">>,<<"ok">>}]}]}]}},
      {<<"clusterMembership">>,<<"active">>},
      {<<"recoveryType">>,<<"none">>},
      {<<"status">>,<<"healthy">>},
      {<<"otpNode">>,<<"ns_1@db2.lan">>},
      {<<"thisNode">>,true},
      {<<"hostname">>,<<"db2.lan:8091">>},
      {<<"nodeUUID">>,<<"b0a71fbd3e8ac51d55b35452995f45cf">>},
      {<<"clusterCompatibility">>,458758},
      {<<"version">>,<<"7.6.2-3721-enterprise">>},
      {<<"os">>,<<"aarch64-unknown-linux-gnu">>},
      {<<"cpuCount">>,10},
      {<<"ports">>,
       {[{<<"direct">>,11210},
         {<<"httpsMgmt">>,18091},
         {<<"httpsCAPI">>,18092},
         {<<"distTCP">>,21100},
         {<<"distTLS">>,21150}]}},
      {<<"services">>,[<<"kv">>]},
      {<<"nodeEncryption">>,false},
      {<<"nodeEncryptionClientCertVerification">>,false},
      {<<"addressFamilyOnly">>,false},
      {<<"configuredHostname">>,<<"db2.lan:8091">>},
      {<<"addressFamily">>,<<"inet">>},
      {<<"externalListeners">>,
       [{[{<<"afamily">>,<<"inet">>},{<<"nodeEncryption">>,false}]}]},
      {<<"serverGroup">>,<<"Group 1">>},
      {<<"couchApiBase">>,<<"http://db2.lan:8092/">>},
      {<<"couchApiBaseHTTPS">>,<<"https://db2.lan:18092/">>},
      {<<"nodeHash">>,6508283},
      {<<"systemStats">>,
       {[{<<"cpu_utilization_rate">>,3.827703377977286},
         {<<"cpu_stolen_rate">>,0},
         {<<"swap_total">>,1073737728},
         {<<"swap_used">>,3956736},
         {<<"mem_total">>,8217968640},
         {<<"mem_free">>,4638806016},
         {<<"mem_limit">>,8217968640},
         {<<"cpu_cores_available">>,10},
         {<<"allocstall">>,17}]}},
      {<<"interestingStats">>,{[]}},
      {<<"uptime">>,<<"42">>},
      {<<"memoryTotal">>,8217968640},
      {<<"memoryFree">>,4638806016},
      {<<"mcdMemoryReserved">>,6269},
      {<<"mcdMemoryAllocated">>,6269},
      {<<"memoryQuota">>,3406},
      {<<"queryMemoryQuota">>,0},
      {<<"indexMemoryQuota">>,512},
      {<<"ftsMemoryQuota">>,512},
      {<<"cbasMemoryQuota">>,1549},
      {<<"eventingMemoryQuota">>,256}]}}
[cluster:debug,2025-05-15T18:47:22.882Z,ns_1@172.19.0.4:ns_cluster<0.273.0>:ns_cluster:get_port_from_epmd:1093]port_please('ns_1@db2.lan', inet, false) = {ok,21100}
[ns_server:debug,2025-05-15T18:47:22.882Z,ns_1@172.19.0.4:ns_cluster<0.273.0>:ns_cluster:check_host_port_connectivity:656]Successfully checked TCP connectivity to "db2.lan":21100
[ns_server:debug,2025-05-15T18:47:22.882Z,ns_1@172.19.0.4:ns_cluster<0.273.0>:chronicle_master:call:71]Calling chronicle_master with {add_replica,'ns_1@db2.lan',undefined,
                                  [index,kv,n1ql]}
[ns_server:debug,2025-05-15T18:47:22.906Z,ns_1@172.19.0.4:<0.2326.0>:chronicle_master:handle_oper:342]Starting kv operation {add_replica,'ns_1@db2.lan',undefined,[index,kv,n1ql]} with lock <<"49d5762e287786c3127905510f7a4115">>
[ns_server:debug,2025-05-15T18:47:22.906Z,ns_1@172.19.0.4:<0.2326.0>:ns_cluster_membership:add_node:308]Add node 'ns_1@db2.lan', GroupUUID = undefined, Services = [index,kv,n1ql]
[ns_server:debug,2025-05-15T18:47:22.935Z,ns_1@172.19.0.4:<0.2326.0>:chronicle_master:handle_oper:345]Starting topology operation {add_replica,'ns_1@db2.lan',undefined,
                                [index,kv,n1ql]} with lock <<"49d5762e287786c3127905510f7a4115">>
[ns_server:debug,2025-05-15T18:47:22.935Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: unfinished_topology_operation, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                  15})
{regular,{add_replica,'ns_1@db2.lan',undefined,[index,kv,n1ql]},
         <<"49d5762e287786c3127905510f7a4115">>,
         #Ref<0.2327127311.3650355203.217418>}
[ns_server:debug,2025-05-15T18:47:22.935Z,ns_1@172.19.0.4:mb_master<0.2313.0>:mb_master:update_peers:543]List of peers has changed from ['ns_1@172.19.0.4'] to ['ns_1@172.19.0.4',
                                                       'ns_1@db2.lan']
[ns_server:debug,2025-05-15T18:47:22.936Z,ns_1@172.19.0.4:ns_cookie_manager<0.238.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[ns_server:debug,2025-05-15T18:47:22.936Z,ns_1@172.19.0.4:<0.2415.0>:ns_node_disco:do_nodes_wanted_updated_fun:210]ns_node_disco: nodes_wanted updated: ['ns_1@172.19.0.4','ns_1@db2.lan'], with cookie: {sanitized,
                                                                                       <<"biSYTaQUFDVndTs/b+IcDmD2L0woktv9naRdv3GCK6A=">>}
[ns_server:debug,2025-05-15T18:47:22.936Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: nodes_wanted, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,15})
['ns_1@172.19.0.4','ns_1@db2.lan']
[error_logger:info,2025-05-15T18:47:22.936Z,ns_1@172.19.0.4:net_kernel<0.2170.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'ns_1@db2.lan',
                          {6513440,#Ref<0.2327127311.3650486282.215961>}}}
[ns_server:debug,2025-05-15T18:47:22.936Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@db2.lan',membership}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                     15})
inactiveAdded
[ns_server:debug,2025-05-15T18:47:22.936Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@db2.lan',services}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                   15})
[index,kv,n1ql]
[ns_server:debug,2025-05-15T18:47:22.936Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: server_groups, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,15})
[[{uuid,<<"0">>},
  {name,<<"Group 1">>},
  {nodes,['ns_1@172.19.0.4','ns_1@db2.lan']}]]
[ns_server:debug,2025-05-15T18:47:22.937Z,ns_1@172.19.0.4:net_kernel<0.2170.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'ns_1@db2.lan' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:22.937Z,ns_1@172.19.0.4:cb_dist<0.2168.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.2327127311.3650355201.220805>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:47:22.937Z,ns_1@172.19.0.4:cb_dist<0.2168.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.2327127311.3650355201.220805>,
                                  inet_tcp_dist,<0.2416.0>,
                                  #Ref<0.2327127311.3650355201.220808>}
[error_logger:info,2025-05-15T18:47:22.939Z,ns_1@172.19.0.4:net_kernel<0.2170.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.2416.0>,{recv_challenge_ack_failed,{error,closed}}}}
[ns_server:debug,2025-05-15T18:47:22.939Z,ns_1@172.19.0.4:cb_dist<0.2168.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.2327127311.3650355201.220805>,
                               inet_tcp_dist,<0.2416.0>,
                               #Ref<0.2327127311.3650355201.220808>}
[error_logger:info,2025-05-15T18:47:22.939Z,ns_1@172.19.0.4:net_kernel<0.2170.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'ns_1@db2.lan'}}
[error_logger:info,2025-05-15T18:47:22.939Z,ns_1@172.19.0.4:net_kernel<0.2170.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{disconnect,'ns_1@db2.lan'}}
[ns_server:debug,2025-05-15T18:47:22.939Z,ns_1@172.19.0.4:<0.2415.0>:ns_node_disco:do_nodes_wanted_updated_fun:213]ns_node_disco: nodes_wanted pong: ['ns_1@172.19.0.4'], with cookie: {sanitized,
                                                                     <<"biSYTaQUFDVndTs/b+IcDmD2L0woktv9naRdv3GCK6A=">>}
[error_logger:info,2025-05-15T18:47:22.957Z,ns_1@172.19.0.4:net_kernel<0.2170.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'ns_1@db2.lan',
                          {6513441,#Ref<0.2327127311.3650486282.215961>}}}
[ns_server:debug,2025-05-15T18:47:22.958Z,ns_1@172.19.0.4:net_kernel<0.2170.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'ns_1@db2.lan' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:22.958Z,ns_1@172.19.0.4:cb_dist<0.2168.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.2327127311.3650355205.217050>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:47:22.958Z,ns_1@172.19.0.4:cb_dist<0.2168.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.2327127311.3650355205.217050>,
                                  inet_tcp_dist,<0.2422.0>,
                                  #Ref<0.2327127311.3650355205.217053>}
[error_logger:info,2025-05-15T18:47:22.960Z,ns_1@172.19.0.4:net_kernel<0.2170.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.2422.0>,{recv_challenge_ack_failed,{error,closed}}}}
[ns_server:debug,2025-05-15T18:47:22.960Z,ns_1@172.19.0.4:cb_dist<0.2168.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.2327127311.3650355205.217050>,
                               inet_tcp_dist,<0.2422.0>,
                               #Ref<0.2327127311.3650355205.217053>}
[error_logger:info,2025-05-15T18:47:22.960Z,ns_1@172.19.0.4:net_kernel<0.2170.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'ns_1@db2.lan'}}
[chronicle:info,2025-05-15T18:47:22.960Z,ns_1@172.19.0.4:chronicle_proposer<0.2185.0>:chronicle_proposer:handle_down:1142]Observed agent {chronicle_agent,'ns_1@db2.lan'} on peer 'ns_1@db2.lan' go down with reason noconnection
[ns_server:debug,2025-05-15T18:47:22.966Z,ns_1@172.19.0.4:<0.2326.0>:chronicle_master:handle_topology_oper:332]Cluster info: #{committed_seqno => 17,compat_version => 0,
                config =>
                    {log_entry,<<"5aab03dbecad06b50ffb474da59a00c9">>,
                        {4,'ns_1@172.19.0.4'},
                        17,
                        {config,
                            {<<"1dd5991463d519e23729dda2c80c376b">>,0,1},
                            0,<<"49d5762e287786c3127905510f7a4115">>,
                            #{'ns_1@172.19.0.4' =>
                                  #{id =>
                                        <<"1dd5991463d519e23729dda2c80c376b">>,
                                    role => voter},
                              'ns_1@db2.lan' =>
                                  #{id =>
                                        <<"9f6894b51ffa497cdfc8b792d9e6726b">>,
                                    role => replica}},
                            undefined,
                            #{chronicle_config_rsm =>
                                  {rsm_config,chronicle_config_rsm,[]},
                              kv => {rsm_config,chronicle_kv,[]}},
                            #{},undefined,
                            [{<<"5aab03dbecad06b50ffb474da59a00c9">>,0}]}},
                history_id => <<"5aab03dbecad06b50ffb474da59a00c9">>}
[ns_server:debug,2025-05-15T18:47:22.966Z,ns_1@172.19.0.4:<0.2326.0>:chronicle_master:remove_oper_key:309]Removing operation key with lock <<"49d5762e287786c3127905510f7a4115">>
[cluster:info,2025-05-15T18:47:22.988Z,ns_1@172.19.0.4:ns_cluster<0.273.0>:ns_cluster:node_add_transaction_finish:1318]Started node add transaction by adding node 'ns_1@db2.lan' to nodes_wanted (group: undefined)
[ns_server:debug,2025-05-15T18:47:22.988Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:handle_info:42]delete (key: unfinished_topology_operation, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                  18})
[cluster:debug,2025-05-15T18:47:22.989Z,ns_1@172.19.0.4:ns_cluster<0.273.0>:ns_cluster:post_json_to_joinee:920]Posting the following to [https,"db2.lan",18091,"/completeJoin"]:
{{[{<<"targetNode">>,'ns_1@db2.lan'},
   {<<"requestedServices">>,[index,kv,n1ql]},
   {<<"chronicleInfo">>,
    <<"g3QAAAAEZAAPY29tbWl0dGVkX3NlcW5vYRFkAA5jb21wYXRfdmVyc2lvbmEAZAAGY29uZmlnaAVkAAlsb2dfZW50cnltAAAAIDVhYWIwM2RiZWNhZDA2YjUwZmZiNDc0ZGE1OWEwMGM5aAJhBGQAD25zXzFAMTcyLjE5LjAuNGERaApkAAZjb25maWdoA20AAAAgMWRkNTk5MTQ2M2Q1MTllMjM3MjlkZGEyYzgwYzM3NmJhAGEBYQBtAAAAIDQ5ZDU3NjJlMjg3Nzg2YzMxMjc5MDU1MTBmN2E0MTE1dAAAAAJkAA9uc18xQDE3Mi4xOS4wLjR0AAAAAmQAAmlkbQAAACAxZGQ1OTkxNDYzZDUxOWUyMzcyOWRkYTJjODBjMzc2YmQABHJvbGVkAAV2b3RlcmQADG5zXzFAZGIyLmxhbnQAAAACZAACaWRtAAAAIDlmNjg5NGI1MWZmYTQ5N2NkZmM4Yjc5MmQ5ZTY3MjZiZAAEcm9sZWQAB3JlcGxpY2FkAAl1bmRlZmluZWR0AAAAAmQAFGNocm9uaWNsZV9jb25maWdfcnNtaANkAApyc21fY29uZmlnZAAUY2hyb25pY2xlX2NvbmZpZ19yc21qZAACa3ZoA2QACnJzbV9jb25maWdkAAxjaHJvbmljbGVfa3ZqdAAAAABkAAl1bmRlZmluZWRsAAAAAWgCbQAAACA1YWFiMDNkYmVjYWQwNmI1MGZmYjQ3NGRhNTlhMDBjOWEAamQACmhpc3RvcnlfaWRtAAAAIDVhYWIwM2RiZWNhZDA2YjUwZmZiNDc0ZGE1OWEwMGM5">>},
   {availableStorage,{[{hdd,[{[{path,<<"/">>},
                               {sizeKBytes,1056557396},
                               {usagePercent,2}]},
                             {[{path,<<"/dev">>},
                               {sizeKBytes,65536},
                               {usagePercent,0}]},
                             {[{path,<<"/dev/shm">>},
                               {sizeKBytes,65536},
                               {usagePercent,0}]},
                             {[{path,<<"/etc/resolv.conf">>},
                               {sizeKBytes,1056557396},
                               {usagePercent,2}]},
                             {[{path,<<"/etc/hostname">>},
                               {sizeKBytes,1056557396},
                               {usagePercent,2}]},
                             {[{path,<<"/etc/hosts">>},
                               {sizeKBytes,1056557396},
                               {usagePercent,2}]},
                             {[{path,<<"/opt/couchbase/var">>},
                               {sizeKBytes,482797652},
                               {usagePercent,92}]},
                             {[{path,<<"/proc/kcore">>},
                               {sizeKBytes,65536},
                               {usagePercent,0}]},
                             {[{path,<<"/proc/keys">>},
                               {sizeKBytes,65536},
                               {usagePercent,0}]},
                             {[{path,<<"/proc/timer_list">>},
                               {sizeKBytes,65536},
                               {usagePercent,0}]},
                             {[{path,<<"/proc/scsi">>},
                               {sizeKBytes,4012680},
                               {usagePercent,0}]},
                             {[{path,<<"/sys/firmware">>},
                               {sizeKBytes,4012680},
                               {usagePercent,0}]}]}]}},
   {storageTotals,{[{ram,{[{total,8217968640},
                           {quotaTotal,3221225472},
                           {quotaUsed,0},
                           {used,4251119616},
                           {usedByData,0},
                           {quotaUsedPerNode,0},
                           {quotaTotalPerNode,3221225472}]}},
                    {hdd,{[{total,494384795648},
                           {quotaTotal,494384795648},
                           {used,454834011996},
                           {usedByData,0},
                           {free,39550783652}]}}]}},
   {storage,{[{ssd,[]},
              {hdd,[{[{path,<<"/opt/couchbase/var/lib/couchbase/data">>},
                      {index_path,<<"/opt/couchbase/var/lib/couchbase/data">>},
                      {cbas_dirs,[<<"/opt/couchbase/var/lib/couchbase/data">>]},
                      {eventing_path,<<"/opt/couchbase/var/lib/couchbase/data">>},
                      {java_home,<<>>},
                      {quotaMb,none},
                      {state,ok}]}]}]}},
   {clusterMembership,active},
   {recoveryType,none},
   {status,<<"healthy">>},
   {otpNode,'ns_1@172.19.0.4'},
   {thisNode,true},
   {hostname,<<"172.19.0.4:8091">>},
   {nodeUUID,<<"28569ac00b9c1d7c50e39741027d428c">>},
   {clusterCompatibility,458758},
   {version,<<"7.6.2-3721-enterprise">>},
   {os,<<"aarch64-unknown-linux-gnu">>},
   {cpuCount,10},
   {ports,{[{direct,11210},
            {httpsMgmt,18091},
            {httpsCAPI,18092},
            {distTCP,21100},
            {distTLS,21150}]}},
   {services,[index,kv,n1ql]},
   {nodeEncryption,false},
   {nodeEncryptionClientCertVerification,false},
   {addressFamilyOnly,false},
   {configuredHostname,<<"172.19.0.4:8091">>},
   {addressFamily,inet},
   {externalListeners,[{[{afamily,inet},{nodeEncryption,false}]}]},
   {serverGroup,<<"Group 1">>},
   {otpCookie,{sanitized,<<"biSYTaQUFDVndTs/b+IcDmD2L0woktv9naRdv3GCK6A=">>}},
   {couchApiBase,<<"http://172.19.0.4:8092/">>},
   {couchApiBaseHTTPS,<<"https://172.19.0.4:18092/">>},
   {nodeHash,38045879},
   {systemStats,{[{cpu_utilization_rate,3.89883035089008},
                  {cpu_stolen_rate,0},
                  {swap_total,1073737728},
                  {swap_used,3956736},
                  {mem_total,8217968640},
                  {mem_free,4805804032},
                  {mem_limit,8217968640},
                  {cpu_cores_available,10},
                  {allocstall,17}]}},
   {interestingStats,{[]}},
   {uptime,<<"41">>},
   {memoryTotal,8217968640},
   {memoryFree,4805804032},
   {mcdMemoryReserved,6269},
   {mcdMemoryAllocated,6269},
   {memoryQuota,3072},
   {queryMemoryQuota,0},
   {indexMemoryQuota,512},
   {ftsMemoryQuota,512},
   {cbasMemoryQuota,1024},
   {eventingMemoryQuota,256}]}}
[error_logger:error,2025-05-15T18:47:23.024Z,ns_1@172.19.0.4:<0.514.0>:ale_error_logger_handler:do_log:101]
=========================CRASH REPORT=========================
  crasher:
    initial call: ns_log:babysitter_log_consumption_loop_tramp/0
    pid: <0.514.0>
    registered_name: []
    exception exit: {{nodedown,'babysitter_of_ns_1@cb.local'},
                     {gen_server,call,
                         [{ns_babysitter_log,'babysitter_of_ns_1@cb.local'},
                          consume,infinity]}}
      in function  gen_server:call/3 (gen_server.erl, line 385)
      in call from ns_log:babysitter_log_consumption_loop/0 (src/ns_log.erl, line 66)
      in call from misc:delaying_crash/2 (src/misc.erl, line 1808)
    ancestors: [<0.513.0>,<0.512.0>,<0.511.0>,ns_server_sup,
                  ns_server_nodes_sup,<0.290.0>,ns_server_cluster_sup,
                  root_sup,<0.154.0>]
    message_queue_len: 5
    messages: [{[alias|#Ref<0.2327127311.3650420741.215685>],superseded},
                  {[alias|#Ref<16336.2327127311.3650420741.215687>],
                   superseded},
                  {[alias|#Ref<16336.2327127311.3650420739.216019>],
                   superseded},
                  {[alias|#Ref<16336.2327127311.3650420739.216261>],
                   superseded},
                  {[alias|#Ref<16336.2327127311.3650420738.216347>],
                   superseded}]
    links: [<0.513.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 4185
    stack_size: 28
    reductions: 14632
  neighbours:

[error_logger:error,2025-05-15T18:47:23.024Z,ns_1@172.19.0.4:<0.513.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {<0.513.0>,suppress_max_restart_intensity}
    errorContext: child_terminated
    reason: {{nodedown,'babysitter_of_ns_1@cb.local'},
             {gen_server,call,
                         [{ns_babysitter_log,'babysitter_of_ns_1@cb.local'},
                          consume,infinity]}}
    offender: [{pid,<0.514.0>},
               {id,ns_babysitter_log_consumer},
               {mfargs,{ns_log,start_link_babysitter_log_consumer,[]}},
               {restart_type,permanent},
               {significant,false},
               {shutdown,1000},
               {child_type,worker}]

[error_logger:info,2025-05-15T18:47:23.024Z,ns_1@172.19.0.4:<0.513.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.513.0>,suppress_max_restart_intensity}
    started: [{pid,<0.2429.0>},
              {id,ns_babysitter_log_consumer},
              {mfargs,{ns_log,start_link_babysitter_log_consumer,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:error,2025-05-15T18:47:23.038Z,ns_1@172.19.0.4:ns_ports_setup<0.2158.0>:ale_error_logger_handler:do_log:101]
=========================CRASH REPORT=========================
  crasher:
    initial call: ns_ports_setup:setup_body_tramp/0
    pid: <0.2158.0>
    registered_name: ns_ports_setup
    exception error: distribution_not_started
      in function  auth:set_cookie/2 (auth.erl, line 143)
      in call from ns_server:get_babysitter_node/0 (src/ns_server.erl, line 330)
      in call from ns_ports_setup:set_children/2 (src/ns_ports_setup.erl, line 61)
      in call from ns_ports_setup:set_children_and_loop/3 (src/ns_ports_setup.erl, line 77)
      in call from misc:delaying_crash/2 (src/misc.erl, line 1808)
    ancestors: [<0.674.0>,<0.673.0>,<0.672.0>,ns_server_sup,
                  ns_server_nodes_sup,<0.290.0>,ns_server_cluster_sup,
                  root_sup,<0.154.0>]
    message_queue_len: 86
    messages: [check_children_update,check_children_update,
                  check_children_update,check_children_update,
                  check_children_update,check_children_update,
                  check_children_update,check_children_update,
                  check_children_update,check_children_update,
                  check_children_update,check_children_update,
                  check_children_update,check_children_update,
                  check_children_update,check_children_update,
                  check_children_update,check_children_update,
                  check_children_update,check_children_update,
                  check_children_update,check_children_update,
                  check_children_update,check_children_update,
                  check_children_update,check_children_update,
                  check_children_update,check_children_update,
                  check_children_update,check_children_update,
                  check_children_update,check_children_update,
                  check_children_update,check_children_update,
                  check_children_update,check_children_update,
                  check_children_update,check_children_update,
                  check_children_update,check_children_update,
                  check_children_update,check_children_update,
                  check_children_update,check_children_update,
                  check_children_update,check_children_update,
                  check_children_update,check_children_update,
                  check_children_update|...]
    links: [<0.2159.0>,<0.2161.0>,<0.674.0>]
    dictionary: [{'ns_ports_setup-cbq-engine-available',
                      "/opt/couchbase/bin/cbq-engine"},
                  {'ns_ports_setup-backup-available',
                      "/opt/couchbase/bin/backup"},
                  {'ns_ports_setup-indexer-available',
                      "/opt/couchbase/bin/indexer"},
                  {'ns_ports_setup-saslauthd-port-available',
                      "/opt/couchbase/bin/saslauthd-port"},
                  {'ns_ports_setup-goxdcr-available',
                      "/opt/couchbase/bin/goxdcr"},
                  {'ns_ports_setup-cbft-available',"/opt/couchbase/bin/cbft"},
                  {'ns_ports_setup-eventing-producer-available',
                      "/opt/couchbase/bin/eventing-producer"},
                  {'ns_ports_setup-cbas-available',"/opt/couchbase/bin/cbas"},
                  {'ns_ports_setup-projector-available',
                      "/opt/couchbase/bin/projector"}]
    trap_exit: false
    status: running
    heap_size: 4185
    stack_size: 28
    reductions: 12283
  neighbours:

[ns_server:debug,2025-05-15T18:47:23.038Z,ns_1@172.19.0.4:<0.2159.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.2158.0>} exited with reason {distribution_not_started,
                                                                                               [{auth,
                                                                                                 set_cookie,
                                                                                                 2,
                                                                                                 [{file,
                                                                                                   "auth.erl"},
                                                                                                  {line,
                                                                                                   143}]},
                                                                                                {ns_server,
                                                                                                 get_babysitter_node,
                                                                                                 0,
                                                                                                 [{file,
                                                                                                   "src/ns_server.erl"},
                                                                                                  {line,
                                                                                                   330}]},
                                                                                                {ns_ports_setup,
                                                                                                 set_children,
                                                                                                 2,
                                                                                                 [{file,
                                                                                                   "src/ns_ports_setup.erl"},
                                                                                                  {line,
                                                                                                   61}]},
                                                                                                {ns_ports_setup,
                                                                                                 set_children_and_loop,
                                                                                                 3,
                                                                                                 [{file,
                                                                                                   "src/ns_ports_setup.erl"},
                                                                                                  {line,
                                                                                                   77}]},
                                                                                                {misc,
                                                                                                 delaying_crash,
                                                                                                 2,
                                                                                                 [{file,
                                                                                                   "src/misc.erl"},
                                                                                                  {line,
                                                                                                   1808}]},
                                                                                                {proc_lib,
                                                                                                 init_p_do_apply,
                                                                                                 3,
                                                                                                 [{file,
                                                                                                   "proc_lib.erl"},
                                                                                                  {line,
                                                                                                   240}]}]}
[error_logger:error,2025-05-15T18:47:23.038Z,ns_1@172.19.0.4:<0.674.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {<0.674.0>,suppress_max_restart_intensity}
    errorContext: child_terminated
    reason: {distribution_not_started,
                [{auth,set_cookie,2,[{file,"auth.erl"},{line,143}]},
                 {ns_server,get_babysitter_node,0,
                     [{file,"src/ns_server.erl"},{line,330}]},
                 {ns_ports_setup,set_children,2,
                     [{file,"src/ns_ports_setup.erl"},{line,61}]},
                 {ns_ports_setup,set_children_and_loop,3,
                     [{file,"src/ns_ports_setup.erl"},{line,77}]},
                 {misc,delaying_crash,2,[{file,"src/misc.erl"},{line,1808}]},
                 {proc_lib,init_p_do_apply,3,
                     [{file,"proc_lib.erl"},{line,240}]}]}
    offender: [{pid,<0.2158.0>},
               {id,ns_ports_setup},
               {mfargs,{ns_ports_setup,start,[]}},
               {restart_type,permanent},
               {significant,false},
               {shutdown,brutal_kill},
               {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:23.038Z,ns_1@172.19.0.4:<0.2161.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {user_storage_events,<0.2158.0>} exited with reason {distribution_not_started,
                                                                                    [{auth,
                                                                                      set_cookie,
                                                                                      2,
                                                                                      [{file,
                                                                                        "auth.erl"},
                                                                                       {line,
                                                                                        143}]},
                                                                                     {ns_server,
                                                                                      get_babysitter_node,
                                                                                      0,
                                                                                      [{file,
                                                                                        "src/ns_server.erl"},
                                                                                       {line,
                                                                                        330}]},
                                                                                     {ns_ports_setup,
                                                                                      set_children,
                                                                                      2,
                                                                                      [{file,
                                                                                        "src/ns_ports_setup.erl"},
                                                                                       {line,
                                                                                        61}]},
                                                                                     {ns_ports_setup,
                                                                                      set_children_and_loop,
                                                                                      3,
                                                                                      [{file,
                                                                                        "src/ns_ports_setup.erl"},
                                                                                       {line,
                                                                                        77}]},
                                                                                     {misc,
                                                                                      delaying_crash,
                                                                                      2,
                                                                                      [{file,
                                                                                        "src/misc.erl"},
                                                                                       {line,
                                                                                        1808}]},
                                                                                     {proc_lib,
                                                                                      init_p_do_apply,
                                                                                      3,
                                                                                      [{file,
                                                                                        "proc_lib.erl"},
                                                                                       {line,
                                                                                        240}]}]}
[error_logger:info,2025-05-15T18:47:23.038Z,ns_1@172.19.0.4:<0.674.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.674.0>,suppress_max_restart_intensity}
    started: [{pid,<0.2430.0>},
              {id,ns_ports_setup},
              {mfargs,{ns_ports_setup,start,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:error,2025-05-15T18:47:23.038Z,ns_1@172.19.0.4:<0.2359.0>:ale_error_logger_handler:do_log:101]
=========================CRASH REPORT=========================
  crasher:
    initial call: terse_cluster_info_uploader:init/1
    pid: <0.2359.0>
    registered_name: []
    exception exit: {{distribution_not_started,
                         [{auth,set_cookie,2,[{file,"auth.erl"},{line,143}]},
                          {ns_server,get_babysitter_node,0,
                              [{file,"src/ns_server.erl"},{line,330}]},
                          {ns_ports_setup,set_children,2,
                              [{file,"src/ns_ports_setup.erl"},{line,61}]},
                          {ns_ports_setup,set_children_and_loop,3,
                              [{file,"src/ns_ports_setup.erl"},{line,77}]},
                          {misc,delaying_crash,2,
                              [{file,"src/misc.erl"},{line,1808}]},
                          {proc_lib,init_p_do_apply,3,
                              [{file,"proc_lib.erl"},{line,240}]}]},
                     {gen_server,call,[ns_ports_setup,sync,infinity]}}
      in function  gen_server:call/3 (gen_server.erl, line 385)
      in call from memcached_config_mgr:memcached_port_pid/0 (src/memcached_config_mgr.erl, line 155)
      in call from terse_cluster_info_uploader:init/1 (src/terse_cluster_info_uploader.erl, line 43)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.290.0>,
                  ns_server_cluster_sup,root_sup,<0.154.0>]
    message_queue_len: 3
    messages: [refresh,refresh,refresh]
    links: [<0.494.0>,<0.2360.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 987
    stack_size: 28
    reductions: 1685
  neighbours:

[error_logger:error,2025-05-15T18:47:23.038Z,ns_1@172.19.0.4:memcached_config_mgr<0.2357.0>:ale_error_logger_handler:do_log:101]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.2357.0>
    registered_name: memcached_config_mgr
    exception exit: {{distribution_not_started,
                         [{auth,set_cookie,2,[{file,"auth.erl"},{line,143}]},
                          {ns_server,get_babysitter_node,0,
                              [{file,"src/ns_server.erl"},{line,330}]},
                          {ns_ports_setup,set_children,2,
                              [{file,"src/ns_ports_setup.erl"},{line,61}]},
                          {ns_ports_setup,set_children_and_loop,3,
                              [{file,"src/ns_ports_setup.erl"},{line,77}]},
                          {misc,delaying_crash,2,
                              [{file,"src/misc.erl"},{line,1808}]},
                          {proc_lib,init_p_do_apply,3,
                              [{file,"proc_lib.erl"},{line,240}]}]},
                     {gen_server,call,[ns_ports_setup,sync,infinity]}}
      in function  gen_server:call/3 (gen_server.erl, line 385)
      in call from memcached_config_mgr:memcached_port_pid/0 (src/memcached_config_mgr.erl, line 155)
      in call from memcached_config_mgr:init/1 (src/memcached_config_mgr.erl, line 56)
    ancestors: [<0.700.0>,<0.699.0>,<0.698.0>,ns_server_sup,
                  ns_server_nodes_sup,<0.290.0>,ns_server_cluster_sup,
                  root_sup,<0.154.0>]
    message_queue_len: 0
    messages: []
    links: [<0.700.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 610
    stack_size: 28
    reductions: 1661
  neighbours:

[ns_server:debug,2025-05-15T18:47:23.039Z,ns_1@172.19.0.4:<0.2360.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {bucket_info_cache_invalidations,<0.2359.0>} exited with reason {{distribution_not_started,
                                                                                                 [{auth,
                                                                                                   set_cookie,
                                                                                                   2,
                                                                                                   [{file,
                                                                                                     "auth.erl"},
                                                                                                    {line,
                                                                                                     143}]},
                                                                                                  {ns_server,
                                                                                                   get_babysitter_node,
                                                                                                   0,
                                                                                                   [{file,
                                                                                                     "src/ns_server.erl"},
                                                                                                    {line,
                                                                                                     330}]},
                                                                                                  {ns_ports_setup,
                                                                                                   set_children,
                                                                                                   2,
                                                                                                   [{file,
                                                                                                     "src/ns_ports_setup.erl"},
                                                                                                    {line,
                                                                                                     61}]},
                                                                                                  {ns_ports_setup,
                                                                                                   set_children_and_loop,
                                                                                                   3,
                                                                                                   [{file,
                                                                                                     "src/ns_ports_setup.erl"},
                                                                                                    {line,
                                                                                                     77}]},
                                                                                                  {misc,
                                                                                                   delaying_crash,
                                                                                                   2,
                                                                                                   [{file,
                                                                                                     "src/misc.erl"},
                                                                                                    {line,
                                                                                                     1808}]},
                                                                                                  {proc_lib,
                                                                                                   init_p_do_apply,
                                                                                                   3,
                                                                                                   [{file,
                                                                                                     "proc_lib.erl"},
                                                                                                    {line,
                                                                                                     240}]}]},
                                                                                                {gen_server,
                                                                                                 call,
                                                                                                 [ns_ports_setup,
                                                                                                  sync,
                                                                                                  infinity]}}
[error_logger:error,2025-05-15T18:47:23.039Z,ns_1@172.19.0.4:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {local,ns_server_sup}
    errorContext: child_terminated
    reason: {{distribution_not_started,
                 [{auth,set_cookie,2,[{file,"auth.erl"},{line,143}]},
                  {ns_server,get_babysitter_node,0,
                      [{file,"src/ns_server.erl"},{line,330}]},
                  {ns_ports_setup,set_children,2,
                      [{file,"src/ns_ports_setup.erl"},{line,61}]},
                  {ns_ports_setup,set_children_and_loop,3,
                      [{file,"src/ns_ports_setup.erl"},{line,77}]},
                  {misc,delaying_crash,2,[{file,"src/misc.erl"},{line,1808}]},
                  {proc_lib,init_p_do_apply,3,
                      [{file,"proc_lib.erl"},{line,240}]}]},
             {gen_server,call,[ns_ports_setup,sync,infinity]}}
    offender: [{pid,<0.2359.0>},
               {id,terse_cluster_info_uploader},
               {mfargs,{terse_cluster_info_uploader,start_link,[]}},
               {restart_type,permanent},
               {significant,false},
               {shutdown,1000},
               {child_type,worker}]

[error_logger:error,2025-05-15T18:47:23.039Z,ns_1@172.19.0.4:<0.700.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {<0.700.0>,suppress_max_restart_intensity}
    errorContext: child_terminated
    reason: {{distribution_not_started,
                 [{auth,set_cookie,2,[{file,"auth.erl"},{line,143}]},
                  {ns_server,get_babysitter_node,0,
                      [{file,"src/ns_server.erl"},{line,330}]},
                  {ns_ports_setup,set_children,2,
                      [{file,"src/ns_ports_setup.erl"},{line,61}]},
                  {ns_ports_setup,set_children_and_loop,3,
                      [{file,"src/ns_ports_setup.erl"},{line,77}]},
                  {misc,delaying_crash,2,[{file,"src/misc.erl"},{line,1808}]},
                  {proc_lib,init_p_do_apply,3,
                      [{file,"proc_lib.erl"},{line,240}]}]},
             {gen_server,call,[ns_ports_setup,sync,infinity]}}
    offender: [{pid,<0.2357.0>},
               {id,memcached_config_mgr},
               {mfargs,{memcached_config_mgr,start_link,[]}},
               {restart_type,permanent},
               {significant,false},
               {shutdown,1000},
               {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:23.039Z,ns_1@172.19.0.4:memcached_config_mgr<0.2435.0>:memcached_config_mgr:memcached_port_pid:154]waiting for completion of initial ns_ports_setup round
[ns_server:debug,2025-05-15T18:47:23.039Z,ns_1@172.19.0.4:<0.2433.0>:memcached_config_mgr:memcached_port_pid:154]waiting for completion of initial ns_ports_setup round
[error_logger:info,2025-05-15T18:47:23.039Z,ns_1@172.19.0.4:ns_server_sup<0.494.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.2433.0>},
              {id,terse_cluster_info_uploader},
              {mfargs,{terse_cluster_info_uploader,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:23.039Z,ns_1@172.19.0.4:<0.700.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.700.0>,suppress_max_restart_intensity}
    started: [{pid,<0.2435.0>},
              {id,memcached_config_mgr},
              {mfargs,{memcached_config_mgr,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:23.040Z,ns_1@172.19.0.4:ns_ports_setup<0.2430.0>:ns_ports_manager:set_dynamic_children:48]Setting children [memcached,saslauthd_port,goxdcr,index,n1ql,projector]
[ns_server:debug,2025-05-15T18:47:23.040Z,ns_1@172.19.0.4:ns_ports_setup<0.2430.0>:ns_ports_setup:set_children:65]Monitor ns_child_ports_sup <16971.133.0>
[ns_server:debug,2025-05-15T18:47:23.040Z,ns_1@172.19.0.4:memcached_config_mgr<0.2435.0>:memcached_config_mgr:memcached_port_pid:156]ns_ports_setup seems to be ready
[ns_server:debug,2025-05-15T18:47:23.040Z,ns_1@172.19.0.4:<0.2433.0>:memcached_config_mgr:memcached_port_pid:156]ns_ports_setup seems to be ready
[ns_server:debug,2025-05-15T18:47:23.041Z,ns_1@172.19.0.4:memcached_config_mgr<0.2435.0>:memcached_config_mgr:find_port_pid_loop:164]Found memcached port <16971.139.0>
[ns_server:debug,2025-05-15T18:47:23.041Z,ns_1@172.19.0.4:<0.2433.0>:memcached_config_mgr:find_port_pid_loop:164]Found memcached port <16971.139.0>
[ns_server:debug,2025-05-15T18:47:23.041Z,ns_1@172.19.0.4:<0.2433.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":38,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true,\"hostname\":\"172.19.0.4\",\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[ns_server:debug,2025-05-15T18:47:23.042Z,ns_1@172.19.0.4:memcached_config_mgr<0.2435.0>:memcached_config_mgr:do_read_current_memcached_config:371]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2025-05-15T18:47:23.042Z,ns_1@172.19.0.4:memcached_config_mgr<0.2435.0>:memcached_config_mgr:init:100]found memcached port to be already active
[ns_server:info,2025-05-15T18:47:23.042Z,ns_1@172.19.0.4:memcached_config_mgr<0.2435.0>:memcached_config_mgr:push_tls_config:233]Pushing TLS config to memcached:
{[{<<"private key">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem">>},
  {<<"certificate chain">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem">>},
  {<<"CA file">>,<<"/opt/couchbase/var/lib/couchbase/config/certs/ca.pem">>},
  {<<"minimum version">>,<<"TLS 1.2">>},
  {<<"cipher list">>,
   {[{<<"TLS 1.2">>,<<"HIGH">>},
     {<<"TLS 1.3">>,
      <<"TLS_AES_256_GCM_SHA384:TLS_AES_128_GCM_SHA256:TLS_CHACHA20_POLY1305_SHA256">>}]}},
  {<<"cipher order">>,true},
  {<<"client cert auth">>,<<"disabled">>}]}
[ns_server:info,2025-05-15T18:47:23.044Z,ns_1@172.19.0.4:memcached_config_mgr<0.2435.0>:memcached_config_mgr:push_tls_config:237]Successfully pushed TLS config to memcached
[ns_server:debug,2025-05-15T18:47:23.149Z,ns_1@172.19.0.4:cb_dist<0.2168.0>:cb_dist:info_msg:1098]cb_dist: Accepted new connection from <0.2171.0> DistCtrl #Port<0.117>: {con,
                                                                         #Ref<0.2327127311.3650355201.220846>,
                                                                         inet_tcp_dist,
                                                                         undefined,
                                                                         undefined}
[ns_server:debug,2025-05-15T18:47:23.149Z,ns_1@172.19.0.4:net_kernel<0.2170.0>:cb_dist:info_msg:1098]cb_dist: Accepting connection from <0.2171.0> using module inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:23.149Z,ns_1@172.19.0.4:cb_dist<0.2168.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.2327127311.3650355201.220846>,
                                  inet_tcp_dist,<0.2444.0>,
                                  #Ref<0.2327127311.3650355201.220849>}
[chronicle:info,2025-05-15T18:47:23.150Z,ns_1@172.19.0.4:chronicle_proposer<0.2185.0>:chronicle_proposer:handle_nodeup:1093]Peer 'ns_1@db2.lan' came up
[ns_server:debug,2025-05-15T18:47:23.150Z,ns_1@172.19.0.4:<0.446.0>:doc_replicator:nodeup_monitoring_loop:145]got nodeup event. Considering ddocs replication
[user:info,2025-05-15T18:47:23.150Z,ns_1@172.19.0.4:ns_node_disco<0.537.0>:ns_node_disco:handle_info:163]Node 'ns_1@172.19.0.4' saw that node 'ns_1@db2.lan' came up. Tags: []
[ns_server:debug,2025-05-15T18:47:23.150Z,ns_1@172.19.0.4:users_replicator<0.434.0>:doc_replicator:loop:79]Replicating all docs to new nodes: ['ns_1@db2.lan']
[ns_server:debug,2025-05-15T18:47:23.150Z,ns_1@172.19.0.4:ns_node_disco_events<0.536.0>:ns_config_rep:handle_node_disco_event:513]Detected new nodes (['ns_1@db2.lan']).  Moving config around.
[ns_server:debug,2025-05-15T18:47:23.150Z,ns_1@172.19.0.4:users_replicator<0.434.0>:replicated_dets:select_from_table:312][dets] Starting select with {users_storage,[{'_',[],['$_']}],500}
[ns_server:info,2025-05-15T18:47:23.150Z,ns_1@172.19.0.4:ns_node_disco_events<0.536.0>:ns_node_disco_log:handle_event:40]ns_node_disco_log: nodes changed: ['ns_1@172.19.0.4','ns_1@db2.lan']
[ns_server:warn,2025-05-15T18:47:23.150Z,ns_1@172.19.0.4:users_replicator<0.434.0>:doc_replicator:loop:110]Remote server node {users_storage,'ns_1@db2.lan'} process down: noproc
[chronicle:debug,2025-05-15T18:47:23.151Z,ns_1@172.19.0.4:chronicle_proposer<0.2185.0>:chronicle_proposer:catchup_peers:1719]Catching up peer 'ns_1@db2.lan' from seqno 0
[chronicle:debug,2025-05-15T18:47:23.151Z,ns_1@172.19.0.4:chronicle_catchup<0.2186.0>:chronicle_catchup:spawn_catchup:151]Starting catchup for peer 'ns_1@db2.lan' at seqno 0
[ns_server:warn,2025-05-15T18:47:23.151Z,ns_1@172.19.0.4:<0.2447.0>:leader_lease_acquire_worker:handle_exception:244]Failed to acquire lease from 'ns_1@db2.lan': {exit,
                                              {noproc,
                                               {gen_server,call,
                                                [{leader_lease_agent,
                                                  'ns_1@db2.lan'},
                                                 {acquire_lease,
                                                  'ns_1@172.19.0.4',
                                                  <<"60878074b48f15926e51cf002c4824e4">>,
                                                  [{timeout,15000},
                                                   {period,15000}]},
                                                 infinity]}}}
[chronicle:debug,2025-05-15T18:47:23.151Z,ns_1@172.19.0.4:chronicle_proposer<0.2185.0>:chronicle_proposer:handle_catchup_result:981]Caught up peer 'ns_1@db2.lan' to seqno 18
[ns_server:debug,2025-05-15T18:47:23.210Z,ns_1@172.19.0.4:users_replicator<0.434.0>:doc_replicator:loop:79]Replicating all docs to new nodes: ['ns_1@db2.lan']
[ns_server:debug,2025-05-15T18:47:23.210Z,ns_1@172.19.0.4:users_replicator<0.434.0>:replicated_dets:select_from_table:312][dets] Starting select with {users_storage,[{'_',[],['$_']}],500}
[ns_server:debug,2025-05-15T18:47:23.285Z,ns_1@172.19.0.4:<0.2340.0>:auto_failover_logic:log_master_activity:130]Transitioned node {'ns_1@172.19.0.4',<<"28569ac00b9c1d7c50e39741027d428c">>} state new -> up
[ns_server:warn,2025-05-15T18:47:24.153Z,ns_1@172.19.0.4:<0.2447.0>:leader_lease_acquire_worker:handle_exception:244]Failed to acquire lease from 'ns_1@db2.lan': {exit,
                                              {noproc,
                                               {gen_server,call,
                                                [{leader_lease_agent,
                                                  'ns_1@db2.lan'},
                                                 {acquire_lease,
                                                  'ns_1@172.19.0.4',
                                                  <<"60878074b48f15926e51cf002c4824e4">>,
                                                  [{timeout,15000},
                                                   {period,15000}]},
                                                 infinity]}}}
[ns_server:debug,2025-05-15T18:47:24.406Z,ns_1@172.19.0.4:json_rpc_connection-cbq-engine-service_api<0.2502.0>:json_rpc_connection:init:71]Observed revrpc connection: label "cbq-engine-service_api", handling process <0.2502.0>
[ns_server:debug,2025-05-15T18:47:24.406Z,ns_1@172.19.0.4:service_agent-n1ql<0.2113.0>:service_agent:do_handle_connection:410]Observed new json rpc connection for n1ql: <0.2502.0>
[ns_server:debug,2025-05-15T18:47:24.406Z,ns_1@172.19.0.4:<0.2116.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {json_rpc_events,<0.2114.0>} exited with reason normal
[ns_server:error,2025-05-15T18:47:24.671Z,ns_1@172.19.0.4:service_status_keeper_worker<0.733.0>:rest_utils:get_json:62]Request to (indexer) getIndexStatus with headers [] failed: {error,
                                                             {econnrefused,
                                                              [{lhttpc_client,
                                                                send_request,
                                                                1,
                                                                [{file,
                                                                  "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                                                 {line,220}]},
                                                               {lhttpc_client,
                                                                execute,9,
                                                                [{file,
                                                                  "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                                                 {line,169}]},
                                                               {lhttpc_client,
                                                                request,9,
                                                                [{file,
                                                                  "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                                                 {line,
                                                                  93}]}]}}
[ns_server:error,2025-05-15T18:47:24.671Z,ns_1@172.19.0.4:service_status_keeper-index<0.736.0>:service_status_keeper:handle_cast:103]Service service_index returned incorrect status
[ns_server:debug,2025-05-15T18:47:25.068Z,ns_1@172.19.0.4:json_rpc_connection-index-service_api<0.2561.0>:json_rpc_connection:init:71]Observed revrpc connection: label "index-service_api", handling process <0.2561.0>
[ns_server:debug,2025-05-15T18:47:25.069Z,ns_1@172.19.0.4:service_agent-index<0.2109.0>:service_agent:do_handle_connection:410]Observed new json rpc connection for index: <0.2561.0>
[ns_server:debug,2025-05-15T18:47:25.069Z,ns_1@172.19.0.4:<0.2112.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {json_rpc_events,<0.2110.0>} exited with reason normal
[ns_server:debug,2025-05-15T18:47:25.070Z,ns_1@172.19.0.4:service_manager-index-worker<0.2568.0>:service_manager:do_run_op:250]Got node infos:
[{'ns_1@172.19.0.4',[{node_id,<<"28569ac00b9c1d7c50e39741027d428c">>},
                     {priority,7060000},
                     {opaque,null}]}]
[ns_server:debug,2025-05-15T18:47:25.071Z,ns_1@172.19.0.4:service_manager-index-worker<0.2568.0>:service_manager:do_run_op:255]Using node 'ns_1@172.19.0.4' as a leader
[rebalance:info,2025-05-15T18:47:25.071Z,ns_1@172.19.0.4:service_manager-index-worker<0.2568.0>:service_manager:rebalance_op:327]Rebalancing service index with id <<"35d452100b72fa9639f18b2cc28124e2">>.
KeepNodes: ['ns_1@172.19.0.4']
EjectNodes: []
DeltaNodes: []
[rebalance:info,2025-05-15T18:47:25.071Z,ns_1@172.19.0.4:service_manager-index-worker<0.2568.0>:service_manager:update_service_map:311]Updating service map for index:
['ns_1@172.19.0.4']
[ns_server:debug,2025-05-15T18:47:25.071Z,ns_1@172.19.0.4:service_manager-index-worker<0.2568.0>:ns_cluster_membership:set_service_map:589]Set service map for service index to ['ns_1@172.19.0.4']
[ns_server:debug,2025-05-15T18:47:25.077Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{3,63914554045}}]}|
 <<"{\"indexer.plasma.backIndex.enablePageBloomFilter\":false,\"indexer.settings.compaction.abort_exceed_interval\":false,\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.en"...>>]
[ns_server:debug,2025-05-15T18:47:25.077Z,ns_1@172.19.0.4:ns_config_rep<0.545.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([{metakv,<<"/indexing/settings/config">>}]..)
[ns_server:debug,2025-05-15T18:47:25.094Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: {service_map,index}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                        19})
['ns_1@172.19.0.4']
[ns_server:debug,2025-05-15T18:47:25.095Z,ns_1@172.19.0.4:<0.2433.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":41,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"indexAdmin\":9100,\"indexHttp\":9102,\"indexHttps\":19102,\"indexScan\":9101,\"indexStreamCatchup\":9104,\"indexStreamInit\":9103,\"indexStreamMaint\":9105,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true,\"hostname\":\"172.19.0.4\",\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[ns_server:debug,2025-05-15T18:47:25.107Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/indexing/rebalance/RebalanceToken">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554045}}]}|
 <<"{\"MasterId\":\"28569ac00b9c1d7c50e39741027d428c\",\"RebalId\":\"35d452100b72fa9639f18b2cc28124e2\",\"Source\":0,\"Error\":\"\",\"MasterIP\":\"172.19.0.4\",\"Version\":0,\"RebalPhase\":0,\"ActiveRebalancer\":1}">>]
[ns_server:debug,2025-05-15T18:47:25.107Z,ns_1@172.19.0.4:ns_config_rep<0.545.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([{metakv,
                                   <<"/indexing/rebalance/RebalanceToken">>}]..)
[ns_server:debug,2025-05-15T18:47:25.108Z,ns_1@172.19.0.4:service_manager-index-worker<0.2568.0>:service_janitor:orchestrate_initial_rebalance:97]Initial rebalance progress for `index': [{'ns_1@172.19.0.4',0}]
[ns_server:debug,2025-05-15T18:47:25.110Z,ns_1@172.19.0.4:service_manager-index-worker<0.2568.0>:service_janitor:orchestrate_initial_rebalance:97]Initial rebalance progress for `index': [{'ns_1@172.19.0.4',0.1}]
[ns_server:debug,2025-05-15T18:47:25.126Z,ns_1@172.19.0.4:service_manager-index-worker<0.2568.0>:service_janitor:orchestrate_initial_rebalance:97]Initial rebalance progress for `index': [{'ns_1@172.19.0.4',1}]
[ns_server:debug,2025-05-15T18:47:25.171Z,ns_1@172.19.0.4:ns_config_rep_merger<0.544.0>:ns_config:log_conflict:1423]Conflicting configuration changes to field scramsha_fallback_salt:
<<55,187,62,55,248,32,6,50,18,123,218,116>> and
<<199,70,35,121,194,74,234,136,104,77,98,154>>, choosing the former.
[ns_server:debug,2025-05-15T18:47:25.172Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
scramsha_fallback_salt ->
<<55,187,62,55,248,32,6,50,18,123,218,116>>
[ns_server:info,2025-05-15T18:47:25.173Z,ns_1@172.19.0.4:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:handle_info:764]cert_and_pkey changed
[ns_server:debug,2025-05-15T18:47:25.173Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
cluster_compat_mode ->
undefined
[ns_server:debug,2025-05-15T18:47:25.173Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',address_family} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|inet]
[ns_server:debug,2025-05-15T18:47:25.173Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',audit} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}]
[ns_server:debug,2025-05-15T18:47:25.173Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',backup_grpc_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9124]
[ns_server:debug,2025-05-15T18:47:25.173Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',backup_http_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|8097]
[ns_server:debug,2025-05-15T18:47:25.174Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',backup_https_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|18097]
[ns_server:debug,2025-05-15T18:47:25.174Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',capi_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|8092]
[ns_server:debug,2025-05-15T18:47:25.174Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_admin_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9110]
[ns_server:debug,2025-05-15T18:47:25.174Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_cc_client_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9113]
[ns_server:debug,2025-05-15T18:47:25.174Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_cc_cluster_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9112]
[ns_server:debug,2025-05-15T18:47:25.174Z,ns_1@172.19.0.4:prometheus_cfg<0.515.0>:prometheus_cfg:maybe_apply_new_settings:704]Settings didn't change, ignoring update
[ns_server:debug,2025-05-15T18:47:25.174Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_cc_http_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9111]
[ns_server:debug,2025-05-15T18:47:25.174Z,ns_1@172.19.0.4:ns_config_rep<0.545.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([scramsha_fallback_salt]..)
[ns_server:debug,2025-05-15T18:47:25.174Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_cluster_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9115]
[ns_server:debug,2025-05-15T18:47:25.174Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_console_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9114]
[ns_server:debug,2025-05-15T18:47:25.174Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_data_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9116]
[ns_server:debug,2025-05-15T18:47:25.174Z,ns_1@172.19.0.4:<0.2433.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":53,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"indexAdmin\":9100,\"indexHttp\":9102,\"indexHttps\":19102,\"indexScan\":9101,\"indexStreamCatchup\":9104,\"indexStreamInit\":9103,\"indexStreamMaint\":9105,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true,\"hostname\":\"172.19.0.4\",\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[ns_server:debug,2025-05-15T18:47:25.174Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_debug_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|-1]
[ns_server:debug,2025-05-15T18:47:25.174Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_dirs} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
 "/opt/couchbase/var/lib/couchbase/data"]
[ns_server:debug,2025-05-15T18:47:25.174Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_http_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|8095]
[ns_server:debug,2025-05-15T18:47:25.174Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_messaging_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9118]
[ns_server:debug,2025-05-15T18:47:25.174Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_metadata_callback_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9119]
[ns_server:debug,2025-05-15T18:47:25.174Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_metadata_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9121]
[ns_server:debug,2025-05-15T18:47:25.174Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_parent_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9122]
[ns_server:debug,2025-05-15T18:47:25.174Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_replication_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9120]
[ns_server:debug,2025-05-15T18:47:25.174Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_result_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9117]
[ns_server:debug,2025-05-15T18:47:25.174Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',cbas_ssl_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|18095]
[ns_server:debug,2025-05-15T18:47:25.175Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',client_cert} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{2,63914554043}}]},
 {certs_epoch,0},
 {subject,<<"CN=Couchbase Internal Client (116550952)">>},
 {not_after,63985747642},
 {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
 {load_timestamp,63914554042},
 {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hX"...>>},
 {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDWjCCAkKgAwIBAgIIGD/HyA7ClpEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjJaFw0y\nNzA4MTcxODQ3MjJaMDAxLjAsBgNVBAMTJUNvdWNoYmFzZSBJbnRlcm5hbCBDbGll\nbnQgKDExNjU1MDk1MikwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDg\nAaBAyJnrm54fDG0xlsEQH9E51WTLpkwtleiz980c498XKRPsgRWnjY920KTUIH2/\ndQc"...>>},
 {pkey_passphrase_settings,[]},
 {type,generated},
 {name,"@internal"}]
[ns_server:debug,2025-05-15T18:47:25.175Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',compaction_daemon} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
 {check_interval,30},
 {min_db_file_size,131072},
 {min_view_file_size,20971520}]
[ns_server:debug,2025-05-15T18:47:25.175Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',config_version} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|{7,6}]
[ns_server:debug,2025-05-15T18:47:25.175Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',database_dir} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2025-05-15T18:47:25.175Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',erl_external_listeners} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
 {inet,false}]
[ns_server:debug,2025-05-15T18:47:25.175Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',event_log} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
 {filename,"/opt/couchbase/var/lib/couchbase/event_log"}]
[ns_server:debug,2025-05-15T18:47:25.175Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',eventing_debug_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9140]
[ns_server:debug,2025-05-15T18:47:25.175Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',eventing_dir} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2025-05-15T18:47:25.175Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',eventing_http_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|8096]
[ns_server:debug,2025-05-15T18:47:25.175Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',eventing_https_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|18096]
[ns_server:debug,2025-05-15T18:47:25.176Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',fts_grpc_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9130]
[ns_server:debug,2025-05-15T18:47:25.176Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',fts_grpc_ssl_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|19130]
[ns_server:debug,2025-05-15T18:47:25.176Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',fts_http_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|8094]
[ns_server:debug,2025-05-15T18:47:25.176Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',fts_ssl_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|18094]
[ns_server:debug,2025-05-15T18:47:25.176Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',index_dir} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2025-05-15T18:47:25.176Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',indexer_admin_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9100]
[ns_server:debug,2025-05-15T18:47:25.176Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',indexer_http_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9102]
[ns_server:debug,2025-05-15T18:47:25.176Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',indexer_https_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|19102]
[ns_server:debug,2025-05-15T18:47:25.176Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',indexer_scan_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9101]
[ns_server:debug,2025-05-15T18:47:25.176Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',indexer_stcatchup_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9104]
[ns_server:debug,2025-05-15T18:47:25.176Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',indexer_stinit_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9103]
[ns_server:debug,2025-05-15T18:47:25.176Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',indexer_stmaint_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9105]
[ns_server:debug,2025-05-15T18:47:25.176Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',is_enterprise} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|true]
[ns_server:debug,2025-05-15T18:47:25.177Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',isasl} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
 {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]
[ns_server:debug,2025-05-15T18:47:25.177Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',membership} ->
inactiveAdded
[ns_server:debug,2025-05-15T18:47:25.177Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',memcached} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
 {port,11210},
 {dedicated_port,11209},
 {dedicated_ssl_port,11206},
 {ssl_port,11207},
 {admin_user,"@ns_server"},
 {other_users,["@cbq-engine","@projector","@goxdcr","@index","@fts",
               "@eventing","@cbas","@backup"]},
 {admin_pass,"*****"},
 {engines,[{membase,[{engine,"/opt/couchbase/lib/memcached/ep.so"},
                     {static_config_string,"failpartialwarmup=false"}]},
           {memcached,[{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
                       {static_config_string,"vb0=true"}]}]},
 {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
 {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
 {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
 {log_prefix,"memcached.log"},
 {log_generations,20},
 {log_cyclesize,10485760},
 {log_rotation_period,39003}]
[ns_server:debug,2025-05-15T18:47:25.177Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',memcached_config} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
 {[{interfaces,{memcached_config_mgr,get_interfaces,[]}},
   {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
   {connection_idle_time,connection_idle_time},
   {privilege_debug,privilege_debug},
   {breakpad,
    {[{enabled,breakpad_enabled},
      {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
   {deployment_model,{memcached_config_mgr,get_config_profile,[]}},
   {verbosity,verbosity},
   {audit_file,{"~s",[audit_file]}},
   {rbac_file,{"~s",[rbac_file]}},
   {dedupe_nmvb_maps,dedupe_nmvb_maps},
   {tracing_enabled,tracing_enabled},
   {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
   {xattr_enabled,true},
   {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
   {collections_enabled,true},
   {max_connections,max_connections},
   {system_connections,system_connections},
   {num_reader_threads,num_reader_threads},
   {num_writer_threads,num_writer_threads},
   {num_auxio_threads,num_auxio_threads},
   {num_nonio_threads,num_nonio_threads},
   {num_storage_threads,num_storage_threads},
   {logger,
    {[{filename,{"~s/~s",[log_path,log_prefix]}},{cyclesize,log_cyclesize}]}},
   {external_auth_service,{memcached_config_mgr,get_external_auth_service,[]}},
   {active_external_users_push_interval,
    {memcached_config_mgr,get_external_users_push_interval,[]}},
   {prometheus,{memcached_config_mgr,prometheus_cfg,[]}},
   {sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
   {ssl_sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
   {tcp_keepalive_idle,tcp_keepalive_idle},
   {tcp_keepalive_interval,tcp_keepalive_interval},
   {tcp_keepalive_probes,tcp_keepalive_probes},
   {tcp_user_timeout,tcp_user_timeout},
   {always_collect_trace_info,always_collect_trace_info},
   {connection_limit_mode,connection_limit_mode},
   {free_connection_pool_size,free_connection_pool_size},
   {max_client_connection_details,max_client_connection_details}]}]
[ns_server:debug,2025-05-15T18:47:25.177Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',memcached_dedicated_ssl_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|11206]
[ns_server:debug,2025-05-15T18:47:25.177Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',memcached_defaults} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
 {max_connections,65000},
 {system_connections,5000},
 {connection_idle_time,0},
 {verbosity,0},
 {privilege_debug,false},
 {breakpad_enabled,true},
 {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
 {dedupe_nmvb_maps,false},
 {je_malloc_conf,undefined},
 {tracing_enabled,true},
 {datatype_snappy,true},
 {num_reader_threads,<<"default">>},
 {num_writer_threads,<<"default">>},
 {num_auxio_threads,<<"default">>},
 {num_nonio_threads,<<"default">>},
 {num_storage_threads,<<"default">>},
 {tcp_keepalive_idle,360},
 {tcp_keepalive_interval,10},
 {tcp_keepalive_probes,3},
 {tcp_user_timeout,30},
 {always_collect_trace_info,true},
 {connection_limit_mode,<<"disconnect">>},
 {free_connection_pool_size,0},
 {max_client_connection_details,0}]
[ns_server:debug,2025-05-15T18:47:25.177Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',memcached_prometheus} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|11280]
[ns_server:debug,2025-05-15T18:47:25.177Z,ns_1@172.19.0.4:<0.2433.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":53,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"indexAdmin\":9100,\"indexHttp\":9102,\"indexHttps\":19102,\"indexScan\":9101,\"indexStreamCatchup\":9104,\"indexStreamInit\":9103,\"indexStreamMaint\":9105,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true,\"hostname\":\"172.19.0.4\",\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[ns_server:debug,2025-05-15T18:47:25.177Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',n2n_client_cert_auth} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|false]
[ns_server:debug,2025-05-15T18:47:25.177Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',node_cert} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{2,63914554043}}]},
 {certs_epoch,0},
 {subject,<<"CN=Couchbase Server Node (db2.lan)">>},
 {not_after,63985747642},
 {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
 {load_timestamp,63914554042},
 {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hX"...>>},
 {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDOjCCAiKgAwIBAgIIGD/HyAMALgMwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjJaFw0y\nNzA4MTcxODQ3MjJaMCoxKDAmBgNVBAMTH0NvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nZGIyLmxhbikwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC63r7EIhIk\nOyMwE9mw7bzVJfstC9FfdZvk0bP8vxZkuhNt/bVcND8p00ihMP8FCbfsa5d8VgWC\nDoa"...>>},
 {pkey_passphrase_settings,[]},
 {type,generated},
 {hostname,"db2.lan"}]
[ns_server:debug,2025-05-15T18:47:25.177Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',node_encryption} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|false]
[ns_server:debug,2025-05-15T18:47:25.178Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',ns_log} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
 {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]
[ns_server:debug,2025-05-15T18:47:25.178Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',port_servers} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}]
[ns_server:debug,2025-05-15T18:47:25.178Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',projector_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9999]
[ns_server:debug,2025-05-15T18:47:25.178Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',projector_ssl_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9999]
[ns_server:debug,2025-05-15T18:47:25.178Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',prometheus_auth_info} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{2,63914554045}}]}|
 {"@prometheus",
  {auth,
   [{<<"hash">>,
     {[{<<"hashes">>,
        {sanitized,<<"E4iwPDExOnOjGXs0qP6la711Y5qTaST+1nzp3WWzVqw=">>}},
       {<<"algorithm">>,<<"argon2id">>},
       {<<"salt">>,<<"61T+47ADFbRDEeE8WMrAGw==">>},
       {<<"parallelism">>,1},
       {<<"time">>,3},
       {<<"memory">>,524288}]}},
    {<<"scram-sha-512">>,
     {[{<<"salt">>,
        <<"kGjfTLY3stMxG3p1BzztyO2J8qheD+hlb8euMQoyGg1cr+r9pUQyPsr32qJ3H9T4JpSJnBUEyXbjZ/tnoEGXMw==">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"KuADGEOAaLhVMSkwIKxrztvRUhXdeMaZA8c8MPeK2qw=">>}}]}},
    {<<"scram-sha-256">>,
     {[{<<"salt">>,<<"ga97SSKrWv6mIdojxSSVO+hBOALInVDadwYP0X8uqjU=">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"QaFsRlNDNHpIcZSkMUdzIYnBSJznlCdgo3U/gZzgIW8=">>}}]}},
    {<<"scram-sha-1">>,
     {[{<<"salt">>,<<"kN6RJArUekQYl2C69D1t3KNt2Zc=">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"JyPn7MPQQ8xQtBYdBohgEg3u+BTyMU7ioflt+xVGsak=">>}}]}}]}}]
[ns_server:debug,2025-05-15T18:47:25.178Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',prometheus_http_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9123]
[ns_server:debug,2025-05-15T18:47:25.178Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',query_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|8093]
[ns_server:debug,2025-05-15T18:47:25.178Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',rest} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]},
 {port,8091},
 {port_meta,global}]
[ns_server:debug,2025-05-15T18:47:25.178Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',saslauthd_enabled} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|true]
[ns_server:debug,2025-05-15T18:47:25.178Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',ssl_capi_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|18092]
[ns_server:debug,2025-05-15T18:47:25.178Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',ssl_query_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|18093]
[ns_server:debug,2025-05-15T18:47:25.178Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',ssl_rest_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|18091]
[ns_server:debug,2025-05-15T18:47:25.178Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',uuid} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|
 <<"7a33d46fd1976e65466c5efc8b45ef2e">>]
[ns_server:debug,2025-05-15T18:47:25.178Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',xdcr_rest_port} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|9998]
[ns_server:debug,2025-05-15T18:47:25.178Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db2.lan',{project_intact,is_vulnerable}} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554043}}]}|false]
[ns_server:debug,2025-05-15T18:47:25.179Z,ns_1@172.19.0.4:memcached_config_mgr<0.2435.0>:memcached_config_mgr:apply_changed_memcached_config:262]New memcached config is hot-reloadable.
[ns_server:debug,2025-05-15T18:47:25.180Z,ns_1@172.19.0.4:memcached_config_mgr<0.2435.0>:memcached_config_mgr:do_read_current_memcached_config:371]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2025-05-15T18:47:25.185Z,ns_1@172.19.0.4:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:restart_regenerate_client_cert_timer:1447]Time left before client cert regeneration: 70588761000
[user:info,2025-05-15T18:47:25.192Z,ns_1@172.19.0.4:memcached_config_mgr<0.2435.0>:memcached_config_mgr:hot_reload_config:331]Hot-reloaded memcached.json for config change of the following keys: [<<"scramsha_fallback_salt">>]
[cluster:debug,2025-05-15T18:47:25.211Z,ns_1@172.19.0.4:ns_cluster<0.273.0>:ns_cluster:post_json_to_joinee:927]Reply from [https,"db2.lan",18091,"/completeJoin"]:
{ok,[]}
[cluster:debug,2025-05-15T18:47:25.211Z,ns_1@172.19.0.4:ns_cluster<0.273.0>:ns_cluster:handle_call:418]add_node(https, "db2.lan", 18091, undefined, ..) -> {ok,'ns_1@db2.lan'}
[ns_server:debug,2025-05-15T18:47:25.211Z,ns_1@172.19.0.4:ns_audit<0.696.0>:ns_audit:handle_call:168]Audit add_node: [{local,{[{ip,<<"172.19.0.4">>},{port,8091}]}},
                 {remote,{[{ip,<<"172.19.0.4">>},{port,42782}]}},
                 {real_userid,{[{domain,builtin},
                                {user,<<"<ud>jaba_admin</ud>">>}]}},
                 {timestamp,<<"2025-05-15T18:47:25.211Z">>},
                 {user,<<"<ud>jaba_admin</ud>">>},
                 {services,[index,kv,n1ql]},
                 {port,18091},
                 {hostname,<<"db2.lan">>},
                 {node,'ns_1@db2.lan'}]
[cluster:debug,2025-05-15T18:47:25.422Z,ns_1@172.19.0.4:ns_cluster<0.273.0>:ns_cluster:handle_call:415]handling add_node(https, "db3.lan", 18091, undefined, ..)
[ns_server:info,2025-05-15T18:47:25.422Z,ns_1@172.19.0.4:ns_cluster<0.273.0>:ns_cluster:check_add_possible:781]Checking if host 'db3.lan' is allowed to join the cluster (allowed hosts: [<<"*">>])
[ns_server:debug,2025-05-15T18:47:25.424Z,ns_1@172.19.0.4:ns_cluster<0.273.0>:ns_cluster:check_host_port_connectivity:656]Successfully checked TCP connectivity to "db3.lan":18091
[ns_server:debug,2025-05-15T18:47:25.527Z,ns_1@172.19.0.4:<0.2663.0>:goport:handle_eof:592]Stream 'stdout' closed
[ns_server:debug,2025-05-15T18:47:25.528Z,ns_1@172.19.0.4:<0.2663.0>:goport:handle_eof:592]Stream 'stderr' closed
[ns_server:info,2025-05-15T18:47:25.528Z,ns_1@172.19.0.4:<0.2663.0>:goport:handle_process_exit:573]Port exited with status 0.
[ns_server:debug,2025-05-15T18:47:25.590Z,ns_1@172.19.0.4:<0.2666.0>:goport:handle_eof:592]Stream 'stdout' closed
[ns_server:debug,2025-05-15T18:47:25.590Z,ns_1@172.19.0.4:<0.2666.0>:goport:handle_eof:592]Stream 'stderr' closed
[ns_server:info,2025-05-15T18:47:25.590Z,ns_1@172.19.0.4:<0.2666.0>:goport:handle_process_exit:573]Port exited with status 0.
[cluster:debug,2025-05-15T18:47:25.590Z,ns_1@172.19.0.4:ns_cluster<0.273.0>:ns_cluster:post_json_to_joinee:920]Posting the following to [https,"db3.lan",18091,"/engageCluster2"]:
{{[{<<"requestedTargetNodeHostname">>,<<"db3.lan">>},
   {<<"requestedServices">>,[index,kv,n1ql]},
   {<<"isDeveloperPreview">>,false},
   {availableStorage,{[{hdd,[{[{path,<<"/">>},
                               {sizeKBytes,1056557396},
                               {usagePercent,2}]},
                             {[{path,<<"/dev">>},
                               {sizeKBytes,65536},
                               {usagePercent,0}]},
                             {[{path,<<"/dev/shm">>},
                               {sizeKBytes,65536},
                               {usagePercent,0}]},
                             {[{path,<<"/etc/resolv.conf">>},
                               {sizeKBytes,1056557396},
                               {usagePercent,2}]},
                             {[{path,<<"/etc/hostname">>},
                               {sizeKBytes,1056557396},
                               {usagePercent,2}]},
                             {[{path,<<"/etc/hosts">>},
                               {sizeKBytes,1056557396},
                               {usagePercent,2}]},
                             {[{path,<<"/opt/couchbase/var">>},
                               {sizeKBytes,482797652},
                               {usagePercent,92}]},
                             {[{path,<<"/proc/kcore">>},
                               {sizeKBytes,65536},
                               {usagePercent,0}]},
                             {[{path,<<"/proc/keys">>},
                               {sizeKBytes,65536},
                               {usagePercent,0}]},
                             {[{path,<<"/proc/timer_list">>},
                               {sizeKBytes,65536},
                               {usagePercent,0}]},
                             {[{path,<<"/proc/scsi">>},
                               {sizeKBytes,4012680},
                               {usagePercent,0}]},
                             {[{path,<<"/sys/firmware">>},
                               {sizeKBytes,4012680},
                               {usagePercent,0}]}]}]}},
   {storageTotals,{[{ram,{[{total,8217968640},
                           {quotaTotal,3221225472},
                           {quotaUsed,0},
                           {used,4251119616},
                           {usedByData,0},
                           {quotaUsedPerNode,0},
                           {quotaTotalPerNode,3221225472}]}},
                    {hdd,{[{total,494384795648},
                           {quotaTotal,494384795648},
                           {used,454834011996},
                           {usedByData,0},
                           {free,39550783652}]}}]}},
   {storage,{[{ssd,[]},
              {hdd,[{[{path,<<"/opt/couchbase/var/lib/couchbase/data">>},
                      {index_path,<<"/opt/couchbase/var/lib/couchbase/data">>},
                      {cbas_dirs,[<<"/opt/couchbase/var/lib/couchbase/data">>]},
                      {eventing_path,<<"/opt/couchbase/var/lib/couchbase/data">>},
                      {java_home,<<>>},
                      {quotaMb,none},
                      {state,ok}]}]}]}},
   {clusterMembership,active},
   {recoveryType,none},
   {status,<<"healthy">>},
   {otpNode,'ns_1@172.19.0.4'},
   {thisNode,true},
   {hostname,<<"172.19.0.4:8091">>},
   {nodeUUID,<<"28569ac00b9c1d7c50e39741027d428c">>},
   {clusterCompatibility,458758},
   {version,<<"7.6.2-3721-enterprise">>},
   {os,<<"aarch64-unknown-linux-gnu">>},
   {cpuCount,10},
   {ports,{[{direct,11210},
            {httpsMgmt,18091},
            {httpsCAPI,18092},
            {distTCP,21100},
            {distTLS,21150}]}},
   {services,[index,kv,n1ql]},
   {nodeEncryption,false},
   {nodeEncryptionClientCertVerification,false},
   {addressFamilyOnly,false},
   {configuredHostname,<<"172.19.0.4:8091">>},
   {addressFamily,inet},
   {externalListeners,[{[{afamily,inet},{nodeEncryption,false}]}]},
   {serverGroup,<<"Group 1">>},
   {couchApiBase,<<"http://172.19.0.4:8092/">>},
   {couchApiBaseHTTPS,<<"https://172.19.0.4:18092/">>},
   {nodeHash,10838665},
   {systemStats,{[{cpu_utilization_rate,3.89883035089008},
                  {cpu_stolen_rate,0},
                  {swap_total,1073737728},
                  {swap_used,3956736},
                  {mem_total,8217968640},
                  {mem_free,4805804032},
                  {mem_limit,8217968640},
                  {cpu_cores_available,10},
                  {allocstall,17}]}},
   {interestingStats,{[]}},
   {uptime,<<"44">>},
   {memoryTotal,8217968640},
   {memoryFree,4805804032},
   {mcdMemoryReserved,6269},
   {mcdMemoryAllocated,6269},
   {memoryQuota,3072},
   {queryMemoryQuota,0},
   {indexMemoryQuota,512},
   {ftsMemoryQuota,512},
   {cbasMemoryQuota,1024},
   {eventingMemoryQuota,256},
   {<<"autogeneratedCA">>,
    <<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hXLFvQCuy0dFqLPrJ6xY21RJ5ttVPR78ssxxXSe9JdTj/IsUczkwyxfZfwK\npBszlGYuHO0Utnwq1K4ruMPYFYfpLacJSfsb3KWEmZwAoNuRpjvE24AsH3tvj7cB\nB5v49HJ0axvNAFm75GWDyUr7T7UwcTJaFIFtfy+J0Umt2TjFNY0DPpt1WEzkKFKx\nOdBs+7cNc35ZG4rwVu/EJ14OXhQMXkpbUpYJxS8jzkzSI+1Eb67kH4j5QhnR9H+w\nw2S9AgMBAAGjQjBAMA4GA1UdDwEB/wQEAwIBhjAPBgNVHRMBAf8EBTADAQH/MB0G\nA1UdDgQWBBS8ASq2Mkh0UsZe4fIvzs4mCSfTmjANBgkqhkiG9w0BAQsFAAOCAQEA\nB3q1mzdJc3VY17JDCaRPKY/Veni3oZ6zGJ9r9LnWKE7IO9AxBAKYGMELHMf9httQ\nmRZrXiGQi/tYXLlFFk7IcIID6iOOZLLagDhCQJPi52dGhBB4a2EnBvGF9OJI4zW9\nJw04HNR5cF1cxWIrRdGJAX/kbs/M9jz0STlXziVNwiAYeGIxkjq/fwKk6weai8iI\nwQVu6GsbtSBL8f4FRCk2vK6a1MD54BQDasRShdi3k/DJAEtY92muu13M0jc48mCM\ndEVeXxM8rIrG6LavziTwtO+jkFyWjlRyNbSr6il+neSWJ7ZspTo+ZYFy55CAh5ws\nwk6Ljo2wlQgfGflJbKcxAw==\n-----END CERTIFICATE-----\n">>},
   {<<"autogeneratedClientCert">>,
    <<"-----BEGIN CERTIFICATE-----\nMIIDWTCCAkGgAwIBAgIIGD/HyMO1PBAwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjVaFw0y\nNzA4MTcxODQ3MjVaMC8xLTArBgNVBAMTJENvdWNoYmFzZSBJbnRlcm5hbCBDbGll\nbnQgKDMyNjYwNzI2KTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAKHe\n9VAt+X6MU19Fg7ok9H0Zxkmq5mDd0acYZWMO8WqPC7jBY2dopAKDJoF/AKgk8TLT\ncz5AYGMiY3CokE1WbBNVZZ8+VMft6H+06YxTTsLvqWttVghc4Q3UKSuDpvQrIbxj\n7PxG12wiWoQUCfnBMjrL2ILgJKAQIMjO7UoWin3LmZo8LTdR4ugoBoS3cfQJhz7R\nT7iuSbe/44TwrcYrDzrvg3UCsFrXLnKs/LJ+nfQyl0fjkS/MP33lCHkdZ6+teEaJ\ncsNTaJuYKYIQIR1ICQnsFCsXrf2D4h0v8ZUsRVoaB3TFebmEqYtNB7V3NnP/sxFf\nqQvn6gwzDmvlb0O5+/MCAwEAAaOBgzCBgDAOBgNVHQ8BAf8EBAMCBaAwEwYDVR0l\nBAwwCgYIKwYBBQUHAwIwDAYDVR0TAQH/BAIwADAfBgNVHSMEGDAWgBS8ASq2Mkh0\nUsZe4fIvzs4mCSfTmjAqBgNVHREEIzAhgR9pbnRlcm5hbEBpbnRlcm5hbC5jb3Vj\naGJhc2UuY29tMA0GCSqGSIb3DQEBCwUAA4IBAQAtkwpYapli88YqNcz52BW8oUhZ\nqHNUis2IieXUTRZSTpTQlvywLUYl+7UvOl9QZjGGowKnD7bdTGgR0qyP1Bab1eIs\nZTbE8+MMa/2zeKjsDQlv7LKoKRI6eBa7nadz2AsyuYJ6h6M8Pj8XI1fXTSdEMZyU\ny32ahUlOq2qNPC485fFLJs/6ggN+3NCZT4H/ohbv8cZvoSUuHOj/wzbuNMZg4d18\n57CxIpHFCtyqUkXfQabWuPSsJRadCqs+Z59XLqEmyVdTeX8HcPON7IXtI7lNDdWB\nS0M6ljMG3ONZVVzxAowbDDbacQ1BK+kY8qlk0KzgCydCP3z86KPAsXEFSqTr\n-----END CERTIFICATE-----\n">>},
   {<<"autogeneratedClientKey">>,<<"********">>},
   {<<"autogeneratedCert">>,
    <<"-----BEGIN CERTIFICATE-----\nMIIDOjCCAiKgAwIBAgIIGD/HyL2BZrYwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjVaFw0y\nNzA4MTcxODQ3MjVaMCoxKDAmBgNVBAMTH0NvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nZGIzLmxhbikwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCoBie+jXEG\nM9mqURIO0hHIXr1th5jf7TJD7f69DGF3znRjw1VTjnaMZAQOk8Gg44nwI1o1KtJj\nUmflnTGWWtZU+43sI8tDXLtymAbYoxEkodGUqqPCEPQkCWLubJvTbSZVEyegLo9P\nKw4RPwJGAkbAFq1poCecaE9pf4qeOhibUM7ivJx5nFMcTkUGKqJt6vVH4oA/FcHp\nfOLz51gjD0O/U2Ev8NqFvx6ftWkIU/+LGaHaMjoDj4x8SldegqvGtDmpfhg2+97T\nftALlkUkvvJqd4beilSDxCiurYlIKOki8GWaGFfleyV8HwD/hOIZ/wj5yz607CDj\nuny5Z11yQr7/AgMBAAGjajBoMA4GA1UdDwEB/wQEAwIFoDATBgNVHSUEDDAKBggr\nBgEFBQcDATAMBgNVHRMBAf8EAjAAMB8GA1UdIwQYMBaAFLwBKrYySHRSxl7h8i/O\nziYJJ9OaMBIGA1UdEQQLMAmCB2RiMy5sYW4wDQYJKoZIhvcNAQELBQADggEBAAPS\nNPGKGEFgoq/sasnMadhR33aGK60LnjgknxG0R9lT+/RHZKEoDMbYw4gp7UUrh+JZ\n+cneP93RcOIpvfRP7AUlGviu8HDldR5yFQBRXpDdKyWvIvPyhwZK4v65yIYUcLq4\ngxMZUzynscYxC4eN1l2eyD2Rf4FuZ7G2ehW4zcei1DD35WId1pR+YKjwKNZ8Q5Sk\nyjT9F2mftICgpSdxyZJLdNXaXDGfodNVzuCKnMnYUoRWmddt5UqqLLkPM0+7HEpU\nn5wAUJD2kSCGspqGnAZbcBEbGKbW2hmgXAKxOQVyVfi4b1ZGo/IAScLuUoMZw8hh\n+6kwlsLDy19h3nkTfxo=\n-----END CERTIFICATE-----\n">>},
   {<<"autogeneratedKey">>,<<"********">>}]}}
[ns_server:debug,2025-05-15T18:47:25.709Z,ns_1@172.19.0.4:cb_dist<0.2168.0>:cb_dist:info_msg:1098]cb_dist: Accepted new connection from <0.2171.0> DistCtrl #Port<0.167>: {con,
                                                                         #Ref<0.2327127311.3650355201.221354>,
                                                                         inet_tcp_dist,
                                                                         undefined,
                                                                         undefined}
[ns_server:debug,2025-05-15T18:47:25.709Z,ns_1@172.19.0.4:net_kernel<0.2170.0>:cb_dist:info_msg:1098]cb_dist: Accepting connection from <0.2171.0> using module inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:25.710Z,ns_1@172.19.0.4:cb_dist<0.2168.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.2327127311.3650355201.221354>,
                                  inet_tcp_dist,<0.2688.0>,
                                  #Ref<0.2327127311.3650355201.221357>}
[ns_server:debug,2025-05-15T18:47:25.710Z,ns_1@172.19.0.4:cb_dist<0.2168.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.2327127311.3650355201.221354>,
                               inet_tcp_dist,<0.2688.0>,
                               #Ref<0.2327127311.3650355201.221357>}
[error_logger:info,2025-05-15T18:47:25.710Z,ns_1@172.19.0.4:net_kernel<0.2170.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.2688.0>,shutdown}}
[cluster:debug,2025-05-15T18:47:25.775Z,ns_1@172.19.0.4:ns_cluster<0.273.0>:ns_cluster:post_json_to_joinee:927]Reply from [https,"db3.lan",18091,"/engageCluster2"]:
{ok,{[{<<"availableStorage">>,
       {[{<<"hdd">>,
          [{[{<<"path">>,<<"/">>},
             {<<"sizeKBytes">>,1056557396},
             {<<"usagePercent">>,2}]},
           {[{<<"path">>,<<"/dev">>},
             {<<"sizeKBytes">>,65536},
             {<<"usagePercent">>,0}]},
           {[{<<"path">>,<<"/dev/shm">>},
             {<<"sizeKBytes">>,65536},
             {<<"usagePercent">>,0}]},
           {[{<<"path">>,<<"/etc/resolv.conf">>},
             {<<"sizeKBytes">>,1056557396},
             {<<"usagePercent">>,2}]},
           {[{<<"path">>,<<"/etc/hostname">>},
             {<<"sizeKBytes">>,1056557396},
             {<<"usagePercent">>,2}]},
           {[{<<"path">>,<<"/etc/hosts">>},
             {<<"sizeKBytes">>,1056557396},
             {<<"usagePercent">>,2}]},
           {[{<<"path">>,<<"/opt/couchbase/var">>},
             {<<"sizeKBytes">>,482797652},
             {<<"usagePercent">>,92}]},
           {[{<<"path">>,<<"/proc/kcore">>},
             {<<"sizeKBytes">>,65536},
             {<<"usagePercent">>,0}]},
           {[{<<"path">>,<<"/proc/keys">>},
             {<<"sizeKBytes">>,65536},
             {<<"usagePercent">>,0}]},
           {[{<<"path">>,<<"/proc/timer_list">>},
             {<<"sizeKBytes">>,65536},
             {<<"usagePercent">>,0}]},
           {[{<<"path">>,<<"/proc/scsi">>},
             {<<"sizeKBytes">>,4012680},
             {<<"usagePercent">>,0}]},
           {[{<<"path">>,<<"/sys/firmware">>},
             {<<"sizeKBytes">>,4012680},
             {<<"usagePercent">>,0}]}]}]}},
      {<<"storageTotals">>,
       {[{<<"ram">>,
          {[{<<"total">>,8217968640},
            {<<"quotaTotal">>,3571449856},
            {<<"quotaUsed">>,0},
            {<<"used">>,4296560640},
            {<<"usedByData">>,0},
            {<<"quotaUsedPerNode">>,0},
            {<<"quotaTotalPerNode">>,3571449856}]}},
         {<<"hdd">>,
          {[{<<"total">>,494384795648},
            {<<"quotaTotal">>,494384795648},
            {<<"used">>,454834011996},
            {<<"usedByData">>,0},
            {<<"free">>,39550783652}]}}]}},
      {<<"storage">>,
       {[{<<"ssd">>,[]},
         {<<"hdd">>,
          [{[{<<"path">>,<<"/opt/couchbase/var/lib/couchbase/data">>},
             {<<"index_path">>,<<"/opt/couchbase/var/lib/couchbase/data">>},
             {<<"cbas_dirs">>,[<<"/opt/couchbase/var/lib/couchbase/data">>]},
             {<<"eventing_path">>,<<"/opt/couchbase/var/lib/couchbase/data">>},
             {<<"java_home">>,<<>>},
             {<<"quotaMb">>,<<"none">>},
             {<<"state">>,<<"ok">>}]}]}]}},
      {<<"clusterMembership">>,<<"active">>},
      {<<"recoveryType">>,<<"none">>},
      {<<"status">>,<<"healthy">>},
      {<<"otpNode">>,<<"ns_1@db3.lan">>},
      {<<"thisNode">>,true},
      {<<"hostname">>,<<"db3.lan:8091">>},
      {<<"nodeUUID">>,<<"e0d520cb35700b4a473cf359e3354528">>},
      {<<"clusterCompatibility">>,458758},
      {<<"version">>,<<"7.6.2-3721-enterprise">>},
      {<<"os">>,<<"aarch64-unknown-linux-gnu">>},
      {<<"cpuCount">>,10},
      {<<"ports">>,
       {[{<<"direct">>,11210},
         {<<"httpsMgmt">>,18091},
         {<<"httpsCAPI">>,18092},
         {<<"distTCP">>,21100},
         {<<"distTLS">>,21150}]}},
      {<<"services">>,[<<"kv">>]},
      {<<"nodeEncryption">>,false},
      {<<"nodeEncryptionClientCertVerification">>,false},
      {<<"addressFamilyOnly">>,false},
      {<<"configuredHostname">>,<<"db3.lan:8091">>},
      {<<"addressFamily">>,<<"inet">>},
      {<<"externalListeners">>,
       [{[{<<"afamily">>,<<"inet">>},{<<"nodeEncryption">>,false}]}]},
      {<<"serverGroup">>,<<"Group 1">>},
      {<<"couchApiBase">>,<<"http://db3.lan:8092/">>},
      {<<"couchApiBaseHTTPS">>,<<"https://db3.lan:18092/">>},
      {<<"nodeHash">>,76895453},
      {<<"systemStats">>,
       {[{<<"cpu_utilization_rate">>,3.979204159160034},
         {<<"cpu_stolen_rate">>,0},
         {<<"swap_total">>,1073737728},
         {<<"swap_used">>,3956736},
         {<<"mem_total">>,8217968640},
         {<<"mem_free">>,4803461120},
         {<<"mem_limit">>,8217968640},
         {<<"cpu_cores_available">>,10},
         {<<"allocstall">>,17}]}},
      {<<"interestingStats">>,{[]}},
      {<<"uptime">>,<<"45">>},
      {<<"memoryTotal">>,8217968640},
      {<<"memoryFree">>,4803461120},
      {<<"mcdMemoryReserved">>,6269},
      {<<"mcdMemoryAllocated">>,6269},
      {<<"memoryQuota">>,3406},
      {<<"queryMemoryQuota">>,0},
      {<<"indexMemoryQuota">>,512},
      {<<"ftsMemoryQuota">>,512},
      {<<"cbasMemoryQuota">>,1549},
      {<<"eventingMemoryQuota">>,256}]}}
[cluster:debug,2025-05-15T18:47:25.776Z,ns_1@172.19.0.4:ns_cluster<0.273.0>:ns_cluster:get_port_from_epmd:1093]port_please('ns_1@db3.lan', inet, false) = {ok,21100}
[ns_server:debug,2025-05-15T18:47:25.776Z,ns_1@172.19.0.4:ns_cluster<0.273.0>:ns_cluster:check_host_port_connectivity:656]Successfully checked TCP connectivity to "db3.lan":21100
[ns_server:debug,2025-05-15T18:47:25.776Z,ns_1@172.19.0.4:ns_cluster<0.273.0>:chronicle_master:call:71]Calling chronicle_master with {add_replica,'ns_1@db3.lan',undefined,
                                  [index,kv,n1ql]}
[ns_server:debug,2025-05-15T18:47:25.799Z,ns_1@172.19.0.4:<0.2326.0>:chronicle_master:handle_oper:342]Starting kv operation {add_replica,'ns_1@db3.lan',undefined,[index,kv,n1ql]} with lock <<"45724915daa2a489c0a6816d20198585">>
[ns_server:debug,2025-05-15T18:47:25.799Z,ns_1@172.19.0.4:<0.2326.0>:ns_cluster_membership:add_node:308]Add node 'ns_1@db3.lan', GroupUUID = undefined, Services = [index,kv,n1ql]
[ns_server:debug,2025-05-15T18:47:25.828Z,ns_1@172.19.0.4:<0.2326.0>:chronicle_master:handle_oper:345]Starting topology operation {add_replica,'ns_1@db3.lan',undefined,
                                [index,kv,n1ql]} with lock <<"45724915daa2a489c0a6816d20198585">>
[ns_server:debug,2025-05-15T18:47:25.828Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: unfinished_topology_operation, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                  22})
{regular,{add_replica,'ns_1@db3.lan',undefined,[index,kv,n1ql]},
         <<"45724915daa2a489c0a6816d20198585">>,
         #Ref<0.2327127311.3650355203.217418>}
[ns_server:debug,2025-05-15T18:47:25.828Z,ns_1@172.19.0.4:mb_master<0.2313.0>:mb_master:update_peers:543]List of peers has changed from ['ns_1@172.19.0.4','ns_1@db2.lan'] to ['ns_1@172.19.0.4',
                                                                      'ns_1@db2.lan',
                                                                      'ns_1@db3.lan']
[ns_server:debug,2025-05-15T18:47:25.828Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: nodes_wanted, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,22})
['ns_1@172.19.0.4','ns_1@db2.lan','ns_1@db3.lan']
[ns_server:debug,2025-05-15T18:47:25.828Z,ns_1@172.19.0.4:ns_cookie_manager<0.238.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[ns_server:debug,2025-05-15T18:47:25.828Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@db3.lan',membership}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                     22})
inactiveAdded
[ns_server:debug,2025-05-15T18:47:25.828Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@db3.lan',services}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                   22})
[index,kv,n1ql]
[ns_server:debug,2025-05-15T18:47:25.828Z,ns_1@172.19.0.4:<0.2690.0>:ns_node_disco:do_nodes_wanted_updated_fun:210]ns_node_disco: nodes_wanted updated: ['ns_1@172.19.0.4','ns_1@db2.lan',
                                      'ns_1@db3.lan'], with cookie: {sanitized,
                                                                     <<"biSYTaQUFDVndTs/b+IcDmD2L0woktv9naRdv3GCK6A=">>}
[ns_server:debug,2025-05-15T18:47:25.828Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: server_groups, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,22})
[[{uuid,<<"0">>},
  {name,<<"Group 1">>},
  {nodes,['ns_1@172.19.0.4','ns_1@db2.lan','ns_1@db3.lan']}]]
[ns_server:debug,2025-05-15T18:47:25.828Z,ns_1@172.19.0.4:<0.2433.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":56,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"indexAdmin\":9100,\"indexHttp\":9102,\"indexHttps\":19102,\"indexScan\":9101,\"indexStreamCatchup\":9104,\"indexStreamInit\":9103,\"indexStreamMaint\":9105,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true,\"hostname\":\"172.19.0.4\",\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[error_logger:info,2025-05-15T18:47:25.829Z,ns_1@172.19.0.4:net_kernel<0.2170.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'ns_1@db3.lan',
                          {14108582,#Ref<0.2327127311.3650486281.215813>}}}
[ns_server:debug,2025-05-15T18:47:25.830Z,ns_1@172.19.0.4:net_kernel<0.2170.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'ns_1@db3.lan' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:25.830Z,ns_1@172.19.0.4:cb_dist<0.2168.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.2327127311.3650355204.218068>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:47:25.830Z,ns_1@172.19.0.4:cb_dist<0.2168.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.2327127311.3650355204.218068>,
                                  inet_tcp_dist,<0.2693.0>,
                                  #Ref<0.2327127311.3650355209.215822>}
[error_logger:info,2025-05-15T18:47:25.832Z,ns_1@172.19.0.4:net_kernel<0.2170.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.2693.0>,{recv_challenge_ack_failed,{error,closed}}}}
[ns_server:debug,2025-05-15T18:47:25.832Z,ns_1@172.19.0.4:cb_dist<0.2168.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.2327127311.3650355204.218068>,
                               inet_tcp_dist,<0.2693.0>,
                               #Ref<0.2327127311.3650355209.215822>}
[error_logger:info,2025-05-15T18:47:25.832Z,ns_1@172.19.0.4:net_kernel<0.2170.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'ns_1@db3.lan'}}
[error_logger:info,2025-05-15T18:47:25.833Z,ns_1@172.19.0.4:net_kernel<0.2170.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{disconnect,'ns_1@db3.lan'}}
[ns_server:debug,2025-05-15T18:47:25.833Z,ns_1@172.19.0.4:<0.2690.0>:ns_node_disco:do_nodes_wanted_updated_fun:213]ns_node_disco: nodes_wanted pong: ['ns_1@172.19.0.4','ns_1@db2.lan'], with cookie: {sanitized,
                                                                                    <<"biSYTaQUFDVndTs/b+IcDmD2L0woktv9naRdv3GCK6A=">>}
[error_logger:info,2025-05-15T18:47:25.849Z,ns_1@172.19.0.4:net_kernel<0.2170.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'ns_1@db3.lan',
                          {14108583,#Ref<0.2327127311.3650486281.215813>}}}
[ns_server:debug,2025-05-15T18:47:25.850Z,ns_1@172.19.0.4:net_kernel<0.2170.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'ns_1@db3.lan' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:25.850Z,ns_1@172.19.0.4:cb_dist<0.2168.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.2327127311.3650355202.218218>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:47:25.850Z,ns_1@172.19.0.4:cb_dist<0.2168.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.2327127311.3650355202.218218>,
                                  inet_tcp_dist,<0.2695.0>,
                                  #Ref<0.2327127311.3650355208.216372>}
[ns_server:debug,2025-05-15T18:47:25.852Z,ns_1@172.19.0.4:cb_dist<0.2168.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.2327127311.3650355202.218218>,
                               inet_tcp_dist,<0.2695.0>,
                               #Ref<0.2327127311.3650355208.216372>}
[error_logger:info,2025-05-15T18:47:25.852Z,ns_1@172.19.0.4:net_kernel<0.2170.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.2695.0>,{recv_challenge_ack_failed,{error,closed}}}}
[error_logger:info,2025-05-15T18:47:25.852Z,ns_1@172.19.0.4:net_kernel<0.2170.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'ns_1@db3.lan'}}
[chronicle:info,2025-05-15T18:47:25.852Z,ns_1@172.19.0.4:chronicle_proposer<0.2185.0>:chronicle_proposer:handle_down:1142]Observed agent {chronicle_agent,'ns_1@db3.lan'} on peer 'ns_1@db3.lan' go down with reason noconnection
[ns_server:debug,2025-05-15T18:47:25.857Z,ns_1@172.19.0.4:<0.2326.0>:chronicle_master:handle_topology_oper:332]Cluster info: #{committed_seqno => 24,compat_version => 0,
                config =>
                    {log_entry,<<"5aab03dbecad06b50ffb474da59a00c9">>,
                        {4,'ns_1@172.19.0.4'},
                        24,
                        {config,
                            {<<"1dd5991463d519e23729dda2c80c376b">>,0,3},
                            0,<<"45724915daa2a489c0a6816d20198585">>,
                            #{'ns_1@172.19.0.4' =>
                                  #{id =>
                                        <<"1dd5991463d519e23729dda2c80c376b">>,
                                    role => voter},
                              'ns_1@db2.lan' =>
                                  #{id =>
                                        <<"9f6894b51ffa497cdfc8b792d9e6726b">>,
                                    role => replica},
                              'ns_1@db3.lan' =>
                                  #{id =>
                                        <<"659ec03c812f5cc9456bd71fe3c1a047">>,
                                    role => replica}},
                            undefined,
                            #{chronicle_config_rsm =>
                                  {rsm_config,chronicle_config_rsm,[]},
                              kv => {rsm_config,chronicle_kv,[]}},
                            #{},undefined,
                            [{<<"5aab03dbecad06b50ffb474da59a00c9">>,0}]}},
                history_id => <<"5aab03dbecad06b50ffb474da59a00c9">>}
[ns_server:debug,2025-05-15T18:47:25.857Z,ns_1@172.19.0.4:<0.2326.0>:chronicle_master:remove_oper_key:309]Removing operation key with lock <<"45724915daa2a489c0a6816d20198585">>
[cluster:info,2025-05-15T18:47:25.882Z,ns_1@172.19.0.4:ns_cluster<0.273.0>:ns_cluster:node_add_transaction_finish:1318]Started node add transaction by adding node 'ns_1@db3.lan' to nodes_wanted (group: undefined)
[ns_server:debug,2025-05-15T18:47:25.882Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:handle_info:42]delete (key: unfinished_topology_operation, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                  25})
[cluster:debug,2025-05-15T18:47:25.883Z,ns_1@172.19.0.4:ns_cluster<0.273.0>:ns_cluster:post_json_to_joinee:920]Posting the following to [https,"db3.lan",18091,"/completeJoin"]:
{{[{<<"targetNode">>,'ns_1@db3.lan'},
   {<<"requestedServices">>,[index,kv,n1ql]},
   {<<"chronicleInfo">>,
    <<"g3QAAAAEZAAPY29tbWl0dGVkX3NlcW5vYRhkAA5jb21wYXRfdmVyc2lvbmEAZAAGY29uZmlnaAVkAAlsb2dfZW50cnltAAAAIDVhYWIwM2RiZWNhZDA2YjUwZmZiNDc0ZGE1OWEwMGM5aAJhBGQAD25zXzFAMTcyLjE5LjAuNGEYaApkAAZjb25maWdoA20AAAAgMWRkNTk5MTQ2M2Q1MTllMjM3MjlkZGEyYzgwYzM3NmJhAGEDYQBtAAAAIDQ1NzI0OTE1ZGFhMmE0ODljMGE2ODE2ZDIwMTk4NTg1dAAAAANkAA9uc18xQDE3Mi4xOS4wLjR0AAAAAmQAAmlkbQAAACAxZGQ1OTkxNDYzZDUxOWUyMzcyOWRkYTJjODBjMzc2YmQABHJvbGVkAAV2b3RlcmQADG5zXzFAZGIyLmxhbnQAAAACZAACaWRtAAAAIDlmNjg5NGI1MWZmYTQ5N2NkZmM4Yjc5MmQ5ZTY3MjZiZAAEcm9sZWQAB3JlcGxpY2FkAAxuc18xQGRiMy5sYW50AAAAAmQAAmlkbQAAACA2NTllYzAzYzgxMmY1Y2M5NDU2YmQ3MWZlM2MxYTA0N2QABHJvbGVkAAdyZXBsaWNhZAAJdW5kZWZpbmVkdAAAAAJkABRjaHJvbmljbGVfY29uZmlnX3JzbWgDZAAKcnNtX2NvbmZpZ2QAFGNocm9uaWNsZV9jb25maWdfcnNtamQAAmt2aANkAApyc21fY29uZmlnZAAMY2hyb25pY2xlX2t2anQAAAAAZAAJdW5kZWZpbmVkbAAAAAFoAm0AAAAgNWFhYjAzZGJlY2FkMDZiNTBmZmI0NzRkYTU5YTAwYzlhAGpkAApoaXN0b3J5X2lkbQAAACA1YWFiMDNkYmVjYWQwNmI1MGZmYjQ3NGRhNTlhMDBjOQ==">>},
   {availableStorage,{[{hdd,[{[{path,<<"/">>},
                               {sizeKBytes,1056557396},
                               {usagePercent,2}]},
                             {[{path,<<"/dev">>},
                               {sizeKBytes,65536},
                               {usagePercent,0}]},
                             {[{path,<<"/dev/shm">>},
                               {sizeKBytes,65536},
                               {usagePercent,0}]},
                             {[{path,<<"/etc/resolv.conf">>},
                               {sizeKBytes,1056557396},
                               {usagePercent,2}]},
                             {[{path,<<"/etc/hostname">>},
                               {sizeKBytes,1056557396},
                               {usagePercent,2}]},
                             {[{path,<<"/etc/hosts">>},
                               {sizeKBytes,1056557396},
                               {usagePercent,2}]},
                             {[{path,<<"/opt/couchbase/var">>},
                               {sizeKBytes,482797652},
                               {usagePercent,92}]},
                             {[{path,<<"/proc/kcore">>},
                               {sizeKBytes,65536},
                               {usagePercent,0}]},
                             {[{path,<<"/proc/keys">>},
                               {sizeKBytes,65536},
                               {usagePercent,0}]},
                             {[{path,<<"/proc/timer_list">>},
                               {sizeKBytes,65536},
                               {usagePercent,0}]},
                             {[{path,<<"/proc/scsi">>},
                               {sizeKBytes,4012680},
                               {usagePercent,0}]},
                             {[{path,<<"/sys/firmware">>},
                               {sizeKBytes,4012680},
                               {usagePercent,0}]}]}]}},
   {storageTotals,{[{ram,{[{total,8217968640},
                           {quotaTotal,3221225472},
                           {quotaUsed,0},
                           {used,4251119616},
                           {usedByData,0},
                           {quotaUsedPerNode,0},
                           {quotaTotalPerNode,3221225472}]}},
                    {hdd,{[{total,494384795648},
                           {quotaTotal,494384795648},
                           {used,454834011996},
                           {usedByData,0},
                           {free,39550783652}]}}]}},
   {storage,{[{ssd,[]},
              {hdd,[{[{path,<<"/opt/couchbase/var/lib/couchbase/data">>},
                      {index_path,<<"/opt/couchbase/var/lib/couchbase/data">>},
                      {cbas_dirs,[<<"/opt/couchbase/var/lib/couchbase/data">>]},
                      {eventing_path,<<"/opt/couchbase/var/lib/couchbase/data">>},
                      {java_home,<<>>},
                      {quotaMb,none},
                      {state,ok}]}]}]}},
   {clusterMembership,active},
   {recoveryType,none},
   {status,<<"healthy">>},
   {otpNode,'ns_1@172.19.0.4'},
   {thisNode,true},
   {hostname,<<"172.19.0.4:8091">>},
   {nodeUUID,<<"28569ac00b9c1d7c50e39741027d428c">>},
   {clusterCompatibility,458758},
   {version,<<"7.6.2-3721-enterprise">>},
   {os,<<"aarch64-unknown-linux-gnu">>},
   {cpuCount,10},
   {ports,{[{direct,11210},
            {httpsMgmt,18091},
            {httpsCAPI,18092},
            {distTCP,21100},
            {distTLS,21150}]}},
   {services,[index,kv,n1ql]},
   {nodeEncryption,false},
   {nodeEncryptionClientCertVerification,false},
   {addressFamilyOnly,false},
   {configuredHostname,<<"172.19.0.4:8091">>},
   {addressFamily,inet},
   {externalListeners,[{[{afamily,inet},{nodeEncryption,false}]}]},
   {serverGroup,<<"Group 1">>},
   {otpCookie,{sanitized,<<"biSYTaQUFDVndTs/b+IcDmD2L0woktv9naRdv3GCK6A=">>}},
   {couchApiBase,<<"http://172.19.0.4:8092/">>},
   {couchApiBaseHTTPS,<<"https://172.19.0.4:18092/">>},
   {nodeHash,38045879},
   {systemStats,{[{cpu_utilization_rate,3.89883035089008},
                  {cpu_stolen_rate,0},
                  {swap_total,1073737728},
                  {swap_used,3956736},
                  {mem_total,8217968640},
                  {mem_free,4805804032},
                  {mem_limit,8217968640},
                  {cpu_cores_available,10},
                  {allocstall,17}]}},
   {interestingStats,{[]}},
   {uptime,<<"44">>},
   {memoryTotal,8217968640},
   {memoryFree,4805804032},
   {mcdMemoryReserved,6269},
   {mcdMemoryAllocated,6269},
   {memoryQuota,3072},
   {queryMemoryQuota,0},
   {indexMemoryQuota,512},
   {ftsMemoryQuota,512},
   {cbasMemoryQuota,1024},
   {eventingMemoryQuota,256}]}}
[error_logger:info,2025-05-15T18:47:25.927Z,ns_1@172.19.0.4:net_kernel<0.2170.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'ns_1@db3.lan',
                          {14108584,#Ref<0.2327127311.3650486281.215813>}}}
[ns_server:debug,2025-05-15T18:47:25.928Z,ns_1@172.19.0.4:net_kernel<0.2170.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'ns_1@db3.lan' using inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:25.928Z,ns_1@172.19.0.4:cb_dist<0.2168.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.2327127311.3650355202.218285>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-05-15T18:47:25.928Z,ns_1@172.19.0.4:cb_dist<0.2168.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.2327127311.3650355202.218285>,
                                  inet_tcp_dist,<0.2712.0>,
                                  #Ref<0.2327127311.3650355202.218288>}
[ns_server:debug,2025-05-15T18:47:25.929Z,ns_1@172.19.0.4:cb_dist<0.2168.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.2327127311.3650355202.218285>,
                               inet_tcp_dist,<0.2712.0>,
                               #Ref<0.2327127311.3650355202.218288>}
[error_logger:info,2025-05-15T18:47:25.929Z,ns_1@172.19.0.4:net_kernel<0.2170.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.2712.0>,{recv_challenge_ack_failed,{error,closed}}}}
[error_logger:info,2025-05-15T18:47:25.929Z,ns_1@172.19.0.4:net_kernel<0.2170.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'ns_1@db3.lan'}}
[ns_server:debug,2025-05-15T18:47:26.042Z,ns_1@172.19.0.4:cb_dist<0.2168.0>:cb_dist:info_msg:1098]cb_dist: Accepted new connection from <0.2171.0> DistCtrl #Port<0.175>: {con,
                                                                         #Ref<0.2327127311.3650355201.221461>,
                                                                         inet_tcp_dist,
                                                                         undefined,
                                                                         undefined}
[ns_server:debug,2025-05-15T18:47:26.042Z,ns_1@172.19.0.4:net_kernel<0.2170.0>:cb_dist:info_msg:1098]cb_dist: Accepting connection from <0.2171.0> using module inet_tcp_dist
[ns_server:debug,2025-05-15T18:47:26.042Z,ns_1@172.19.0.4:cb_dist<0.2168.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.2327127311.3650355201.221461>,
                                  inet_tcp_dist,<0.2718.0>,
                                  #Ref<0.2327127311.3650355205.217917>}
[ns_server:debug,2025-05-15T18:47:26.043Z,ns_1@172.19.0.4:<0.446.0>:doc_replicator:nodeup_monitoring_loop:145]got nodeup event. Considering ddocs replication
[user:info,2025-05-15T18:47:26.043Z,ns_1@172.19.0.4:ns_node_disco<0.537.0>:ns_node_disco:handle_info:163]Node 'ns_1@172.19.0.4' saw that node 'ns_1@db3.lan' came up. Tags: []
[chronicle:info,2025-05-15T18:47:26.043Z,ns_1@172.19.0.4:chronicle_proposer<0.2185.0>:chronicle_proposer:handle_nodeup:1093]Peer 'ns_1@db3.lan' came up
[ns_server:debug,2025-05-15T18:47:26.043Z,ns_1@172.19.0.4:users_replicator<0.434.0>:doc_replicator:loop:79]Replicating all docs to new nodes: ['ns_1@db3.lan']
[ns_server:debug,2025-05-15T18:47:26.043Z,ns_1@172.19.0.4:users_replicator<0.434.0>:replicated_dets:select_from_table:312][dets] Starting select with {users_storage,[{'_',[],['$_']}],500}
[ns_server:debug,2025-05-15T18:47:26.043Z,ns_1@172.19.0.4:ns_node_disco_events<0.536.0>:ns_config_rep:handle_node_disco_event:513]Detected new nodes (['ns_1@db3.lan']).  Moving config around.
[ns_server:info,2025-05-15T18:47:26.043Z,ns_1@172.19.0.4:ns_node_disco_events<0.536.0>:ns_node_disco_log:handle_event:40]ns_node_disco_log: nodes changed: ['ns_1@172.19.0.4','ns_1@db2.lan',
                                   'ns_1@db3.lan']
[ns_server:warn,2025-05-15T18:47:26.044Z,ns_1@172.19.0.4:users_replicator<0.434.0>:doc_replicator:loop:110]Remote server node {users_storage,'ns_1@db3.lan'} process down: noproc
[ns_server:warn,2025-05-15T18:47:26.044Z,ns_1@172.19.0.4:<0.2721.0>:leader_lease_acquire_worker:handle_exception:244]Failed to acquire lease from 'ns_1@db3.lan': {exit,
                                              {noproc,
                                               {gen_server,call,
                                                [{leader_lease_agent,
                                                  'ns_1@db3.lan'},
                                                 {acquire_lease,
                                                  'ns_1@172.19.0.4',
                                                  <<"60878074b48f15926e51cf002c4824e4">>,
                                                  [{timeout,15000},
                                                   {period,15000}]},
                                                 infinity]}}}
[chronicle:debug,2025-05-15T18:47:26.045Z,ns_1@172.19.0.4:chronicle_proposer<0.2185.0>:chronicle_proposer:catchup_peers:1719]Catching up peer 'ns_1@db3.lan' from seqno 0
[chronicle:debug,2025-05-15T18:47:26.045Z,ns_1@172.19.0.4:chronicle_catchup<0.2186.0>:chronicle_catchup:spawn_catchup:151]Starting catchup for peer 'ns_1@db3.lan' at seqno 0
[chronicle:debug,2025-05-15T18:47:26.046Z,ns_1@172.19.0.4:chronicle_proposer<0.2185.0>:chronicle_proposer:handle_catchup_result:981]Caught up peer 'ns_1@db3.lan' to seqno 25
[ns_server:debug,2025-05-15T18:47:26.069Z,ns_1@172.19.0.4:service_manager-index-worker<0.2568.0>:service_janitor:orchestrate_initial_rebalance:97]Initial rebalance progress for `index': [{'ns_1@172.19.0.4',1}]
[ns_server:debug,2025-05-15T18:47:26.116Z,ns_1@172.19.0.4:users_replicator<0.434.0>:doc_replicator:loop:79]Replicating all docs to new nodes: ['ns_1@db3.lan']
[ns_server:debug,2025-05-15T18:47:26.116Z,ns_1@172.19.0.4:users_replicator<0.434.0>:replicated_dets:select_from_table:312][dets] Starting select with {users_storage,[{'_',[],['$_']}],500}
[ns_server:warn,2025-05-15T18:47:26.155Z,ns_1@172.19.0.4:<0.2447.0>:leader_lease_acquire_worker:handle_lease_already_acquired:226]Failed to acquire lease from 'ns_1@db2.lan' because its already taken by {'ns_1@cb.local',
                                                                          <<"445e053598a21dcd845301efeef2f950">>} (valid for 13332ms)
[ns_server:warn,2025-05-15T18:47:27.046Z,ns_1@172.19.0.4:<0.2721.0>:leader_lease_acquire_worker:handle_exception:244]Failed to acquire lease from 'ns_1@db3.lan': {exit,
                                              {noproc,
                                               {gen_server,call,
                                                [{leader_lease_agent,
                                                  'ns_1@db3.lan'},
                                                 {acquire_lease,
                                                  'ns_1@172.19.0.4',
                                                  <<"60878074b48f15926e51cf002c4824e4">>,
                                                  [{timeout,15000},
                                                   {period,15000}]},
                                                 infinity]}}}
[ns_server:info,2025-05-15T18:47:27.285Z,ns_1@172.19.0.4:<0.2338.0>:ns_orchestrator:handle_event:670]Skipping janitor in state janitor_running
[ns_server:debug,2025-05-15T18:47:27.758Z,ns_1@172.19.0.4:compiled_roles_cache<0.454.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@prometheus",stats_reader}
[ns_server:debug,2025-05-15T18:47:27.825Z,ns_1@172.19.0.4:<0.2433.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":68,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"indexAdmin\":9100,\"indexHttp\":9102,\"indexHttps\":19102,\"indexScan\":9101,\"indexStreamCatchup\":9104,\"indexStreamInit\":9103,\"indexStreamMaint\":9105,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true,\"hostname\":\"172.19.0.4\",\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[ns_server:info,2025-05-15T18:47:27.826Z,ns_1@172.19.0.4:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:handle_info:764]cert_and_pkey changed
[ns_server:debug,2025-05-15T18:47:27.826Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',address_family} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|inet]
[ns_server:debug,2025-05-15T18:47:27.826Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',audit} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}]
[ns_server:debug,2025-05-15T18:47:27.826Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',backup_grpc_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9124]
[ns_server:debug,2025-05-15T18:47:27.826Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',backup_http_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|8097]
[ns_server:debug,2025-05-15T18:47:27.826Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',backup_https_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|18097]
[ns_server:debug,2025-05-15T18:47:27.826Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',capi_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|8092]
[ns_server:debug,2025-05-15T18:47:27.826Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_admin_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9110]
[ns_server:debug,2025-05-15T18:47:27.826Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_cc_client_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9113]
[ns_server:debug,2025-05-15T18:47:27.826Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_cc_cluster_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9112]
[ns_server:debug,2025-05-15T18:47:27.826Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_cc_http_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9111]
[ns_server:debug,2025-05-15T18:47:27.826Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_cluster_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9115]
[ns_server:debug,2025-05-15T18:47:27.826Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_console_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9114]
[ns_server:debug,2025-05-15T18:47:27.826Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_data_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9116]
[ns_server:debug,2025-05-15T18:47:27.826Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_debug_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|-1]
[ns_server:debug,2025-05-15T18:47:27.827Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_dirs} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
 "/opt/couchbase/var/lib/couchbase/data"]
[ns_server:debug,2025-05-15T18:47:27.827Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_http_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|8095]
[ns_server:debug,2025-05-15T18:47:27.827Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_messaging_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9118]
[ns_server:debug,2025-05-15T18:47:27.827Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_metadata_callback_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9119]
[ns_server:debug,2025-05-15T18:47:27.827Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_metadata_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9121]
[ns_server:debug,2025-05-15T18:47:27.827Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_parent_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9122]
[ns_server:debug,2025-05-15T18:47:27.827Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_replication_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9120]
[ns_server:debug,2025-05-15T18:47:27.827Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_result_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9117]
[ns_server:debug,2025-05-15T18:47:27.827Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',cbas_ssl_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|18095]
[ns_server:debug,2025-05-15T18:47:27.827Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',client_cert} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{2,63914554046}}]},
 {certs_epoch,0},
 {subject,<<"CN=Couchbase Internal Client (32660726)">>},
 {not_after,63985747645},
 {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
 {load_timestamp,63914554045},
 {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hX"...>>},
 {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDWTCCAkGgAwIBAgIIGD/HyMO1PBAwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjVaFw0y\nNzA4MTcxODQ3MjVaMC8xLTArBgNVBAMTJENvdWNoYmFzZSBJbnRlcm5hbCBDbGll\nbnQgKDMyNjYwNzI2KTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAKHe\n9VAt+X6MU19Fg7ok9H0Zxkmq5mDd0acYZWMO8WqPC7jBY2dopAKDJoF/AKgk8TLT\ncz5"...>>},
 {pkey_passphrase_settings,[]},
 {type,generated},
 {name,"@internal"}]
[ns_server:debug,2025-05-15T18:47:27.827Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',compaction_daemon} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
 {check_interval,30},
 {min_db_file_size,131072},
 {min_view_file_size,20971520}]
[ns_server:debug,2025-05-15T18:47:27.827Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',config_version} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|{7,6}]
[ns_server:debug,2025-05-15T18:47:27.827Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',database_dir} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2025-05-15T18:47:27.827Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',erl_external_listeners} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
 {inet,false}]
[ns_server:debug,2025-05-15T18:47:27.827Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',event_log} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
 {filename,"/opt/couchbase/var/lib/couchbase/event_log"}]
[ns_server:debug,2025-05-15T18:47:27.827Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',eventing_debug_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9140]
[ns_server:debug,2025-05-15T18:47:27.827Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',eventing_dir} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2025-05-15T18:47:27.827Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',eventing_http_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|8096]
[ns_server:debug,2025-05-15T18:47:27.827Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',eventing_https_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|18096]
[ns_server:debug,2025-05-15T18:47:27.827Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',fts_grpc_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9130]
[ns_server:debug,2025-05-15T18:47:27.828Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',fts_grpc_ssl_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|19130]
[ns_server:debug,2025-05-15T18:47:27.828Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',fts_http_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|8094]
[ns_server:debug,2025-05-15T18:47:27.828Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',fts_ssl_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|18094]
[ns_server:debug,2025-05-15T18:47:27.828Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',index_dir} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2025-05-15T18:47:27.828Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',indexer_admin_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9100]
[ns_server:debug,2025-05-15T18:47:27.828Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',indexer_http_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9102]
[ns_server:debug,2025-05-15T18:47:27.828Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',indexer_https_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|19102]
[ns_server:debug,2025-05-15T18:47:27.828Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',indexer_scan_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9101]
[ns_server:debug,2025-05-15T18:47:27.828Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',indexer_stcatchup_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9104]
[ns_server:debug,2025-05-15T18:47:27.828Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',indexer_stinit_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9103]
[ns_server:debug,2025-05-15T18:47:27.828Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',indexer_stmaint_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9105]
[ns_server:debug,2025-05-15T18:47:27.828Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',is_enterprise} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|true]
[ns_server:debug,2025-05-15T18:47:27.828Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',isasl} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
 {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]
[ns_server:debug,2025-05-15T18:47:27.828Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',membership} ->
inactiveAdded
[ns_server:debug,2025-05-15T18:47:27.828Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',memcached} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
 {port,11210},
 {dedicated_port,11209},
 {dedicated_ssl_port,11206},
 {ssl_port,11207},
 {admin_user,"@ns_server"},
 {other_users,["@cbq-engine","@projector","@goxdcr","@index","@fts",
               "@eventing","@cbas","@backup"]},
 {admin_pass,"*****"},
 {engines,[{membase,[{engine,"/opt/couchbase/lib/memcached/ep.so"},
                     {static_config_string,"failpartialwarmup=false"}]},
           {memcached,[{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
                       {static_config_string,"vb0=true"}]}]},
 {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
 {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
 {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
 {log_prefix,"memcached.log"},
 {log_generations,20},
 {log_cyclesize,10485760},
 {log_rotation_period,39003}]
[ns_server:debug,2025-05-15T18:47:27.828Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',memcached_config} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
 {[{interfaces,{memcached_config_mgr,get_interfaces,[]}},
   {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
   {connection_idle_time,connection_idle_time},
   {privilege_debug,privilege_debug},
   {breakpad,
    {[{enabled,breakpad_enabled},
      {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
   {deployment_model,{memcached_config_mgr,get_config_profile,[]}},
   {verbosity,verbosity},
   {audit_file,{"~s",[audit_file]}},
   {rbac_file,{"~s",[rbac_file]}},
   {dedupe_nmvb_maps,dedupe_nmvb_maps},
   {tracing_enabled,tracing_enabled},
   {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
   {xattr_enabled,true},
   {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
   {collections_enabled,true},
   {max_connections,max_connections},
   {system_connections,system_connections},
   {num_reader_threads,num_reader_threads},
   {num_writer_threads,num_writer_threads},
   {num_auxio_threads,num_auxio_threads},
   {num_nonio_threads,num_nonio_threads},
   {num_storage_threads,num_storage_threads},
   {logger,
    {[{filename,{"~s/~s",[log_path,log_prefix]}},{cyclesize,log_cyclesize}]}},
   {external_auth_service,{memcached_config_mgr,get_external_auth_service,[]}},
   {active_external_users_push_interval,
    {memcached_config_mgr,get_external_users_push_interval,[]}},
   {prometheus,{memcached_config_mgr,prometheus_cfg,[]}},
   {sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
   {ssl_sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
   {tcp_keepalive_idle,tcp_keepalive_idle},
   {tcp_keepalive_interval,tcp_keepalive_interval},
   {tcp_keepalive_probes,tcp_keepalive_probes},
   {tcp_user_timeout,tcp_user_timeout},
   {always_collect_trace_info,always_collect_trace_info},
   {connection_limit_mode,connection_limit_mode},
   {free_connection_pool_size,free_connection_pool_size},
   {max_client_connection_details,max_client_connection_details}]}]
[ns_server:debug,2025-05-15T18:47:27.829Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',memcached_dedicated_ssl_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|11206]
[ns_server:debug,2025-05-15T18:47:27.829Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',memcached_defaults} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
 {max_connections,65000},
 {system_connections,5000},
 {connection_idle_time,0},
 {verbosity,0},
 {privilege_debug,false},
 {breakpad_enabled,true},
 {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
 {dedupe_nmvb_maps,false},
 {je_malloc_conf,undefined},
 {tracing_enabled,true},
 {datatype_snappy,true},
 {num_reader_threads,<<"default">>},
 {num_writer_threads,<<"default">>},
 {num_auxio_threads,<<"default">>},
 {num_nonio_threads,<<"default">>},
 {num_storage_threads,<<"default">>},
 {tcp_keepalive_idle,360},
 {tcp_keepalive_interval,10},
 {tcp_keepalive_probes,3},
 {tcp_user_timeout,30},
 {always_collect_trace_info,true},
 {connection_limit_mode,<<"disconnect">>},
 {free_connection_pool_size,0},
 {max_client_connection_details,0}]
[ns_server:debug,2025-05-15T18:47:27.829Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',memcached_prometheus} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|11280]
[ns_server:debug,2025-05-15T18:47:27.829Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',n2n_client_cert_auth} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|false]
[ns_server:debug,2025-05-15T18:47:27.829Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',node_cert} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{2,63914554046}}]},
 {certs_epoch,0},
 {subject,<<"CN=Couchbase Server Node (db3.lan)">>},
 {not_after,63985747645},
 {verified_with,<<192,166,69,7,195,50,150,215,16,31,180,201,215,60,73,246>>},
 {load_timestamp,63914554045},
 {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGD/Hv5zFUhEwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYzY0ZDkx\nNGUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC9RweKonTKraxQqc+3\n5YMcZ+d2cRe4o3aEMozFZAkODd4puG9ZhAJmURS5fNmlRFhdYNO4vCQRI7CnbVIB\nUMl8+hX"...>>},
 {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDOjCCAiKgAwIBAgIIGD/HyL2BZrYwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBjNjRkOTE0ZTAeFw0yNTA1MTQxODQ3MjVaFw0y\nNzA4MTcxODQ3MjVaMCoxKDAmBgNVBAMTH0NvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nZGIzLmxhbikwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCoBie+jXEG\nM9mqURIO0hHIXr1th5jf7TJD7f69DGF3znRjw1VTjnaMZAQOk8Gg44nwI1o1KtJj\nUmf"...>>},
 {pkey_passphrase_settings,[]},
 {type,generated},
 {hostname,"db3.lan"}]
[ns_server:debug,2025-05-15T18:47:27.829Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',node_encryption} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|false]
[ns_server:debug,2025-05-15T18:47:27.829Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',ns_log} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
 {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]
[ns_server:debug,2025-05-15T18:47:27.829Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',port_servers} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}]
[ns_server:debug,2025-05-15T18:47:27.829Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',projector_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9999]
[ns_server:debug,2025-05-15T18:47:27.829Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',projector_ssl_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9999]
[ns_server:debug,2025-05-15T18:47:27.829Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',prometheus_auth_info} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{2,63914554047}}]}|
 {"@prometheus",
  {auth,
   [{<<"hash">>,
     {[{<<"hashes">>,
        {sanitized,<<"K8k/NfCHEgbJA5Gjm/Uf6LIWuV5wdCnmlHHxqgxuFcg=">>}},
       {<<"algorithm">>,<<"argon2id">>},
       {<<"salt">>,<<"DJB2Ptc3EvRFRaPMQY8lxQ==">>},
       {<<"parallelism">>,1},
       {<<"time">>,3},
       {<<"memory">>,524288}]}},
    {<<"scram-sha-512">>,
     {[{<<"salt">>,
        <<"kQZBb8aKY+QE2LMIih5sI+B6I8bZhl0CgtizD2Fc7mLeSYkH0HpqLrT4l2I+g3feMZ02/xAzulfE7pW7LKeNOg==">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"3t2jsPPn+9DcO1shgR+zoz6ZswYB3twV7AiZzEoEZaw=">>}}]}},
    {<<"scram-sha-256">>,
     {[{<<"salt">>,<<"t1Mu2m4fYVtuDH4+kaa1tY0l5MgkTK5scos+czMBlSU=">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"3/WNUnYG1nDOZfZnux37H+3kYuv6M9rzdGe6kGBsDos=">>}}]}},
    {<<"scram-sha-1">>,
     {[{<<"salt">>,<<"ejCbw2caHZmnb35/ZwMKng0kuTY=">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"koVlXdNIWFlL5fpQq2C/7QKV+kuonc55IuSVFkJpchc=">>}}]}}]}}]
[ns_server:debug,2025-05-15T18:47:27.829Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',prometheus_http_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9123]
[ns_server:debug,2025-05-15T18:47:27.829Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',query_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|8093]
[ns_server:debug,2025-05-15T18:47:27.830Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',rest} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]},
 {port,8091},
 {port_meta,global}]
[ns_server:debug,2025-05-15T18:47:27.830Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',saslauthd_enabled} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|true]
[ns_server:debug,2025-05-15T18:47:27.830Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',ssl_capi_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|18092]
[ns_server:debug,2025-05-15T18:47:27.830Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',ssl_query_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|18093]
[ns_server:debug,2025-05-15T18:47:27.830Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',ssl_rest_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|18091]
[ns_server:debug,2025-05-15T18:47:27.830Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',uuid} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|
 <<"80960cc3730024bccd4fc49444efdc14">>]
[ns_server:debug,2025-05-15T18:47:27.830Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',xdcr_rest_port} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|9998]
[ns_server:debug,2025-05-15T18:47:27.830Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@db3.lan',{project_intact,is_vulnerable}} ->
[{'_vclock',[{<<"80960cc3730024bccd4fc49444efdc14">>,{1,63914554046}}]}|false]
[ns_server:debug,2025-05-15T18:47:27.834Z,ns_1@172.19.0.4:<0.2433.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":68,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"indexAdmin\":9100,\"indexHttp\":9102,\"indexHttps\":19102,\"indexScan\":9101,\"indexStreamCatchup\":9104,\"indexStreamInit\":9103,\"indexStreamMaint\":9105,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true,\"hostname\":\"172.19.0.4\",\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[ns_server:debug,2025-05-15T18:47:27.837Z,ns_1@172.19.0.4:ns_ssl_services_setup<0.308.0>:ns_ssl_services_setup:restart_regenerate_client_cert_timer:1447]Time left before client cert regeneration: 70588759000
[cluster:debug,2025-05-15T18:47:27.863Z,ns_1@172.19.0.4:ns_cluster<0.273.0>:ns_cluster:post_json_to_joinee:927]Reply from [https,"db3.lan",18091,"/completeJoin"]:
{ok,[]}
[cluster:debug,2025-05-15T18:47:27.863Z,ns_1@172.19.0.4:ns_cluster<0.273.0>:ns_cluster:handle_call:418]add_node(https, "db3.lan", 18091, undefined, ..) -> {ok,'ns_1@db3.lan'}
[ns_server:debug,2025-05-15T18:47:27.863Z,ns_1@172.19.0.4:ns_audit<0.696.0>:ns_audit:handle_call:168]Audit add_node: [{local,{[{ip,<<"172.19.0.4">>},{port,8091}]}},
                 {remote,{[{ip,<<"172.19.0.4">>},{port,42790}]}},
                 {real_userid,{[{domain,builtin},
                                {user,<<"<ud>jaba_admin</ud>">>}]}},
                 {timestamp,<<"2025-05-15T18:47:27.863Z">>},
                 {user,<<"<ud>jaba_admin</ud>">>},
                 {services,[index,kv,n1ql]},
                 {port,18091},
                 {hostname,<<"db3.lan">>},
                 {node,'ns_1@db3.lan'}]
[ns_server:info,2025-05-15T18:47:28.072Z,ns_1@172.19.0.4:<0.2749.0>:menelaus_web_cluster:do_handle_rebalance:1118]Starting rebalance with params [['ns_1@172.19.0.4','ns_1@db2.lan',
                                 'ns_1@db3.lan'],
                                [],all,[],all]
[ns_server:debug,2025-05-15T18:47:28.080Z,ns_1@172.19.0.4:cleanup_process<0.2339.0>:service_manager:with_trap_exit_spawn_monitor:111]Got an exit signal while running op: rebalance for service: index. Exit message: {'EXIT',
                                                                                  <0.2332.0>,
                                                                                  shutdown}
[ns_server:error,2025-05-15T18:47:28.080Z,ns_1@172.19.0.4:service_manager-index<0.2361.0>:service_manager:run_op_worker:222]Got exit message from parent: {'EXIT',<0.2339.0>,shutdown}
[ns_server:error,2025-05-15T18:47:28.080Z,ns_1@172.19.0.4:service_agent-index<0.2109.0>:service_agent:handle_info:325]Service Manager <0.2361.0> died unexpectedly: shutdown
[ns_server:debug,2025-05-15T18:47:28.081Z,ns_1@172.19.0.4:service_agent-index<0.2109.0>:service_agent:cleanup_service:641]Cleaning up stale tasks:
[[{<<"rev">>,<<"AAAAAAAAAAI=">>},
  {<<"id">>,<<"rebalance/35d452100b72fa9639f18b2cc28124e2">>},
  {<<"type">>,<<"task-rebalance">>},
  {<<"status">>,<<"task-running">>},
  {<<"isCancelable">>,true},
  {<<"progress">>,1},
  {<<"extra">>,
   {[{<<"rebalanceId">>,<<"35d452100b72fa9639f18b2cc28124e2">>}]}}],
 [{<<"rev">>,<<"AAAAAAAAAAA=">>},
  {<<"id">>,<<"prepare/35d452100b72fa9639f18b2cc28124e2">>},
  {<<"type">>,<<"task-prepared">>},
  {<<"status">>,<<"task-running">>},
  {<<"isCancelable">>,true},
  {<<"progress">>,0},
  {<<"extra">>,
   {[{<<"rebalanceId">>,<<"35d452100b72fa9639f18b2cc28124e2">>}]}}]]
[ns_server:info,2025-05-15T18:47:28.083Z,ns_1@172.19.0.4:<0.2338.0>:ns_orchestrator:idle:927]Starting rebalance, KeepNodes = ['ns_1@172.19.0.4','ns_1@db2.lan',
                                 'ns_1@db3.lan'], EjectNodes = [], Failed over and being ejected nodes = []; no delta recovery nodes; Operation Id = 8a34a6593102287e30e8ecd2a3f50285
[user:info,2025-05-15T18:47:28.083Z,ns_1@172.19.0.4:<0.2338.0>:ns_orchestrator:idle:930]Starting rebalance, KeepNodes = ['ns_1@172.19.0.4','ns_1@db2.lan',
                                 'ns_1@db3.lan'], EjectNodes = [], Failed over and being ejected nodes = []; no delta recovery nodes; Operation Id = 8a34a6593102287e30e8ecd2a3f50285
[ns_server:debug,2025-05-15T18:47:28.106Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: counters, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,26})
[{rebalance_start,{1747334848,1}}]
[ns_server:debug,2025-05-15T18:47:28.107Z,ns_1@172.19.0.4:<0.2433.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":69,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"indexAdmin\":9100,\"indexHttp\":9102,\"indexHttps\":19102,\"indexScan\":9101,\"indexStreamCatchup\":9104,\"indexStreamInit\":9103,\"indexStreamMaint\":9105,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true,\"hostname\":\"172.19.0.4\",\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[ns_server:debug,2025-05-15T18:47:28.133Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: rebalance_status, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                     27})
running
[ns_server:debug,2025-05-15T18:47:28.133Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: rebalance_status_uuid, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                          27})
<<"1827da82069c38267f07251824989d63">>
[ns_server:debug,2025-05-15T18:47:28.133Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: rebalancer_pid, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,27})
<0.2826.0>
[ns_server:debug,2025-05-15T18:47:28.133Z,ns_1@172.19.0.4:ns_audit<0.696.0>:ns_audit:handle_call:168]Audit rebalance_initiated: [{local,{[{ip,<<"172.19.0.4">>},{port,8091}]}},
                            {remote,{[{ip,<<"172.19.0.4">>},{port,42794}]}},
                            {real_userid,
                                {[{domain,builtin},
                                  {user,<<"<ud>jaba_admin</ud>">>}]}},
                            {timestamp,<<"2025-05-15T18:47:28.133Z">>},
                            {delta_recovery_buckets,all},
                            {ejected_nodes,[]},
                            {known_nodes,
                                ['ns_1@172.19.0.4','ns_1@db2.lan',
                                 'ns_1@db3.lan']}]
[ns_server:debug,2025-05-15T18:47:28.133Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: rebalance_type, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,27})
rebalance
[ns_server:debug,2025-05-15T18:47:29.041Z,ns_1@172.19.0.4:cb_dist<0.2168.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.2327127311.3650355204.217326>,
                               inet_tcp_dist,<0.2175.0>,
                               #Ref<0.2327127311.3650355204.217328>}
[error_logger:info,2025-05-15T18:47:29.041Z,ns_1@172.19.0.4:net_kernel<0.2170.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.2175.0>,setup_timer_timeout}}
[error_logger:info,2025-05-15T18:47:29.041Z,ns_1@172.19.0.4:net_kernel<0.2170.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'ns_1@cb.local'}}
[ns_server:warn,2025-05-15T18:47:29.049Z,ns_1@172.19.0.4:<0.2721.0>:leader_lease_acquire_worker:handle_lease_already_acquired:226]Failed to acquire lease from 'ns_1@db3.lan' because its already taken by {'ns_1@cb.local',
                                                                          <<"9a39908e5882941cd97af5604964e304">>} (valid for 12423ms)
[ns_server:debug,2025-05-15T18:47:30.111Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/indexing/rebalance/RebalanceToken">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554050}}]}|
 '_deleted']
[ns_server:debug,2025-05-15T18:47:30.111Z,ns_1@172.19.0.4:ns_config_rep<0.545.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([{metakv,
                                   <<"/indexing/rebalance/RebalanceToken">>}]..)
[ns_server:debug,2025-05-15T18:47:30.131Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/indexing/info/versionToken">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554050}}]}|
 <<"{\"Version\":9}">>]
[ns_server:debug,2025-05-15T18:47:30.132Z,ns_1@172.19.0.4:ns_config_rep<0.545.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([{metakv,<<"/indexing/info/versionToken">>}]..)
[ns_server:info,2025-05-15T18:47:32.286Z,ns_1@172.19.0.4:<0.2338.0>:ns_orchestrator:handle_event:670]Skipping janitor in state rebalancing
[ns_server:debug,2025-05-15T18:47:36.755Z,ns_1@172.19.0.4:leader_lease_agent<0.2308.0>:leader_lease_agent:handle_lease_expired:277]Lease held by {lease_holder,<<"12235e8cca1e7367c554bc33cea65983">>,
                            'ns_1@cb.local'} expired. Starting expirer.
[ns_server:warn,2025-05-15T18:47:36.756Z,ns_1@172.19.0.4:<0.2318.0>:leader_lease_acquire_worker:handle_lease_already_acquired:226]Failed to acquire lease from 'ns_1@172.19.0.4' because its already taken by {'ns_1@cb.local',
                                                                             <<"12235e8cca1e7367c554bc33cea65983">>} (valid for 0ms)
[ns_server:debug,2025-05-15T18:47:36.761Z,ns_1@172.19.0.4:leader_lease_agent<0.2308.0>:leader_lease_agent:do_handle_acquire_lease:140]Granting lease to {lease_holder,<<"60878074b48f15926e51cf002c4824e4">>,
                                'ns_1@172.19.0.4'} for 15000ms
[ns_server:info,2025-05-15T18:47:36.764Z,ns_1@172.19.0.4:<0.2318.0>:leader_lease_acquire_worker:handle_fresh_lease_acquired:296]Acquired lease from node 'ns_1@172.19.0.4' (lease uuid: <<"60878074b48f15926e51cf002c4824e4">>)
[ns_server:debug,2025-05-15T18:47:36.764Z,ns_1@172.19.0.4:leader_activities<0.2307.0>:leader_activities:add_activity:590]Added activity:
{activity,<0.3244.0>,#Ref<0.2327127311.3650355202.218843>,default,
          <<"c877e25a74ca744747716ee247d77fee">>,
          [rebalance],
          majority,[]}
[rebalance:info,2025-05-15T18:47:36.765Z,ns_1@172.19.0.4:<0.3245.0>:ns_rebalancer:drop_old_2i_indexes:1354]Going to drop possible old 2i indexes on nodes ['ns_1@db2.lan','ns_1@db3.lan']
[rebalance:info,2025-05-15T18:47:36.766Z,ns_1@172.19.0.4:<0.3245.0>:ns_rebalancer:drop_old_2i_indexes:1360]Going to keep possible 2i indexes on nodes []
[rebalance:debug,2025-05-15T18:47:36.766Z,ns_1@172.19.0.4:<0.3245.0>:ns_rebalancer:drop_old_2i_indexes:1379]Cleanup succeeded: [{'ns_1@db3.lan',ok},{'ns_1@db2.lan',ok}]
[ns_server:debug,2025-05-15T18:47:36.816Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@172.19.0.4',recovery_type}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                           30})
none
[ns_server:debug,2025-05-15T18:47:36.817Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@db2.lan',recovery_type}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                        30})
none
[ns_server:debug,2025-05-15T18:47:36.817Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@db3.lan',recovery_type}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                        30})
none
[ns_server:debug,2025-05-15T18:47:36.817Z,ns_1@172.19.0.4:<0.3245.0>:service_janitor:init_topology_aware_service:82]Doing initial topology change for service `n1ql'
[ns_server:debug,2025-05-15T18:47:36.817Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@172.19.0.4',failover_vbuckets}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                               30})
[]
[ns_server:debug,2025-05-15T18:47:36.817Z,ns_1@172.19.0.4:service_manager-n1ql<0.3252.0>:service_agent:wait_for_agents:74]Waiting for the service agents for service n1ql to come up on nodes:
['ns_1@172.19.0.4']
[ns_server:debug,2025-05-15T18:47:36.817Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@db2.lan',failover_vbuckets}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                            30})
[]
[ns_server:debug,2025-05-15T18:47:36.817Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@db3.lan',failover_vbuckets}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                            30})
[]
[ns_server:debug,2025-05-15T18:47:36.817Z,ns_1@172.19.0.4:service_manager-n1ql<0.3252.0>:service_agent:wait_for_agents_loop:92]All service agents are ready for n1ql
[ns_server:debug,2025-05-15T18:47:36.821Z,ns_1@172.19.0.4:service_manager-n1ql-worker<0.3262.0>:service_manager:do_run_op:250]Got node infos:
[{'ns_1@172.19.0.4',[{node_id,<<"28569ac00b9c1d7c50e39741027d428c">>},
                     {priority,0},
                     {opaque,null}]}]
[ns_server:debug,2025-05-15T18:47:36.821Z,ns_1@172.19.0.4:service_manager-n1ql-worker<0.3262.0>:service_manager:do_run_op:255]Using node 'ns_1@172.19.0.4' as a leader
[rebalance:info,2025-05-15T18:47:36.821Z,ns_1@172.19.0.4:service_manager-n1ql-worker<0.3262.0>:service_manager:rebalance_op:327]Rebalancing service n1ql with id <<"8499a85fbc27d92b66fc0cf657a9e45a">>.
KeepNodes: ['ns_1@172.19.0.4']
EjectNodes: []
DeltaNodes: []
[rebalance:info,2025-05-15T18:47:36.821Z,ns_1@172.19.0.4:service_manager-n1ql-worker<0.3262.0>:service_manager:update_service_map:311]Updating service map for n1ql:
['ns_1@172.19.0.4']
[ns_server:debug,2025-05-15T18:47:36.821Z,ns_1@172.19.0.4:service_manager-n1ql-worker<0.3262.0>:ns_cluster_membership:set_service_map:589]Set service map for service n1ql to ['ns_1@172.19.0.4']
[ns_server:debug,2025-05-15T18:47:36.845Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: {service_map,n1ql}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                       31})
['ns_1@172.19.0.4']
[ns_server:debug,2025-05-15T18:47:36.847Z,ns_1@172.19.0.4:<0.2433.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":76,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"indexAdmin\":9100,\"indexHttp\":9102,\"indexHttps\":19102,\"indexScan\":9101,\"indexStreamCatchup\":9104,\"indexStreamInit\":9103,\"indexStreamMaint\":9105,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"n1ql\":8093,\"n1qlSSL\":18093,\"projector\":9999},\"thisNode\":true,\"hostname\":\"172.19.0.4\",\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[ns_server:debug,2025-05-15T18:47:36.857Z,ns_1@172.19.0.4:service_manager-n1ql<0.3252.0>:service_manager:run_op_worker:216]Worker terminated normally
[ns_server:debug,2025-05-15T18:47:36.860Z,ns_1@172.19.0.4:<0.3245.0>:ns_cluster_membership:set_service_map:589]Set service map for service n1ql to ['ns_1@172.19.0.4']
[ns_server:debug,2025-05-15T18:47:36.883Z,ns_1@172.19.0.4:<0.3245.0>:service_janitor:init_topology_aware_service:85]Initial rebalance for `n1ql` finished successfully
[ns_server:debug,2025-05-15T18:47:36.883Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: {service_map,n1ql}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                       32})
['ns_1@172.19.0.4']
[ns_server:debug,2025-05-15T18:47:36.884Z,ns_1@172.19.0.4:<0.2433.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":77,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"indexAdmin\":9100,\"indexHttp\":9102,\"indexHttps\":19102,\"indexScan\":9101,\"indexStreamCatchup\":9104,\"indexStreamInit\":9103,\"indexStreamMaint\":9105,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"n1ql\":8093,\"n1qlSSL\":18093,\"projector\":9999},\"thisNode\":true,\"hostname\":\"172.19.0.4\",\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[ns_server:debug,2025-05-15T18:47:36.890Z,ns_1@172.19.0.4:<0.3245.0>:chronicle_master:call:71]Calling chronicle_master with {activate_nodes,
                                  ['ns_1@172.19.0.4','ns_1@db2.lan',
                                   'ns_1@db3.lan']}
[ns_server:debug,2025-05-15T18:47:36.920Z,ns_1@172.19.0.4:<0.2326.0>:chronicle_master:handle_oper:342]Starting kv operation {activate_nodes,['ns_1@172.19.0.4','ns_1@db2.lan',
                                       'ns_1@db3.lan']} with lock <<"2c2b97b761cc8e298503d744e5749bcb">>
[ns_server:debug,2025-05-15T18:47:36.920Z,ns_1@172.19.0.4:<0.2326.0>:ns_cluster_membership:activate:223]Activate nodes ['ns_1@172.19.0.4','ns_1@db2.lan','ns_1@db3.lan']
[ns_server:debug,2025-05-15T18:47:36.952Z,ns_1@172.19.0.4:<0.2326.0>:chronicle_master:handle_oper:345]Starting topology operation {activate_nodes,
                                ['ns_1@172.19.0.4','ns_1@db2.lan',
                                 'ns_1@db3.lan']} with lock <<"2c2b97b761cc8e298503d744e5749bcb">>
[ns_server:debug,2025-05-15T18:47:36.953Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: unfinished_topology_operation, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                  35})
{regular,{activate_nodes,['ns_1@172.19.0.4','ns_1@db2.lan','ns_1@db3.lan']},
         <<"2c2b97b761cc8e298503d744e5749bcb">>,
         #Ref<0.2327127311.3650355203.217418>}
[ns_server:debug,2025-05-15T18:47:36.953Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@172.19.0.4',membership}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                        35})
active
[ns_server:debug,2025-05-15T18:47:36.953Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@db2.lan',membership}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                     35})
active
[ns_server:debug,2025-05-15T18:47:36.953Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@db3.lan',membership}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                     35})
active
[ns_server:debug,2025-05-15T18:47:36.955Z,ns_1@172.19.0.4:<0.2433.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":80,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"indexAdmin\":9100,\"indexHttp\":9102,\"indexHttps\":19102,\"indexScan\":9101,\"indexStreamCatchup\":9104,\"indexStreamInit\":9103,\"indexStreamMaint\":9105,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"n1ql\":8093,\"n1qlSSL\":18093,\"projector\":9999},\"thisNode\":true,\"hostname\":\"172.19.0.4\",\"serverGroup\":\"Group 1\"},{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"hostname\":\"db2.lan\",\"serverGroup\":\"Group 1\"},{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"hostname\":\"db3.lan\",\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[ns_server:debug,2025-05-15T18:47:36.984Z,ns_1@172.19.0.4:<0.2326.0>:chronicle_master:remove_oper_key:309]Removing operation key with lock <<"2c2b97b761cc8e298503d744e5749bcb">>
[ns_server:debug,2025-05-15T18:47:37.023Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:handle_info:42]delete (key: unfinished_topology_operation, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                  39})
[ns_server:debug,2025-05-15T18:47:37.023Z,ns_1@172.19.0.4:leader_activities<0.2307.0>:leader_activities:add_activity:590]Added activity:
{activity,<0.3319.0>,#Ref<0.2327127311.3650355208.216755>,default,
          <<"c877e25a74ca744747716ee247d77fee">>,
          [rebalance,update_quorum_nodes],
          majority,[]}
[ns_server:info,2025-05-15T18:47:37.289Z,ns_1@172.19.0.4:<0.2338.0>:ns_orchestrator:handle_event:670]Skipping janitor in state rebalancing
[ns_server:debug,2025-05-15T18:47:37.318Z,ns_1@172.19.0.4:<0.2340.0>:auto_failover_logic:log_master_activity:130]Transitioned node {'ns_1@db2.lan',<<"7a33d46fd1976e65466c5efc8b45ef2e">>} state new -> up
[ns_server:debug,2025-05-15T18:47:37.318Z,ns_1@172.19.0.4:<0.2340.0>:auto_failover_logic:log_master_activity:130]Transitioned node {'ns_1@db3.lan',<<"80960cc3730024bccd4fc49444efdc14">>} state new -> up
[ns_server:warn,2025-05-15T18:47:39.489Z,ns_1@172.19.0.4:<0.2447.0>:leader_lease_acquire_worker:handle_lease_already_acquired:226]Failed to acquire lease from 'ns_1@db2.lan' because its already taken by {'ns_1@cb.local',
                                                                          <<"445e053598a21dcd845301efeef2f950">>} (valid for 0ms)
[ns_server:info,2025-05-15T18:47:39.496Z,ns_1@172.19.0.4:<0.2447.0>:leader_lease_acquire_worker:handle_fresh_lease_acquired:296]Acquired lease from node 'ns_1@db2.lan' (lease uuid: <<"60878074b48f15926e51cf002c4824e4">>)
[ns_server:debug,2025-05-15T18:47:39.497Z,ns_1@172.19.0.4:leader_activities<0.2307.0>:leader_activities:handle_switch_quorum:898]Updating quorum for activity {default,[rebalance,update_quorum_nodes],
                                      <0.3319.0>} to [majority,
                                                      {majority,
                                                       {set,3,16,16,8,80,48,
                                                        {[],[],[],[],[],[],[],
                                                         [],[],[],[],[],[],[],
                                                         [],[]},
                                                        {{[],[],[],[],
                                                          ['ns_1@172.19.0.4'],
                                                          [],[],[],[],[],[],
                                                          [],[],[],
                                                          ['ns_1@db3.lan',
                                                           'ns_1@db2.lan'],
                                                          []}}}}]
[ns_server:info,2025-05-15T18:47:39.497Z,ns_1@172.19.0.4:leader_quorum_nodes_manager<0.2319.0>:leader_quorum_nodes_manager:handle_set_quorum_nodes:121]Updating quorum nodes.
Old quorum nodes: ['ns_1@172.19.0.4']
New quorum nodes: ['ns_1@172.19.0.4','ns_1@db2.lan','ns_1@db3.lan']
[ns_server:debug,2025-05-15T18:47:39.500Z,ns_1@172.19.0.4:leader_quorum_nodes_manager<0.2319.0>:leader_quorum_nodes_manager:push_config:135]Attempting to synchronize config to ['ns_1@db2.lan','ns_1@db3.lan']
[ns_server:debug,2025-05-15T18:47:39.500Z,ns_1@172.19.0.4:ns_config_rep<0.545.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([quorum_nodes]..)
[ns_server:debug,2025-05-15T18:47:39.500Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
quorum_nodes ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{2,63914554059}}]},
 'ns_1@172.19.0.4','ns_1@db2.lan','ns_1@db3.lan']
[ns_server:debug,2025-05-15T18:47:39.502Z,ns_1@172.19.0.4:ns_config_rep<0.545.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 788 us
[ns_server:debug,2025-05-15T18:47:39.512Z,ns_1@172.19.0.4:leader_activities<0.2307.0>:leader_activities:handle_activity_down:457]Activity terminated with reason normal. Activity:
{activity,<0.3319.0>,#Ref<0.2327127311.3650355208.216755>,default,
          <<"c877e25a74ca744747716ee247d77fee">>,
          [rebalance,update_quorum_nodes],
          [majority,
           {majority,{set,3,16,16,8,80,48,
                          {[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]},
                          {{[],[],[],[],
                            ['ns_1@172.19.0.4'],
                            [],[],[],[],[],[],[],[],[],
                            ['ns_1@db3.lan','ns_1@db2.lan'],
                            []}}}}],
          []}
[rebalance:debug,2025-05-15T18:47:39.519Z,ns_1@172.19.0.4:<0.3245.0>:ns_rebalancer:rebalance_kv:579]BucketConfigs = []
[ns_server:debug,2025-05-15T18:47:39.529Z,ns_1@172.19.0.4:service_manager-n1ql<0.3442.0>:service_agent:wait_for_agents:74]Waiting for the service agents for service n1ql to come up on nodes:
['ns_1@172.19.0.4','ns_1@db2.lan','ns_1@db3.lan']
[ns_server:debug,2025-05-15T18:47:39.530Z,ns_1@172.19.0.4:service_manager-n1ql<0.3442.0>:service_agent:wait_for_agents_loop:92]All service agents are ready for n1ql
[ns_server:debug,2025-05-15T18:47:39.662Z,ns_1@172.19.0.4:service_manager-n1ql-worker<0.3476.0>:service_manager:do_run_op:250]Got node infos:
[{'ns_1@db3.lan',[{node_id,<<"80960cc3730024bccd4fc49444efdc14">>},
                  {priority,0},
                  {opaque,null}]},
 {'ns_1@db2.lan',[{node_id,<<"7a33d46fd1976e65466c5efc8b45ef2e">>},
                  {priority,0},
                  {opaque,null}]},
 {'ns_1@172.19.0.4',[{node_id,<<"28569ac00b9c1d7c50e39741027d428c">>},
                     {priority,0},
                     {opaque,null}]}]
[ns_server:debug,2025-05-15T18:47:39.663Z,ns_1@172.19.0.4:service_manager-n1ql-worker<0.3476.0>:service_manager:do_run_op:255]Using node 'ns_1@172.19.0.4' as a leader
[rebalance:info,2025-05-15T18:47:39.663Z,ns_1@172.19.0.4:service_manager-n1ql-worker<0.3476.0>:service_manager:rebalance_op:327]Rebalancing service n1ql with id <<"1b481865da30bbe878d8f78e4808e91d">>.
KeepNodes: ['ns_1@172.19.0.4','ns_1@db2.lan','ns_1@db3.lan']
EjectNodes: []
DeltaNodes: []
[rebalance:info,2025-05-15T18:47:39.663Z,ns_1@172.19.0.4:service_manager-n1ql-worker<0.3476.0>:service_manager:update_service_map:311]Updating service map for n1ql:
['ns_1@172.19.0.4','ns_1@db2.lan','ns_1@db3.lan']
[ns_server:debug,2025-05-15T18:47:39.663Z,ns_1@172.19.0.4:service_manager-n1ql-worker<0.3476.0>:ns_cluster_membership:set_service_map:589]Set service map for service n1ql to ['ns_1@172.19.0.4','ns_1@db2.lan',
                                     'ns_1@db3.lan']
[ns_server:debug,2025-05-15T18:47:39.687Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: {service_map,n1ql}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                       40})
['ns_1@172.19.0.4','ns_1@db2.lan','ns_1@db3.lan']
[ns_server:debug,2025-05-15T18:47:39.689Z,ns_1@172.19.0.4:<0.2433.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":86,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"indexAdmin\":9100,\"indexHttp\":9102,\"indexHttps\":19102,\"indexScan\":9101,\"indexStreamCatchup\":9104,\"indexStreamInit\":9103,\"indexStreamMaint\":9105,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"n1ql\":8093,\"n1qlSSL\":18093,\"projector\":9999},\"thisNode\":true,\"hostname\":\"172.19.0.4\",\"serverGroup\":\"Group 1\"},{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"n1ql\":8093,\"n1qlSSL\":18093,\"projector\":9999},\"hostname\":\"db2.lan\",\"serverGroup\":\"Group 1\"},{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"n1ql\":8093,\"n1qlSSL\":18093,\"projector\":9999},\"hostname\":\"db3.lan\",\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[ns_server:debug,2025-05-15T18:47:39.696Z,ns_1@172.19.0.4:service_manager-n1ql<0.3442.0>:service_manager:run_op_worker:216]Worker terminated normally
[ns_server:debug,2025-05-15T18:47:39.699Z,ns_1@172.19.0.4:service_manager-index<0.3508.0>:service_agent:wait_for_agents:74]Waiting for the service agents for service index to come up on nodes:
['ns_1@172.19.0.4','ns_1@db2.lan','ns_1@db3.lan']
[ns_server:debug,2025-05-15T18:47:39.699Z,ns_1@172.19.0.4:service_manager-index<0.3508.0>:service_agent:wait_for_agents_loop:92]All service agents are ready for index
[ns_server:debug,2025-05-15T18:47:39.702Z,ns_1@172.19.0.4:service_manager-index-worker<0.3526.0>:service_manager:do_run_op:250]Got node infos:
[{'ns_1@db2.lan',[{node_id,<<"7a33d46fd1976e65466c5efc8b45ef2e">>},
                  {priority,7060000},
                  {opaque,null}]},
 {'ns_1@db3.lan',[{node_id,<<"80960cc3730024bccd4fc49444efdc14">>},
                  {priority,7060000},
                  {opaque,null}]},
 {'ns_1@172.19.0.4',[{node_id,<<"28569ac00b9c1d7c50e39741027d428c">>},
                     {priority,7060000},
                     {opaque,null}]}]
[ns_server:debug,2025-05-15T18:47:39.702Z,ns_1@172.19.0.4:service_manager-index-worker<0.3526.0>:service_manager:do_run_op:255]Using node 'ns_1@172.19.0.4' as a leader
[rebalance:info,2025-05-15T18:47:39.702Z,ns_1@172.19.0.4:service_manager-index-worker<0.3526.0>:service_manager:rebalance_op:327]Rebalancing service index with id <<"62c17460b6c2fb91a6a08c24ea499705">>.
KeepNodes: ['ns_1@172.19.0.4','ns_1@db2.lan','ns_1@db3.lan']
EjectNodes: []
DeltaNodes: []
[rebalance:info,2025-05-15T18:47:39.702Z,ns_1@172.19.0.4:service_manager-index-worker<0.3526.0>:service_manager:update_service_map:311]Updating service map for index:
['ns_1@172.19.0.4','ns_1@db2.lan','ns_1@db3.lan']
[ns_server:debug,2025-05-15T18:47:39.702Z,ns_1@172.19.0.4:service_manager-index-worker<0.3526.0>:ns_cluster_membership:set_service_map:589]Set service map for service index to ['ns_1@172.19.0.4','ns_1@db2.lan',
                                      'ns_1@db3.lan']
[ns_server:debug,2025-05-15T18:47:39.728Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: {service_map,index}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                        41})
['ns_1@172.19.0.4','ns_1@db2.lan','ns_1@db3.lan']
[ns_server:debug,2025-05-15T18:47:39.729Z,ns_1@172.19.0.4:<0.2433.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":87,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"indexAdmin\":9100,\"indexHttp\":9102,\"indexHttps\":19102,\"indexScan\":9101,\"indexStreamCatchup\":9104,\"indexStreamInit\":9103,\"indexStreamMaint\":9105,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"n1ql\":8093,\"n1qlSSL\":18093,\"projector\":9999},\"thisNode\":true,\"hostname\":\"172.19.0.4\",\"serverGroup\":\"Group 1\"},{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"indexAdmin\":9100,\"indexHttp\":9102,\"indexHttps\":19102,\"indexScan\":9101,\"indexStreamCatchup\":9104,\"indexStreamInit\":9103,\"indexStreamMaint\":9105,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"n1ql\":8093,\"n1qlSSL\":18093,\"projector\":9999},\"hostname\":\"db2.lan\",\"serverGroup\":\"Group 1\"},{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"indexAdmin\":9100,\"indexHttp\":9102,\"indexHttps\":19102,\"indexScan\":9101,\"indexStreamCatchup\":9104,\"indexStreamInit\":9103,\"indexStreamMaint\":9105,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"n1ql\":8093,\"n1qlSSL\":18093,\"projector\":9999},\"hostname\":\"db3.lan\",\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[ns_server:debug,2025-05-15T18:47:39.753Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/indexing/rebalance/RebalanceToken">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{3,63914554059}}]}|
 <<"{\"MasterId\":\"28569ac00b9c1d7c50e39741027d428c\",\"RebalId\":\"62c17460b6c2fb91a6a08c24ea499705\",\"Source\":0,\"Error\":\"\",\"MasterIP\":\"172.19.0.4\",\"Version\":0,\"RebalPhase\":0,\"ActiveRebalancer\":1}">>]
[ns_server:debug,2025-05-15T18:47:39.755Z,ns_1@172.19.0.4:ns_config_rep<0.545.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([{metakv,
                                   <<"/indexing/rebalance/RebalanceToken">>}]..)
[ns_server:debug,2025-05-15T18:47:40.873Z,ns_1@172.19.0.4:cb_saml<0.429.0>:cb_saml:refresh_metadata:526]Refreshing metadata
[ns_server:debug,2025-05-15T18:47:40.873Z,ns_1@172.19.0.4:cb_saml<0.429.0>:cb_saml:restart_refresh_timer:583]Disabling refresh timer
[ns_server:debug,2025-05-15T18:47:41.090Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{4,63914554061}}]}|
 <<"{\"indexer.plasma.backIndex.enablePageBloomFilter\":true,\"indexer.settings.allow_large_keys\":true,\"indexer.settings.bufferPoolBlockSize\":16384,\"indexer.settings.build.batch_size\":5,\"indexer.settings.compaction.abort_exceed_interval\":false,\"indexer.settings.compaction.check_period\":30,\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.days_of_week\":\"Sund"...>>]
[ns_server:debug,2025-05-15T18:47:41.091Z,ns_1@172.19.0.4:ns_config_rep<0.545.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([{metakv,<<"/indexing/settings/config">>}]..)
[ns_server:debug,2025-05-15T18:47:41.091Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/indexing/settings/config/features/PlasmaEnablePageBloomFilterBackIndex">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554061}}]}|
 <<"{}">>]
[ns_server:debug,2025-05-15T18:47:41.092Z,ns_1@172.19.0.4:ns_config_rep<0.545.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([{metakv,
                                   <<"/indexing/settings/config/features/PlasmaEnablePageBloomFilterBackIndex">>}]..)
[ns_server:warn,2025-05-15T18:47:41.476Z,ns_1@172.19.0.4:<0.2721.0>:leader_lease_acquire_worker:handle_lease_already_acquired:226]Failed to acquire lease from 'ns_1@db3.lan' because its already taken by {'ns_1@cb.local',
                                                                          <<"9a39908e5882941cd97af5604964e304">>} (valid for 0ms)
[ns_server:info,2025-05-15T18:47:41.482Z,ns_1@172.19.0.4:<0.2721.0>:leader_lease_acquire_worker:handle_fresh_lease_acquired:296]Acquired lease from node 'ns_1@db3.lan' (lease uuid: <<"60878074b48f15926e51cf002c4824e4">>)
[ns_server:info,2025-05-15T18:47:42.291Z,ns_1@172.19.0.4:<0.2338.0>:ns_orchestrator:handle_event:670]Skipping janitor in state rebalancing
[ns_server:info,2025-05-15T18:47:47.292Z,ns_1@172.19.0.4:<0.2338.0>:ns_orchestrator:handle_event:670]Skipping janitor in state rebalancing
[ns_server:debug,2025-05-15T18:47:47.293Z,ns_1@172.19.0.4:ns_audit<0.696.0>:ns_audit:handle_call:168]Audit auth_failure: [{local,{[{ip,<<"172.19.0.4">>},{port,8091}]}},
                     {remote,{[{ip,<<"172.19.0.1">>},{port,60564}]}},
                     {timestamp,<<"2025-05-15T18:47:47.293Z">>},
                     {raw_url,<<"<ud>/pools/default/tasks</ud>">>}]
[ns_server:debug,2025-05-15T18:47:47.293Z,ns_1@172.19.0.4:ns_audit<0.696.0>:ns_audit:handle_call:168]Audit auth_failure: [{local,{[{ip,<<"172.19.0.4">>},{port,8091}]}},
                     {remote,{[{ip,<<"172.19.0.1">>},{port,60548}]}},
                     {timestamp,<<"2025-05-15T18:47:47.293Z">>},
                     {raw_url,<<"<ud>/settings/autoFailover</ud>">>}]
[ns_server:debug,2025-05-15T18:47:47.293Z,ns_1@172.19.0.4:ns_audit<0.696.0>:ns_audit:handle_call:168]Audit auth_failure: [{local,{[{ip,<<"172.19.0.4">>},{port,8091}]}},
                     {remote,{[{ip,<<"172.19.0.1">>},{port,60580}]}},
                     {timestamp,<<"2025-05-15T18:47:47.293Z">>},
                     {raw_url,<<"<ud>/pools/default/stats/range/</ud>">>}]
[ns_server:debug,2025-05-15T18:47:47.293Z,ns_1@172.19.0.4:ns_audit<0.696.0>:ns_audit:handle_call:168]Audit auth_failure: [{local,{[{ip,<<"172.19.0.4">>},{port,8091}]}},
                     {remote,{[{ip,<<"172.19.0.1">>},{port,60568}]}},
                     {timestamp,<<"2025-05-15T18:47:47.293Z">>},
                     {raw_url,<<"<ud>/pools/default/pendingRetryRebalance</ud>">>}]
[ns_server:info,2025-05-15T18:47:48.777Z,ns_1@172.19.0.4:ns_config_rep<0.545.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@db3.lan'
[user:info,2025-05-15T18:47:49.539Z,ns_1@172.19.0.4:<0.4051.0>:menelaus_web_alerts_srv:global_alert:227]Approaching full disk warning. Usage of disk "/opt/couchbase/var" on node "172.19.0.4" is around 92%.
[ns_server:debug,2025-05-15T18:47:49.671Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2025-05-15T18:47:49.672Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:47:49.672Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2025-05-15T18:47:49.672Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:47:49.773Z,ns_1@172.19.0.4:ns_config_rep<0.545.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([{metakv,
                                   <<"/indexing/rebalance/RebalanceToken">>}]..)
[ns_server:debug,2025-05-15T18:47:49.773Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/indexing/rebalance/RebalanceToken">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{4,63914554069}}]}|
 '_deleted']
[ns_server:debug,2025-05-15T18:47:49.787Z,ns_1@172.19.0.4:service_manager-index<0.3508.0>:service_manager:run_op_worker:216]Worker terminated normally
[ns_server:debug,2025-05-15T18:47:49.800Z,ns_1@172.19.0.4:leader_activities<0.2307.0>:leader_activities:add_activity:590]Added activity:
{activity,<0.4099.0>,#Ref<0.2327127311.3650355206.216713>,default,
          <<"c877e25a74ca744747716ee247d77fee">>,
          [rebalance,update_quorum_nodes],
          majority,[]}
[ns_server:debug,2025-05-15T18:47:49.801Z,ns_1@172.19.0.4:leader_activities<0.2307.0>:leader_activities:handle_switch_quorum:898]Updating quorum for activity {default,[rebalance,update_quorum_nodes],
                                      <0.4099.0>} to [majority,
                                                      {majority,
                                                       {set,3,16,16,8,80,48,
                                                        {[],[],[],[],[],[],[],
                                                         [],[],[],[],[],[],[],
                                                         [],[]},
                                                        {{[],[],[],[],
                                                          ['ns_1@172.19.0.4'],
                                                          [],[],[],[],[],[],
                                                          [],[],[],
                                                          ['ns_1@db3.lan',
                                                           'ns_1@db2.lan'],
                                                          []}}}}]
[ns_server:debug,2025-05-15T18:47:49.801Z,ns_1@172.19.0.4:leader_activities<0.2307.0>:leader_activities:handle_activity_down:457]Activity terminated with reason normal. Activity:
{activity,<0.4099.0>,#Ref<0.2327127311.3650355206.216713>,default,
          <<"c877e25a74ca744747716ee247d77fee">>,
          [rebalance,update_quorum_nodes],
          [majority,
           {majority,{set,3,16,16,8,80,48,
                          {[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]},
                          {{[],[],[],[],
                            ['ns_1@172.19.0.4'],
                            [],[],[],[],[],[],[],[],[],
                            ['ns_1@db3.lan','ns_1@db2.lan'],
                            []}}}}],
          []}
[ns_server:debug,2025-05-15T18:47:49.802Z,ns_1@172.19.0.4:leader_activities<0.2307.0>:leader_activities:handle_activity_down:457]Activity terminated with reason normal. Activity:
{activity,<0.3244.0>,#Ref<0.2327127311.3650355202.218843>,default,
          <<"c877e25a74ca744747716ee247d77fee">>,
          [rebalance],
          majority,[]}
[user:info,2025-05-15T18:47:49.802Z,ns_1@172.19.0.4:<0.2338.0>:ns_orchestrator:log_rebalance_completion:1661]Rebalance completed successfully.
Rebalance Operation Id = 8a34a6593102287e30e8ecd2a3f50285
[ns_server:debug,2025-05-15T18:47:49.828Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: counters, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,44})
[{rebalance_success,{1747334869,1}},{rebalance_start,{1747334848,1}}]
[ns_server:debug,2025-05-15T18:47:49.830Z,ns_1@172.19.0.4:<0.2433.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":94,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"indexAdmin\":9100,\"indexHttp\":9102,\"indexHttps\":19102,\"indexScan\":9101,\"indexStreamCatchup\":9104,\"indexStreamInit\":9103,\"indexStreamMaint\":9105,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"n1ql\":8093,\"n1qlSSL\":18093,\"projector\":9999},\"thisNode\":true,\"hostname\":\"172.19.0.4\",\"serverGroup\":\"Group 1\"},{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"indexAdmin\":9100,\"indexHttp\":9102,\"indexHttps\":19102,\"indexScan\":9101,\"indexStreamCatchup\":9104,\"indexStreamInit\":9103,\"indexStreamMaint\":9105,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"n1ql\":8093,\"n1qlSSL\":18093,\"projector\":9999},\"hostname\":\"db2.lan\",\"serverGroup\":\"Group 1\"},{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"indexAdmin\":9100,\"indexHttp\":9102,\"indexHttps\":19102,\"indexScan\":9101,\"indexStreamCatchup\":9104,\"indexStreamInit\":9103,\"indexStreamMaint\":9105,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"n1ql\":8093,\"n1qlSSL\":18093,\"projector\":9999},\"hostname\":\"db3.lan\",\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[ns_server:debug,2025-05-15T18:47:49.834Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
rebalance_reports ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554069}}]},
 {<<"c1b1e82bc3018faaa44cc124f5be1eeb">>,
  [{node,'ns_1@172.19.0.4'},
   {filename,"rebalance_report_20250515T184749.json"}]}]
[ns_server:debug,2025-05-15T18:47:49.834Z,ns_1@172.19.0.4:ns_config_rep<0.545.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([rebalance_reports]..)
[ns_server:debug,2025-05-15T18:47:49.857Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: rebalance_status, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                     45})
none
[ns_server:info,2025-05-15T18:47:49.857Z,ns_1@172.19.0.4:leader_registry<0.2311.0>:leader_registry:handle_down:286]Process <0.2824.0> registered as 'ns_rebalance_observer' terminated.
[ns_server:debug,2025-05-15T18:47:49.857Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: rebalance_status_uuid, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                          45})
<<"e30f0e0de45bc0da5422537090074df6">>
[ns_server:debug,2025-05-15T18:47:49.857Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: rebalancer_pid, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,45})
undefined
[ns_server:debug,2025-05-15T18:47:49.857Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: rebalance_type, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,45})
rebalance
[ns_server:debug,2025-05-15T18:47:50.300Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: bucket_names, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,46})
["doom-scrolling"]
[ns_server:debug,2025-05-15T18:47:50.300Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: {bucket,"doom-scrolling",props}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                    46})
[{replica_index,true},
 {ram_quota,536870912},
 {durability_min_level,none},
 {num_vbuckets,1024},
 {cross_cluster_versioning_enabled,false},
 {version_pruning_window_hrs,720},
 {pitr_enabled,false},
 {pitr_granularity,600},
 {pitr_max_history_age,86400},
 {flush_enabled,false},
 {num_threads,3},
 {eviction_policy,value_only},
 {conflict_resolution_type,seqno},
 {storage_mode,couchstore},
 {max_ttl,0},
 {compression_mode,passive},
 {rank,0},
 {type,membase},
 {num_replicas,1},
 {replication_topology,star},
 {repl_type,dcp},
 {servers,[]}]
[ns_server:debug,2025-05-15T18:47:50.300Z,ns_1@172.19.0.4:roles_cache<0.458.0>:active_cache:clean:181]Clearing the roles_cache cache
[ns_server:debug,2025-05-15T18:47:50.300Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: {bucket,"doom-scrolling",uuid}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                   46})
<<"f70f8d64d14cf7513107bff35eb6561d">>
[ns_server:debug,2025-05-15T18:47:50.300Z,ns_1@172.19.0.4:roles_cache<0.458.0>:active_cache:clean:181]Clearing the roles_cache cache
[ns_server:debug,2025-05-15T18:47:50.300Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: {bucket,"doom-scrolling",collections}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                          46})
[{uid,1},
 {next_uid,2},
 {next_scope_uid,9},
 {next_coll_uid,10},
 {num_scopes,0},
 {num_collections,0},
 {scopes,[{"_default",
           [{uid,0},{collections,[{"_default",[{uid,0},{maxTTL,0}]}]}]},
          {"_system",
           [{uid,8},
            {collections,[{"_query",[{uid,9},{maxTTL,-1},{history,false}]},
                          {"_mobile",
                           [{uid,8},{maxTTL,-1},{history,false}]}]}]}]}]
[ns_server:debug,2025-05-15T18:47:50.300Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@172.19.0.4',
                   {"doom-scrolling",last_seen_collection_ids}}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                                       46})
[2,9,10]
[ns_server:debug,2025-05-15T18:47:50.300Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@db2.lan',{"doom-scrolling",last_seen_collection_ids}}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                                                      46})
[2,9,10]
[ns_server:debug,2025-05-15T18:47:50.300Z,ns_1@172.19.0.4:compiled_roles_cache<0.454.0>:versioned_cache:handle_info:89]Flushing cache compiled_roles_cache due to version change from {[7,6],
                                                                {0,249091395},
                                                                {0,249091395},
                                                                true,[]} to {[7,
                                                                              6],
                                                                             {0,
                                                                              249091395},
                                                                             {0,
                                                                              249091395},
                                                                             true,
                                                                             [{"doom-scrolling",
                                                                               <<"f70f8d64d14cf7513107bff35eb6561d">>,
                                                                               1}]}
[ns_server:debug,2025-05-15T18:47:50.300Z,ns_1@172.19.0.4:ns_bucket_worker<0.709.0>:ns_bucket_worker:start_one_uploader:153]Starting uploader for bucket: "doom-scrolling"
[ns_server:debug,2025-05-15T18:47:50.300Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@db3.lan',{"doom-scrolling",last_seen_collection_ids}}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                                                      46})
[2,9,10]
[ns_server:debug,2025-05-15T18:47:50.302Z,ns_1@172.19.0.4:ns_audit<0.696.0>:ns_audit:handle_call:168]Audit create_bucket: [{local,{[{ip,<<"172.19.0.4">>},{port,8091}]}},
                      {remote,{[{ip,<<"172.19.0.4">>},{port,46536}]}},
                      {real_userid,{[{domain,builtin},
                                     {user,<<"<ud>jaba_admin</ud>">>}]}},
                      {timestamp,<<"2025-05-15T18:47:50.300Z">>},
                      {props,{[{rank,0},
                               {compression_mode,passive},
                               {max_ttl,0},
                               {storage_mode,couchstore},
                               {conflict_resolution_type,seqno},
                               {eviction_policy,value_only},
                               {num_threads,3},
                               {flush_enabled,false},
                               {pitr_max_history_age,86400},
                               {pitr_granularity,600},
                               {pitr_enabled,false},
                               {version_pruning_window_hrs,720},
                               {cross_cluster_versioning_enabled,false},
                               {num_vbuckets,1024},
                               {durability_min_level,none},
                               {ram_quota,536870912},
                               {replica_index,true}]}},
                      {type,membase},
                      {bucket_name,<<"doom-scrolling">>}]
[menelaus:info,2025-05-15T18:47:50.302Z,ns_1@172.19.0.4:<0.4006.0>:menelaus_web_buckets:do_bucket_create:880]Created bucket "doom-scrolling" of type: couchbase
[{replica_index,true},
 {ram_quota,536870912},
 {durability_min_level,none},
 {num_vbuckets,1024},
 {cross_cluster_versioning_enabled,false},
 {version_pruning_window_hrs,720},
 {pitr_enabled,false},
 {pitr_granularity,600},
 {pitr_max_history_age,86400},
 {flush_enabled,false},
 {num_threads,3},
 {eviction_policy,value_only},
 {conflict_resolution_type,seqno},
 {storage_mode,couchstore},
 {max_ttl,0},
 {compression_mode,passive},
 {rank,0}]
[ns_server:debug,2025-05-15T18:47:50.303Z,ns_1@172.19.0.4:memcached_permissions<0.531.0>:memcached_cfg:write_cfg:156]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2025-05-15T18:47:50.306Z,ns_1@172.19.0.4:memcached_permissions<0.531.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,{user,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2025-05-15T18:47:50.308Z,ns_1@172.19.0.4:memcached_refresh<0.304.0>:memcached_refresh:handle_call:57]File rename from "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac" is requested
[ns_server:debug,2025-05-15T18:47:50.310Z,ns_1@172.19.0.4:memcached_permissions<0.531.0>:memcached_cfg:rename_and_refresh:178]Successfully renamed "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2025-05-15T18:47:50.311Z,ns_1@172.19.0.4:memcached_refresh<0.304.0>:memcached_refresh:handle_cast:67]Refresh of rbac requested
[ns_server:debug,2025-05-15T18:47:50.311Z,ns_1@172.19.0.4:terse_bucket_info_uploader-doom-scrolling<0.4194.0>:memcached_config_mgr:memcached_port_pid:154]waiting for completion of initial ns_ports_setup round
[ns_server:debug,2025-05-15T18:47:50.312Z,ns_1@172.19.0.4:memcached_permissions<0.531.0>:memcached_cfg:write_cfg:156]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[error_logger:info,2025-05-15T18:47:50.312Z,ns_1@172.19.0.4:ns_bucket_sup<0.708.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_bucket_sup}
    started: [{pid,<0.4194.0>},
              {id,{terse_bucket_info_uploader,"doom-scrolling"}},
              {mfargs,
                  {terse_bucket_info_uploader,start_link,["doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:50.314Z,ns_1@172.19.0.4:memcached_permissions<0.531.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,{user,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2025-05-15T18:47:50.316Z,ns_1@172.19.0.4:<0.4192.0>:memcached_refresh:do_refresh:153]Successfully connected to memcached, Trying to refresh [rbac]
[ns_server:debug,2025-05-15T18:47:50.317Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:update_servers:147]janitor decided to update servers list for bucket "doom-scrolling" to ['ns_1@172.19.0.4',
                                                                       'ns_1@db2.lan',
                                                                       'ns_1@db3.lan']
[ns_server:debug,2025-05-15T18:47:50.318Z,ns_1@172.19.0.4:memcached_refresh<0.304.0>:memcached_refresh:handle_call:57]File rename from "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac" is requested
[ns_server:debug,2025-05-15T18:47:50.318Z,ns_1@172.19.0.4:memcached_permissions<0.531.0>:memcached_cfg:rename_and_refresh:178]Successfully renamed "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2025-05-15T18:47:50.318Z,ns_1@172.19.0.4:memcached_refresh<0.304.0>:memcached_refresh:handle_cast:67]Refresh of rbac requested
[ns_server:debug,2025-05-15T18:47:50.319Z,ns_1@172.19.0.4:memcached_refresh<0.304.0>:memcached_refresh:update_refresh_state:116]Refresh of [rbac] succeeded
[ns_server:debug,2025-05-15T18:47:50.321Z,ns_1@172.19.0.4:<0.4203.0>:memcached_refresh:do_refresh:153]Successfully connected to memcached, Trying to refresh [rbac]
[ns_server:debug,2025-05-15T18:47:50.322Z,ns_1@172.19.0.4:memcached_refresh<0.304.0>:memcached_refresh:update_refresh_state:116]Refresh of [rbac] succeeded
[ns_server:info,2025-05-15T18:47:50.341Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:cleanup_with_membase_bucket_check_map:198]janitor decided to generate initial vbucket map
[ns_server:debug,2025-05-15T18:47:50.341Z,ns_1@172.19.0.4:roles_cache<0.458.0>:active_cache:clean:181]Clearing the roles_cache cache
[ns_server:debug,2025-05-15T18:47:50.341Z,ns_1@172.19.0.4:ns_bucket_worker<0.709.0>:ns_bucket_worker:start_one_bucket:125]Starting new bucket: "doom-scrolling"
[ns_server:debug,2025-05-15T18:47:50.341Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: {bucket,"doom-scrolling",props}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                    47})
[{replica_index,true},
 {ram_quota,536870912},
 {durability_min_level,none},
 {num_vbuckets,1024},
 {cross_cluster_versioning_enabled,false},
 {version_pruning_window_hrs,720},
 {pitr_enabled,false},
 {pitr_granularity,600},
 {pitr_max_history_age,86400},
 {flush_enabled,false},
 {num_threads,3},
 {eviction_policy,value_only},
 {conflict_resolution_type,seqno},
 {storage_mode,couchstore},
 {max_ttl,0},
 {compression_mode,passive},
 {rank,0},
 {type,membase},
 {num_replicas,1},
 {replication_topology,star},
 {repl_type,dcp},
 {servers,['ns_1@172.19.0.4','ns_1@db2.lan','ns_1@db3.lan']},
 {map_diff,[]},
 {fastForwardMap_diff,[]}]
[ns_server:debug,2025-05-15T18:47:50.345Z,ns_1@172.19.0.4:<0.4202.0>:mb_map:generate_map_new:341]Scores for past maps:
[]
[ns_server:debug,2025-05-15T18:47:50.345Z,ns_1@172.19.0.4:<0.4213.0>:mb_map:do_invoke_vbmap_body:766]Node Id Map: [{'ns_1@db3.lan',0},{'ns_1@172.19.0.4',1},{'ns_1@db2.lan',2}]
[ns_server:debug,2025-05-15T18:47:50.347Z,ns_1@172.19.0.4:single_bucket_kv_sup-doom-scrolling<0.4214.0>:single_bucket_kv_sup:sync_config_to_couchdb_node:64]Syncing config to couchdb node
[ns_server:debug,2025-05-15T18:47:50.347Z,ns_1@172.19.0.4:ns_config_rep<0.545.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 55 us
[ns_server:debug,2025-05-15T18:47:50.348Z,ns_1@172.19.0.4:single_bucket_kv_sup-doom-scrolling<0.4214.0>:single_bucket_kv_sup:sync_config_to_couchdb_node:70]Synced config to couchdb node successfully
[ns_server:debug,2025-05-15T18:47:50.348Z,ns_1@172.19.0.4:<0.4213.0>:mb_map:do_invoke_vbmap_body:775]Wrote vbmap to "/opt/couchbase/var/lib/couchbase/tmp/prev-vbmap-576460752303422812_118.json"
[ns_server:debug,2025-05-15T18:47:50.352Z,ns_1@172.19.0.4:compiled_roles_cache<0.454.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@index",admin}
[ns_server:debug,2025-05-15T18:47:50.352Z,ns_1@172.19.0.4:compiled_roles_cache<0.454.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@cbq-engine",admin}
[ns_server:debug,2025-05-15T18:47:50.353Z,ns_1@172.19.0.4:compiled_roles_cache<0.454.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@projector",admin}
[ns_server:debug,2025-05-15T18:47:50.353Z,ns_1@172.19.0.4:compiled_roles_cache<0.454.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@goxdcr",admin}
[ns_server:debug,2025-05-15T18:47:50.353Z,ns_1@172.19.0.4:terse_bucket_info_uploader-doom-scrolling<0.4194.0>:memcached_config_mgr:memcached_port_pid:156]ns_ports_setup seems to be ready
[ns_server:debug,2025-05-15T18:47:50.359Z,ns_1@172.19.0.4:terse_bucket_info_uploader-doom-scrolling<0.4194.0>:memcached_config_mgr:find_port_pid_loop:164]Found memcached port <16971.139.0>
[ns_server:debug,2025-05-15T18:47:50.359Z,ns_1@172.19.0.4:terse_bucket_info_uploader-doom-scrolling<0.4194.0>:terse_bucket_info_uploader:flush_refresh_msgs:94]Flushed 1 refresh messages
[ns_server:debug,2025-05-15T18:47:50.360Z,ns_1@172.19.0.4:terse_bucket_info_uploader-doom-scrolling<0.4194.0>:terse_bucket_info_uploader:maybe_set_cluster_config:144]Bucket 'doom-scrolling' needs vbmap before setting cluster config
[ns_server:debug,2025-05-15T18:47:50.361Z,ns_1@172.19.0.4:capi_doc_replicator-doom-scrolling<0.4222.0>:replicated_storage:wait_for_startup:47]Start waiting for startup
[error_logger:info,2025-05-15T18:47:50.361Z,ns_1@172.19.0.4:<0.4220.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.4220.0>,docs_kv_sup}
    started: [{pid,<0.4222.0>},
              {id,doc_replicator},
              {mfargs,{capi_ddoc_manager,start_replicator,["doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:50.361Z,ns_1@172.19.0.4:capi_ddoc_replication_srv-doom-scrolling<0.4223.0>:replicated_storage:wait_for_startup:47]Start waiting for startup
[error_logger:info,2025-05-15T18:47:50.363Z,ns_1@172.19.0.4:<0.4220.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.4220.0>,docs_kv_sup}
    started: [{pid,<0.4223.0>},
              {id,doc_replication_srv},
              {mfargs,{doc_replication_srv,start_link,["doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:50.379Z,ns_1@172.19.0.4:logger_proxy<0.70.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'capi_ddoc_manager_sup-doom-scrolling'}
    started: [{pid,<16972.1059.0>},
              {id,capi_ddoc_manager_events},
              {mfargs,
                  {capi_ddoc_manager,start_link_event_manager,
                      ["doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:50.387Z,ns_1@172.19.0.4:<0.4220.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.4220.0>,docs_kv_sup}
    started: [{pid,<16972.1058.0>},
              {id,capi_ddoc_manager_sup},
              {mfargs,
                  {capi_ddoc_manager_sup,start_link_remote,
                      ['couchdb_ns_1@cb.local',"doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:47:50.387Z,ns_1@172.19.0.4:capi_doc_replicator-doom-scrolling<0.4222.0>:replicated_storage:wait_for_startup:50]Received replicated storage registration from <16972.1060.0>
[ns_server:debug,2025-05-15T18:47:50.387Z,ns_1@172.19.0.4:capi_ddoc_replication_srv-doom-scrolling<0.4223.0>:replicated_storage:wait_for_startup:50]Received replicated storage registration from <16972.1060.0>
[error_logger:info,2025-05-15T18:47:50.387Z,ns_1@172.19.0.4:logger_proxy<0.70.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'capi_ddoc_manager_sup-doom-scrolling'}
    started: [{pid,<16972.1060.0>},
              {id,capi_ddoc_manager},
              {mfargs,
                  {capi_ddoc_manager,start_link,
                      ["doom-scrolling",<0.4222.0>,<0.4223.0>]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:50.397Z,ns_1@172.19.0.4:<0.4220.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.4220.0>,docs_kv_sup}
    started: [{pid,<16972.1062.0>},
              {id,capi_set_view_manager},
              {mfargs,
                  {capi_set_view_manager,start_link_remote,
                      ['couchdb_ns_1@cb.local',"doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:50.405Z,ns_1@172.19.0.4:<0.4220.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.4220.0>,docs_kv_sup}
    started: [{pid,<16972.1064.0>},
              {id,couch_stats_reader},
              {mfargs,
                  {couch_stats_reader,start_link_remote,
                      ['couchdb_ns_1@cb.local',"doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:50.405Z,ns_1@172.19.0.4:single_bucket_kv_sup-doom-scrolling<0.4214.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'single_bucket_kv_sup-doom-scrolling'}
    started: [{pid,<0.4220.0>},
              {id,{docs_kv_sup,"doom-scrolling"}},
              {mfargs,{docs_kv_sup,start_link,["doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:47:50.409Z,ns_1@172.19.0.4:ns_memcached-doom-scrolling<0.4260.0>:ns_memcached:init:161]Starting ns_memcached
[ns_server:debug,2025-05-15T18:47:50.409Z,ns_1@172.19.0.4:<0.4261.0>:ns_memcached:run_connect_phase:188]Started 'connecting' phase of ns_memcached-doom-scrolling. Parent is <0.4260.0>
[error_logger:info,2025-05-15T18:47:50.409Z,ns_1@172.19.0.4:<0.4259.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.4259.0>,ns_memcached_sup}
    started: [{pid,<0.4260.0>},
              {id,{ns_memcached,"doom-scrolling"}},
              {mfargs,{ns_memcached,start_link,["doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,86400000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:50.409Z,ns_1@172.19.0.4:single_bucket_kv_sup-doom-scrolling<0.4214.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'single_bucket_kv_sup-doom-scrolling'}
    started: [{pid,<0.4259.0>},
              {id,{ns_memcached_sup,"doom-scrolling"}},
              {mfargs,{ns_memcached_sup,start_link,["doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:50.411Z,ns_1@172.19.0.4:single_bucket_kv_sup-doom-scrolling<0.4214.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'single_bucket_kv_sup-doom-scrolling'}
    started: [{pid,<0.4262.0>},
              {id,{dcp_sup,"doom-scrolling"}},
              {mfargs,{dcp_sup,start_link,["doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:50.416Z,ns_1@172.19.0.4:single_bucket_kv_sup-doom-scrolling<0.4214.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'single_bucket_kv_sup-doom-scrolling'}
    started: [{pid,<0.4263.0>},
              {id,{dcp_replication_manager,"doom-scrolling"}},
              {mfargs,{dcp_replication_manager,start_link,["doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:50.419Z,ns_1@172.19.0.4:single_bucket_kv_sup-doom-scrolling<0.4214.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'single_bucket_kv_sup-doom-scrolling'}
    started: [{pid,<0.4266.0>},
              {id,{replication_manager,"doom-scrolling"}},
              {mfargs,{replication_manager,start_link,["doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:50.443Z,ns_1@172.19.0.4:janitor_agent_sup-doom-scrolling<0.4267.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'janitor_agent_sup-doom-scrolling'}
    started: [{pid,<0.4268.0>},
              {id,rebalance_subprocesses_registry},
              {mfargs,
                  {ns_process_registry,start_link,
                      ['rebalance_subprocesses_registry-doom-scrolling',
                       [{terminate_command,kill}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,86400000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:50.451Z,ns_1@172.19.0.4:janitor_agent_sup-doom-scrolling<0.4267.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'janitor_agent_sup-doom-scrolling'}
    started: [{pid,<0.4269.0>},
              {id,janitor_agent},
              {mfargs,{janitor_agent,start_link,["doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:50.452Z,ns_1@172.19.0.4:janitor_agent-doom-scrolling<0.4269.0>:dcp_sup:nuke:110]Nuking DCP replicators for bucket "doom-scrolling":
[]
[error_logger:info,2025-05-15T18:47:50.452Z,ns_1@172.19.0.4:single_bucket_kv_sup-doom-scrolling<0.4214.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'single_bucket_kv_sup-doom-scrolling'}
    started: [{pid,<0.4267.0>},
              {id,{janitor_agent_sup,"doom-scrolling"}},
              {mfargs,{janitor_agent_sup,start_link,["doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-05-15T18:47:50.452Z,ns_1@172.19.0.4:single_bucket_kv_sup-doom-scrolling<0.4214.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'single_bucket_kv_sup-doom-scrolling'}
    started: [{pid,<0.4270.0>},
              {id,{stats_reader,"doom-scrolling"}},
              {mfargs,{stats_reader,start_link,["doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:50.452Z,ns_1@172.19.0.4:single_bucket_kv_sup-doom-scrolling<0.4214.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'single_bucket_kv_sup-doom-scrolling'}
    started: [{pid,<0.4272.0>},
              {id,{goxdcr_stats_reader,"doom-scrolling"}},
              {mfargs,{stats_reader,start_link,["@xdcr-doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-05-15T18:47:50.452Z,ns_1@172.19.0.4:ns_bucket_sup<0.708.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_bucket_sup}
    started: [{pid,<0.4214.0>},
              {id,{single_bucket_kv_sup,"doom-scrolling"}},
              {mfargs,{single_bucket_kv_sup,start_link,["doom-scrolling"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-05-15T18:47:50.458Z,ns_1@172.19.0.4:capi_doc_replicator-doom-scrolling<0.4222.0>:doc_replicator:loop:79]Replicating all docs to new nodes: ['ns_1@db2.lan','ns_1@db3.lan']
[ns_server:info,2025-05-15T18:47:50.470Z,ns_1@172.19.0.4:ns_memcached-doom-scrolling<0.4260.0>:ns_memcached:do_ensure_bucket:1592]Created bucket "doom-scrolling" with config string "max_size=536870912;dbname=/opt/couchbase/var/lib/couchbase/data/doom-scrolling;backend=couchdb;couch_bucket=doom-scrolling;max_vbuckets=1024;alog_path=/opt/couchbase/var/lib/couchbase/data/doom-scrolling/access.log;data_traffic_enabled=false;max_num_workers=3;uuid=f70f8d64d14cf7513107bff35eb6561d;conflict_resolution_type=seqno;bucket_type=persistent;durability_min_level=none;pitr_enabled=false;pitr_granularity=600;pitr_max_history_age=86400;item_eviction_policy=value_only;persistent_metadata_purge_age=259200;max_ttl=0;ht_locks=47;compression_mode=passive;max_num_shards=0;failpartialwarmup=false"
[ns_server:debug,2025-05-15T18:47:50.470Z,ns_1@172.19.0.4:ns_memcached-doom-scrolling<0.4260.0>:memcached_bucket_config:ensure_collections:291]Applying collection manifest to bucket "doom-scrolling" due to id change from <<"0">> to <<"1">>.
[ns_server:info,2025-05-15T18:47:50.481Z,ns_1@172.19.0.4:ns_memcached-doom-scrolling<0.4260.0>:ns_memcached:handle_info:837]Main ns_memcached connection established: {ok,#Port<0.218>}
[ns_server:debug,2025-05-15T18:47:50.482Z,ns_1@172.19.0.4:terse_bucket_info_uploader-doom-scrolling<0.4194.0>:terse_bucket_info_uploader:maybe_set_cluster_config:144]Bucket 'doom-scrolling' needs vbmap before setting cluster config
[user:info,2025-05-15T18:47:50.482Z,ns_1@172.19.0.4:ns_memcached-doom-scrolling<0.4260.0>:ns_memcached:handle_cast:806]Bucket "doom-scrolling" loaded on node 'ns_1@172.19.0.4' in 0 seconds.
[ns_server:info,2025-05-15T18:47:50.489Z,ns_1@172.19.0.4:janitor_agent-doom-scrolling<0.4269.0>:janitor_agent:read_flush_counter:969]Loading flushseq failed: {error,enoent}. Assuming it's equal to global config.
[ns_server:info,2025-05-15T18:47:50.489Z,ns_1@172.19.0.4:janitor_agent-doom-scrolling<0.4269.0>:janitor_agent:read_flush_counter_from_config:977]Initialized flushseq 0 from bucket config
[ns_server:debug,2025-05-15T18:47:50.600Z,ns_1@172.19.0.4:<0.4213.0>:mb_map:do_invoke_vbmap_body:799]vbmap diag output:
Started as:
  /opt/couchbase/bin/vbmap --diag /opt/couchbase/var/lib/couchbase/tmp/vbmap_diag-576460752303422844_118 --output-format json --num-vbuckets 1024 --num-nodes 3 --num-slaves 10 --num-replicas 1 --relax-all --greedy --current-map /opt/couchbase/var/lib/couchbase/tmp/prev-vbmap-576460752303422812_118.json
Using 1747334870352046009 as a seed
Tags are not specified. Assuming every node on a separate tag.
Finalized parameters
  Number of nodes: 3
  Number of slaves: 2
  Number of vbuckets: 1024
  Number of replicas: 1
  Tags assignments:
    0 -> 0
    1 -> 1
    2 -> 2
Succesfully read current vbucket map:
[
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1]
]
Prev R:
RI (greedy):
  N |    |  0   1   2 |
----|-----------------|
    |  T |  0   1   2 |
----|-----------------|
  0 |  0 |  0   1   1 | 2
  1 |  1 |  1   0   1 | 2
  2 |  2 |  1   1   0 | 2
____|_________________|
    |    |  2   2   2 |
R (greedy):
  N |    |  0   1   2 |
----|-----------------|
    |  T |  0   1   2 |
----|-----------------|
  0 |  0 |  0 171 171 | 342
  1 |  1 |170   0 171 | 341
  2 |  2 |171 170   0 | 341
____|_________________|
    |    |341 341 342 |
Time spent for vbmap generation (greedy) - 66.14425ms
Per-node Vb Stats (greedy):
   Active Vbs:   map[0:342 1:341 2:341]
   Replica Vbs:  map[0:341 1:341 2:342]
Move stats:
    Active takeovers - 1024
    Total new replicas - 2048

[ns_server:debug,2025-05-15T18:47:50.601Z,ns_1@172.19.0.4:<0.4213.0>:mb_map:do_invoke_vbmap_body:817]Score before simple minimization: {2048,1024}
[ns_server:debug,2025-05-15T18:47:50.602Z,ns_1@172.19.0.4:<0.4213.0>:mb_map:do_invoke_vbmap_body:823]Score after simple minimization: {2048,1024}
[ns_server:debug,2025-05-15T18:47:50.602Z,ns_1@172.19.0.4:<0.4213.0>:mb_map:do_invoke_vbmap_body:832]Map better after simple minimization;
                                       using it
[ns_server:debug,2025-05-15T18:47:50.603Z,ns_1@172.19.0.4:<0.4290.0>:mb_map:do_invoke_vbmap_body:766]Node Id Map: [{'ns_1@db3.lan',0},{'ns_1@172.19.0.4',1},{'ns_1@db2.lan',2}]
[ns_server:debug,2025-05-15T18:47:50.605Z,ns_1@172.19.0.4:<0.4290.0>:mb_map:do_invoke_vbmap_body:775]Wrote vbmap to "/opt/couchbase/var/lib/couchbase/tmp/prev-vbmap-576460752303422748_118.json"
[ns_server:debug,2025-05-15T18:47:50.624Z,ns_1@172.19.0.4:compiled_roles_cache<0.454.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@",admin}
[ns_server:debug,2025-05-15T18:47:50.708Z,ns_1@172.19.0.4:compiled_roles_cache<0.454.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@prometheus",stats_reader}
[ns_server:debug,2025-05-15T18:47:50.792Z,ns_1@172.19.0.4:<0.4290.0>:mb_map:do_invoke_vbmap_body:799]vbmap diag output:
Started as:
  /opt/couchbase/bin/vbmap --diag /opt/couchbase/var/lib/couchbase/tmp/vbmap_diag-576460752303422780_118 --output-format json --num-vbuckets 1024 --num-nodes 3 --num-slaves 10 --num-replicas 1 --relax-all --greedy --current-map /opt/couchbase/var/lib/couchbase/tmp/prev-vbmap-576460752303422748_118.json
Using 1747334870606360176 as a seed
Tags are not specified. Assuming every node on a separate tag.
Finalized parameters
  Number of nodes: 3
  Number of slaves: 2
  Number of vbuckets: 1024
  Number of replicas: 1
  Tags assignments:
    0 -> 0
    1 -> 1
    2 -> 2
Succesfully read current vbucket map:
[
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1]
]
Prev R:
RI (greedy):
  N |    |  0   1   2 |
----|-----------------|
    |  T |  0   1   2 |
----|-----------------|
  0 |  0 |  0   1   1 | 2
  1 |  1 |  1   0   1 | 2
  2 |  2 |  1   1   0 | 2
____|_________________|
    |    |  2   2   2 |
R (greedy):
  N |    |  0   1   2 |
----|-----------------|
    |  T |  0   1   2 |
----|-----------------|
  0 |  0 |  0 170 171 | 341
  1 |  1 |171   0 171 | 342
  2 |  2 |170 171   0 | 341
____|_________________|
    |    |341 341 342 |
Time spent for vbmap generation (greedy) - 66.39675ms
Per-node Vb Stats (greedy):
   Active Vbs:   map[0:341 1:342 2:341]
   Replica Vbs:  map[0:341 1:341 2:342]
Move stats:
    Active takeovers - 1024
    Total new replicas - 2048

[ns_server:debug,2025-05-15T18:47:50.793Z,ns_1@172.19.0.4:<0.4290.0>:mb_map:do_invoke_vbmap_body:817]Score before simple minimization: {2048,1024}
[ns_server:debug,2025-05-15T18:47:50.794Z,ns_1@172.19.0.4:<0.4290.0>:mb_map:do_invoke_vbmap_body:823]Score after simple minimization: {2048,1024}
[ns_server:debug,2025-05-15T18:47:50.794Z,ns_1@172.19.0.4:<0.4290.0>:mb_map:do_invoke_vbmap_body:832]Map better after simple minimization;
                                       using it
[ns_server:debug,2025-05-15T18:47:50.795Z,ns_1@172.19.0.4:<0.4320.0>:mb_map:do_invoke_vbmap_body:766]Node Id Map: [{'ns_1@db3.lan',0},{'ns_1@172.19.0.4',1},{'ns_1@db2.lan',2}]
[ns_server:debug,2025-05-15T18:47:50.797Z,ns_1@172.19.0.4:<0.4320.0>:mb_map:do_invoke_vbmap_body:775]Wrote vbmap to "/opt/couchbase/var/lib/couchbase/tmp/prev-vbmap-576460752303422684_118.json"
[ns_server:debug,2025-05-15T18:47:50.944Z,ns_1@172.19.0.4:<0.4320.0>:mb_map:do_invoke_vbmap_body:799]vbmap diag output:
Started as:
  /opt/couchbase/bin/vbmap --diag /opt/couchbase/var/lib/couchbase/tmp/vbmap_diag-576460752303422716_118 --output-format json --num-vbuckets 1024 --num-nodes 3 --num-slaves 10 --num-replicas 1 --relax-all --greedy --current-map /opt/couchbase/var/lib/couchbase/tmp/prev-vbmap-576460752303422684_118.json
Using 1747334870798221134 as a seed
Tags are not specified. Assuming every node on a separate tag.
Finalized parameters
  Number of nodes: 3
  Number of slaves: 2
  Number of vbuckets: 1024
  Number of replicas: 1
  Tags assignments:
    0 -> 0
    1 -> 1
    2 -> 2
Succesfully read current vbucket map:
[
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1]
]
Prev R:
RI (greedy):
  N |    |  0   1   2 |
----|-----------------|
    |  T |  0   1   2 |
----|-----------------|
  0 |  0 |  0   1   1 | 2
  1 |  1 |  1   0   1 | 2
  2 |  2 |  1   1   0 | 2
____|_________________|
    |    |  2   2   2 |
R (greedy):
  N |    |  0   1   2 |
----|-----------------|
    |  T |  0   1   2 |
----|-----------------|
  0 |  0 |  0 171 170 | 341
  1 |  1 |171   0 171 | 342
  2 |  2 |171 170   0 | 341
____|_________________|
    |    |342 341 341 |
Time spent for vbmap generation (greedy) - 69.346667ms
Per-node Vb Stats (greedy):
   Active Vbs:   map[0:341 1:342 2:341]
   Replica Vbs:  map[0:342 1:341 2:341]
Move stats:
    Active takeovers - 1024
    Total new replicas - 2048

[ns_server:debug,2025-05-15T18:47:50.945Z,ns_1@172.19.0.4:<0.4320.0>:mb_map:do_invoke_vbmap_body:817]Score before simple minimization: {2048,1024}
[ns_server:debug,2025-05-15T18:47:50.946Z,ns_1@172.19.0.4:<0.4320.0>:mb_map:do_invoke_vbmap_body:823]Score after simple minimization: {2048,1024}
[ns_server:debug,2025-05-15T18:47:50.946Z,ns_1@172.19.0.4:<0.4320.0>:mb_map:do_invoke_vbmap_body:832]Map better after simple minimization;
                                       using it
[ns_server:debug,2025-05-15T18:47:50.947Z,ns_1@172.19.0.4:<0.4322.0>:mb_map:do_invoke_vbmap_body:766]Node Id Map: [{'ns_1@db2.lan',0},{'ns_1@172.19.0.4',1},{'ns_1@db3.lan',2}]
[ns_server:debug,2025-05-15T18:47:50.949Z,ns_1@172.19.0.4:<0.4322.0>:mb_map:do_invoke_vbmap_body:775]Wrote vbmap to "/opt/couchbase/var/lib/couchbase/tmp/prev-vbmap-576460752303422620_118.json"
[ns_server:debug,2025-05-15T18:47:51.103Z,ns_1@172.19.0.4:<0.4322.0>:mb_map:do_invoke_vbmap_body:799]vbmap diag output:
Started as:
  /opt/couchbase/bin/vbmap --diag /opt/couchbase/var/lib/couchbase/tmp/vbmap_diag-576460752303422652_118 --output-format json --num-vbuckets 1024 --num-nodes 3 --num-slaves 10 --num-replicas 1 --relax-all --greedy --current-map /opt/couchbase/var/lib/couchbase/tmp/prev-vbmap-576460752303422620_118.json
Using 1747334870951723010 as a seed
Tags are not specified. Assuming every node on a separate tag.
Finalized parameters
  Number of nodes: 3
  Number of slaves: 2
  Number of vbuckets: 1024
  Number of replicas: 1
  Tags assignments:
    0 -> 0
    1 -> 1
    2 -> 2
Succesfully read current vbucket map:
[
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1]
]
Prev R:
RI (greedy):
  N |    |  0   1   2 |
----|-----------------|
    |  T |  0   1   2 |
----|-----------------|
  0 |  0 |  0   1   1 | 2
  1 |  1 |  1   0   1 | 2
  2 |  2 |  1   1   0 | 2
____|_________________|
    |    |  2   2   2 |
R (greedy):
  N |    |  0   1   2 |
----|-----------------|
    |  T |  0   1   2 |
----|-----------------|
  0 |  0 |  0 171 170 | 341
  1 |  1 |170   0 171 | 341
  2 |  2 |171 171   0 | 342
____|_________________|
    |    |341 342 341 |
Time spent for vbmap generation (greedy) - 71.974458ms
Per-node Vb Stats (greedy):
   Active Vbs:   map[0:341 1:341 2:342]
   Replica Vbs:  map[0:341 1:342 2:341]
Move stats:
    Active takeovers - 1024
    Total new replicas - 2048

[ns_server:debug,2025-05-15T18:47:51.104Z,ns_1@172.19.0.4:<0.4322.0>:mb_map:do_invoke_vbmap_body:817]Score before simple minimization: {2048,1024}
[ns_server:debug,2025-05-15T18:47:51.105Z,ns_1@172.19.0.4:<0.4322.0>:mb_map:do_invoke_vbmap_body:823]Score after simple minimization: {2048,1024}
[ns_server:debug,2025-05-15T18:47:51.105Z,ns_1@172.19.0.4:<0.4322.0>:mb_map:do_invoke_vbmap_body:832]Map better after simple minimization;
                                       using it
[ns_server:debug,2025-05-15T18:47:51.106Z,ns_1@172.19.0.4:<0.4336.0>:mb_map:do_invoke_vbmap_body:766]Node Id Map: [{'ns_1@db2.lan',0},{'ns_1@172.19.0.4',1},{'ns_1@db3.lan',2}]
[ns_server:debug,2025-05-15T18:47:51.107Z,ns_1@172.19.0.4:<0.4336.0>:mb_map:do_invoke_vbmap_body:775]Wrote vbmap to "/opt/couchbase/var/lib/couchbase/tmp/prev-vbmap-576460752303422556_118.json"
[ns_server:debug,2025-05-15T18:47:51.321Z,ns_1@172.19.0.4:<0.4336.0>:mb_map:do_invoke_vbmap_body:799]vbmap diag output:
Started as:
  /opt/couchbase/bin/vbmap --diag /opt/couchbase/var/lib/couchbase/tmp/vbmap_diag-576460752303422588_118 --output-format json --num-vbuckets 1024 --num-nodes 3 --num-slaves 10 --num-replicas 1 --relax-all --greedy --current-map /opt/couchbase/var/lib/couchbase/tmp/prev-vbmap-576460752303422556_118.json
Using 1747334871110477301 as a seed
Tags are not specified. Assuming every node on a separate tag.
Finalized parameters
  Number of nodes: 3
  Number of slaves: 2
  Number of vbuckets: 1024
  Number of replicas: 1
  Tags assignments:
    0 -> 0
    1 -> 1
    2 -> 2
Succesfully read current vbucket map:
[
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1]
]
Prev R:
RI (greedy):
  N |    |  0   1   2 |
----|-----------------|
    |  T |  0   1   2 |
----|-----------------|
  0 |  0 |  0   1   1 | 2
  1 |  1 |  1   0   1 | 2
  2 |  2 |  1   1   0 | 2
____|_________________|
    |    |  2   2   2 |
R (greedy):
  N |    |  0   1   2 |
----|-----------------|
    |  T |  0   1   2 |
----|-----------------|
  0 |  0 |  0 172 170 | 342
  1 |  1 |170   0 171 | 341
  2 |  2 |171 170   0 | 341
____|_________________|
    |    |341 342 341 |
Time spent for vbmap generation (greedy) - 72.491375ms
Per-node Vb Stats (greedy):
   Active Vbs:   map[0:342 1:341 2:341]
   Replica Vbs:  map[0:341 1:342 2:341]
Move stats:
    Active takeovers - 1024
    Total new replicas - 2048

[ns_server:debug,2025-05-15T18:47:51.323Z,ns_1@172.19.0.4:<0.4336.0>:mb_map:do_invoke_vbmap_body:817]Score before simple minimization: {2048,1024}
[ns_server:debug,2025-05-15T18:47:51.324Z,ns_1@172.19.0.4:<0.4336.0>:mb_map:do_invoke_vbmap_body:823]Score after simple minimization: {2048,1024}
[ns_server:debug,2025-05-15T18:47:51.324Z,ns_1@172.19.0.4:<0.4336.0>:mb_map:do_invoke_vbmap_body:832]Map better after simple minimization;
                                       using it
[ns_server:debug,2025-05-15T18:47:51.325Z,ns_1@172.19.0.4:<0.4343.0>:mb_map:do_invoke_vbmap_body:766]Node Id Map: [{'ns_1@db2.lan',0},{'ns_1@172.19.0.4',1},{'ns_1@db3.lan',2}]
[ns_server:debug,2025-05-15T18:47:51.327Z,ns_1@172.19.0.4:<0.4343.0>:mb_map:do_invoke_vbmap_body:775]Wrote vbmap to "/opt/couchbase/var/lib/couchbase/tmp/prev-vbmap-576460752303422815_118.json"
[ns_server:debug,2025-05-15T18:47:51.361Z,ns_1@172.19.0.4:terse_bucket_info_uploader-doom-scrolling<0.4194.0>:terse_bucket_info_uploader:maybe_set_cluster_config:144]Bucket 'doom-scrolling' needs vbmap before setting cluster config
[ns_server:debug,2025-05-15T18:47:51.472Z,ns_1@172.19.0.4:<0.4343.0>:mb_map:do_invoke_vbmap_body:799]vbmap diag output:
Started as:
  /opt/couchbase/bin/vbmap --diag /opt/couchbase/var/lib/couchbase/tmp/vbmap_diag-576460752303422847_118 --output-format json --num-vbuckets 1024 --num-nodes 3 --num-slaves 10 --num-replicas 1 --relax-all --greedy --current-map /opt/couchbase/var/lib/couchbase/tmp/prev-vbmap-576460752303422815_118.json
Using 1747334871330558051 as a seed
Tags are not specified. Assuming every node on a separate tag.
Finalized parameters
  Number of nodes: 3
  Number of slaves: 2
  Number of vbuckets: 1024
  Number of replicas: 1
  Tags assignments:
    0 -> 0
    1 -> 1
    2 -> 2
Succesfully read current vbucket map:
[
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1]
]
Prev R:
RI (greedy):
  N |    |  0   1   2 |
----|-----------------|
    |  T |  0   1   2 |
----|-----------------|
  0 |  0 |  0   1   1 | 2
  1 |  1 |  1   0   1 | 2
  2 |  2 |  1   1   0 | 2
____|_________________|
    |    |  2   2   2 |
R (greedy):
  N |    |  0   1   2 |
----|-----------------|
    |  T |  0   1   2 |
----|-----------------|
  0 |  0 |  0 170 171 | 341
  1 |  1 |171   0 171 | 342
  2 |  2 |170 171   0 | 341
____|_________________|
    |    |341 341 342 |
Time spent for vbmap generation (greedy) - 67.9395ms
Per-node Vb Stats (greedy):
   Active Vbs:   map[0:341 1:342 2:341]
   Replica Vbs:  map[0:341 1:341 2:342]
Move stats:
    Active takeovers - 1024
    Total new replicas - 2048

[ns_server:debug,2025-05-15T18:47:51.473Z,ns_1@172.19.0.4:<0.4343.0>:mb_map:do_invoke_vbmap_body:817]Score before simple minimization: {2048,1024}
[ns_server:debug,2025-05-15T18:47:51.491Z,ns_1@172.19.0.4:terse_bucket_info_uploader-doom-scrolling<0.4194.0>:terse_bucket_info_uploader:maybe_set_cluster_config:144]Bucket 'doom-scrolling' needs vbmap before setting cluster config
[ns_server:debug,2025-05-15T18:47:51.493Z,ns_1@172.19.0.4:<0.4343.0>:mb_map:do_invoke_vbmap_body:823]Score after simple minimization: {2048,1024}
[ns_server:debug,2025-05-15T18:47:51.494Z,ns_1@172.19.0.4:<0.4343.0>:mb_map:do_invoke_vbmap_body:832]Map better after simple minimization;
                                       using it
[ns_server:debug,2025-05-15T18:47:51.497Z,ns_1@172.19.0.4:<0.4347.0>:mb_map:do_invoke_vbmap_body:766]Node Id Map: [{'ns_1@db2.lan',0},{'ns_1@db3.lan',1},{'ns_1@172.19.0.4',2}]
[ns_server:debug,2025-05-15T18:47:51.500Z,ns_1@172.19.0.4:<0.4347.0>:mb_map:do_invoke_vbmap_body:775]Wrote vbmap to "/opt/couchbase/var/lib/couchbase/tmp/prev-vbmap-576460752303422751_118.json"
[ns_server:debug,2025-05-15T18:47:51.658Z,ns_1@172.19.0.4:<0.4347.0>:mb_map:do_invoke_vbmap_body:799]vbmap diag output:
Started as:
  /opt/couchbase/bin/vbmap --diag /opt/couchbase/var/lib/couchbase/tmp/vbmap_diag-576460752303422783_118 --output-format json --num-vbuckets 1024 --num-nodes 3 --num-slaves 10 --num-replicas 1 --relax-all --greedy --current-map /opt/couchbase/var/lib/couchbase/tmp/prev-vbmap-576460752303422751_118.json
Using 1747334871501591135 as a seed
Tags are not specified. Assuming every node on a separate tag.
Finalized parameters
  Number of nodes: 3
  Number of slaves: 2
  Number of vbuckets: 1024
  Number of replicas: 1
  Tags assignments:
    0 -> 0
    1 -> 1
    2 -> 2
Succesfully read current vbucket map:
[
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1]
]
Prev R:
RI (greedy):
  N |    |  0   1   2 |
----|-----------------|
    |  T |  0   1   2 |
----|-----------------|
  0 |  0 |  0   1   1 | 2
  1 |  1 |  1   0   1 | 2
  2 |  2 |  1   1   0 | 2
____|_________________|
    |    |  2   2   2 |
R (greedy):
  N |    |  0   1   2 |
----|-----------------|
    |  T |  0   1   2 |
----|-----------------|
  0 |  0 |  0 171 171 | 342
  1 |  1 |171   0 170 | 341
  2 |  2 |170 171   0 | 341
____|_________________|
    |    |341 342 341 |
Time spent for vbmap generation (greedy) - 77.878959ms
Per-node Vb Stats (greedy):
   Active Vbs:   map[0:342 1:341 2:341]
   Replica Vbs:  map[0:341 1:342 2:341]
Move stats:
    Active takeovers - 1024
    Total new replicas - 2048

[ns_server:debug,2025-05-15T18:47:51.659Z,ns_1@172.19.0.4:<0.4347.0>:mb_map:do_invoke_vbmap_body:817]Score before simple minimization: {2048,1024}
[ns_server:debug,2025-05-15T18:47:51.659Z,ns_1@172.19.0.4:<0.4347.0>:mb_map:do_invoke_vbmap_body:823]Score after simple minimization: {2048,1024}
[ns_server:debug,2025-05-15T18:47:51.659Z,ns_1@172.19.0.4:<0.4347.0>:mb_map:do_invoke_vbmap_body:832]Map better after simple minimization;
                                       using it
[ns_server:debug,2025-05-15T18:47:51.660Z,ns_1@172.19.0.4:<0.4367.0>:mb_map:do_invoke_vbmap_body:766]Node Id Map: [{'ns_1@db2.lan',0},{'ns_1@db3.lan',1},{'ns_1@172.19.0.4',2}]
[ns_server:debug,2025-05-15T18:47:51.662Z,ns_1@172.19.0.4:<0.4367.0>:mb_map:do_invoke_vbmap_body:775]Wrote vbmap to "/opt/couchbase/var/lib/couchbase/tmp/prev-vbmap-576460752303422687_118.json"
[ns_server:debug,2025-05-15T18:47:51.821Z,ns_1@172.19.0.4:<0.4367.0>:mb_map:do_invoke_vbmap_body:799]vbmap diag output:
Started as:
  /opt/couchbase/bin/vbmap --diag /opt/couchbase/var/lib/couchbase/tmp/vbmap_diag-576460752303422719_118 --output-format json --num-vbuckets 1024 --num-nodes 3 --num-slaves 10 --num-replicas 1 --relax-all --greedy --current-map /opt/couchbase/var/lib/couchbase/tmp/prev-vbmap-576460752303422687_118.json
Using 1747334871664664552 as a seed
Tags are not specified. Assuming every node on a separate tag.
Finalized parameters
  Number of nodes: 3
  Number of slaves: 2
  Number of vbuckets: 1024
  Number of replicas: 1
  Tags assignments:
    0 -> 0
    1 -> 1
    2 -> 2
Succesfully read current vbucket map:
[
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1]
]
Prev R:
RI (greedy):
  N |    |  0   1   2 |
----|-----------------|
    |  T |  0   1   2 |
----|-----------------|
  0 |  0 |  0   1   1 | 2
  1 |  1 |  1   0   1 | 2
  2 |  2 |  1   1   0 | 2
____|_________________|
    |    |  2   2   2 |
R (greedy):
  N |    |  0   1   2 |
----|-----------------|
    |  T |  0   1   2 |
----|-----------------|
  0 |  0 |  0 171 170 | 341
  1 |  1 |170   0 171 | 341
  2 |  2 |171 171   0 | 342
____|_________________|
    |    |341 342 341 |
Time spent for vbmap generation (greedy) - 63.796917ms
Per-node Vb Stats (greedy):
   Active Vbs:   map[0:341 1:341 2:342]
   Replica Vbs:  map[0:341 1:342 2:341]
Move stats:
    Active takeovers - 1024
    Total new replicas - 2048

[ns_server:debug,2025-05-15T18:47:51.823Z,ns_1@172.19.0.4:<0.4367.0>:mb_map:do_invoke_vbmap_body:817]Score before simple minimization: {2048,1024}
[ns_server:debug,2025-05-15T18:47:51.823Z,ns_1@172.19.0.4:<0.4367.0>:mb_map:do_invoke_vbmap_body:823]Score after simple minimization: {2048,1024}
[ns_server:debug,2025-05-15T18:47:51.823Z,ns_1@172.19.0.4:<0.4367.0>:mb_map:do_invoke_vbmap_body:832]Map better after simple minimization;
                                       using it
[ns_server:debug,2025-05-15T18:47:51.824Z,ns_1@172.19.0.4:<0.4371.0>:mb_map:do_invoke_vbmap_body:766]Node Id Map: [{'ns_1@db2.lan',0},{'ns_1@db3.lan',1},{'ns_1@172.19.0.4',2}]
[ns_server:debug,2025-05-15T18:47:51.825Z,ns_1@172.19.0.4:<0.4371.0>:mb_map:do_invoke_vbmap_body:775]Wrote vbmap to "/opt/couchbase/var/lib/couchbase/tmp/prev-vbmap-576460752303422623_118.json"
[ns_server:debug,2025-05-15T18:47:51.962Z,ns_1@172.19.0.4:<0.4371.0>:mb_map:do_invoke_vbmap_body:799]vbmap diag output:
Started as:
  /opt/couchbase/bin/vbmap --diag /opt/couchbase/var/lib/couchbase/tmp/vbmap_diag-576460752303422655_118 --output-format json --num-vbuckets 1024 --num-nodes 3 --num-slaves 10 --num-replicas 1 --relax-all --greedy --current-map /opt/couchbase/var/lib/couchbase/tmp/prev-vbmap-576460752303422623_118.json
Using 1747334871826806010 as a seed
Tags are not specified. Assuming every node on a separate tag.
Finalized parameters
  Number of nodes: 3
  Number of slaves: 2
  Number of vbuckets: 1024
  Number of replicas: 1
  Tags assignments:
    0 -> 0
    1 -> 1
    2 -> 2
Succesfully read current vbucket map:
[
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1],
  [-1 -1]
]
Prev R:
RI (greedy):
  N |    |  0   1   2 |
----|-----------------|
    |  T |  0   1   2 |
----|-----------------|
  0 |  0 |  0   1   1 | 2
  1 |  1 |  1   0   1 | 2
  2 |  2 |  1   1   0 | 2
____|_________________|
    |    |  2   2   2 |
R (greedy):
  N |    |  0   1   2 |
----|-----------------|
    |  T |  0   1   2 |
----|-----------------|
  0 |  0 |  0 171 170 | 341
  1 |  1 |170   0 171 | 341
  2 |  2 |171 171   0 | 342
____|_________________|
    |    |341 342 341 |
Time spent for vbmap generation (greedy) - 65.193958ms
Per-node Vb Stats (greedy):
   Active Vbs:   map[0:341 1:341 2:342]
   Replica Vbs:  map[0:341 1:342 2:341]
Move stats:
    Active takeovers - 1024
    Total new replicas - 2048

[ns_server:debug,2025-05-15T18:47:51.963Z,ns_1@172.19.0.4:<0.4371.0>:mb_map:do_invoke_vbmap_body:817]Score before simple minimization: {2048,1024}
[ns_server:debug,2025-05-15T18:47:51.964Z,ns_1@172.19.0.4:<0.4371.0>:mb_map:do_invoke_vbmap_body:823]Score after simple minimization: {2048,1024}
[ns_server:debug,2025-05-15T18:47:51.964Z,ns_1@172.19.0.4:<0.4371.0>:mb_map:do_invoke_vbmap_body:832]Map better after simple minimization;
                                       using it
[ns_server:debug,2025-05-15T18:47:51.965Z,ns_1@172.19.0.4:<0.4202.0>:mb_map:generate_map_new:351]Scores for generated maps:
[{2048,1024},
 {2048,1024},
 {2048,1024},
 {2048,1024},
 {2048,1024},
 {2048,1024},
 {2048,1024},
 {2048,1024},
 {2048,1024}]
[ns_server:debug,2025-05-15T18:47:51.965Z,ns_1@172.19.0.4:<0.4202.0>:mb_map:generate_map_new:355]Considering 6 maps:
[{2048,1024},{2048,1024},{2048,1024},{2048,1024},{2048,1024},{2048,1024}]
[ns_server:debug,2025-05-15T18:47:51.965Z,ns_1@172.19.0.4:<0.4202.0>:mb_map:generate_map_new:359]Best map score: {2048,1024} (true)
[ns_server:debug,2025-05-15T18:47:51.990Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: {bucket,"doom-scrolling",last_balanced_vbmap}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                                  48})
{[['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4','ns_1@db2.lan'],
  ['ns_1@172.19.0.4'|...],
  [...]|...],
 [{replication_topology,star},
  {tags,undefined},
  {use_vbmap_greedy_optimization,true},
  {max_slaves,10}]}
[ns_server:debug,2025-05-15T18:47:51.991Z,ns_1@172.19.0.4:terse_bucket_info_uploader-doom-scrolling<0.4194.0>:terse_bucket_info_uploader:maybe_set_cluster_config:144]Bucket 'doom-scrolling' needs vbmap before setting cluster config
[ns_server:debug,2025-05-15T18:47:52.014Z,ns_1@172.19.0.4:roles_cache<0.458.0>:active_cache:clean:181]Clearing the roles_cache cache
[ns_server:debug,2025-05-15T18:47:52.015Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: {bucket,"doom-scrolling",props}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                    49})
[{map_diff,[{0,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {1,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {2,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {3,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {4,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {5,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {6,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {7,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {8,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {9,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {10,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {11,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {12,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {13,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {14,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {15,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {16,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {17,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {18,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {19,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {20,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {21,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {22,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {23,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {24,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {25,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {26,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {27,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {28,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {29,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {30,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {31,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {32,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {33,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {34,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {35,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {36,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {37,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {38,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {39,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {40,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {41,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {42,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {43,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {44,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {45,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {46,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {47,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {48,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {49,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {50,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {51,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {52,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {53,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {54,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {55,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {56,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {57,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {58,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {59,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {60,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {61,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {62,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {63,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {64,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {65,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {66,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {67,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {68,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {69,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {70,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {71,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {72,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {73,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {74,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {75,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {76,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {77,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {78,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {79,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {80,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {81,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {82,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {83,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {84,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {85,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {86,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {87,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {88,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {89,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {90,[],['ns_1@172.19.0.4','ns_1@db2.lan']},
            {91,[],['ns_1@172.19.0.4'|...]},
            {92,[],[...]},
            {93,[],...},
            {94,...},
            {...}|...]},
 {map_opts_hash,42591107},
 {replica_index,true},
 {ram_quota,536870912},
 {durability_min_level,none},
 {num_vbuckets,1024},
 {cross_cluster_versioning_enabled,false},
 {version_pruning_window_hrs,720},
 {pitr_enabled,false},
 {pitr_granularity,600},
 {pitr_max_history_age,86400},
 {flush_enabled,false},
 {num_threads,3},
 {eviction_policy,value_only},
 {conflict_resolution_type,seqno},
 {storage_mode,couchstore},
 {max_ttl,0},
 {compression_mode,passive},
 {rank,0},
 {type,membase},
 {num_replicas,1},
 {replication_topology,star},
 {repl_type,dcp},
 {servers,['ns_1@172.19.0.4','ns_1@db2.lan','ns_1@db3.lan']},
 {fastForwardMap_diff,[]}]
[ns_server:debug,2025-05-15T18:47:52.017Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:check_states_match:399]Found states mismatch in bucket "doom-scrolling":
[{0,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {1,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {2,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {3,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {4,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {5,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {6,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {7,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {8,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {9,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {10,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {11,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {12,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {13,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {14,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {15,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {16,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {17,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {18,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {19,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {20,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {21,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {22,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {23,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {24,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {25,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {26,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {27,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {28,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {29,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {30,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {31,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {32,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {33,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {34,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {35,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {36,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {37,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {38,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {39,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {40,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {41,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {42,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {43,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {44,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {45,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {46,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {47,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {48,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {49,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {50,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {51,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {52,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {53,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {54,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {55,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {56,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {57,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {58,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {59,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {60,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {61,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {62,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {63,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {64,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {65,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {66,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {67,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {68,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {69,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {70,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {71,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {72,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {73,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {74,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {75,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {76,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {77,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {78,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {79,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {80,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {81,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {82,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {83,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {84,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {85,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {86,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {87,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {88,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {89,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {90,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {91,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {92,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {93,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {94,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {95,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {96,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {97,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {98,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {99,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {100,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {101,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {102,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {103,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {104,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {105,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {106,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {107,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {108,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {109,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {110,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {111,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {112,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {113,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {114,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {115,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {116,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {117,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {118,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {119,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {120,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {121,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {122,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {123,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {124,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {125,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {126,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {127,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {128,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {129,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {130,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {131,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {132,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {133,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {134,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {135,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {136,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {137,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {138,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {139,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {140,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {141,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {142,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {143,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {144,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {145,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {146,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {147,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {148,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {149,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {150,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {151,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {152,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {153,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {154,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {155,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {156,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {157,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {158,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {159,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {160,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {161,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {162,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {163,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {164,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {165,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {166,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {167,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {168,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {169,['ns_1@172.19.0.4','ns_1@db2.lan'],[]},
 {170,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {171,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {172,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {173,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {174,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {175,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {176,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {177,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {178,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {179,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {180,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {181,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {182,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {183,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {184,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {185,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {186,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {187,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {188,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {189,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {190,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {191,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {192,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {193,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {194,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {195,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {196,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {197,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {198,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {199,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {200,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {201,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {202,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {203,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {204,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {205,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {206,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {207,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {208,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {209,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {210,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {211,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {212,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {213,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {214,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {215,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {216,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {217,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {218,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {219,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {220,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {221,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {222,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {223,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {224,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {225,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {226,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {227,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {228,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {229,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {230,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {231,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {232,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {233,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {234,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {235,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {236,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {237,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {238,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {239,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {240,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {241,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {242,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {243,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {244,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {245,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {246,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {247,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {248,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {249,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {250,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {251,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {252,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {253,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {254,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {255,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {256,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {257,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {258,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {259,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {260,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {261,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {262,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {263,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {264,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {265,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {266,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {267,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {268,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {269,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {270,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {271,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {272,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {273,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {274,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {275,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {276,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {277,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {278,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {279,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {280,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {281,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {282,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {283,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {284,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {285,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {286,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {287,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {288,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {289,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {290,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {291,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {292,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {293,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {294,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {295,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {296,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {297,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {298,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {299,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {300,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {301,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {302,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {303,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {304,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {305,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {306,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {307,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {308,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {309,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {310,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {311,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {312,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {313,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {314,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {315,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {316,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {317,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {318,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {319,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {320,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {321,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {322,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {323,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {324,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {325,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {326,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {327,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {328,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {329,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {330,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {331,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {332,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {333,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {334,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {335,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {336,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {337,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {338,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {339,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {340,['ns_1@172.19.0.4','ns_1@db3.lan'],[]},
 {341,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {342,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {343,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {344,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {345,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {346,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {347,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {348,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {349,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {350,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {351,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {352,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {353,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {354,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {355,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {356,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {357,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {358,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {359,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {360,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {361,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {362,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {363,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {364,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {365,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {366,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {367,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {368,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {369,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {370,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {371,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {372,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {373,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {374,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {375,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {376,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {377,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {378,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {379,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {380,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {381,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {382,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {383,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {384,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {385,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {386,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {387,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {388,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {389,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {390,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {391,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {392,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {393,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {394,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {395,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {396,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {397,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {398,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {399,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {400,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {401,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {402,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {403,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {404,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {405,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {406,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {407,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {408,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {409,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {410,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {411,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {412,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {413,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {414,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {415,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {416,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {417,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {418,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {419,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {420,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {421,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {422,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {423,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {424,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {425,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {426,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {427,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {428,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {429,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {430,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {431,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {432,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {433,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {434,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {435,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {436,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {437,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {438,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {439,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {440,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {441,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {442,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {443,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {444,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {445,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {446,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {447,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {448,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {449,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {450,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {451,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {452,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {453,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {454,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {455,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {456,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {457,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {458,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {459,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {460,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {461,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {462,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {463,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {464,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {465,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {466,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {467,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {468,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {469,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {470,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {471,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {472,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {473,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {474,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {475,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {476,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {477,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {478,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {479,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {480,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {481,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {482,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {483,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {484,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {485,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {486,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {487,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {488,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {489,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {490,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {491,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {492,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {493,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {494,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {495,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {496,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {497,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {498,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {499,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {500,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {501,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {502,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {503,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {504,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {505,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {506,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {507,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {508,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {509,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {510,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {511,['ns_1@db2.lan','ns_1@172.19.0.4'],[]},
 {512,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {513,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {514,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {515,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {516,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {517,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {518,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {519,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {520,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {521,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {522,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {523,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {524,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {525,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {526,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {527,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {528,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {529,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {530,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {531,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {532,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {533,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {534,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {535,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {536,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {537,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {538,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {539,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {540,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {541,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {542,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {543,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {544,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {545,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {546,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {547,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {548,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {549,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {550,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {551,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {552,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {553,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {554,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {555,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {556,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {557,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {558,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {559,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {560,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {561,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {562,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {563,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {564,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {565,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {566,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {567,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {568,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {569,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {570,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {571,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {572,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {573,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {574,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {575,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {576,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {577,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {578,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {579,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {580,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {581,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {582,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {583,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {584,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {585,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {586,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {587,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {588,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {589,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {590,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {591,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {592,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {593,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {594,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {595,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {596,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {597,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {598,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {599,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {600,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {601,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {602,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {603,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {604,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {605,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {606,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {607,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {608,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {609,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {610,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {611,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {612,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {613,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {614,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {615,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {616,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {617,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {618,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {619,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {620,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {621,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {622,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {623,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {624,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {625,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {626,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {627,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {628,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {629,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {630,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {631,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {632,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {633,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {634,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {635,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {636,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {637,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {638,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {639,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {640,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {641,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {642,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {643,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {644,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {645,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {646,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {647,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {648,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {649,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {650,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {651,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {652,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {653,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {654,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {655,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {656,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {657,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {658,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {659,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {660,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {661,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {662,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {663,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {664,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {665,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {666,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {667,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {668,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {669,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {670,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {671,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {672,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {673,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {674,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {675,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {676,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {677,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {678,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {679,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {680,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {681,['ns_1@db2.lan','ns_1@db3.lan'],[]},
 {682,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {683,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {684,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {685,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {686,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {687,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {688,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {689,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {690,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {691,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {692,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {693,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {694,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {695,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {696,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {697,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {698,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {699,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {700,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {701,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {702,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {703,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {704,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {705,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {706,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {707,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {708,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {709,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {710,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {711,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {712,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {713,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {714,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {715,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {716,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {717,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {718,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {719,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {720,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {721,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {722,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {723,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {724,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {725,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {726,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {727,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {728,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {729,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {730,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {731,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {732,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {733,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {734,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {735,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {736,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {737,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {738,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {739,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {740,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {741,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {742,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {743,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {744,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {745,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {746,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {747,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {748,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {749,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {750,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {751,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {752,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {753,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {754,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {755,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {756,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {757,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {758,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {759,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {760,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {761,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {762,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {763,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {764,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {765,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {766,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {767,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {768,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {769,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {770,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {771,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {772,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {773,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {774,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {775,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {776,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {777,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {778,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {779,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {780,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {781,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {782,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {783,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {784,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {785,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {786,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {787,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {788,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {789,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {790,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {791,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {792,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {793,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {794,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {795,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {796,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {797,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {798,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {799,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {800,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {801,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {802,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {803,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {804,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {805,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {806,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {807,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {808,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {809,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {810,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {811,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {812,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {813,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {814,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {815,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {816,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {817,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {818,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {819,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {820,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {821,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {822,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {823,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {824,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {825,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {826,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {827,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {828,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {829,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {830,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {831,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {832,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {833,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {834,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {835,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {836,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {837,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {838,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {839,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {840,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {841,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {842,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {843,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {844,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {845,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {846,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {847,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {848,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {849,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {850,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {851,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {852,['ns_1@db3.lan','ns_1@172.19.0.4'],[]},
 {853,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {854,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {855,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {856,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {857,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {858,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {859,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {860,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {861,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {862,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {863,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {864,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {865,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {866,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {867,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {868,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {869,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {870,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {871,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {872,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {873,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {874,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {875,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {876,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {877,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {878,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {879,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {880,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {881,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {882,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {883,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {884,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {885,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {886,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {887,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {888,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {889,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {890,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {891,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {892,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {893,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {894,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {895,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {896,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {897,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {898,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {899,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {900,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {901,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {902,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {903,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {904,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {905,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {906,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {907,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {908,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {909,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {910,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {911,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {912,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {913,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {914,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {915,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {916,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {917,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {918,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {919,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {920,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {921,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {922,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {923,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {924,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {925,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {926,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {927,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {928,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {929,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {930,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {931,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {932,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {933,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {934,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {935,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {936,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {937,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {938,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {939,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {940,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {941,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {942,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {943,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {944,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {945,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {946,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {947,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {948,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {949,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {950,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {951,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {952,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {953,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {954,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {955,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {956,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {957,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {958,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {959,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {960,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {961,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {962,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {963,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {964,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {965,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {966,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {967,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {968,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {969,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {970,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {971,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {972,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {973,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {974,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {975,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {976,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {977,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {978,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {979,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {980,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {981,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {982,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {983,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {984,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {985,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {986,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {987,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {988,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {989,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {990,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {991,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {992,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {993,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {994,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {995,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {996,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {997,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {998,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {999,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {1000,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {1001,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {1002,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {1003,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {1004,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {1005,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {1006,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {1007,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {1008,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {1009,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {1010,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {1011,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {1012,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {1013,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {1014,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {1015,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {1016,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {1017,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {1018,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {1019,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {1020,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {1021,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {1022,['ns_1@db3.lan','ns_1@db2.lan'],[]},
 {1023,['ns_1@db3.lan','ns_1@db2.lan'],[]}]
[ns_server:debug,2025-05-15T18:47:52.034Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:pull_config:418]Going to pull config
[ns_server:info,2025-05-15T18:47:52.042Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 0 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.042Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 1 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.042Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 2 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.042Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 3 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.042Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 4 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.042Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 5 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.042Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 6 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.042Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 7 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.042Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 8 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.043Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 9 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.043Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 10 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.043Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 11 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.043Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 12 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.043Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 13 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.043Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 14 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.043Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 15 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.043Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 16 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.043Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 17 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.043Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 18 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.043Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 19 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.043Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 20 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.043Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 21 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.043Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 22 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.043Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 23 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.043Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 24 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.043Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 25 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.043Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 26 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.043Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 27 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.043Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 28 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.044Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 29 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.044Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 30 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.044Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 31 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.044Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 32 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.044Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 33 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.044Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 34 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.044Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 35 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.044Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 36 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.044Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 37 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.044Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 38 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.044Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 39 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.044Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 40 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.044Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 41 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.044Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 42 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.044Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 43 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.044Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 44 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.044Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 45 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.044Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 46 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.044Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 47 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.044Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 48 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.045Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 49 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.045Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 50 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.045Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 51 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.045Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 52 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.045Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 53 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.045Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 54 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.045Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 55 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.045Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 56 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.047Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 57 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.047Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 58 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.047Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 59 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.047Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 60 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.047Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 61 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.047Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 62 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.047Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 63 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.047Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 64 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.047Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 65 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.047Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 66 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.047Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 67 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.047Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 68 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.047Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 69 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.047Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 70 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.047Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 71 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.047Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 72 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.047Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 73 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.047Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 74 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.047Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 75 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.047Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 76 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.047Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 77 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.047Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 78 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.047Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 79 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.047Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 80 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.047Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 81 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.047Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 82 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.047Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 83 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.047Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 84 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.047Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 85 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.048Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 86 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.048Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 87 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.048Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 88 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.048Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 89 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.048Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 90 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.048Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 91 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.048Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 92 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.048Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 93 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.048Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 94 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.048Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 95 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.048Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 96 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.048Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 97 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.048Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 98 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.048Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 99 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.048Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 100 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.048Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 101 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.048Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 102 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.048Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 103 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.048Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 104 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.048Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 105 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.048Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 106 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.048Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 107 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.048Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 108 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.048Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 109 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.048Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 110 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.048Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 111 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.048Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 112 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.048Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 113 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.048Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 114 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.048Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 115 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.048Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 116 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.048Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 117 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.049Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 118 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.049Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 119 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.049Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 120 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.049Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 121 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.049Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 122 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.049Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 123 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.049Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 124 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.049Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 125 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.049Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 126 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.049Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 127 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.049Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 128 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.049Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 129 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.049Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 130 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.049Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 131 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.049Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 132 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.049Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 133 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.049Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 134 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.049Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 135 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.049Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 136 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.049Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 137 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.049Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 138 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.049Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 139 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.049Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 140 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.049Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 141 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.049Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 142 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.049Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 143 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.049Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 144 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.049Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 145 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.049Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 146 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.049Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 147 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.049Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 148 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.049Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 149 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.049Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 150 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.049Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 151 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.049Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 152 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.049Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 153 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.049Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 154 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.049Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 155 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.049Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 156 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.049Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 157 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.049Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 158 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.049Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 159 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.049Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 160 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 161 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 162 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 163 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 164 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 165 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 166 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 167 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 168 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 169 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 170 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 171 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 172 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 173 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 174 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 175 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 176 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 177 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 178 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 179 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 180 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 181 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 182 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 183 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 184 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 185 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 186 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 187 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 188 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 189 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 190 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 191 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 192 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 193 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 194 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 195 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 196 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 197 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 198 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 199 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 200 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 201 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 202 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 203 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 204 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 205 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 206 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 207 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 208 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 209 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.050Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 210 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.051Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 211 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.051Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 212 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.051Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 213 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.051Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 214 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.051Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 215 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.051Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 216 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.051Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 217 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.051Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 218 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.051Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 219 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.051Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 220 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.051Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 221 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.051Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 222 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.051Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 223 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.051Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 224 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.051Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 225 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.051Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 226 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.051Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 227 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.051Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 228 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.051Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 229 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.051Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 230 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.051Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 231 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.051Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 232 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.051Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 233 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.051Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 234 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.051Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 235 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.051Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 236 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.052Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 237 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.052Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 238 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.052Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 239 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.052Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 240 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.052Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 241 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.052Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 242 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.052Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 243 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.052Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 244 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.052Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 245 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.052Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 246 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.052Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 247 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.052Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 248 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.052Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 249 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.052Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 250 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.052Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 251 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.052Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 252 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.052Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 253 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.052Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 254 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.052Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 255 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.052Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 256 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.052Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 257 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.052Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 258 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.052Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 259 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.052Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 260 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.052Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 261 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.053Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 262 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.053Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 263 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.053Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 264 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.053Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 265 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.053Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 266 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.053Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 267 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.053Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 268 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.053Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 269 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.053Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 270 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.053Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 271 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.053Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 272 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.053Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 273 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.053Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 274 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.053Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 275 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.053Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 276 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.053Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 277 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.053Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 278 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.053Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 279 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.053Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 280 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.053Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 281 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.053Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 282 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.053Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 283 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.053Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 284 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.053Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 285 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.053Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 286 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.053Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 287 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.053Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 288 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.053Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 289 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.053Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 290 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.053Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 291 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.053Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 292 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.053Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 293 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.053Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 294 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.053Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 295 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.054Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 296 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.054Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 297 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.054Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 298 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.054Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 299 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.054Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 300 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.054Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 301 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.054Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 302 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.054Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 303 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.054Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 304 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.054Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 305 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.054Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 306 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.054Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 307 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.054Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 308 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.054Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 309 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.054Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 310 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.054Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 311 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.054Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 312 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.054Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 313 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.054Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 314 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.054Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 315 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.054Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 316 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.054Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 317 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.054Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 318 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.054Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 319 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 320 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 321 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 322 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 323 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 324 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 325 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 326 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 327 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 328 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 329 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 330 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 331 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 332 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 333 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 334 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 335 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 336 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 337 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 338 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 339 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 340 in "doom-scrolling" on 'ns_1@172.19.0.4' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 341 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 342 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 343 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 344 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 345 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 346 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 347 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 348 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 349 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 350 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 351 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 352 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 353 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 354 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 355 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 356 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 357 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 358 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 359 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 360 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 361 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 362 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 363 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 364 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 365 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 366 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 367 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.055Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 368 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.056Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 369 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.056Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 370 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.056Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 371 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.056Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 372 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.056Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 373 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.056Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 374 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.056Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 375 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.056Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 376 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.056Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 377 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.056Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 378 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.056Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 379 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.056Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 380 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.056Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 381 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.056Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 382 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.056Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 383 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.056Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 384 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.056Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 385 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.056Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 386 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.056Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 387 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.056Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 388 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.056Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 389 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.056Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 390 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.056Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 391 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.056Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 392 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.056Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 393 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.056Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 394 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.056Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 395 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.056Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 396 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.056Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 397 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.056Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 398 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.056Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 399 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.056Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 400 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.056Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 401 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.056Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 402 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.056Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 403 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.056Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 404 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.056Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 405 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.056Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 406 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.056Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 407 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.057Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 408 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.057Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 409 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.057Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 410 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.057Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 411 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.057Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 412 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.057Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 413 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.057Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 414 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.057Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 415 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.057Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 416 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.057Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 417 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.057Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 418 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.057Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 419 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.057Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 420 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.057Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 421 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.057Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 422 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.057Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 423 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.057Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 424 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.057Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 425 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.058Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 426 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.058Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 427 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.058Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 428 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.058Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 429 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.058Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 430 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.058Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 431 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.058Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 432 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.058Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 433 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.058Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 434 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.058Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 435 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.058Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 436 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.058Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 437 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.058Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 438 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.058Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 439 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.059Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 440 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.059Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 441 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.059Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 442 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.059Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 443 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.059Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 444 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.059Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 445 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.059Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 446 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.059Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 447 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.059Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 448 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.059Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 449 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.059Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 450 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.059Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 451 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.059Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 452 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.059Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 453 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.059Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 454 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.059Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 455 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.059Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 456 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.059Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 457 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.059Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 458 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.059Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 459 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.059Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 460 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.059Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 461 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.059Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 462 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.059Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 463 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.059Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 464 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.059Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 465 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.059Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 466 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.059Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 467 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.059Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 468 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.059Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 469 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.059Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 470 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.059Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 471 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.059Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 472 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.060Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 473 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.060Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 474 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.060Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 475 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.060Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 476 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.060Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 477 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.060Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 478 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.060Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 479 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.060Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 480 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.060Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 481 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.060Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 482 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.060Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 483 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.060Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 484 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.060Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 485 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.060Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 486 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.060Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 487 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.060Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 488 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.060Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 489 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.060Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 490 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.060Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 491 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.060Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 492 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.060Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 493 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.060Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 494 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.060Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 495 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.060Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 496 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.060Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 497 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.060Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 498 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.060Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 499 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.060Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 500 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.061Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 501 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.061Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 502 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.061Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 503 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.061Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 504 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.061Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 505 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.061Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 506 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.061Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 507 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.061Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 508 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.061Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 509 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.061Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 510 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.061Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 511 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.061Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 512 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.061Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 513 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.061Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 514 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.061Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 515 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.061Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 516 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.061Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 517 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.061Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 518 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.061Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 519 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.061Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 520 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.061Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 521 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.061Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 522 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.061Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 523 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.061Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 524 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.061Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 525 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.061Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 526 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.061Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 527 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.061Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 528 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.061Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 529 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.061Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 530 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.061Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 531 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.061Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 532 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.061Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 533 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.061Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 534 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.061Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 535 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.061Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 536 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.061Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 537 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.061Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 538 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.061Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 539 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.062Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 540 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.062Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 541 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.062Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 542 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.062Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 543 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.062Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 544 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.062Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 545 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.062Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 546 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.062Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 547 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.062Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 548 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.062Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 549 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.062Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 550 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.062Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 551 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.062Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 552 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.062Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 553 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.062Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 554 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.062Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 555 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.062Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 556 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.062Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 557 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.062Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 558 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.062Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 559 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.062Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 560 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.062Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 561 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.062Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 562 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.062Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 563 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.062Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 564 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.062Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 565 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.062Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 566 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.062Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 567 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.062Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 568 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.062Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 569 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.062Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 570 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.062Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 571 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.062Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 572 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.062Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 573 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.062Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 574 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.062Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 575 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.062Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 576 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.062Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 577 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.062Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 578 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.062Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 579 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.063Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 580 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.063Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 581 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.063Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 582 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.063Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 583 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.063Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 584 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.063Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 585 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.063Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 586 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.063Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 587 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.063Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 588 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.063Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 589 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.063Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 590 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.063Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 591 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.063Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 592 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.063Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 593 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.063Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 594 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.063Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 595 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.063Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 596 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.063Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 597 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.063Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 598 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.063Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 599 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.063Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 600 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.063Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 601 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.063Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 602 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.064Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 603 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.064Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 604 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.064Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 605 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.064Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 606 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.064Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 607 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.064Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 608 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.064Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 609 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.064Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 610 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.064Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 611 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.064Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 612 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.064Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 613 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.064Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 614 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.064Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 615 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.064Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 616 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.064Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 617 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.064Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 618 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.064Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 619 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.064Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 620 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.064Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 621 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.064Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 622 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.064Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 623 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.064Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 624 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.064Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 625 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.064Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 626 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.064Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 627 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.064Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 628 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.064Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 629 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.064Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 630 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.065Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 631 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.065Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 632 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.065Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 633 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.065Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 634 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.065Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 635 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.065Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 636 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.065Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 637 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.065Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 638 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.065Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 639 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.065Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 640 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.065Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 641 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.065Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 642 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.065Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 643 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.065Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 644 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.065Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 645 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.065Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 646 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.065Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 647 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.065Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 648 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.065Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 649 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.065Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 650 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.065Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 651 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.065Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 652 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.065Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 653 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.065Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 654 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.065Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 655 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.065Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 656 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.065Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 657 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.065Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 658 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.065Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 659 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.065Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 660 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.065Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 661 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.065Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 662 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.065Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 663 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.065Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 664 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.065Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 665 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.065Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 666 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.065Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 667 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.065Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 668 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.065Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 669 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.065Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 670 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.065Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 671 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.065Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 672 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.066Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 673 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.066Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 674 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.066Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 675 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.066Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 676 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.066Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 677 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.066Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 678 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.066Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 679 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.066Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 680 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.066Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 681 in "doom-scrolling" on 'ns_1@db2.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.066Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 682 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.066Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 683 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.066Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 684 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.066Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 685 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.066Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 686 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.066Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 687 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.066Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 688 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.066Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 689 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.066Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 690 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.066Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 691 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.066Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 692 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.066Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 693 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.066Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 694 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.066Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 695 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.066Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 696 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.066Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 697 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.066Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 698 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.066Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 699 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.066Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 700 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.066Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 701 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.066Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 702 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.066Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 703 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.066Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 704 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.066Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 705 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.066Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 706 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.066Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 707 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.066Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 708 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.066Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 709 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.066Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 710 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.066Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 711 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.066Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 712 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.066Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 713 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.066Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 714 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.066Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 715 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.066Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 716 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.066Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 717 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.067Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 718 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.067Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 719 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.067Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 720 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.067Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 721 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.067Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 722 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.067Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 723 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.067Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 724 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.067Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 725 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.067Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 726 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.067Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 727 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.067Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 728 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.067Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 729 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.067Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 730 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.067Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 731 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.067Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 732 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.067Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 733 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.067Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 734 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.067Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 735 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.067Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 736 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.067Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 737 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.067Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 738 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.067Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 739 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.067Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 740 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.067Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 741 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.067Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 742 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.067Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 743 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.067Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 744 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.067Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 745 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.067Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 746 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.067Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 747 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.067Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 748 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.067Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 749 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.067Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 750 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.068Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 751 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.068Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 752 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.068Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 753 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.068Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 754 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.068Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 755 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.068Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 756 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.068Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 757 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.068Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 758 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.068Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 759 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.068Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 760 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.068Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 761 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.068Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 762 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.068Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 763 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.068Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 764 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.068Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 765 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.068Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 766 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.068Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 767 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.068Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 768 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.068Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 769 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.068Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 770 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.068Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 771 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.068Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 772 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.068Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 773 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.068Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 774 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.069Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 775 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.069Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 776 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.069Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 777 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.069Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 778 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.069Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 779 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.069Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 780 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.069Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 781 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.069Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 782 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.069Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 783 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.069Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 784 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.069Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 785 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.069Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 786 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.069Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 787 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.069Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 788 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.069Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 789 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.069Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 790 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.069Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 791 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.069Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 792 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.069Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 793 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.069Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 794 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.069Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 795 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.069Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 796 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.069Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 797 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.069Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 798 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.069Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 799 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.069Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 800 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.069Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 801 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.069Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 802 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.069Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 803 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.069Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 804 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.069Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 805 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.069Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 806 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.069Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 807 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.069Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 808 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.069Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 809 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.069Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 810 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.069Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 811 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.069Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 812 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.069Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 813 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.069Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 814 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.070Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 815 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.070Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 816 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.070Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 817 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.070Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 818 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.070Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 819 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.070Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 820 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.070Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 821 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.070Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 822 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.070Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 823 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.070Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 824 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.070Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 825 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.070Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 826 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.070Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 827 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.070Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 828 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.070Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 829 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.070Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 830 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.070Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 831 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.070Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 832 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.070Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 833 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.070Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 834 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.070Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 835 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.070Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 836 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.070Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 837 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.070Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 838 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.070Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 839 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.070Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 840 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.070Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 841 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.070Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 842 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.070Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 843 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.070Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 844 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.070Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 845 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.070Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 846 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.070Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 847 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.070Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 848 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.070Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 849 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.070Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 850 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.070Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 851 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.070Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 852 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.070Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 853 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.070Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 854 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.070Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 855 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.070Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 856 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.070Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 857 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.070Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 858 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.070Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 859 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.070Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 860 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.071Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 861 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.071Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 862 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.071Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 863 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.071Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 864 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.071Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 865 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.071Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 866 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.071Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 867 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.071Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 868 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.071Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 869 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.071Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 870 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.071Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 871 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.071Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 872 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.071Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 873 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.071Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 874 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.071Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 875 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.071Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 876 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.071Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 877 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.071Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 878 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.071Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 879 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.071Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 880 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.071Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 881 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.071Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 882 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.071Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 883 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.071Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 884 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.071Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 885 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.071Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 886 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.071Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 887 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.071Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 888 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.071Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 889 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.071Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 890 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.071Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 891 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.071Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 892 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.071Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 893 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.071Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 894 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.071Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 895 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.071Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 896 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.071Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 897 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.071Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 898 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.071Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 899 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.071Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 900 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.072Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 901 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.072Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 902 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.072Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 903 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.072Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 904 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.072Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 905 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.072Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 906 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.072Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 907 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.072Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 908 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.072Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 909 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.072Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 910 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.072Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 911 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.072Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 912 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.072Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 913 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.072Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 914 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.072Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 915 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.072Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 916 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.072Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 917 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.072Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 918 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.072Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 919 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.072Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 920 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.072Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 921 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.072Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 922 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.072Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 923 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.072Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 924 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.072Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 925 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.072Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 926 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.072Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 927 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.072Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 928 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.072Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 929 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.072Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 930 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.072Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 931 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.072Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 932 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.072Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 933 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.072Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 934 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.072Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 935 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.072Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 936 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.073Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 937 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.073Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 938 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.073Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 939 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.073Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 940 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.073Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 941 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.073Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 942 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.073Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 943 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.073Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 944 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.073Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 945 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.073Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 946 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.073Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 947 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.073Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 948 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.073Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 949 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.073Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 950 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.073Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 951 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.073Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 952 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.073Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 953 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.073Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 954 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.073Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 955 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.073Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 956 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.073Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 957 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.073Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 958 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.073Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 959 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.073Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 960 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.073Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 961 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.073Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 962 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.073Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 963 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.073Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 964 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.073Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 965 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.073Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 966 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.073Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 967 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.073Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 968 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.073Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 969 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.073Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 970 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.073Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 971 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.073Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 972 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.074Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 973 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.074Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 974 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.074Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 975 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.074Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 976 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.074Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 977 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.074Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 978 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.074Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 979 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.074Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 980 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.074Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 981 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.074Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 982 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.074Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 983 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.074Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 984 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.074Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 985 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.074Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 986 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.074Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 987 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.074Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 988 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.074Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 989 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.074Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 990 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.074Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 991 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.074Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 992 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.074Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 993 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.074Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 994 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.074Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 995 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.074Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 996 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.074Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 997 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.075Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 998 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.075Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 999 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.075Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 1000 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.075Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 1001 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.075Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 1002 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.075Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 1003 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.075Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 1004 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.075Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 1005 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.075Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 1006 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.075Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 1007 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.075Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 1008 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.075Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 1009 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.075Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 1010 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.075Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 1011 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.075Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 1012 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.075Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 1013 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.075Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 1014 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.076Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 1015 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.076Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 1016 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.076Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 1017 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.076Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 1018 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.076Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 1019 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.076Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 1020 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.076Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 1021 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.076Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 1022 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.076Z,ns_1@172.19.0.4:<0.4202.0>:ns_janitor:sanify_chain:670]Setting vbucket 1023 in "doom-scrolling" on 'ns_1@db3.lan' from missing to active.
[ns_server:info,2025-05-15T18:47:52.254Z,ns_1@172.19.0.4:<0.4279.0>:ns_memcached:do_handle_call:710]Changed vbucket state 
[{852,replica,undefined},
 {851,replica,undefined},
 {850,replica,undefined},
 {849,replica,undefined},
 {848,replica,undefined},
 {847,replica,undefined},
 {846,replica,undefined},
 {845,replica,undefined},
 {844,replica,undefined},
 {843,replica,undefined},
 {842,replica,undefined},
 {841,replica,undefined},
 {840,replica,undefined},
 {839,replica,undefined},
 {838,replica,undefined},
 {837,replica,undefined},
 {836,replica,undefined},
 {835,replica,undefined},
 {834,replica,undefined},
 {833,replica,undefined},
 {832,replica,undefined},
 {831,replica,undefined},
 {830,replica,undefined},
 {829,replica,undefined},
 {828,replica,undefined},
 {827,replica,undefined},
 {826,replica,undefined},
 {825,replica,undefined},
 {824,replica,undefined},
 {823,replica,undefined},
 {822,replica,undefined},
 {821,replica,undefined},
 {820,replica,undefined},
 {819,replica,undefined},
 {818,replica,undefined},
 {817,replica,undefined},
 {816,replica,undefined},
 {815,replica,undefined},
 {814,replica,undefined},
 {813,replica,undefined},
 {812,replica,undefined},
 {811,replica,undefined},
 {810,replica,undefined},
 {809,replica,undefined},
 {808,replica,undefined},
 {807,replica,undefined},
 {806,replica,undefined},
 {805,replica,undefined},
 {804,replica,undefined},
 {803,replica,undefined},
 {802,replica,undefined},
 {801,replica,undefined},
 {800,replica,undefined},
 {799,replica,undefined},
 {798,replica,undefined},
 {797,replica,undefined},
 {796,replica,undefined},
 {795,replica,undefined},
 {794,replica,undefined},
 {793,replica,undefined},
 {792,replica,undefined},
 {791,replica,undefined},
 {790,replica,undefined},
 {789,replica,undefined},
 {788,replica,undefined},
 {787,replica,undefined},
 {786,replica,undefined},
 {785,replica,undefined},
 {784,replica,undefined},
 {783,replica,undefined},
 {782,replica,undefined},
 {781,replica,undefined},
 {780,replica,undefined},
 {779,replica,undefined},
 {778,replica,undefined},
 {777,replica,undefined},
 {776,replica,undefined},
 {775,replica,undefined},
 {774,replica,undefined},
 {773,replica,undefined},
 {772,replica,undefined},
 {771,replica,undefined},
 {770,replica,undefined},
 {769,replica,undefined},
 {768,replica,undefined},
 {767,replica,undefined},
 {766,replica,undefined},
 {765,replica,undefined},
 {764,replica,undefined},
 {763,replica,undefined},
 {762,replica,undefined},
 {761,replica,undefined},
 {760,replica,undefined},
 {759,replica,undefined},
 {758,replica,undefined},
 {757,replica,undefined},
 {756,replica,undefined},
 {755,replica,undefined},
 {754,replica,undefined},
 {753,replica,undefined},
 {752,replica,undefined},
 {751,replica,undefined},
 {750,replica,undefined},
 {749,replica,undefined},
 {748,replica,undefined},
 {747,replica,undefined},
 {746,replica,undefined},
 {745,replica,undefined},
 {744,replica,undefined},
 {743,replica,undefined},
 {742,replica,undefined},
 {741,replica,undefined},
 {740,replica,undefined},
 {739,replica,undefined},
 {738,replica,undefined},
 {737,replica,undefined},
 {736,replica,undefined},
 {735,replica,undefined},
 {734,replica,undefined},
 {733,replica,undefined},
 {732,replica,undefined},
 {731,replica,undefined},
 {730,replica,undefined},
 {729,replica,undefined},
 {728,replica,undefined},
 {727,replica,undefined},
 {726,replica,undefined},
 {725,replica,undefined},
 {724,replica,undefined},
 {723,replica,undefined},
 {722,replica,undefined},
 {721,replica,undefined},
 {720,replica,undefined},
 {719,replica,undefined},
 {718,replica,undefined},
 {717,replica,undefined},
 {716,replica,undefined},
 {715,replica,undefined},
 {714,replica,undefined},
 {713,replica,undefined},
 {712,replica,undefined},
 {711,replica,undefined},
 {710,replica,undefined},
 {709,replica,undefined},
 {708,replica,undefined},
 {707,replica,undefined},
 {706,replica,undefined},
 {705,replica,undefined},
 {704,replica,undefined},
 {703,replica,undefined},
 {702,replica,undefined},
 {701,replica,undefined},
 {700,replica,undefined},
 {699,replica,undefined},
 {698,replica,undefined},
 {697,replica,undefined},
 {696,replica,undefined},
 {695,replica,undefined},
 {694,replica,undefined},
 {693,replica,undefined},
 {692,replica,undefined},
 {691,replica,undefined},
 {690,replica,undefined},
 {689,replica,undefined},
 {688,replica,undefined},
 {687,replica,undefined},
 {686,replica,undefined},
 {685,replica,undefined},
 {684,replica,undefined},
 {683,replica,undefined},
 {682,replica,undefined},
 {511,replica,undefined},
 {510,replica,undefined},
 {509,replica,undefined},
 {508,replica,undefined},
 {507,replica,undefined},
 {506,replica,undefined},
 {505,replica,undefined},
 {504,replica,undefined},
 {503,replica,undefined},
 {502,replica,undefined},
 {501,replica,undefined},
 {500,replica,undefined},
 {499,replica,undefined},
 {498,replica,undefined},
 {497,replica,undefined},
 {496,replica,undefined},
 {495,replica,undefined},
 {494,replica,undefined},
 {493,replica,undefined},
 {492,replica,undefined},
 {491,replica,undefined},
 {490,replica,undefined},
 {489,replica,undefined},
 {488,replica,undefined},
 {487,replica,undefined},
 {486,replica,undefined},
 {485,replica,undefined},
 {484,replica,undefined},
 {483,replica,undefined},
 {482,replica,undefined},
 {481,replica,undefined},
 {480,replica,undefined},
 {479,replica,undefined},
 {478,replica,undefined},
 {477,replica,undefined},
 {476,replica,undefined},
 {475,replica,undefined},
 {474,replica,undefined},
 {473,replica,undefined},
 {472,replica,undefined},
 {471,replica,undefined},
 {470,replica,undefined},
 {469,replica,undefined},
 {468,replica,undefined},
 {467,replica,undefined},
 {466,replica,undefined},
 {465,replica,undefined},
 {464,replica,undefined},
 {463,replica,undefined},
 {462,replica,undefined},
 {461,replica,undefined},
 {460,replica,undefined},
 {459,replica,undefined},
 {458,replica,undefined},
 {457,replica,undefined},
 {456,replica,undefined},
 {455,replica,undefined},
 {454,replica,undefined},
 {453,replica,undefined},
 {452,replica,undefined},
 {451,replica,undefined},
 {450,replica,undefined},
 {449,replica,undefined},
 {448,replica,undefined},
 {447,replica,undefined},
 {446,replica,undefined},
 {445,replica,undefined},
 {444,replica,undefined},
 {443,replica,undefined},
 {442,replica,undefined},
 {441,replica,undefined},
 {440,replica,undefined},
 {439,replica,undefined},
 {438,replica,undefined},
 {437,replica,undefined},
 {436,replica,undefined},
 {435,replica,undefined},
 {434,replica,undefined},
 {433,replica,undefined},
 {432,replica,undefined},
 {431,replica,undefined},
 {430,replica,undefined},
 {429,replica,undefined},
 {428,replica,undefined},
 {427,replica,undefined},
 {426,replica,undefined},
 {425,replica,undefined},
 {424,replica,undefined},
 {423,replica,undefined},
 {422,replica,undefined},
 {421,replica,undefined},
 {420,replica,undefined},
 {419,replica,undefined},
 {418,replica,undefined},
 {417,replica,undefined},
 {416,replica,undefined},
 {415,replica,undefined},
 {414,replica,undefined},
 {413,replica,undefined},
 {412,replica,undefined},
 {411,replica,undefined},
 {410,replica,undefined},
 {409,replica,undefined},
 {408,replica,undefined},
 {407,replica,undefined},
 {406,replica,undefined},
 {405,replica,undefined},
 {404,replica,undefined},
 {403,replica,undefined},
 {402,replica,undefined},
 {401,replica,undefined},
 {400,replica,undefined},
 {399,replica,undefined},
 {398,replica,undefined},
 {397,replica,undefined},
 {396,replica,undefined},
 {395,replica,undefined},
 {394,replica,undefined},
 {393,replica,undefined},
 {392,replica,undefined},
 {391,replica,undefined},
 {390,replica,undefined},
 {389,replica,undefined},
 {388,replica,undefined},
 {387,replica,undefined},
 {386,replica,undefined},
 {385,replica,undefined},
 {384,replica,undefined},
 {383,replica,undefined},
 {382,replica,undefined},
 {381,replica,undefined},
 {380,replica,undefined},
 {379,replica,undefined},
 {378,replica,undefined},
 {377,replica,undefined},
 {376,replica,undefined},
 {375,replica,undefined},
 {374,replica,undefined},
 {373,replica,undefined},
 {372,replica,undefined},
 {371,replica,undefined},
 {370,replica,undefined},
 {369,replica,undefined},
 {368,replica,undefined},
 {367,replica,undefined},
 {366,replica,undefined},
 {365,replica,undefined},
 {364,replica,undefined},
 {363,replica,undefined},
 {362,replica,undefined},
 {361,replica,undefined},
 {360,replica,undefined},
 {359,replica,undefined},
 {358,replica,undefined},
 {357,replica,undefined},
 {356,replica,undefined},
 {355,replica,undefined},
 {354,replica,undefined},
 {353,replica,undefined},
 {352,replica,undefined},
 {351,replica,undefined},
 {350,replica,undefined},
 {349,replica,undefined},
 {348,replica,undefined},
 {347,replica,undefined},
 {346,replica,undefined},
 {345,replica,undefined},
 {344,replica,undefined},
 {343,replica,undefined},
 {342,replica,undefined},
 {341,replica,undefined},
 {340,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {339,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {338,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {337,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {336,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {335,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {334,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {333,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {332,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {331,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {330,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {329,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {328,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {327,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {326,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {325,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {324,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {323,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {322,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {321,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {320,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {319,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {318,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {317,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {316,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {315,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {314,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {313,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {312,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {311,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {310,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {309,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {308,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {307,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {306,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {305,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {304,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {303,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {302,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {301,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {300,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {299,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {298,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {297,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {296,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {295,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {294,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {293,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {292,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {291,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {290,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {289,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {288,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {287,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {286,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {285,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {284,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {283,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {282,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {281,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {280,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {279,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {278,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {277,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {276,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {275,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {274,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {273,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {272,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {271,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {270,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {269,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {268,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {267,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {266,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {265,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {264,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {263,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {262,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {261,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {260,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {259,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {258,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {257,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {256,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {255,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {254,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {253,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {252,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {251,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {250,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {249,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {248,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {247,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {246,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {245,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {244,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {243,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {242,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {241,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {240,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {239,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {238,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {237,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {236,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {235,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {234,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {233,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {232,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {231,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {230,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {229,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {228,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {227,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {226,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {225,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {224,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {223,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {222,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {221,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {220,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {219,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {218,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {217,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {216,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {215,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {214,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {213,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {212,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {211,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {210,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {209,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {208,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {207,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {206,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {205,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {204,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {203,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {202,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {201,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {200,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {199,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {198,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {197,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {196,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {195,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {194,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {193,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {192,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {191,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {190,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {189,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {188,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {187,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {186,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {185,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {184,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {183,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {182,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {181,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {180,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {179,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {178,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {177,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {176,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {175,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {174,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {173,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {172,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {171,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {170,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db3.lan']]}]}},
 {169,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {168,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {167,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {166,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {165,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {164,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {163,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {162,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {161,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {160,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {159,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {158,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {157,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {156,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {155,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {154,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {153,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {152,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {151,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {150,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {149,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {148,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {147,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {146,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {145,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {144,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {143,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {142,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {141,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {140,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {139,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {138,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {137,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {136,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {135,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {134,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {133,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {132,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {131,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {130,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {129,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {128,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {127,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {126,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {125,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {124,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {123,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {122,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {121,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {120,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {119,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {118,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {117,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {116,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {115,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {114,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {113,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {112,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {111,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {110,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {109,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {108,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {107,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {106,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {105,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {104,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {103,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {102,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {101,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {100,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {99,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {98,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {97,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {96,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {95,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {94,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {93,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {92,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {91,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {90,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {89,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {88,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {87,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {86,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {85,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {84,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {83,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {82,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {81,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {80,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {79,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {78,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {77,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {76,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {75,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {74,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {73,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {72,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {71,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {70,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {69,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {68,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {67,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {66,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {65,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {64,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {63,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {62,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {61,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {60,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {59,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {58,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {57,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {56,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {55,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {54,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {53,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {52,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {51,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {50,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {49,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {48,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {47,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {46,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {45,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {44,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {43,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {42,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {41,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {40,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {39,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {38,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {37,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {36,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {35,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {34,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {33,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {32,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {31,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {30,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {29,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {28,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {27,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {26,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {25,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {24,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {23,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {22,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {21,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {20,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {19,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {18,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {17,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {16,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {15,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {14,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {13,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {12,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {11,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {10,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {9,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {8,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {7,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {6,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {5,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {4,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {3,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {2,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {1,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}},
 {0,active,{[{topology,[['ns_1@172.19.0.4','ns_1@db2.lan']]}]}}]
[ns_server:debug,2025-05-15T18:47:52.261Z,ns_1@172.19.0.4:dcp_replication_manager-doom-scrolling<0.4263.0>:dcp_sup:start_replicator:48]Starting DCP replication from 'ns_1@db2.lan' for bucket "doom-scrolling" (Features = [collections,
                                                                                      del_times,
                                                                                      del_user_xattr,
                                                                                      json,
                                                                                      set_consumer_name,
                                                                                      snappy,
                                                                                      xattr])
[ns_server:debug,2025-05-15T18:47:52.277Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:open_connection:72]Open consumer connection "replication:ns_1@db2.lan->ns_1@172.19.0.4:doom-scrolling" on socket #Port<0.231>: Body <<"{\"consumer_name\":\"ns_1@172.19.0.4\"}">>
[ns_server:debug,2025-05-15T18:47:52.278Z,ns_1@172.19.0.4:dcp_replicator-doom-scrolling-ns_1@db2.lan<0.4432.0>:dcp_replicator:init:53]Opened connection to local memcached <0.4434.0>
[error_logger:info,2025-05-15T18:47:52.282Z,ns_1@172.19.0.4:dcp_sup-doom-scrolling<0.4262.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'dcp_sup-doom-scrolling'}
    started: [{pid,<0.4432.0>},
              {id,{'ns_1@db2.lan',[collections,del_times,del_user_xattr,json,
                                   set_consumer_name,snappy,xattr]}},
              {mfargs,{dcp_replicator,start_link,
                                      ['ns_1@db2.lan',"doom-scrolling",
                                       [collections,del_times,del_user_xattr,
                                        json,set_consumer_name,snappy,
                                        xattr]]}},
              {restart_type,temporary},
              {significant,false},
              {shutdown,60000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:52.282Z,ns_1@172.19.0.4:dcp_replication_manager-doom-scrolling<0.4263.0>:dcp_sup:start_replicator:48]Starting DCP replication from 'ns_1@db3.lan' for bucket "doom-scrolling" (Features = [collections,
                                                                                      del_times,
                                                                                      del_user_xattr,
                                                                                      json,
                                                                                      set_consumer_name,
                                                                                      snappy,
                                                                                      xattr])
[ns_server:debug,2025-05-15T18:47:52.286Z,ns_1@172.19.0.4:<0.4467.0>:dcp_commands:open_connection:72]Open producer connection "replication:ns_1@db2.lan->ns_1@172.19.0.4:doom-scrolling" on socket #Port<0.232>: Body undefined
[ns_server:debug,2025-05-15T18:47:52.287Z,ns_1@172.19.0.4:<0.4468.0>:dcp_replicator:connect_to_producer:81]initiated new dcp replication with consumer side: <0.4434.0> and producer side: <0.4467.0>
[ns_server:debug,2025-05-15T18:47:52.287Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:open_connection:72]Open consumer connection "replication:ns_1@db3.lan->ns_1@172.19.0.4:doom-scrolling" on socket #Port<0.233>: Body <<"{\"consumer_name\":\"ns_1@172.19.0.4\"}">>
[ns_server:debug,2025-05-15T18:47:52.288Z,ns_1@172.19.0.4:dcp_replicator-doom-scrolling-ns_1@db3.lan<0.4469.0>:dcp_replicator:init:53]Opened connection to local memcached <0.4470.0>
[error_logger:info,2025-05-15T18:47:52.288Z,ns_1@172.19.0.4:dcp_sup-doom-scrolling<0.4262.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'dcp_sup-doom-scrolling'}
    started: [{pid,<0.4469.0>},
              {id,{'ns_1@db3.lan',[collections,del_times,del_user_xattr,json,
                                   set_consumer_name,snappy,xattr]}},
              {mfargs,{dcp_replicator,start_link,
                                      ['ns_1@db3.lan',"doom-scrolling",
                                       [collections,del_times,del_user_xattr,
                                        json,set_consumer_name,snappy,
                                        xattr]]}},
              {restart_type,temporary},
              {significant,false},
              {shutdown,60000},
              {child_type,worker}]

[ns_server:debug,2025-05-15T18:47:52.288Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 341, opaque = 0x155, type = add
[ns_server:debug,2025-05-15T18:47:52.288Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 342, opaque = 0x156, type = add
[ns_server:debug,2025-05-15T18:47:52.288Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 343, opaque = 0x157, type = add
[ns_server:debug,2025-05-15T18:47:52.288Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 344, opaque = 0x158, type = add
[ns_server:debug,2025-05-15T18:47:52.289Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 345, opaque = 0x159, type = add
[ns_server:debug,2025-05-15T18:47:52.289Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 346, opaque = 0x15A, type = add
[ns_server:debug,2025-05-15T18:47:52.289Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 347, opaque = 0x15B, type = add
[ns_server:debug,2025-05-15T18:47:52.289Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 348, opaque = 0x15C, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 349, opaque = 0x15D, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 350, opaque = 0x15E, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 351, opaque = 0x15F, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 352, opaque = 0x160, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 353, opaque = 0x161, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 354, opaque = 0x162, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 355, opaque = 0x163, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 356, opaque = 0x164, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 357, opaque = 0x165, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 358, opaque = 0x166, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 359, opaque = 0x167, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 360, opaque = 0x168, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 361, opaque = 0x169, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 362, opaque = 0x16A, type = add
[ns_server:debug,2025-05-15T18:47:52.290Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 363, opaque = 0x16B, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 364, opaque = 0x16C, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 365, opaque = 0x16D, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 366, opaque = 0x16E, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 367, opaque = 0x16F, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 368, opaque = 0x170, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 369, opaque = 0x171, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 370, opaque = 0x172, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 371, opaque = 0x173, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 372, opaque = 0x174, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 373, opaque = 0x175, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 374, opaque = 0x176, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 375, opaque = 0x177, type = add
[ns_server:debug,2025-05-15T18:47:52.291Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 376, opaque = 0x178, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 377, opaque = 0x179, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 378, opaque = 0x17A, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 379, opaque = 0x17B, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 380, opaque = 0x17C, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 381, opaque = 0x17D, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 382, opaque = 0x17E, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 383, opaque = 0x17F, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 384, opaque = 0x180, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 385, opaque = 0x181, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 386, opaque = 0x182, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 387, opaque = 0x183, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 388, opaque = 0x184, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 389, opaque = 0x185, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 390, opaque = 0x186, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 391, opaque = 0x187, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 392, opaque = 0x188, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 393, opaque = 0x189, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 394, opaque = 0x18A, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 395, opaque = 0x18B, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 396, opaque = 0x18C, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 397, opaque = 0x18D, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 398, opaque = 0x18E, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 399, opaque = 0x18F, type = add
[ns_server:debug,2025-05-15T18:47:52.292Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 400, opaque = 0x190, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 401, opaque = 0x191, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 402, opaque = 0x192, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 403, opaque = 0x193, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 404, opaque = 0x194, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 405, opaque = 0x195, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 406, opaque = 0x196, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 407, opaque = 0x197, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 408, opaque = 0x198, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 409, opaque = 0x199, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 410, opaque = 0x19A, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 411, opaque = 0x19B, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 412, opaque = 0x19C, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 413, opaque = 0x19D, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 414, opaque = 0x19E, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 415, opaque = 0x19F, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 416, opaque = 0x1A0, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 417, opaque = 0x1A1, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 418, opaque = 0x1A2, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 419, opaque = 0x1A3, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 420, opaque = 0x1A4, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 421, opaque = 0x1A5, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 422, opaque = 0x1A6, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 423, opaque = 0x1A7, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 424, opaque = 0x1A8, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 425, opaque = 0x1A9, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 426, opaque = 0x1AA, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 427, opaque = 0x1AB, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 428, opaque = 0x1AC, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 429, opaque = 0x1AD, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 430, opaque = 0x1AE, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 431, opaque = 0x1AF, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 432, opaque = 0x1B0, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 433, opaque = 0x1B1, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 434, opaque = 0x1B2, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 435, opaque = 0x1B3, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 436, opaque = 0x1B4, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 437, opaque = 0x1B5, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 438, opaque = 0x1B6, type = add
[ns_server:debug,2025-05-15T18:47:52.293Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 439, opaque = 0x1B7, type = add
[ns_server:debug,2025-05-15T18:47:52.294Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 440, opaque = 0x1B8, type = add
[ns_server:debug,2025-05-15T18:47:52.294Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 441, opaque = 0x1B9, type = add
[ns_server:debug,2025-05-15T18:47:52.294Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 442, opaque = 0x1BA, type = add
[ns_server:debug,2025-05-15T18:47:52.294Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 443, opaque = 0x1BB, type = add
[ns_server:debug,2025-05-15T18:47:52.294Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 444, opaque = 0x1BC, type = add
[ns_server:debug,2025-05-15T18:47:52.294Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 445, opaque = 0x1BD, type = add
[ns_server:debug,2025-05-15T18:47:52.294Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 446, opaque = 0x1BE, type = add
[ns_server:debug,2025-05-15T18:47:52.294Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 447, opaque = 0x1BF, type = add
[ns_server:debug,2025-05-15T18:47:52.294Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 448, opaque = 0x1C0, type = add
[ns_server:debug,2025-05-15T18:47:52.294Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 449, opaque = 0x1C1, type = add
[ns_server:debug,2025-05-15T18:47:52.294Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 450, opaque = 0x1C2, type = add
[ns_server:debug,2025-05-15T18:47:52.294Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 451, opaque = 0x1C3, type = add
[ns_server:debug,2025-05-15T18:47:52.294Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 452, opaque = 0x1C4, type = add
[ns_server:debug,2025-05-15T18:47:52.294Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 453, opaque = 0x1C5, type = add
[ns_server:debug,2025-05-15T18:47:52.294Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 454, opaque = 0x1C6, type = add
[ns_server:debug,2025-05-15T18:47:52.294Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 455, opaque = 0x1C7, type = add
[ns_server:debug,2025-05-15T18:47:52.294Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 456, opaque = 0x1C8, type = add
[ns_server:debug,2025-05-15T18:47:52.294Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 457, opaque = 0x1C9, type = add
[ns_server:debug,2025-05-15T18:47:52.294Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 458, opaque = 0x1CA, type = add
[ns_server:debug,2025-05-15T18:47:52.294Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 459, opaque = 0x1CB, type = add
[ns_server:debug,2025-05-15T18:47:52.294Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 460, opaque = 0x1CC, type = add
[ns_server:debug,2025-05-15T18:47:52.294Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 461, opaque = 0x1CD, type = add
[ns_server:debug,2025-05-15T18:47:52.294Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 462, opaque = 0x1CE, type = add
[ns_server:debug,2025-05-15T18:47:52.295Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 463, opaque = 0x1CF, type = add
[ns_server:debug,2025-05-15T18:47:52.295Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 464, opaque = 0x1D0, type = add
[ns_server:debug,2025-05-15T18:47:52.295Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 465, opaque = 0x1D1, type = add
[ns_server:debug,2025-05-15T18:47:52.295Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 466, opaque = 0x1D2, type = add
[ns_server:debug,2025-05-15T18:47:52.295Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 467, opaque = 0x1D3, type = add
[ns_server:debug,2025-05-15T18:47:52.295Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 468, opaque = 0x1D4, type = add
[ns_server:debug,2025-05-15T18:47:52.295Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 469, opaque = 0x1D5, type = add
[ns_server:debug,2025-05-15T18:47:52.295Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 470, opaque = 0x1D6, type = add
[ns_server:debug,2025-05-15T18:47:52.295Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 471, opaque = 0x1D7, type = add
[ns_server:debug,2025-05-15T18:47:52.295Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 472, opaque = 0x1D8, type = add
[ns_server:debug,2025-05-15T18:47:52.295Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 473, opaque = 0x1D9, type = add
[ns_server:debug,2025-05-15T18:47:52.295Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 474, opaque = 0x1DA, type = add
[ns_server:debug,2025-05-15T18:47:52.295Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 475, opaque = 0x1DB, type = add
[ns_server:debug,2025-05-15T18:47:52.295Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 476, opaque = 0x1DC, type = add
[ns_server:debug,2025-05-15T18:47:52.295Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 477, opaque = 0x1DD, type = add
[ns_server:debug,2025-05-15T18:47:52.295Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 478, opaque = 0x1DE, type = add
[ns_server:debug,2025-05-15T18:47:52.295Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 479, opaque = 0x1DF, type = add
[ns_server:debug,2025-05-15T18:47:52.295Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 480, opaque = 0x1E0, type = add
[ns_server:debug,2025-05-15T18:47:52.295Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 481, opaque = 0x1E1, type = add
[ns_server:debug,2025-05-15T18:47:52.295Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 482, opaque = 0x1E2, type = add
[ns_server:debug,2025-05-15T18:47:52.295Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 483, opaque = 0x1E3, type = add
[ns_server:debug,2025-05-15T18:47:52.295Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 484, opaque = 0x1E4, type = add
[ns_server:debug,2025-05-15T18:47:52.296Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 485, opaque = 0x1E5, type = add
[ns_server:debug,2025-05-15T18:47:52.296Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 486, opaque = 0x1E6, type = add
[ns_server:debug,2025-05-15T18:47:52.296Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 487, opaque = 0x1E7, type = add
[ns_server:debug,2025-05-15T18:47:52.296Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 488, opaque = 0x1E8, type = add
[ns_server:debug,2025-05-15T18:47:52.296Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 489, opaque = 0x1E9, type = add
[ns_server:debug,2025-05-15T18:47:52.296Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 490, opaque = 0x1EA, type = add
[ns_server:debug,2025-05-15T18:47:52.296Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 491, opaque = 0x1EB, type = add
[ns_server:debug,2025-05-15T18:47:52.296Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 492, opaque = 0x1EC, type = add
[ns_server:debug,2025-05-15T18:47:52.296Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 493, opaque = 0x1ED, type = add
[ns_server:debug,2025-05-15T18:47:52.297Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 494, opaque = 0x1EE, type = add
[ns_server:debug,2025-05-15T18:47:52.297Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 495, opaque = 0x1EF, type = add
[ns_server:debug,2025-05-15T18:47:52.297Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 496, opaque = 0x1F0, type = add
[ns_server:debug,2025-05-15T18:47:52.297Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 497, opaque = 0x1F1, type = add
[ns_server:debug,2025-05-15T18:47:52.297Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 498, opaque = 0x1F2, type = add
[ns_server:debug,2025-05-15T18:47:52.297Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 499, opaque = 0x1F3, type = add
[ns_server:debug,2025-05-15T18:47:52.297Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 500, opaque = 0x1F4, type = add
[ns_server:debug,2025-05-15T18:47:52.297Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 501, opaque = 0x1F5, type = add
[ns_server:debug,2025-05-15T18:47:52.297Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 502, opaque = 0x1F6, type = add
[ns_server:debug,2025-05-15T18:47:52.298Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 503, opaque = 0x1F7, type = add
[ns_server:debug,2025-05-15T18:47:52.298Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 504, opaque = 0x1F8, type = add
[ns_server:debug,2025-05-15T18:47:52.298Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 505, opaque = 0x1F9, type = add
[ns_server:debug,2025-05-15T18:47:52.298Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 506, opaque = 0x1FA, type = add
[ns_server:debug,2025-05-15T18:47:52.298Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 507, opaque = 0x1FB, type = add
[ns_server:debug,2025-05-15T18:47:52.299Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 508, opaque = 0x1FC, type = add
[ns_server:debug,2025-05-15T18:47:52.299Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 509, opaque = 0x1FD, type = add
[ns_server:debug,2025-05-15T18:47:52.299Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 510, opaque = 0x1FE, type = add
[ns_server:debug,2025-05-15T18:47:52.299Z,ns_1@172.19.0.4:<0.4434.0>:dcp_commands:add_stream:83]Add stream for partition 511, opaque = 0x1FF, type = add
[ns_server:debug,2025-05-15T18:47:52.299Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_call:206]Setup DCP streams:
Current []
Streams to open [341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511]
Streams to close []

[ns_server:debug,2025-05-15T18:47:52.300Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0x2000000
80 5E 00 16
00 00 00 00
00 00 00 1E
02 00 00 00
00 00 00 00
00 00 00 00
63 6F 6E 6E
65 63 74 69
6F 6E 5F 62
75 66 66 65
72 5F 73 69
7A 65 31 33
34 32 31 37
37 33 
[ns_server:debug,2025-05-15T18:47:52.300Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0xFE (cmd_get_error_map) vbucket = 0 opaque = 0x3000000
80 FE 00 00
00 00 00 00
00 00 00 02
03 00 00 00
00 00 00 00
00 00 00 00
00 00 
[ns_server:debug,2025-05-15T18:47:52.301Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0x2000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
02 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.301Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0xFE (cmd_get_error_map) vbucket = 0 opaque = 0x3000000 status = 0x1 (key_enoent)
81 FE 00 00
00 00 00 01
00 00 00 00
03 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.302Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xAE000000
80 5E 00 0B
00 00 00 00
00 00 00 0F
AE 00 00 00
00 00 00 00
00 00 00 00
65 6E 61 62
6C 65 5F 6E
6F 6F 70 74
72 75 65 
[ns_server:debug,2025-05-15T18:47:52.302Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xAF000000
80 5E 00 11
00 00 00 00
00 00 00 19
AF 00 00 00
00 00 00 00
00 00 00 00
73 65 74 5F
6E 6F 6F 70
5F 69 6E 74
65 72 76 61
6C 30 2E 31
30 30 30 30
30 
[ns_server:debug,2025-05-15T18:47:52.302Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xAE000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
AE 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.303Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xAF000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
AF 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.304Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB0000000
80 5E 00 0C
00 00 00 00
00 00 00 10
B0 00 00 00
00 00 00 00
00 00 00 00
73 65 74 5F
70 72 69 6F
72 69 74 79
68 69 67 68

[ns_server:debug,2025-05-15T18:47:52.304Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB1000000
80 5E 00 1F
00 00 00 00
00 00 00 23
B1 00 00 00
00 00 00 00
00 00 00 00
73 75 70 70
6F 72 74 73
5F 63 75 72
73 6F 72 5F
64 72 6F 70
70 69 6E 67
5F 76 75 6C
63 61 6E 74
72 75 65 
[ns_server:debug,2025-05-15T18:47:52.304Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB2000000
80 5E 00 11
00 00 00 00
00 00 00 15
B2 00 00 00
00 00 00 00
00 00 00 00
73 75 70 70
6F 72 74 73
5F 68 69 66
69 5F 4D 46
55 74 72 75
65 
[ns_server:debug,2025-05-15T18:47:52.304Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB3000000
80 5E 00 26
00 00 00 00
00 00 00 2A
B3 00 00 00
00 00 00 00
00 00 00 00
73 65 6E 64
5F 73 74 72
65 61 6D 5F
65 6E 64 5F
6F 6E 5F 63
6C 69 65 6E
74 5F 63 6C
6F 73 65 5F
73 74 72 65
61 6D 74 72
75 65 
[ns_server:debug,2025-05-15T18:47:52.305Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB4000000
80 5E 00 14
00 00 00 00
00 00 00 18
B4 00 00 00
00 00 00 00
00 00 00 00
65 6E 61 62
6C 65 5F 65
78 70 69 72
79 5F 6F 70
63 6F 64 65
74 72 75 65

[ns_server:debug,2025-05-15T18:47:52.305Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB5000000
80 5E 00 12
00 00 00 00
00 00 00 16
B5 00 00 00
00 00 00 00
00 00 00 00
65 6E 61 62
6C 65 5F 73
79 6E 63 5F
77 72 69 74
65 73 74 72
75 65 
[ns_server:debug,2025-05-15T18:47:52.305Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB0000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B0 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.306Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB1000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B1 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.306Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB2000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B2 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.306Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB3000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B3 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.306Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB4000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B4 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.306Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB5000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B5 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.306Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB6000000
80 5E 00 0D
00 00 00 00
00 00 00 1C
B6 00 00 00
00 00 00 00
00 00 00 00
63 6F 6E 73
75 6D 65 72
5F 6E 61 6D
65 6E 73 5F
31 40 31 37
32 2E 31 39
2E 30 2E 34

[ns_server:debug,2025-05-15T18:47:52.307Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB7000000
80 5E 00 1B
00 00 00 00
00 00 00 1F
B7 00 00 00
00 00 00 00
00 00 00 00
69 6E 63 6C
75 64 65 5F
64 65 6C 65
74 65 64 5F
75 73 65 72
5F 78 61 74
74 72 73 74
72 75 65 
[ns_server:debug,2025-05-15T18:47:52.307Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB6000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B6 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.307Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB7000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B7 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.307Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB8000000
80 5E 00 13
00 00 00 00
00 00 00 17
B8 00 00 00
00 00 00 00
00 00 00 00
76 37 5F 64
63 70 5F 73
74 61 74 75
73 5F 63 6F
64 65 73 74
72 75 65 
[ns_server:debug,2025-05-15T18:47:52.308Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB8000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B8 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.308Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB9000000
80 5E 00 19
00 00 00 00
00 00 00 1D
B9 00 00 00
00 00 00 00
00 00 00 00
66 6C 61 74
62 75 66 66
65 72 73 5F
73 79 73 74
65 6D 5F 65
76 65 6E 74
73 74 72 75
65 
[ns_server:debug,2025-05-15T18:47:52.308Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB9000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B9 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.309Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xBA000000
80 5E 00 0E
00 00 00 00
00 00 00 12
BA 00 00 00
00 00 00 00
00 00 00 00
63 68 61 6E
67 65 5F 73
74 72 65 61
6D 73 74 72
75 65 
[ns_server:debug,2025-05-15T18:47:52.310Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xBA000000 status = 0x83 (not_supported)
81 5E 00 00
00 00 00 83
00 00 00 00
BA 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.311Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 341 opaque = 0x1000000
80 53 00 00
30 00 01 55
00 00 00 30
01 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.311Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 342 opaque = 0x4000000
80 53 00 00
30 00 01 56
00 00 00 30
04 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.312Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 343 opaque = 0x5000000
80 53 00 00
30 00 01 57
00 00 00 30
05 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.312Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 344 opaque = 0x6000000
80 53 00 00
30 00 01 58
00 00 00 30
06 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.312Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 345 opaque = 0x7000000
80 53 00 00
30 00 01 59
00 00 00 30
07 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.312Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 346 opaque = 0x8000000
80 53 00 00
30 00 01 5A
00 00 00 30
08 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.312Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 347 opaque = 0x9000000
80 53 00 00
30 00 01 5B
00 00 00 30
09 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.313Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 348 opaque = 0xA000000
80 53 00 00
30 00 01 5C
00 00 00 30
0A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.313Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 349 opaque = 0xB000000
80 53 00 00
30 00 01 5D
00 00 00 30
0B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.313Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 350 opaque = 0xC000000
80 53 00 00
30 00 01 5E
00 00 00 30
0C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.313Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 351 opaque = 0xD000000
80 53 00 00
30 00 01 5F
00 00 00 30
0D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.313Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 352 opaque = 0xE000000
80 53 00 00
30 00 01 60
00 00 00 30
0E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.314Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 353 opaque = 0xF000000
80 53 00 00
30 00 01 61
00 00 00 30
0F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.314Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 354 opaque = 0x10000000
80 53 00 00
30 00 01 62
00 00 00 30
10 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.314Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 355 opaque = 0x11000000
80 53 00 00
30 00 01 63
00 00 00 30
11 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.314Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 356 opaque = 0x12000000
80 53 00 00
30 00 01 64
00 00 00 30
12 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.314Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 357 opaque = 0x13000000
80 53 00 00
30 00 01 65
00 00 00 30
13 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.314Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 358 opaque = 0x14000000
80 53 00 00
30 00 01 66
00 00 00 30
14 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.314Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 359 opaque = 0x15000000
80 53 00 00
30 00 01 67
00 00 00 30
15 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.315Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 360 opaque = 0x16000000
80 53 00 00
30 00 01 68
00 00 00 30
16 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.315Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 361 opaque = 0x17000000
80 53 00 00
30 00 01 69
00 00 00 30
17 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.315Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 362 opaque = 0x18000000
80 53 00 00
30 00 01 6A
00 00 00 30
18 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.315Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 363 opaque = 0x19000000
80 53 00 00
30 00 01 6B
00 00 00 30
19 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.315Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 364 opaque = 0x1A000000
80 53 00 00
30 00 01 6C
00 00 00 30
1A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.315Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 365 opaque = 0x1B000000
80 53 00 00
30 00 01 6D
00 00 00 30
1B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.315Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 366 opaque = 0x1C000000
80 53 00 00
30 00 01 6E
00 00 00 30
1C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.315Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 367 opaque = 0x1D000000
80 53 00 00
30 00 01 6F
00 00 00 30
1D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.315Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 368 opaque = 0x1E000000
80 53 00 00
30 00 01 70
00 00 00 30
1E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.316Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 369 opaque = 0x1F000000
80 53 00 00
30 00 01 71
00 00 00 30
1F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.316Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 370 opaque = 0x20000000
80 53 00 00
30 00 01 72
00 00 00 30
20 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.316Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 371 opaque = 0x21000000
80 53 00 00
30 00 01 73
00 00 00 30
21 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.316Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 372 opaque = 0x22000000
80 53 00 00
30 00 01 74
00 00 00 30
22 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.316Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 373 opaque = 0x23000000
80 53 00 00
30 00 01 75
00 00 00 30
23 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.316Z,ns_1@172.19.0.4:<0.4471.0>:dcp_commands:open_connection:72]Open producer connection "replication:ns_1@db3.lan->ns_1@172.19.0.4:doom-scrolling" on socket #Port<0.234>: Body undefined
[ns_server:debug,2025-05-15T18:47:52.316Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 374 opaque = 0x24000000
80 53 00 00
30 00 01 76
00 00 00 30
24 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.316Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 375 opaque = 0x25000000
80 53 00 00
30 00 01 77
00 00 00 30
25 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.317Z,ns_1@172.19.0.4:<0.4472.0>:dcp_replicator:connect_to_producer:81]initiated new dcp replication with consumer side: <0.4470.0> and producer side: <0.4471.0>
[ns_server:debug,2025-05-15T18:47:52.317Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 376 opaque = 0x26000000
80 53 00 00
30 00 01 78
00 00 00 30
26 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.317Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 377 opaque = 0x27000000
80 53 00 00
30 00 01 79
00 00 00 30
27 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.317Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 378 opaque = 0x28000000
80 53 00 00
30 00 01 7A
00 00 00 30
28 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.317Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 379 opaque = 0x29000000
80 53 00 00
30 00 01 7B
00 00 00 30
29 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.317Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 380 opaque = 0x2A000000
80 53 00 00
30 00 01 7C
00 00 00 30
2A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.317Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 381 opaque = 0x2B000000
80 53 00 00
30 00 01 7D
00 00 00 30
2B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.317Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 382 opaque = 0x2C000000
80 53 00 00
30 00 01 7E
00 00 00 30
2C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.317Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 383 opaque = 0x2D000000
80 53 00 00
30 00 01 7F
00 00 00 30
2D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.318Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 384 opaque = 0x2E000000
80 53 00 00
30 00 01 80
00 00 00 30
2E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.318Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 385 opaque = 0x2F000000
80 53 00 00
30 00 01 81
00 00 00 30
2F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.318Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 386 opaque = 0x30000000
80 53 00 00
30 00 01 82
00 00 00 30
30 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.318Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 387 opaque = 0x31000000
80 53 00 00
30 00 01 83
00 00 00 30
31 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.318Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 388 opaque = 0x32000000
80 53 00 00
30 00 01 84
00 00 00 30
32 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.318Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 389 opaque = 0x33000000
80 53 00 00
30 00 01 85
00 00 00 30
33 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.319Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 390 opaque = 0x34000000
80 53 00 00
30 00 01 86
00 00 00 30
34 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.319Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 391 opaque = 0x35000000
80 53 00 00
30 00 01 87
00 00 00 30
35 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.319Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 392 opaque = 0x36000000
80 53 00 00
30 00 01 88
00 00 00 30
36 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.319Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 393 opaque = 0x37000000
80 53 00 00
30 00 01 89
00 00 00 30
37 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.319Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 394 opaque = 0x38000000
80 53 00 00
30 00 01 8A
00 00 00 30
38 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.319Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 395 opaque = 0x39000000
80 53 00 00
30 00 01 8B
00 00 00 30
39 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.319Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 396 opaque = 0x3A000000
80 53 00 00
30 00 01 8C
00 00 00 30
3A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.319Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 397 opaque = 0x3B000000
80 53 00 00
30 00 01 8D
00 00 00 30
3B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.319Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 398 opaque = 0x3C000000
80 53 00 00
30 00 01 8E
00 00 00 30
3C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.320Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 399 opaque = 0x3D000000
80 53 00 00
30 00 01 8F
00 00 00 30
3D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.320Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 400 opaque = 0x3E000000
80 53 00 00
30 00 01 90
00 00 00 30
3E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.320Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 401 opaque = 0x3F000000
80 53 00 00
30 00 01 91
00 00 00 30
3F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.320Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 402 opaque = 0x40000000
80 53 00 00
30 00 01 92
00 00 00 30
40 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.320Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 403 opaque = 0x41000000
80 53 00 00
30 00 01 93
00 00 00 30
41 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.320Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 404 opaque = 0x42000000
80 53 00 00
30 00 01 94
00 00 00 30
42 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.320Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 405 opaque = 0x43000000
80 53 00 00
30 00 01 95
00 00 00 30
43 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.320Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 406 opaque = 0x44000000
80 53 00 00
30 00 01 96
00 00 00 30
44 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.320Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 407 opaque = 0x45000000
80 53 00 00
30 00 01 97
00 00 00 30
45 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.321Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 408 opaque = 0x46000000
80 53 00 00
30 00 01 98
00 00 00 30
46 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.321Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 409 opaque = 0x47000000
80 53 00 00
30 00 01 99
00 00 00 30
47 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.321Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 410 opaque = 0x48000000
80 53 00 00
30 00 01 9A
00 00 00 30
48 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.321Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 411 opaque = 0x49000000
80 53 00 00
30 00 01 9B
00 00 00 30
49 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.321Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 412 opaque = 0x4A000000
80 53 00 00
30 00 01 9C
00 00 00 30
4A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.321Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 413 opaque = 0x4B000000
80 53 00 00
30 00 01 9D
00 00 00 30
4B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.322Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 414 opaque = 0x4C000000
80 53 00 00
30 00 01 9E
00 00 00 30
4C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.322Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 415 opaque = 0x4D000000
80 53 00 00
30 00 01 9F
00 00 00 30
4D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.322Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 416 opaque = 0x4E000000
80 53 00 00
30 00 01 A0
00 00 00 30
4E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.322Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 417 opaque = 0x4F000000
80 53 00 00
30 00 01 A1
00 00 00 30
4F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.322Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 418 opaque = 0x50000000
80 53 00 00
30 00 01 A2
00 00 00 30
50 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.322Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 419 opaque = 0x51000000
80 53 00 00
30 00 01 A3
00 00 00 30
51 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.322Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 420 opaque = 0x52000000
80 53 00 00
30 00 01 A4
00 00 00 30
52 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.323Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 421 opaque = 0x53000000
80 53 00 00
30 00 01 A5
00 00 00 30
53 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.323Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 422 opaque = 0x54000000
80 53 00 00
30 00 01 A6
00 00 00 30
54 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.323Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 423 opaque = 0x55000000
80 53 00 00
30 00 01 A7
00 00 00 30
55 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.323Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 424 opaque = 0x56000000
80 53 00 00
30 00 01 A8
00 00 00 30
56 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.323Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 425 opaque = 0x57000000
80 53 00 00
30 00 01 A9
00 00 00 30
57 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.323Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 426 opaque = 0x58000000
80 53 00 00
30 00 01 AA
00 00 00 30
58 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.323Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 427 opaque = 0x59000000
80 53 00 00
30 00 01 AB
00 00 00 30
59 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.323Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 428 opaque = 0x5A000000
80 53 00 00
30 00 01 AC
00 00 00 30
5A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.323Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 429 opaque = 0x5B000000
80 53 00 00
30 00 01 AD
00 00 00 30
5B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.324Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 430 opaque = 0x5C000000
80 53 00 00
30 00 01 AE
00 00 00 30
5C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.324Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 431 opaque = 0x5D000000
80 53 00 00
30 00 01 AF
00 00 00 30
5D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.324Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 432 opaque = 0x5E000000
80 53 00 00
30 00 01 B0
00 00 00 30
5E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.324Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 433 opaque = 0x5F000000
80 53 00 00
30 00 01 B1
00 00 00 30
5F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.324Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 434 opaque = 0x60000000
80 53 00 00
30 00 01 B2
00 00 00 30
60 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.324Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 435 opaque = 0x61000000
80 53 00 00
30 00 01 B3
00 00 00 30
61 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.324Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 436 opaque = 0x62000000
80 53 00 00
30 00 01 B4
00 00 00 30
62 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.324Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 437 opaque = 0x63000000
80 53 00 00
30 00 01 B5
00 00 00 30
63 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.324Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 438 opaque = 0x64000000
80 53 00 00
30 00 01 B6
00 00 00 30
64 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.325Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 439 opaque = 0x65000000
80 53 00 00
30 00 01 B7
00 00 00 30
65 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.325Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 440 opaque = 0x66000000
80 53 00 00
30 00 01 B8
00 00 00 30
66 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.325Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 441 opaque = 0x67000000
80 53 00 00
30 00 01 B9
00 00 00 30
67 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.325Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 442 opaque = 0x68000000
80 53 00 00
30 00 01 BA
00 00 00 30
68 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.325Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 443 opaque = 0x69000000
80 53 00 00
30 00 01 BB
00 00 00 30
69 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.325Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 444 opaque = 0x6A000000
80 53 00 00
30 00 01 BC
00 00 00 30
6A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.325Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 445 opaque = 0x6B000000
80 53 00 00
30 00 01 BD
00 00 00 30
6B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.325Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 446 opaque = 0x6C000000
80 53 00 00
30 00 01 BE
00 00 00 30
6C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.325Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 447 opaque = 0x6D000000
80 53 00 00
30 00 01 BF
00 00 00 30
6D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.326Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 448 opaque = 0x6E000000
80 53 00 00
30 00 01 C0
00 00 00 30
6E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.326Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 449 opaque = 0x6F000000
80 53 00 00
30 00 01 C1
00 00 00 30
6F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.326Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 450 opaque = 0x70000000
80 53 00 00
30 00 01 C2
00 00 00 30
70 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.326Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 451 opaque = 0x71000000
80 53 00 00
30 00 01 C3
00 00 00 30
71 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.326Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 452 opaque = 0x72000000
80 53 00 00
30 00 01 C4
00 00 00 30
72 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.326Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 453 opaque = 0x73000000
80 53 00 00
30 00 01 C5
00 00 00 30
73 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.326Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 454 opaque = 0x74000000
80 53 00 00
30 00 01 C6
00 00 00 30
74 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.326Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 455 opaque = 0x75000000
80 53 00 00
30 00 01 C7
00 00 00 30
75 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.326Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 456 opaque = 0x76000000
80 53 00 00
30 00 01 C8
00 00 00 30
76 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.327Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 457 opaque = 0x77000000
80 53 00 00
30 00 01 C9
00 00 00 30
77 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.327Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 458 opaque = 0x78000000
80 53 00 00
30 00 01 CA
00 00 00 30
78 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.327Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 459 opaque = 0x79000000
80 53 00 00
30 00 01 CB
00 00 00 30
79 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.327Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 460 opaque = 0x7A000000
80 53 00 00
30 00 01 CC
00 00 00 30
7A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.327Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 461 opaque = 0x7B000000
80 53 00 00
30 00 01 CD
00 00 00 30
7B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.327Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 462 opaque = 0x7C000000
80 53 00 00
30 00 01 CE
00 00 00 30
7C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.327Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 463 opaque = 0x7D000000
80 53 00 00
30 00 01 CF
00 00 00 30
7D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.327Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 464 opaque = 0x7E000000
80 53 00 00
30 00 01 D0
00 00 00 30
7E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.327Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 465 opaque = 0x7F000000
80 53 00 00
30 00 01 D1
00 00 00 30
7F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.327Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 466 opaque = 0x80000000
80 53 00 00
30 00 01 D2
00 00 00 30
80 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.328Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 467 opaque = 0x81000000
80 53 00 00
30 00 01 D3
00 00 00 30
81 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.328Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 468 opaque = 0x82000000
80 53 00 00
30 00 01 D4
00 00 00 30
82 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.328Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 469 opaque = 0x83000000
80 53 00 00
30 00 01 D5
00 00 00 30
83 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.328Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 470 opaque = 0x84000000
80 53 00 00
30 00 01 D6
00 00 00 30
84 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.328Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 471 opaque = 0x85000000
80 53 00 00
30 00 01 D7
00 00 00 30
85 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.328Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 472 opaque = 0x86000000
80 53 00 00
30 00 01 D8
00 00 00 30
86 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.328Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 473 opaque = 0x87000000
80 53 00 00
30 00 01 D9
00 00 00 30
87 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.329Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 474 opaque = 0x88000000
80 53 00 00
30 00 01 DA
00 00 00 30
88 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.329Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 475 opaque = 0x89000000
80 53 00 00
30 00 01 DB
00 00 00 30
89 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.329Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 476 opaque = 0x8A000000
80 53 00 00
30 00 01 DC
00 00 00 30
8A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.329Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 477 opaque = 0x8B000000
80 53 00 00
30 00 01 DD
00 00 00 30
8B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.329Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 478 opaque = 0x8C000000
80 53 00 00
30 00 01 DE
00 00 00 30
8C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.329Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 479 opaque = 0x8D000000
80 53 00 00
30 00 01 DF
00 00 00 30
8D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.329Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 480 opaque = 0x8E000000
80 53 00 00
30 00 01 E0
00 00 00 30
8E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.329Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 481 opaque = 0x8F000000
80 53 00 00
30 00 01 E1
00 00 00 30
8F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.329Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 482 opaque = 0x90000000
80 53 00 00
30 00 01 E2
00 00 00 30
90 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.330Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 483 opaque = 0x91000000
80 53 00 00
30 00 01 E3
00 00 00 30
91 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.330Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 484 opaque = 0x92000000
80 53 00 00
30 00 01 E4
00 00 00 30
92 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.330Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 485 opaque = 0x93000000
80 53 00 00
30 00 01 E5
00 00 00 30
93 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.330Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 486 opaque = 0x94000000
80 53 00 00
30 00 01 E6
00 00 00 30
94 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.330Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 487 opaque = 0x95000000
80 53 00 00
30 00 01 E7
00 00 00 30
95 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.330Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 488 opaque = 0x96000000
80 53 00 00
30 00 01 E8
00 00 00 30
96 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.330Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 489 opaque = 0x97000000
80 53 00 00
30 00 01 E9
00 00 00 30
97 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.331Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 490 opaque = 0x98000000
80 53 00 00
30 00 01 EA
00 00 00 30
98 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.331Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 491 opaque = 0x99000000
80 53 00 00
30 00 01 EB
00 00 00 30
99 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.331Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 492 opaque = 0x9A000000
80 53 00 00
30 00 01 EC
00 00 00 30
9A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.331Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 493 opaque = 0x9B000000
80 53 00 00
30 00 01 ED
00 00 00 30
9B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.331Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 494 opaque = 0x9C000000
80 53 00 00
30 00 01 EE
00 00 00 30
9C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.331Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 495 opaque = 0x9D000000
80 53 00 00
30 00 01 EF
00 00 00 30
9D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.331Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 496 opaque = 0x9E000000
80 53 00 00
30 00 01 F0
00 00 00 30
9E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.331Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 497 opaque = 0x9F000000
80 53 00 00
30 00 01 F1
00 00 00 30
9F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.332Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 498 opaque = 0xA0000000
80 53 00 00
30 00 01 F2
00 00 00 30
A0 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.332Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 499 opaque = 0xA1000000
80 53 00 00
30 00 01 F3
00 00 00 30
A1 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.332Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 500 opaque = 0xA2000000
80 53 00 00
30 00 01 F4
00 00 00 30
A2 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.332Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 501 opaque = 0xA3000000
80 53 00 00
30 00 01 F5
00 00 00 30
A3 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.332Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 502 opaque = 0xA4000000
80 53 00 00
30 00 01 F6
00 00 00 30
A4 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.332Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 503 opaque = 0xA5000000
80 53 00 00
30 00 01 F7
00 00 00 30
A5 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.332Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 504 opaque = 0xA6000000
80 53 00 00
30 00 01 F8
00 00 00 30
A6 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.332Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 505 opaque = 0xA7000000
80 53 00 00
30 00 01 F9
00 00 00 30
A7 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.332Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 506 opaque = 0xA8000000
80 53 00 00
30 00 01 FA
00 00 00 30
A8 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.333Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 507 opaque = 0xA9000000
80 53 00 00
30 00 01 FB
00 00 00 30
A9 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.333Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 508 opaque = 0xAA000000
80 53 00 00
30 00 01 FC
00 00 00 30
AA 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.333Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 509 opaque = 0xAB000000
80 53 00 00
30 00 01 FD
00 00 00 30
AB 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.333Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 510 opaque = 0xAC000000
80 53 00 00
30 00 01 FE
00 00 00 30
AC 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.333Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 511 opaque = 0xAD000000
80 53 00 00
30 00 01 FF
00 00 00 30
AD 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.336Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x1000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
01 00 00 00
00 00 00 00
00 00 00 00
00 00 B9 64
1E 17 42 3A
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.336Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x4000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
04 00 00 00
00 00 00 00
00 00 00 00
00 00 A1 CD
8A 79 18 DE
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.336Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x5000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
05 00 00 00
00 00 00 00
00 00 00 00
00 00 6C AB
E0 7F 62 4B
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.336Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x6000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
06 00 00 00
00 00 00 00
00 00 00 00
00 00 04 77
3B AE C4 FE
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.337Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x7000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
07 00 00 00
00 00 00 00
00 00 00 00
00 00 B4 D6
0C 45 0E 0D
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.337Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x8000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
08 00 00 00
00 00 00 00
00 00 00 00
00 00 77 C0
09 24 1A 7F
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.337Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x9000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
09 00 00 00
00 00 00 00
00 00 00 00
00 00 CC 83
28 E5 14 B3
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.337Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
0A 00 00 00
00 00 00 00
00 00 00 00
00 00 47 65
C4 15 5A EE
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.337Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xB000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
0B 00 00 00
00 00 00 00
00 00 00 00
00 00 D0 FD
DC A8 03 CB
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.337Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xC000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
0C 00 00 00
00 00 00 00
00 00 00 00
00 00 6C 8E
F9 96 C0 0B
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.337Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xD000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
0D 00 00 00
00 00 00 00
00 00 00 00
00 00 F8 73
18 9A A9 C1
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.338Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xE000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
0E 00 00 00
00 00 00 00
00 00 00 00
00 00 B4 32
71 20 39 65
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.338Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xF000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
0F 00 00 00
00 00 00 00
00 00 00 00
00 00 9A 48
D7 A0 50 9E
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.338Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x10000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
10 00 00 00
00 00 00 00
00 00 00 00
00 00 50 6C
BD 33 DB EF
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.338Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x11000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
11 00 00 00
00 00 00 00
00 00 00 00
00 00 8E C2
AA FF 41 80
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.338Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x12000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
12 00 00 00
00 00 00 00
00 00 00 00
00 00 26 F3
AA 56 80 3D
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.338Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x13000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
13 00 00 00
00 00 00 00
00 00 00 00
00 00 35 84
2D 3D 5C E5
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.338Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x14000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
14 00 00 00
00 00 00 00
00 00 00 00
00 00 DA 3A
FA 46 DC 54
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.339Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x15000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
15 00 00 00
00 00 00 00
00 00 00 00
00 00 89 D1
41 25 3B 4A
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.339Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x16000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
16 00 00 00
00 00 00 00
00 00 00 00
00 00 A3 01
AD C0 3A 95
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.339Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x17000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
17 00 00 00
00 00 00 00
00 00 00 00
00 00 70 88
E9 B3 66 F6
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.339Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x18000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
18 00 00 00
00 00 00 00
00 00 00 00
00 00 A5 EE
80 46 D8 D5
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.339Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x19000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
19 00 00 00
00 00 00 00
00 00 00 00
00 00 BF 7B
C9 E7 8C 45
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.339Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x1A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
1A 00 00 00
00 00 00 00
00 00 00 00
00 00 5F DB
1D 1F 8E 86
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.339Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x1B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
1B 00 00 00
00 00 00 00
00 00 00 00
00 00 F0 A6
15 CA 69 61
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.339Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x1C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
1C 00 00 00
00 00 00 00
00 00 00 00
00 00 4C 59
35 8D 52 FE
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.339Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x1D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
1D 00 00 00
00 00 00 00
00 00 00 00
00 00 37 EA
DA 84 CC A9
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.339Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x1E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
1E 00 00 00
00 00 00 00
00 00 00 00
00 00 74 5E
CD AE 1D 93
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.339Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x1F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
1F 00 00 00
00 00 00 00
00 00 00 00
00 00 33 4A
06 B1 78 2B
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.339Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x20000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
20 00 00 00
00 00 00 00
00 00 00 00
00 00 F8 C4
3B DE 41 15
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.339Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x21000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
21 00 00 00
00 00 00 00
00 00 00 00
00 00 21 9D
70 54 F5 71
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.340Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x22000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
22 00 00 00
00 00 00 00
00 00 00 00
00 00 7B 9B
CB CC 07 CE
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.340Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x23000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
23 00 00 00
00 00 00 00
00 00 00 00
00 00 88 11
C9 93 A9 39
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.340Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x24000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
24 00 00 00
00 00 00 00
00 00 00 00
00 00 77 B8
75 0C 4E 7E
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.340Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x25000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
25 00 00 00
00 00 00 00
00 00 00 00
00 00 70 63
DE EB 5E 44
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.340Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x26000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
26 00 00 00
00 00 00 00
00 00 00 00
00 00 E4 A1
95 A6 04 2D
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.340Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x27000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
27 00 00 00
00 00 00 00
00 00 00 00
00 00 E6 4C
4F 45 2C E0
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.340Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x28000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
28 00 00 00
00 00 00 00
00 00 00 00
00 00 04 20
C5 AA 6B 19
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.340Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x29000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
29 00 00 00
00 00 00 00
00 00 00 00
00 00 04 72
A6 DD 42 05
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.340Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x2A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
2A 00 00 00
00 00 00 00
00 00 00 00
00 00 CD 7E
20 B4 A0 D2
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.340Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x2B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
2B 00 00 00
00 00 00 00
00 00 00 00
00 00 C5 B2
D8 BE 8D E4
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.340Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x2C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
2C 00 00 00
00 00 00 00
00 00 00 00
00 00 23 4C
0F DC 92 E6
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.340Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x2D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
2D 00 00 00
00 00 00 00
00 00 00 00
00 00 48 20
64 77 A6 AD
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.340Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x2E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
2E 00 00 00
00 00 00 00
00 00 00 00
00 00 10 11
5A C1 EB 1D
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.340Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x2F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
2F 00 00 00
00 00 00 00
00 00 00 00
00 00 82 AD
28 B2 D8 37
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.340Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x30000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
30 00 00 00
00 00 00 00
00 00 00 00
00 00 FF 7F
21 99 F9 0D
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.340Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x31000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
31 00 00 00
00 00 00 00
00 00 00 00
00 00 2F 23
F5 F2 CA DC
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.340Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x32000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
32 00 00 00
00 00 00 00
00 00 00 00
00 00 48 A0
E3 B6 95 74
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.341Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x33000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
33 00 00 00
00 00 00 00
00 00 00 00
00 00 42 A0
A6 61 D0 3F
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.341Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x34000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
34 00 00 00
00 00 00 00
00 00 00 00
00 00 FD 86
CF 11 65 D5
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.341Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x35000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
35 00 00 00
00 00 00 00
00 00 00 00
00 00 B1 7F
42 F2 9E 7B
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.341Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x36000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
36 00 00 00
00 00 00 00
00 00 00 00
00 00 F8 18
9B 6A 04 F6
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.341Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x37000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
37 00 00 00
00 00 00 00
00 00 00 00
00 00 37 88
27 96 8A 1F
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.341Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x38000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
38 00 00 00
00 00 00 00
00 00 00 00
00 00 A1 0E
51 D4 C2 14
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.341Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x39000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
39 00 00 00
00 00 00 00
00 00 00 00
00 00 9C A5
CC AB FD 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.341Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x3A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
3A 00 00 00
00 00 00 00
00 00 00 00
00 00 55 81
7E E6 A3 5A
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.346Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x3B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
3B 00 00 00
00 00 00 00
00 00 00 00
00 00 05 3C
70 DA 99 61
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.346Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x3C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
3C 00 00 00
00 00 00 00
00 00 00 00
00 00 48 B7
BF 28 F8 78
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.346Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x3D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
3D 00 00 00
00 00 00 00
00 00 00 00
00 00 82 FC
5E CC 08 40
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.346Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x3E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
3E 00 00 00
00 00 00 00
00 00 00 00
00 00 45 E2
6F 35 CF 25
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.347Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x3F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
3F 00 00 00
00 00 00 00
00 00 00 00
00 00 6C 10
E2 57 0F D0
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.347Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x40000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
40 00 00 00
00 00 00 00
00 00 00 00
00 00 B5 79
D5 E6 F5 B3
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.347Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x41000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
41 00 00 00
00 00 00 00
00 00 00 00
00 00 F6 D4
F0 7D 97 05
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.347Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x42000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
42 00 00 00
00 00 00 00
00 00 00 00
00 00 10 9E
F8 75 AC AA
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.347Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x43000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
43 00 00 00
00 00 00 00
00 00 00 00
00 00 E4 0C
50 00 BB 03
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.347Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x44000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
44 00 00 00
00 00 00 00
00 00 00 00
00 00 24 1D
BE 67 AB BE
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.347Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x45000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
45 00 00 00
00 00 00 00
00 00 00 00
00 00 BC A4
69 BC CA 64
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.348Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x46000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
46 00 00 00
00 00 00 00
00 00 00 00
00 00 5B AE
ED CB AE C4
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.348Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x47000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
47 00 00 00
00 00 00 00
00 00 00 00
00 00 FD 7A
51 A8 06 18
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.348Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x48000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
48 00 00 00
00 00 00 00
00 00 00 00
00 00 F1 B6
28 28 8E 2F
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.348Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x49000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
49 00 00 00
00 00 00 00
00 00 00 00
00 00 7C DB
99 A1 34 3F
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.348Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x4A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
4A 00 00 00
00 00 00 00
00 00 00 00
00 00 9C 08
C6 CB 7A 06
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.348Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x4B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
4B 00 00 00
00 00 00 00
00 00 00 00
00 00 14 14
EA 43 40 65
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.348Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x155 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 55
00 00 00 00
00 00 00 00
00 00 00 01

[rebalance:debug,2025-05-15T18:47:52.348Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 341, stream opaque = 0x1000000
[ns_server:debug,2025-05-15T18:47:52.348Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x156 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 56
00 00 00 00
00 00 00 00
00 00 00 04

[ns_server:debug,2025-05-15T18:47:52.348Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x4C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
4C 00 00 00
00 00 00 00
00 00 00 00
00 00 15 8D
6B 0F 17 D2
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.348Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 342, stream opaque = 0x4000000
[ns_server:debug,2025-05-15T18:47:52.348Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x4D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
4D 00 00 00
00 00 00 00
00 00 00 00
00 00 AF 66
86 94 4F 2F
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.348Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x157 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 57
00 00 00 00
00 00 00 00
00 00 00 05

[rebalance:debug,2025-05-15T18:47:52.348Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 343, stream opaque = 0x5000000
[ns_server:debug,2025-05-15T18:47:52.348Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x4E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
4E 00 00 00
00 00 00 00
00 00 00 00
00 00 B9 C1
34 D1 80 57
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.348Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x158 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 58
00 00 00 00
00 00 00 00
00 00 00 06

[rebalance:debug,2025-05-15T18:47:52.348Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 344, stream opaque = 0x6000000
[ns_server:debug,2025-05-15T18:47:52.348Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x159 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 59
00 00 00 00
00 00 00 00
00 00 00 07

[ns_server:debug,2025-05-15T18:47:52.348Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x4F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
4F 00 00 00
00 00 00 00
00 00 00 00
00 00 76 A1
CB 27 22 D0
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.348Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x50000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
50 00 00 00
00 00 00 00
00 00 00 00
00 00 3F F2
F8 5A EE A9
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.348Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x51000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
51 00 00 00
00 00 00 00
00 00 00 00
00 00 E4 BA
C9 CB A9 FD
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.349Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 345, stream opaque = 0x7000000
[ns_server:debug,2025-05-15T18:47:52.349Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x52000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
52 00 00 00
00 00 00 00
00 00 00 00
00 00 A6 EB
F7 13 DC 69
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.349Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x15A status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 5A
00 00 00 00
00 00 00 00
00 00 00 08

[ns_server:debug,2025-05-15T18:47:52.349Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x53000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
53 00 00 00
00 00 00 00
00 00 00 00
00 00 74 E6
63 B7 BF 80
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.349Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 346, stream opaque = 0x8000000
[ns_server:debug,2025-05-15T18:47:52.349Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x54000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
54 00 00 00
00 00 00 00
00 00 00 00
00 00 F8 A9
1F C1 F7 15
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.349Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x55000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
55 00 00 00
00 00 00 00
00 00 00 00
00 00 EC 4E
FB B2 B5 3A
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.349Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x56000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
56 00 00 00
00 00 00 00
00 00 00 00
00 00 97 06
79 C8 2D 6A
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.349Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x57000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
57 00 00 00
00 00 00 00
00 00 00 00
00 00 B6 DB
5C 5A 52 DB
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.349Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x58000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
58 00 00 00
00 00 00 00
00 00 00 00
00 00 29 56
9C 80 FC 93
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.349Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x59000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
59 00 00 00
00 00 00 00
00 00 00 00
00 00 17 F2
C0 A2 F2 53
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.349Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x5A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
5A 00 00 00
00 00 00 00
00 00 00 00
00 00 80 55
3D 02 B1 81
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.349Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x5B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
5B 00 00 00
00 00 00 00
00 00 00 00
00 00 C6 DF
68 04 AA 10
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.349Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x5C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
5C 00 00 00
00 00 00 00
00 00 00 00
00 00 CB EA
66 2D 4B 9D
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.349Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x5D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
5D 00 00 00
00 00 00 00
00 00 00 00
00 00 86 B2
16 5D 82 4C
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.349Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x5E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
5E 00 00 00
00 00 00 00
00 00 00 00
00 00 63 F2
94 70 67 48
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.349Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x5F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
5F 00 00 00
00 00 00 00
00 00 00 00
00 00 65 1C
F9 A8 69 1E
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.350Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x60000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
60 00 00 00
00 00 00 00
00 00 00 00
00 00 B7 52
35 85 CF 62
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.350Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x61000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
61 00 00 00
00 00 00 00
00 00 00 00
00 00 83 8B
9B 47 28 B7
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.350Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x62000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
62 00 00 00
00 00 00 00
00 00 00 00
00 00 EA 41
2D CC 33 FD
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.350Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x63000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
63 00 00 00
00 00 00 00
00 00 00 00
00 00 B0 8C
72 AF 74 35
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.350Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x64000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
64 00 00 00
00 00 00 00
00 00 00 00
00 00 CD 72
EB DC D0 BE
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.350Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x65000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
65 00 00 00
00 00 00 00
00 00 00 00
00 00 01 75
41 82 0A 85
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.350Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x66000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
66 00 00 00
00 00 00 00
00 00 00 00
00 00 D5 AE
30 D0 EC 28
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.350Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x67000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
67 00 00 00
00 00 00 00
00 00 00 00
00 00 90 6B
89 AA 23 58
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.350Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x68000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
68 00 00 00
00 00 00 00
00 00 00 00
00 00 D0 E7
18 38 A1 5D
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.350Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x69000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
69 00 00 00
00 00 00 00
00 00 00 00
00 00 00 BE
AE F9 12 CA
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.350Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x6A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
6A 00 00 00
00 00 00 00
00 00 00 00
00 00 7A 43
57 11 7D DF
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.350Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x6B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
6B 00 00 00
00 00 00 00
00 00 00 00
00 00 03 1D
DA D7 26 15
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.350Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x6C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
6C 00 00 00
00 00 00 00
00 00 00 00
00 00 48 9A
22 00 E3 92
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.350Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x6D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
6D 00 00 00
00 00 00 00
00 00 00 00
00 00 4A 30
6B BF 80 2C
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.350Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x6E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
6E 00 00 00
00 00 00 00
00 00 00 00
00 00 06 2E
DE D2 E6 FC
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.351Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x6F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
6F 00 00 00
00 00 00 00
00 00 00 00
00 00 AC 79
98 FB 97 74
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.351Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x70000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
70 00 00 00
00 00 00 00
00 00 00 00
00 00 DB F5
E1 3A 97 02
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.351Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x71000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
71 00 00 00
00 00 00 00
00 00 00 00
00 00 91 09
47 87 15 5F
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.351Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x72000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
72 00 00 00
00 00 00 00
00 00 00 00
00 00 5D 93
99 D0 B2 69
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.351Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x73000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
73 00 00 00
00 00 00 00
00 00 00 00
00 00 66 79
08 91 46 11
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.351Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x74000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
74 00 00 00
00 00 00 00
00 00 00 00
00 00 E1 AB
01 FC F6 03
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.351Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x75000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
75 00 00 00
00 00 00 00
00 00 00 00
00 00 C5 88
17 0F C5 34
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.351Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x76000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
76 00 00 00
00 00 00 00
00 00 00 00
00 00 D4 0B
48 E5 82 A7
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.351Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x77000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
77 00 00 00
00 00 00 00
00 00 00 00
00 00 69 E8
98 4D 2A A8
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.351Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x78000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
78 00 00 00
00 00 00 00
00 00 00 00
00 00 66 30
A7 D4 72 F0
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.351Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x79000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
79 00 00 00
00 00 00 00
00 00 00 00
00 00 57 B6
34 8A E4 52
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.351Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x7A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
7A 00 00 00
00 00 00 00
00 00 00 00
00 00 6C 0C
20 FE A4 7D
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.351Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x7B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
7B 00 00 00
00 00 00 00
00 00 00 00
00 00 FA A1
59 20 18 12
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.349Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x15B status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 5B
00 00 00 00
00 00 00 00
00 00 00 09

[rebalance:debug,2025-05-15T18:47:52.351Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 347, stream opaque = 0x9000000
[ns_server:debug,2025-05-15T18:47:52.352Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x15C status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 5C
00 00 00 00
00 00 00 00
00 00 00 0A

[rebalance:debug,2025-05-15T18:47:52.352Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 348, stream opaque = 0xA000000
[ns_server:debug,2025-05-15T18:47:52.352Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x15D status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 5D
00 00 00 00
00 00 00 00
00 00 00 0B

[rebalance:debug,2025-05-15T18:47:52.352Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 349, stream opaque = 0xB000000
[ns_server:debug,2025-05-15T18:47:52.352Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x15E status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 5E
00 00 00 00
00 00 00 00
00 00 00 0C

[rebalance:debug,2025-05-15T18:47:52.352Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 350, stream opaque = 0xC000000
[ns_server:debug,2025-05-15T18:47:52.352Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x15F status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 5F
00 00 00 00
00 00 00 00
00 00 00 0D

[rebalance:debug,2025-05-15T18:47:52.352Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 351, stream opaque = 0xD000000
[ns_server:debug,2025-05-15T18:47:52.352Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x160 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 60
00 00 00 00
00 00 00 00
00 00 00 0E

[ns_server:debug,2025-05-15T18:47:52.352Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x7C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
7C 00 00 00
00 00 00 00
00 00 00 00
00 00 5D B3
A6 D7 3D A9
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.352Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 352, stream opaque = 0xE000000
[ns_server:debug,2025-05-15T18:47:52.352Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x161 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 61
00 00 00 00
00 00 00 00
00 00 00 0F

[ns_server:debug,2025-05-15T18:47:52.352Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x7D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
7D 00 00 00
00 00 00 00
00 00 00 00
00 00 B2 C9
4A 31 39 F1
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.352Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 353, stream opaque = 0xF000000
[ns_server:debug,2025-05-15T18:47:52.352Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x7E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
7E 00 00 00
00 00 00 00
00 00 00 00
00 00 33 C9
65 5C B6 69
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.352Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x162 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 62
00 00 00 00
00 00 00 00
00 00 00 10

[rebalance:debug,2025-05-15T18:47:52.352Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 354, stream opaque = 0x10000000
[ns_server:debug,2025-05-15T18:47:52.352Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x7F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
7F 00 00 00
00 00 00 00
00 00 00 00
00 00 4B B9
FA D4 83 58
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.352Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x163 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 63
00 00 00 00
00 00 00 00
00 00 00 11

[rebalance:debug,2025-05-15T18:47:52.352Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 355, stream opaque = 0x11000000
[ns_server:debug,2025-05-15T18:47:52.352Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x164 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 64
00 00 00 00
00 00 00 00
00 00 00 12

[ns_server:debug,2025-05-15T18:47:52.352Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x80000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
80 00 00 00
00 00 00 00
00 00 00 00
00 00 A0 0E
51 11 9F E9
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.352Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 356, stream opaque = 0x12000000
[ns_server:debug,2025-05-15T18:47:52.352Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x81000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
81 00 00 00
00 00 00 00
00 00 00 00
00 00 B4 24
AF 95 10 3E
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.352Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x82000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
82 00 00 00
00 00 00 00
00 00 00 00
00 00 7E 82
CF FE 0A 4E
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.353Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x83000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
83 00 00 00
00 00 00 00
00 00 00 00
00 00 02 EE
AC E4 D2 77
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.353Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x84000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
84 00 00 00
00 00 00 00
00 00 00 00
00 00 81 D2
21 86 8C FA
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.353Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x85000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
85 00 00 00
00 00 00 00
00 00 00 00
00 00 2F 46
24 7D 86 6D
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.353Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x86000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
86 00 00 00
00 00 00 00
00 00 00 00
00 00 9B 20
3A 73 0B 0A
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.353Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x87000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
87 00 00 00
00 00 00 00
00 00 00 00
00 00 9F 4D
6C 19 3B C5
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.353Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x88000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
88 00 00 00
00 00 00 00
00 00 00 00
00 00 97 AA
B7 19 AC 97
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.353Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x89000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
89 00 00 00
00 00 00 00
00 00 00 00
00 00 29 1E
F1 B8 B1 50
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.353Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x8A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
8A 00 00 00
00 00 00 00
00 00 00 00
00 00 67 91
C0 13 FD 0B
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.353Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x165 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 65
00 00 00 00
00 00 00 00
00 00 00 13

[rebalance:debug,2025-05-15T18:47:52.353Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 357, stream opaque = 0x13000000
[ns_server:debug,2025-05-15T18:47:52.353Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x166 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 66
00 00 00 00
00 00 00 00
00 00 00 14

[rebalance:debug,2025-05-15T18:47:52.353Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 358, stream opaque = 0x14000000
[ns_server:debug,2025-05-15T18:47:52.353Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x167 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 67
00 00 00 00
00 00 00 00
00 00 00 15

[rebalance:debug,2025-05-15T18:47:52.353Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 359, stream opaque = 0x15000000
[ns_server:debug,2025-05-15T18:47:52.353Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x168 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 68
00 00 00 00
00 00 00 00
00 00 00 16

[rebalance:debug,2025-05-15T18:47:52.353Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 360, stream opaque = 0x16000000
[ns_server:debug,2025-05-15T18:47:52.354Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x169 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 69
00 00 00 00
00 00 00 00
00 00 00 17

[rebalance:debug,2025-05-15T18:47:52.354Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 361, stream opaque = 0x17000000
[ns_server:debug,2025-05-15T18:47:52.354Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x16A status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 6A
00 00 00 00
00 00 00 00
00 00 00 18

[rebalance:debug,2025-05-15T18:47:52.354Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 362, stream opaque = 0x18000000
[ns_server:debug,2025-05-15T18:47:52.354Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x8B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
8B 00 00 00
00 00 00 00
00 00 00 00
00 00 2B 1D
07 AB 55 6B
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.354Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x16B status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 6B
00 00 00 00
00 00 00 00
00 00 00 19

[ns_server:debug,2025-05-15T18:47:52.354Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x8C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
8C 00 00 00
00 00 00 00
00 00 00 00
00 00 57 7E
70 CE 05 07
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.354Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x8D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
8D 00 00 00
00 00 00 00
00 00 00 00
00 00 0D 0D
C8 06 06 5F
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.354Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 363, stream opaque = 0x19000000
[ns_server:debug,2025-05-15T18:47:52.354Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x8E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
8E 00 00 00
00 00 00 00
00 00 00 00
00 00 A7 E0
41 91 E0 0C
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.354Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x16C status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 6C
00 00 00 00
00 00 00 00
00 00 00 1A

[rebalance:debug,2025-05-15T18:47:52.354Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 364, stream opaque = 0x1A000000
[ns_server:debug,2025-05-15T18:47:52.354Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x16D status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 6D
00 00 00 00
00 00 00 00
00 00 00 1B

[rebalance:debug,2025-05-15T18:47:52.354Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 365, stream opaque = 0x1B000000
[ns_server:debug,2025-05-15T18:47:52.354Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x16E status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 6E
00 00 00 00
00 00 00 00
00 00 00 1C

[ns_server:debug,2025-05-15T18:47:52.354Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x8F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
8F 00 00 00
00 00 00 00
00 00 00 00
00 00 4E F0
81 31 62 17
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.354Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 366, stream opaque = 0x1C000000
[ns_server:debug,2025-05-15T18:47:52.354Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x16F status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 6F
00 00 00 00
00 00 00 00
00 00 00 1D

[ns_server:debug,2025-05-15T18:47:52.354Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x90000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
90 00 00 00
00 00 00 00
00 00 00 00
00 00 28 EC
23 27 B9 57
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.354Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 367, stream opaque = 0x1D000000
[ns_server:debug,2025-05-15T18:47:52.354Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x91000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
91 00 00 00
00 00 00 00
00 00 00 00
00 00 BC 17
8A F8 F2 6F
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.354Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x170 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 70
00 00 00 00
00 00 00 00
00 00 00 1E

[ns_server:debug,2025-05-15T18:47:52.355Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x92000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
92 00 00 00
00 00 00 00
00 00 00 00
00 00 47 78
6A 52 32 C1
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.355Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 368, stream opaque = 0x1E000000
[ns_server:debug,2025-05-15T18:47:52.355Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x93000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
93 00 00 00
00 00 00 00
00 00 00 00
00 00 E0 59
20 63 EC D3
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.355Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x94000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
94 00 00 00
00 00 00 00
00 00 00 00
00 00 BF CC
08 91 A0 C0
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.355Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x171 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 71
00 00 00 00
00 00 00 00
00 00 00 1F

[ns_server:debug,2025-05-15T18:47:52.356Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x95000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
95 00 00 00
00 00 00 00
00 00 00 00
00 00 39 5C
AB C3 90 82
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.356Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 369, stream opaque = 0x1F000000
[ns_server:debug,2025-05-15T18:47:52.356Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x96000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
96 00 00 00
00 00 00 00
00 00 00 00
00 00 16 37
D7 4D 67 3E
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.356Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x172 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 72
00 00 00 00
00 00 00 00
00 00 00 20

[ns_server:debug,2025-05-15T18:47:52.356Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x97000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
97 00 00 00
00 00 00 00
00 00 00 00
00 00 E7 63
9D C4 5E E3
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.356Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x98000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
98 00 00 00
00 00 00 00
00 00 00 00
00 00 D9 17
E2 81 30 8C
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.356Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 370, stream opaque = 0x20000000
[ns_server:debug,2025-05-15T18:47:52.356Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x173 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 73
00 00 00 00
00 00 00 00
00 00 00 21

[ns_server:debug,2025-05-15T18:47:52.356Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x99000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
99 00 00 00
00 00 00 00
00 00 00 00
00 00 FB 50
C2 7A 9F 72
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.356Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 371, stream opaque = 0x21000000
[ns_server:debug,2025-05-15T18:47:52.356Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x174 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 74
00 00 00 00
00 00 00 00
00 00 00 22

[ns_server:debug,2025-05-15T18:47:52.356Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x9A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
9A 00 00 00
00 00 00 00
00 00 00 00
00 00 DD D0
E0 89 7C 79
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.356Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 372, stream opaque = 0x22000000
[ns_server:debug,2025-05-15T18:47:52.356Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x9B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
9B 00 00 00
00 00 00 00
00 00 00 00
00 00 A6 8B
93 CC DE A5
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.356Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x175 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 75
00 00 00 00
00 00 00 00
00 00 00 23

[rebalance:debug,2025-05-15T18:47:52.357Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 373, stream opaque = 0x23000000
[ns_server:debug,2025-05-15T18:47:52.357Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x176 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 76
00 00 00 00
00 00 00 00
00 00 00 24

[ns_server:debug,2025-05-15T18:47:52.357Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x9C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
9C 00 00 00
00 00 00 00
00 00 00 00
00 00 AB 04
8A 56 91 76
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.357Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 374, stream opaque = 0x24000000
[ns_server:debug,2025-05-15T18:47:52.357Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x177 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 77
00 00 00 00
00 00 00 00
00 00 00 25

[ns_server:debug,2025-05-15T18:47:52.357Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x9D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
9D 00 00 00
00 00 00 00
00 00 00 00
00 00 E0 33
B8 AA F7 E7
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.357Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 375, stream opaque = 0x25000000
[ns_server:debug,2025-05-15T18:47:52.357Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x9E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
9E 00 00 00
00 00 00 00
00 00 00 00
00 00 7F FF
29 09 2C 03
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.357Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x178 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 78
00 00 00 00
00 00 00 00
00 00 00 26

[rebalance:debug,2025-05-15T18:47:52.357Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 376, stream opaque = 0x26000000
[ns_server:debug,2025-05-15T18:47:52.357Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x179 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 79
00 00 00 00
00 00 00 00
00 00 00 27

[rebalance:debug,2025-05-15T18:47:52.357Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 377, stream opaque = 0x27000000
[ns_server:debug,2025-05-15T18:47:52.357Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x17A status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 7A
00 00 00 00
00 00 00 00
00 00 00 28

[rebalance:debug,2025-05-15T18:47:52.357Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 378, stream opaque = 0x28000000
[ns_server:debug,2025-05-15T18:47:52.357Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x17B status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 7B
00 00 00 00
00 00 00 00
00 00 00 29

[ns_server:debug,2025-05-15T18:47:52.357Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x9F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
9F 00 00 00
00 00 00 00
00 00 00 00
00 00 C1 67
41 6C 76 1C
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.357Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 379, stream opaque = 0x29000000
[ns_server:debug,2025-05-15T18:47:52.357Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA0000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A0 00 00 00
00 00 00 00
00 00 00 00
00 00 B1 D3
97 72 5B 0C
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.357Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x17C status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 7C
00 00 00 00
00 00 00 00
00 00 00 2A

[rebalance:debug,2025-05-15T18:47:52.357Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 380, stream opaque = 0x2A000000
[ns_server:debug,2025-05-15T18:47:52.357Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA1000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A1 00 00 00
00 00 00 00
00 00 00 00
00 00 33 76
13 51 68 7E
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.357Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x17D status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 7D
00 00 00 00
00 00 00 00
00 00 00 2B

[rebalance:debug,2025-05-15T18:47:52.357Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 381, stream opaque = 0x2B000000
[ns_server:debug,2025-05-15T18:47:52.358Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x17E status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 7E
00 00 00 00
00 00 00 00
00 00 00 2C

[ns_server:debug,2025-05-15T18:47:52.358Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA2000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A2 00 00 00
00 00 00 00
00 00 00 00
00 00 4D A8
B8 6A 32 0C
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.358Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 382, stream opaque = 0x2C000000
[ns_server:debug,2025-05-15T18:47:52.358Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x17F status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 7F
00 00 00 00
00 00 00 00
00 00 00 2D

[ns_server:debug,2025-05-15T18:47:52.358Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA3000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A3 00 00 00
00 00 00 00
00 00 00 00
00 00 4F 97
D5 2E 7C EF
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.358Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 383, stream opaque = 0x2D000000
[ns_server:debug,2025-05-15T18:47:52.358Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x180 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 80
00 00 00 00
00 00 00 00
00 00 00 2E

[ns_server:debug,2025-05-15T18:47:52.358Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA4000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A4 00 00 00
00 00 00 00
00 00 00 00
00 00 04 BD
B9 F0 7A 79
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.358Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 384, stream opaque = 0x2E000000
[ns_server:debug,2025-05-15T18:47:52.358Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x181 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 81
00 00 00 00
00 00 00 00
00 00 00 2F

[ns_server:debug,2025-05-15T18:47:52.358Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA5000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A5 00 00 00
00 00 00 00
00 00 00 00
00 00 24 45
2F 93 1E DA
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.358Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 385, stream opaque = 0x2F000000
[ns_server:debug,2025-05-15T18:47:52.358Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x182 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 82
00 00 00 00
00 00 00 00
00 00 00 30

[rebalance:debug,2025-05-15T18:47:52.358Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 386, stream opaque = 0x30000000
[ns_server:debug,2025-05-15T18:47:52.358Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x183 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 83
00 00 00 00
00 00 00 00
00 00 00 31

[rebalance:debug,2025-05-15T18:47:52.358Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 387, stream opaque = 0x31000000
[ns_server:debug,2025-05-15T18:47:52.358Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA6000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A6 00 00 00
00 00 00 00
00 00 00 00
00 00 DA 28
F1 2B 59 0E
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.358Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x184 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 84
00 00 00 00
00 00 00 00
00 00 00 32

[ns_server:debug,2025-05-15T18:47:52.358Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA7000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A7 00 00 00
00 00 00 00
00 00 00 00
00 00 B8 30
10 2C D2 D6
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.358Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA8000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A8 00 00 00
00 00 00 00
00 00 00 00
00 00 D9 11
F1 0E 11 07
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.358Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA9000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A9 00 00 00
00 00 00 00
00 00 00 00
00 00 15 AF
7F 69 67 79
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.359Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xAA000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
AA 00 00 00
00 00 00 00
00 00 00 00
00 00 4A A1
85 EE 76 2F
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.359Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xAB000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
AB 00 00 00
00 00 00 00
00 00 00 00
00 00 87 40
D9 5D 41 C3
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.359Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xAC000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
AC 00 00 00
00 00 00 00
00 00 00 00
00 00 94 28
3D BE 24 73
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.359Z,ns_1@172.19.0.4:<0.4467.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xAD000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
AD 00 00 00
00 00 00 00
00 00 00 00
00 00 E0 BB
80 29 1C 8A
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.373Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 388, stream opaque = 0x32000000
[ns_server:debug,2025-05-15T18:47:52.374Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x185 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 85
00 00 00 00
00 00 00 00
00 00 00 33

[rebalance:debug,2025-05-15T18:47:52.374Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 389, stream opaque = 0x33000000
[ns_server:debug,2025-05-15T18:47:52.375Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x186 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 86
00 00 00 00
00 00 00 00
00 00 00 34

[rebalance:debug,2025-05-15T18:47:52.375Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 390, stream opaque = 0x34000000
[ns_server:debug,2025-05-15T18:47:52.375Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x187 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 87
00 00 00 00
00 00 00 00
00 00 00 35

[rebalance:debug,2025-05-15T18:47:52.375Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 391, stream opaque = 0x35000000
[ns_server:debug,2025-05-15T18:47:52.375Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x188 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 88
00 00 00 00
00 00 00 00
00 00 00 36

[rebalance:debug,2025-05-15T18:47:52.375Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 392, stream opaque = 0x36000000
[ns_server:debug,2025-05-15T18:47:52.375Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x189 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 89
00 00 00 00
00 00 00 00
00 00 00 37

[rebalance:debug,2025-05-15T18:47:52.375Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 393, stream opaque = 0x37000000
[ns_server:debug,2025-05-15T18:47:52.375Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x18A status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 8A
00 00 00 00
00 00 00 00
00 00 00 38

[rebalance:debug,2025-05-15T18:47:52.375Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 394, stream opaque = 0x38000000
[ns_server:debug,2025-05-15T18:47:52.375Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x18B status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 8B
00 00 00 00
00 00 00 00
00 00 00 39

[rebalance:debug,2025-05-15T18:47:52.375Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 395, stream opaque = 0x39000000
[ns_server:debug,2025-05-15T18:47:52.375Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x18C status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 8C
00 00 00 00
00 00 00 00
00 00 00 3A

[rebalance:debug,2025-05-15T18:47:52.375Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 396, stream opaque = 0x3A000000
[ns_server:debug,2025-05-15T18:47:52.377Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x18D status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 8D
00 00 00 00
00 00 00 00
00 00 00 3B

[rebalance:debug,2025-05-15T18:47:52.377Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 397, stream opaque = 0x3B000000
[ns_server:debug,2025-05-15T18:47:52.377Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x18E status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 8E
00 00 00 00
00 00 00 00
00 00 00 3C

[rebalance:debug,2025-05-15T18:47:52.377Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 398, stream opaque = 0x3C000000
[ns_server:debug,2025-05-15T18:47:52.377Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x18F status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 8F
00 00 00 00
00 00 00 00
00 00 00 3D

[rebalance:debug,2025-05-15T18:47:52.378Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 399, stream opaque = 0x3D000000
[ns_server:debug,2025-05-15T18:47:52.378Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x190 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 90
00 00 00 00
00 00 00 00
00 00 00 3E

[rebalance:debug,2025-05-15T18:47:52.378Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 400, stream opaque = 0x3E000000
[ns_server:debug,2025-05-15T18:47:52.378Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x191 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 91
00 00 00 00
00 00 00 00
00 00 00 3F

[rebalance:debug,2025-05-15T18:47:52.378Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 401, stream opaque = 0x3F000000
[ns_server:debug,2025-05-15T18:47:52.378Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x192 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 92
00 00 00 00
00 00 00 00
00 00 00 40

[rebalance:debug,2025-05-15T18:47:52.378Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 402, stream opaque = 0x40000000
[ns_server:debug,2025-05-15T18:47:52.378Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x193 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 93
00 00 00 00
00 00 00 00
00 00 00 41

[rebalance:debug,2025-05-15T18:47:52.378Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 403, stream opaque = 0x41000000
[ns_server:debug,2025-05-15T18:47:52.378Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x194 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 94
00 00 00 00
00 00 00 00
00 00 00 42

[rebalance:debug,2025-05-15T18:47:52.378Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 404, stream opaque = 0x42000000
[ns_server:debug,2025-05-15T18:47:52.378Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x195 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 95
00 00 00 00
00 00 00 00
00 00 00 43

[rebalance:debug,2025-05-15T18:47:52.378Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 405, stream opaque = 0x43000000
[ns_server:debug,2025-05-15T18:47:52.378Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x196 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 96
00 00 00 00
00 00 00 00
00 00 00 44

[rebalance:debug,2025-05-15T18:47:52.379Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 406, stream opaque = 0x44000000
[ns_server:debug,2025-05-15T18:47:52.379Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x197 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 97
00 00 00 00
00 00 00 00
00 00 00 45

[rebalance:debug,2025-05-15T18:47:52.379Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 407, stream opaque = 0x45000000
[ns_server:debug,2025-05-15T18:47:52.379Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x198 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 98
00 00 00 00
00 00 00 00
00 00 00 46

[rebalance:debug,2025-05-15T18:47:52.379Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 408, stream opaque = 0x46000000
[ns_server:debug,2025-05-15T18:47:52.379Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x199 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 99
00 00 00 00
00 00 00 00
00 00 00 47

[rebalance:debug,2025-05-15T18:47:52.379Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 409, stream opaque = 0x47000000
[ns_server:debug,2025-05-15T18:47:52.379Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x19A status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 9A
00 00 00 00
00 00 00 00
00 00 00 48

[rebalance:debug,2025-05-15T18:47:52.379Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 410, stream opaque = 0x48000000
[ns_server:debug,2025-05-15T18:47:52.379Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x19B status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 9B
00 00 00 00
00 00 00 00
00 00 00 49

[rebalance:debug,2025-05-15T18:47:52.379Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 411, stream opaque = 0x49000000
[ns_server:debug,2025-05-15T18:47:52.379Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x19C status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 9C
00 00 00 00
00 00 00 00
00 00 00 4A

[rebalance:debug,2025-05-15T18:47:52.379Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 412, stream opaque = 0x4A000000
[ns_server:debug,2025-05-15T18:47:52.379Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x19D status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 9D
00 00 00 00
00 00 00 00
00 00 00 4B

[rebalance:debug,2025-05-15T18:47:52.379Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 413, stream opaque = 0x4B000000
[ns_server:debug,2025-05-15T18:47:52.379Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x19E status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 9E
00 00 00 00
00 00 00 00
00 00 00 4C

[rebalance:debug,2025-05-15T18:47:52.379Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 414, stream opaque = 0x4C000000
[ns_server:debug,2025-05-15T18:47:52.379Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x19F status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 9F
00 00 00 00
00 00 00 00
00 00 00 4D

[rebalance:debug,2025-05-15T18:47:52.379Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 415, stream opaque = 0x4D000000
[ns_server:debug,2025-05-15T18:47:52.379Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1A0 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 A0
00 00 00 00
00 00 00 00
00 00 00 4E

[rebalance:debug,2025-05-15T18:47:52.379Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 416, stream opaque = 0x4E000000
[ns_server:debug,2025-05-15T18:47:52.379Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1A1 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 A1
00 00 00 00
00 00 00 00
00 00 00 4F

[rebalance:debug,2025-05-15T18:47:52.379Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 417, stream opaque = 0x4F000000
[ns_server:debug,2025-05-15T18:47:52.379Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1A2 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 A2
00 00 00 00
00 00 00 00
00 00 00 50

[rebalance:debug,2025-05-15T18:47:52.379Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 418, stream opaque = 0x50000000
[ns_server:debug,2025-05-15T18:47:52.379Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1A3 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 A3
00 00 00 00
00 00 00 00
00 00 00 51

[rebalance:debug,2025-05-15T18:47:52.379Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 419, stream opaque = 0x51000000
[ns_server:debug,2025-05-15T18:47:52.379Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1A4 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 A4
00 00 00 00
00 00 00 00
00 00 00 52

[rebalance:debug,2025-05-15T18:47:52.379Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 420, stream opaque = 0x52000000
[ns_server:debug,2025-05-15T18:47:52.380Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1A5 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 A5
00 00 00 00
00 00 00 00
00 00 00 53

[rebalance:debug,2025-05-15T18:47:52.380Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 421, stream opaque = 0x53000000
[ns_server:debug,2025-05-15T18:47:52.380Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1A6 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 A6
00 00 00 00
00 00 00 00
00 00 00 54

[rebalance:debug,2025-05-15T18:47:52.380Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 422, stream opaque = 0x54000000
[ns_server:debug,2025-05-15T18:47:52.380Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1A7 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 A7
00 00 00 00
00 00 00 00
00 00 00 55

[rebalance:debug,2025-05-15T18:47:52.380Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 423, stream opaque = 0x55000000
[ns_server:debug,2025-05-15T18:47:52.380Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1A8 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 A8
00 00 00 00
00 00 00 00
00 00 00 56

[rebalance:debug,2025-05-15T18:47:52.380Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 424, stream opaque = 0x56000000
[ns_server:debug,2025-05-15T18:47:52.380Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1A9 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 A9
00 00 00 00
00 00 00 00
00 00 00 57

[rebalance:debug,2025-05-15T18:47:52.380Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 425, stream opaque = 0x57000000
[ns_server:debug,2025-05-15T18:47:52.380Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1AA status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 AA
00 00 00 00
00 00 00 00
00 00 00 58

[rebalance:debug,2025-05-15T18:47:52.380Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 426, stream opaque = 0x58000000
[ns_server:debug,2025-05-15T18:47:52.380Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1AB status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 AB
00 00 00 00
00 00 00 00
00 00 00 59

[rebalance:debug,2025-05-15T18:47:52.380Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 427, stream opaque = 0x59000000
[ns_server:debug,2025-05-15T18:47:52.380Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1AC status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 AC
00 00 00 00
00 00 00 00
00 00 00 5A

[rebalance:debug,2025-05-15T18:47:52.380Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 428, stream opaque = 0x5A000000
[ns_server:debug,2025-05-15T18:47:52.380Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1AD status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 AD
00 00 00 00
00 00 00 00
00 00 00 5B

[rebalance:debug,2025-05-15T18:47:52.380Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 429, stream opaque = 0x5B000000
[ns_server:debug,2025-05-15T18:47:52.380Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1AE status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 AE
00 00 00 00
00 00 00 00
00 00 00 5C

[rebalance:debug,2025-05-15T18:47:52.380Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 430, stream opaque = 0x5C000000
[ns_server:debug,2025-05-15T18:47:52.380Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1AF status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 AF
00 00 00 00
00 00 00 00
00 00 00 5D

[rebalance:debug,2025-05-15T18:47:52.380Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 431, stream opaque = 0x5D000000
[ns_server:debug,2025-05-15T18:47:52.380Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1B0 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 B0
00 00 00 00
00 00 00 00
00 00 00 5E

[rebalance:debug,2025-05-15T18:47:52.380Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 432, stream opaque = 0x5E000000
[ns_server:debug,2025-05-15T18:47:52.380Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1B1 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 B1
00 00 00 00
00 00 00 00
00 00 00 5F

[rebalance:debug,2025-05-15T18:47:52.380Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 433, stream opaque = 0x5F000000
[ns_server:debug,2025-05-15T18:47:52.380Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1B2 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 B2
00 00 00 00
00 00 00 00
00 00 00 60

[rebalance:debug,2025-05-15T18:47:52.380Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 434, stream opaque = 0x60000000
[ns_server:debug,2025-05-15T18:47:52.380Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1B3 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 B3
00 00 00 00
00 00 00 00
00 00 00 61

[rebalance:debug,2025-05-15T18:47:52.380Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 435, stream opaque = 0x61000000
[ns_server:debug,2025-05-15T18:47:52.381Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1B4 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 B4
00 00 00 00
00 00 00 00
00 00 00 62

[rebalance:debug,2025-05-15T18:47:52.381Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 436, stream opaque = 0x62000000
[ns_server:debug,2025-05-15T18:47:52.381Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1B5 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 B5
00 00 00 00
00 00 00 00
00 00 00 63

[rebalance:debug,2025-05-15T18:47:52.381Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 437, stream opaque = 0x63000000
[ns_server:debug,2025-05-15T18:47:52.381Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1B6 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 B6
00 00 00 00
00 00 00 00
00 00 00 64

[rebalance:debug,2025-05-15T18:47:52.381Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 438, stream opaque = 0x64000000
[ns_server:debug,2025-05-15T18:47:52.381Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1B7 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 B7
00 00 00 00
00 00 00 00
00 00 00 65

[rebalance:debug,2025-05-15T18:47:52.381Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 439, stream opaque = 0x65000000
[ns_server:debug,2025-05-15T18:47:52.382Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1B8 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 B8
00 00 00 00
00 00 00 00
00 00 00 66

[rebalance:debug,2025-05-15T18:47:52.382Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 440, stream opaque = 0x66000000
[ns_server:debug,2025-05-15T18:47:52.382Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1B9 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 B9
00 00 00 00
00 00 00 00
00 00 00 67

[rebalance:debug,2025-05-15T18:47:52.382Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 441, stream opaque = 0x67000000
[ns_server:debug,2025-05-15T18:47:52.382Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1BA status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 BA
00 00 00 00
00 00 00 00
00 00 00 68

[rebalance:debug,2025-05-15T18:47:52.382Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 442, stream opaque = 0x68000000
[ns_server:debug,2025-05-15T18:47:52.382Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1BB status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 BB
00 00 00 00
00 00 00 00
00 00 00 69

[rebalance:debug,2025-05-15T18:47:52.382Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 443, stream opaque = 0x69000000
[ns_server:debug,2025-05-15T18:47:52.382Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1BC status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 BC
00 00 00 00
00 00 00 00
00 00 00 6A

[rebalance:debug,2025-05-15T18:47:52.382Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 444, stream opaque = 0x6A000000
[ns_server:debug,2025-05-15T18:47:52.382Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1BD status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 BD
00 00 00 00
00 00 00 00
00 00 00 6B

[rebalance:debug,2025-05-15T18:47:52.382Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 445, stream opaque = 0x6B000000
[ns_server:debug,2025-05-15T18:47:52.382Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1BE status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 BE
00 00 00 00
00 00 00 00
00 00 00 6C

[rebalance:debug,2025-05-15T18:47:52.382Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 446, stream opaque = 0x6C000000
[ns_server:debug,2025-05-15T18:47:52.382Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1BF status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 BF
00 00 00 00
00 00 00 00
00 00 00 6D

[rebalance:debug,2025-05-15T18:47:52.382Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 447, stream opaque = 0x6D000000
[ns_server:debug,2025-05-15T18:47:52.382Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1C0 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 C0
00 00 00 00
00 00 00 00
00 00 00 6E

[rebalance:debug,2025-05-15T18:47:52.382Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 448, stream opaque = 0x6E000000
[ns_server:debug,2025-05-15T18:47:52.383Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1C1 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 C1
00 00 00 00
00 00 00 00
00 00 00 6F

[rebalance:debug,2025-05-15T18:47:52.383Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 449, stream opaque = 0x6F000000
[ns_server:debug,2025-05-15T18:47:52.383Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1C2 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 C2
00 00 00 00
00 00 00 00
00 00 00 70

[rebalance:debug,2025-05-15T18:47:52.383Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 450, stream opaque = 0x70000000
[ns_server:debug,2025-05-15T18:47:52.383Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1C3 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 C3
00 00 00 00
00 00 00 00
00 00 00 71

[rebalance:debug,2025-05-15T18:47:52.383Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 451, stream opaque = 0x71000000
[ns_server:debug,2025-05-15T18:47:52.383Z,ns_1@172.19.0.4:dcp_traffic_monitor<0.2118.0>:dcp_traffic_monitor:update_bucket:148]Saw that bucket "doom-scrolling" became alive on node 'ns_1@db2.lan'
[ns_server:debug,2025-05-15T18:47:52.383Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1C4 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 C4
00 00 00 00
00 00 00 00
00 00 00 72

[rebalance:debug,2025-05-15T18:47:52.383Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 452, stream opaque = 0x72000000
[ns_server:debug,2025-05-15T18:47:52.383Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1C5 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 C5
00 00 00 00
00 00 00 00
00 00 00 73

[rebalance:debug,2025-05-15T18:47:52.383Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 453, stream opaque = 0x73000000
[ns_server:debug,2025-05-15T18:47:52.383Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1C6 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 C6
00 00 00 00
00 00 00 00
00 00 00 74

[rebalance:debug,2025-05-15T18:47:52.383Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 454, stream opaque = 0x74000000
[ns_server:debug,2025-05-15T18:47:52.383Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1C7 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 C7
00 00 00 00
00 00 00 00
00 00 00 75

[rebalance:debug,2025-05-15T18:47:52.383Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 455, stream opaque = 0x75000000
[ns_server:debug,2025-05-15T18:47:52.383Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1C8 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 C8
00 00 00 00
00 00 00 00
00 00 00 76

[rebalance:debug,2025-05-15T18:47:52.384Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 456, stream opaque = 0x76000000
[ns_server:debug,2025-05-15T18:47:52.384Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1C9 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 C9
00 00 00 00
00 00 00 00
00 00 00 77

[rebalance:debug,2025-05-15T18:47:52.384Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 457, stream opaque = 0x77000000
[ns_server:debug,2025-05-15T18:47:52.384Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1CA status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 CA
00 00 00 00
00 00 00 00
00 00 00 78

[rebalance:debug,2025-05-15T18:47:52.384Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 458, stream opaque = 0x78000000
[ns_server:debug,2025-05-15T18:47:52.384Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1CB status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 CB
00 00 00 00
00 00 00 00
00 00 00 79

[rebalance:debug,2025-05-15T18:47:52.384Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 459, stream opaque = 0x79000000
[ns_server:debug,2025-05-15T18:47:52.384Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1CC status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 CC
00 00 00 00
00 00 00 00
00 00 00 7A

[rebalance:debug,2025-05-15T18:47:52.384Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 460, stream opaque = 0x7A000000
[ns_server:debug,2025-05-15T18:47:52.384Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1CD status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 CD
00 00 00 00
00 00 00 00
00 00 00 7B

[rebalance:debug,2025-05-15T18:47:52.385Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 461, stream opaque = 0x7B000000
[ns_server:debug,2025-05-15T18:47:52.385Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1CE status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 CE
00 00 00 00
00 00 00 00
00 00 00 7C

[rebalance:debug,2025-05-15T18:47:52.385Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 462, stream opaque = 0x7C000000
[ns_server:debug,2025-05-15T18:47:52.385Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1CF status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 CF
00 00 00 00
00 00 00 00
00 00 00 7D

[rebalance:debug,2025-05-15T18:47:52.385Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 463, stream opaque = 0x7D000000
[ns_server:debug,2025-05-15T18:47:52.385Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1D0 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 D0
00 00 00 00
00 00 00 00
00 00 00 7E

[rebalance:debug,2025-05-15T18:47:52.385Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 464, stream opaque = 0x7E000000
[ns_server:debug,2025-05-15T18:47:52.385Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1D1 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 D1
00 00 00 00
00 00 00 00
00 00 00 7F

[rebalance:debug,2025-05-15T18:47:52.385Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 465, stream opaque = 0x7F000000
[ns_server:debug,2025-05-15T18:47:52.386Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1D2 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 D2
00 00 00 00
00 00 00 00
00 00 00 80

[rebalance:debug,2025-05-15T18:47:52.386Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 466, stream opaque = 0x80000000
[ns_server:debug,2025-05-15T18:47:52.386Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1D3 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 D3
00 00 00 00
00 00 00 00
00 00 00 81

[rebalance:debug,2025-05-15T18:47:52.386Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 467, stream opaque = 0x81000000
[ns_server:debug,2025-05-15T18:47:52.386Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1D4 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 D4
00 00 00 00
00 00 00 00
00 00 00 82

[rebalance:debug,2025-05-15T18:47:52.386Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 468, stream opaque = 0x82000000
[ns_server:debug,2025-05-15T18:47:52.386Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1D5 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 D5
00 00 00 00
00 00 00 00
00 00 00 83

[rebalance:debug,2025-05-15T18:47:52.386Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 469, stream opaque = 0x83000000
[ns_server:debug,2025-05-15T18:47:52.386Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1D6 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 D6
00 00 00 00
00 00 00 00
00 00 00 84

[rebalance:debug,2025-05-15T18:47:52.386Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 470, stream opaque = 0x84000000
[ns_server:debug,2025-05-15T18:47:52.386Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1D7 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 D7
00 00 00 00
00 00 00 00
00 00 00 85

[rebalance:debug,2025-05-15T18:47:52.386Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 471, stream opaque = 0x85000000
[ns_server:debug,2025-05-15T18:47:52.386Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1D8 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 D8
00 00 00 00
00 00 00 00
00 00 00 86

[rebalance:debug,2025-05-15T18:47:52.386Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 472, stream opaque = 0x86000000
[ns_server:debug,2025-05-15T18:47:52.387Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1D9 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 D9
00 00 00 00
00 00 00 00
00 00 00 87

[rebalance:debug,2025-05-15T18:47:52.387Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 473, stream opaque = 0x87000000
[ns_server:debug,2025-05-15T18:47:52.387Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1DA status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 DA
00 00 00 00
00 00 00 00
00 00 00 88

[rebalance:debug,2025-05-15T18:47:52.387Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 474, stream opaque = 0x88000000
[ns_server:debug,2025-05-15T18:47:52.387Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1DB status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 DB
00 00 00 00
00 00 00 00
00 00 00 89

[rebalance:debug,2025-05-15T18:47:52.387Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 475, stream opaque = 0x89000000
[ns_server:debug,2025-05-15T18:47:52.387Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1DC status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 DC
00 00 00 00
00 00 00 00
00 00 00 8A

[rebalance:debug,2025-05-15T18:47:52.387Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 476, stream opaque = 0x8A000000
[ns_server:debug,2025-05-15T18:47:52.387Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1DD status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 DD
00 00 00 00
00 00 00 00
00 00 00 8B

[rebalance:debug,2025-05-15T18:47:52.387Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 477, stream opaque = 0x8B000000
[ns_server:debug,2025-05-15T18:47:52.387Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1DE status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 DE
00 00 00 00
00 00 00 00
00 00 00 8C

[rebalance:debug,2025-05-15T18:47:52.387Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 478, stream opaque = 0x8C000000
[ns_server:debug,2025-05-15T18:47:52.387Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1DF status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 DF
00 00 00 00
00 00 00 00
00 00 00 8D

[rebalance:debug,2025-05-15T18:47:52.387Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 479, stream opaque = 0x8D000000
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1E0 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 E0
00 00 00 00
00 00 00 00
00 00 00 8E

[rebalance:debug,2025-05-15T18:47:52.388Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 480, stream opaque = 0x8E000000
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1E1 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 E1
00 00 00 00
00 00 00 00
00 00 00 8F

[rebalance:debug,2025-05-15T18:47:52.388Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 481, stream opaque = 0x8F000000
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1E2 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 E2
00 00 00 00
00 00 00 00
00 00 00 90

[rebalance:debug,2025-05-15T18:47:52.388Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 482, stream opaque = 0x90000000
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1E3 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 E3
00 00 00 00
00 00 00 00
00 00 00 91

[rebalance:debug,2025-05-15T18:47:52.388Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 483, stream opaque = 0x91000000
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1E4 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 E4
00 00 00 00
00 00 00 00
00 00 00 92

[rebalance:debug,2025-05-15T18:47:52.388Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 484, stream opaque = 0x92000000
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1E5 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 E5
00 00 00 00
00 00 00 00
00 00 00 93

[rebalance:debug,2025-05-15T18:47:52.388Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 485, stream opaque = 0x93000000
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1E6 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 E6
00 00 00 00
00 00 00 00
00 00 00 94

[rebalance:debug,2025-05-15T18:47:52.388Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 486, stream opaque = 0x94000000
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1E7 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 E7
00 00 00 00
00 00 00 00
00 00 00 95

[rebalance:debug,2025-05-15T18:47:52.388Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 487, stream opaque = 0x95000000
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1E8 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 E8
00 00 00 00
00 00 00 00
00 00 00 96

[rebalance:debug,2025-05-15T18:47:52.388Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 488, stream opaque = 0x96000000
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1E9 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 E9
00 00 00 00
00 00 00 00
00 00 00 97

[rebalance:debug,2025-05-15T18:47:52.388Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 489, stream opaque = 0x97000000
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1EA status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 EA
00 00 00 00
00 00 00 00
00 00 00 98

[rebalance:debug,2025-05-15T18:47:52.388Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 490, stream opaque = 0x98000000
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1EB status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 EB
00 00 00 00
00 00 00 00
00 00 00 99

[rebalance:debug,2025-05-15T18:47:52.388Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 491, stream opaque = 0x99000000
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1EC status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 EC
00 00 00 00
00 00 00 00
00 00 00 9A

[rebalance:debug,2025-05-15T18:47:52.388Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 492, stream opaque = 0x9A000000
[ns_server:debug,2025-05-15T18:47:52.388Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1ED status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 ED
00 00 00 00
00 00 00 00
00 00 00 9B

[rebalance:debug,2025-05-15T18:47:52.389Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 493, stream opaque = 0x9B000000
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1EE status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 EE
00 00 00 00
00 00 00 00
00 00 00 9C

[rebalance:debug,2025-05-15T18:47:52.389Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 494, stream opaque = 0x9C000000
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1EF status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 EF
00 00 00 00
00 00 00 00
00 00 00 9D

[rebalance:debug,2025-05-15T18:47:52.389Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 495, stream opaque = 0x9D000000
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1F0 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 F0
00 00 00 00
00 00 00 00
00 00 00 9E

[rebalance:debug,2025-05-15T18:47:52.389Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 496, stream opaque = 0x9E000000
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1F1 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 F1
00 00 00 00
00 00 00 00
00 00 00 9F

[rebalance:debug,2025-05-15T18:47:52.389Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 497, stream opaque = 0x9F000000
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1F2 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 F2
00 00 00 00
00 00 00 00
00 00 00 A0

[rebalance:debug,2025-05-15T18:47:52.389Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 498, stream opaque = 0xA0000000
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1F3 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 F3
00 00 00 00
00 00 00 00
00 00 00 A1

[rebalance:debug,2025-05-15T18:47:52.389Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 499, stream opaque = 0xA1000000
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1F4 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 F4
00 00 00 00
00 00 00 00
00 00 00 A2

[rebalance:debug,2025-05-15T18:47:52.389Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 500, stream opaque = 0xA2000000
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1F5 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 F5
00 00 00 00
00 00 00 00
00 00 00 A3

[rebalance:debug,2025-05-15T18:47:52.389Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 501, stream opaque = 0xA3000000
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1F6 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 F6
00 00 00 00
00 00 00 00
00 00 00 A4

[rebalance:debug,2025-05-15T18:47:52.389Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 502, stream opaque = 0xA4000000
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1F7 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 F7
00 00 00 00
00 00 00 00
00 00 00 A5

[rebalance:debug,2025-05-15T18:47:52.389Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 503, stream opaque = 0xA5000000
[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1F8 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 F8
00 00 00 00
00 00 00 00
00 00 00 A6

[ns_server:debug,2025-05-15T18:47:52.389Z,ns_1@172.19.0.4:dcp_traffic_monitor<0.2118.0>:dcp_traffic_monitor:update_bucket:148]Saw that bucket "doom-scrolling" became alive on node 'ns_1@172.19.0.4'
[rebalance:debug,2025-05-15T18:47:52.389Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 504, stream opaque = 0xA6000000
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1F9 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 F9
00 00 00 00
00 00 00 00
00 00 00 A7

[rebalance:debug,2025-05-15T18:47:52.390Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 505, stream opaque = 0xA7000000
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1FA status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 FA
00 00 00 00
00 00 00 00
00 00 00 A8

[rebalance:debug,2025-05-15T18:47:52.390Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 506, stream opaque = 0xA8000000
[ns_server:debug,2025-05-15T18:47:52.390Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1FB status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 FB
00 00 00 00
00 00 00 00
00 00 00 A9

[rebalance:debug,2025-05-15T18:47:52.390Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 507, stream opaque = 0xA9000000
[ns_server:debug,2025-05-15T18:47:52.391Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1FC status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 FC
00 00 00 00
00 00 00 00
00 00 00 AA

[rebalance:debug,2025-05-15T18:47:52.391Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 508, stream opaque = 0xAA000000
[ns_server:debug,2025-05-15T18:47:52.391Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1FD status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 FD
00 00 00 00
00 00 00 00
00 00 00 AB

[rebalance:debug,2025-05-15T18:47:52.391Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 509, stream opaque = 0xAB000000
[ns_server:debug,2025-05-15T18:47:52.391Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1FE status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 FE
00 00 00 00
00 00 00 00
00 00 00 AC

[rebalance:debug,2025-05-15T18:47:52.391Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 510, stream opaque = 0xAC000000
[ns_server:debug,2025-05-15T18:47:52.391Z,ns_1@172.19.0.4:<0.4434.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x1FF status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 01 FF
00 00 00 00
00 00 00 00
00 00 00 AD

[rebalance:debug,2025-05-15T18:47:52.391Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 511, stream opaque = 0xAD000000
[ns_server:debug,2025-05-15T18:47:52.391Z,ns_1@172.19.0.4:<0.4434.0>:dcp_consumer_conn:maybe_reply_setup_streams:499]Setup stream request completed with ok. Moving to idle state
[ns_server:debug,2025-05-15T18:47:52.391Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 682, opaque = 0x2AA, type = add
[ns_server:debug,2025-05-15T18:47:52.391Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 683, opaque = 0x2AB, type = add
[ns_server:debug,2025-05-15T18:47:52.391Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 684, opaque = 0x2AC, type = add
[ns_server:debug,2025-05-15T18:47:52.391Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 685, opaque = 0x2AD, type = add
[ns_server:debug,2025-05-15T18:47:52.391Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 686, opaque = 0x2AE, type = add
[ns_server:debug,2025-05-15T18:47:52.391Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 687, opaque = 0x2AF, type = add
[ns_server:debug,2025-05-15T18:47:52.391Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 688, opaque = 0x2B0, type = add
[ns_server:debug,2025-05-15T18:47:52.391Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 689, opaque = 0x2B1, type = add
[ns_server:debug,2025-05-15T18:47:52.391Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 690, opaque = 0x2B2, type = add
[ns_server:debug,2025-05-15T18:47:52.391Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 691, opaque = 0x2B3, type = add
[ns_server:debug,2025-05-15T18:47:52.391Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 692, opaque = 0x2B4, type = add
[ns_server:debug,2025-05-15T18:47:52.392Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 693, opaque = 0x2B5, type = add
[ns_server:debug,2025-05-15T18:47:52.392Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 694, opaque = 0x2B6, type = add
[ns_server:debug,2025-05-15T18:47:52.392Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 695, opaque = 0x2B7, type = add
[ns_server:debug,2025-05-15T18:47:52.392Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 696, opaque = 0x2B8, type = add
[ns_server:debug,2025-05-15T18:47:52.392Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 697, opaque = 0x2B9, type = add
[ns_server:debug,2025-05-15T18:47:52.392Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 698, opaque = 0x2BA, type = add
[ns_server:debug,2025-05-15T18:47:52.392Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 699, opaque = 0x2BB, type = add
[ns_server:debug,2025-05-15T18:47:52.392Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 700, opaque = 0x2BC, type = add
[ns_server:debug,2025-05-15T18:47:52.393Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 701, opaque = 0x2BD, type = add
[ns_server:debug,2025-05-15T18:47:52.393Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 702, opaque = 0x2BE, type = add
[ns_server:debug,2025-05-15T18:47:52.393Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 703, opaque = 0x2BF, type = add
[ns_server:debug,2025-05-15T18:47:52.393Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 704, opaque = 0x2C0, type = add
[ns_server:debug,2025-05-15T18:47:52.393Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 705, opaque = 0x2C1, type = add
[ns_server:debug,2025-05-15T18:47:52.393Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 706, opaque = 0x2C2, type = add
[ns_server:debug,2025-05-15T18:47:52.393Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 707, opaque = 0x2C3, type = add
[ns_server:debug,2025-05-15T18:47:52.394Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 708, opaque = 0x2C4, type = add
[ns_server:debug,2025-05-15T18:47:52.394Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 709, opaque = 0x2C5, type = add
[ns_server:debug,2025-05-15T18:47:52.394Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 710, opaque = 0x2C6, type = add
[ns_server:debug,2025-05-15T18:47:52.394Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 711, opaque = 0x2C7, type = add
[ns_server:debug,2025-05-15T18:47:52.394Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 712, opaque = 0x2C8, type = add
[ns_server:debug,2025-05-15T18:47:52.394Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 713, opaque = 0x2C9, type = add
[ns_server:debug,2025-05-15T18:47:52.394Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 714, opaque = 0x2CA, type = add
[ns_server:debug,2025-05-15T18:47:52.394Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 715, opaque = 0x2CB, type = add
[ns_server:debug,2025-05-15T18:47:52.394Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 716, opaque = 0x2CC, type = add
[ns_server:debug,2025-05-15T18:47:52.394Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 717, opaque = 0x2CD, type = add
[ns_server:debug,2025-05-15T18:47:52.395Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 718, opaque = 0x2CE, type = add
[ns_server:debug,2025-05-15T18:47:52.395Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 719, opaque = 0x2CF, type = add
[ns_server:debug,2025-05-15T18:47:52.395Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 720, opaque = 0x2D0, type = add
[ns_server:debug,2025-05-15T18:47:52.395Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 721, opaque = 0x2D1, type = add
[ns_server:debug,2025-05-15T18:47:52.395Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 722, opaque = 0x2D2, type = add
[ns_server:debug,2025-05-15T18:47:52.395Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 723, opaque = 0x2D3, type = add
[ns_server:debug,2025-05-15T18:47:52.395Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 724, opaque = 0x2D4, type = add
[ns_server:debug,2025-05-15T18:47:52.395Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 725, opaque = 0x2D5, type = add
[ns_server:debug,2025-05-15T18:47:52.395Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 726, opaque = 0x2D6, type = add
[ns_server:debug,2025-05-15T18:47:52.395Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 727, opaque = 0x2D7, type = add
[ns_server:debug,2025-05-15T18:47:52.395Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 728, opaque = 0x2D8, type = add
[ns_server:debug,2025-05-15T18:47:52.395Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 729, opaque = 0x2D9, type = add
[ns_server:debug,2025-05-15T18:47:52.395Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 730, opaque = 0x2DA, type = add
[ns_server:debug,2025-05-15T18:47:52.395Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 731, opaque = 0x2DB, type = add
[ns_server:debug,2025-05-15T18:47:52.395Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 732, opaque = 0x2DC, type = add
[ns_server:debug,2025-05-15T18:47:52.395Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 733, opaque = 0x2DD, type = add
[ns_server:debug,2025-05-15T18:47:52.396Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 734, opaque = 0x2DE, type = add
[ns_server:debug,2025-05-15T18:47:52.396Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 735, opaque = 0x2DF, type = add
[ns_server:debug,2025-05-15T18:47:52.396Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 736, opaque = 0x2E0, type = add
[ns_server:debug,2025-05-15T18:47:52.396Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 737, opaque = 0x2E1, type = add
[ns_server:debug,2025-05-15T18:47:52.396Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 738, opaque = 0x2E2, type = add
[ns_server:debug,2025-05-15T18:47:52.396Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 739, opaque = 0x2E3, type = add
[ns_server:debug,2025-05-15T18:47:52.396Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 740, opaque = 0x2E4, type = add
[ns_server:debug,2025-05-15T18:47:52.396Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 741, opaque = 0x2E5, type = add
[ns_server:debug,2025-05-15T18:47:52.396Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 742, opaque = 0x2E6, type = add
[ns_server:debug,2025-05-15T18:47:52.396Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 743, opaque = 0x2E7, type = add
[ns_server:debug,2025-05-15T18:47:52.396Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 744, opaque = 0x2E8, type = add
[ns_server:debug,2025-05-15T18:47:52.396Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 745, opaque = 0x2E9, type = add
[ns_server:debug,2025-05-15T18:47:52.396Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 746, opaque = 0x2EA, type = add
[ns_server:debug,2025-05-15T18:47:52.396Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 747, opaque = 0x2EB, type = add
[ns_server:debug,2025-05-15T18:47:52.396Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 748, opaque = 0x2EC, type = add
[ns_server:debug,2025-05-15T18:47:52.396Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 749, opaque = 0x2ED, type = add
[ns_server:debug,2025-05-15T18:47:52.396Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 750, opaque = 0x2EE, type = add
[ns_server:debug,2025-05-15T18:47:52.396Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 751, opaque = 0x2EF, type = add
[ns_server:debug,2025-05-15T18:47:52.396Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 752, opaque = 0x2F0, type = add
[ns_server:debug,2025-05-15T18:47:52.396Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 753, opaque = 0x2F1, type = add
[ns_server:debug,2025-05-15T18:47:52.396Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 754, opaque = 0x2F2, type = add
[ns_server:debug,2025-05-15T18:47:52.396Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 755, opaque = 0x2F3, type = add
[ns_server:debug,2025-05-15T18:47:52.396Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 756, opaque = 0x2F4, type = add
[ns_server:debug,2025-05-15T18:47:52.396Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 757, opaque = 0x2F5, type = add
[ns_server:debug,2025-05-15T18:47:52.396Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 758, opaque = 0x2F6, type = add
[ns_server:debug,2025-05-15T18:47:52.396Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 759, opaque = 0x2F7, type = add
[ns_server:debug,2025-05-15T18:47:52.396Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 760, opaque = 0x2F8, type = add
[ns_server:debug,2025-05-15T18:47:52.396Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 761, opaque = 0x2F9, type = add
[ns_server:debug,2025-05-15T18:47:52.396Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 762, opaque = 0x2FA, type = add
[ns_server:debug,2025-05-15T18:47:52.396Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 763, opaque = 0x2FB, type = add
[ns_server:debug,2025-05-15T18:47:52.397Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 764, opaque = 0x2FC, type = add
[ns_server:debug,2025-05-15T18:47:52.397Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 765, opaque = 0x2FD, type = add
[ns_server:debug,2025-05-15T18:47:52.397Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 766, opaque = 0x2FE, type = add
[ns_server:debug,2025-05-15T18:47:52.397Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 767, opaque = 0x2FF, type = add
[ns_server:debug,2025-05-15T18:47:52.397Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 768, opaque = 0x300, type = add
[ns_server:debug,2025-05-15T18:47:52.397Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 769, opaque = 0x301, type = add
[ns_server:debug,2025-05-15T18:47:52.397Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 770, opaque = 0x302, type = add
[ns_server:debug,2025-05-15T18:47:52.397Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 771, opaque = 0x303, type = add
[ns_server:debug,2025-05-15T18:47:52.397Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 772, opaque = 0x304, type = add
[ns_server:debug,2025-05-15T18:47:52.397Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 773, opaque = 0x305, type = add
[ns_server:debug,2025-05-15T18:47:52.397Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 774, opaque = 0x306, type = add
[ns_server:debug,2025-05-15T18:47:52.397Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 775, opaque = 0x307, type = add
[ns_server:debug,2025-05-15T18:47:52.397Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 776, opaque = 0x308, type = add
[ns_server:debug,2025-05-15T18:47:52.397Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 777, opaque = 0x309, type = add
[ns_server:debug,2025-05-15T18:47:52.397Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 778, opaque = 0x30A, type = add
[ns_server:debug,2025-05-15T18:47:52.397Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 779, opaque = 0x30B, type = add
[ns_server:debug,2025-05-15T18:47:52.397Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 780, opaque = 0x30C, type = add
[ns_server:debug,2025-05-15T18:47:52.397Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 781, opaque = 0x30D, type = add
[ns_server:debug,2025-05-15T18:47:52.397Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 782, opaque = 0x30E, type = add
[ns_server:debug,2025-05-15T18:47:52.397Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 783, opaque = 0x30F, type = add
[ns_server:debug,2025-05-15T18:47:52.397Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 784, opaque = 0x310, type = add
[ns_server:debug,2025-05-15T18:47:52.398Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 785, opaque = 0x311, type = add
[ns_server:debug,2025-05-15T18:47:52.398Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 786, opaque = 0x312, type = add
[ns_server:debug,2025-05-15T18:47:52.398Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 787, opaque = 0x313, type = add
[ns_server:debug,2025-05-15T18:47:52.398Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 788, opaque = 0x314, type = add
[ns_server:debug,2025-05-15T18:47:52.398Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 789, opaque = 0x315, type = add
[ns_server:debug,2025-05-15T18:47:52.398Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 790, opaque = 0x316, type = add
[ns_server:debug,2025-05-15T18:47:52.398Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 791, opaque = 0x317, type = add
[ns_server:debug,2025-05-15T18:47:52.398Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 792, opaque = 0x318, type = add
[ns_server:debug,2025-05-15T18:47:52.398Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 793, opaque = 0x319, type = add
[ns_server:debug,2025-05-15T18:47:52.398Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 794, opaque = 0x31A, type = add
[ns_server:debug,2025-05-15T18:47:52.398Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 795, opaque = 0x31B, type = add
[ns_server:debug,2025-05-15T18:47:52.398Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 796, opaque = 0x31C, type = add
[ns_server:debug,2025-05-15T18:47:52.398Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 797, opaque = 0x31D, type = add
[ns_server:debug,2025-05-15T18:47:52.398Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 798, opaque = 0x31E, type = add
[ns_server:debug,2025-05-15T18:47:52.398Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 799, opaque = 0x31F, type = add
[ns_server:debug,2025-05-15T18:47:52.398Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 800, opaque = 0x320, type = add
[ns_server:debug,2025-05-15T18:47:52.398Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 801, opaque = 0x321, type = add
[ns_server:debug,2025-05-15T18:47:52.398Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 802, opaque = 0x322, type = add
[ns_server:debug,2025-05-15T18:47:52.398Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 803, opaque = 0x323, type = add
[ns_server:debug,2025-05-15T18:47:52.398Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 804, opaque = 0x324, type = add
[ns_server:debug,2025-05-15T18:47:52.398Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 805, opaque = 0x325, type = add
[ns_server:debug,2025-05-15T18:47:52.398Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 806, opaque = 0x326, type = add
[ns_server:debug,2025-05-15T18:47:52.398Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 807, opaque = 0x327, type = add
[ns_server:debug,2025-05-15T18:47:52.398Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 808, opaque = 0x328, type = add
[ns_server:debug,2025-05-15T18:47:52.398Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 809, opaque = 0x329, type = add
[ns_server:debug,2025-05-15T18:47:52.398Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 810, opaque = 0x32A, type = add
[ns_server:debug,2025-05-15T18:47:52.398Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 811, opaque = 0x32B, type = add
[ns_server:debug,2025-05-15T18:47:52.398Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 812, opaque = 0x32C, type = add
[ns_server:debug,2025-05-15T18:47:52.398Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 813, opaque = 0x32D, type = add
[ns_server:debug,2025-05-15T18:47:52.398Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 814, opaque = 0x32E, type = add
[ns_server:debug,2025-05-15T18:47:52.398Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 815, opaque = 0x32F, type = add
[ns_server:debug,2025-05-15T18:47:52.398Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 816, opaque = 0x330, type = add
[ns_server:debug,2025-05-15T18:47:52.398Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 817, opaque = 0x331, type = add
[ns_server:debug,2025-05-15T18:47:52.398Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 818, opaque = 0x332, type = add
[ns_server:debug,2025-05-15T18:47:52.399Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 819, opaque = 0x333, type = add
[ns_server:debug,2025-05-15T18:47:52.399Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 820, opaque = 0x334, type = add
[ns_server:debug,2025-05-15T18:47:52.399Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 821, opaque = 0x335, type = add
[ns_server:debug,2025-05-15T18:47:52.400Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 822, opaque = 0x336, type = add
[ns_server:debug,2025-05-15T18:47:52.400Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 823, opaque = 0x337, type = add
[ns_server:debug,2025-05-15T18:47:52.400Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 824, opaque = 0x338, type = add
[ns_server:debug,2025-05-15T18:47:52.400Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 825, opaque = 0x339, type = add
[ns_server:debug,2025-05-15T18:47:52.400Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 826, opaque = 0x33A, type = add
[ns_server:debug,2025-05-15T18:47:52.400Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 827, opaque = 0x33B, type = add
[ns_server:debug,2025-05-15T18:47:52.400Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 828, opaque = 0x33C, type = add
[ns_server:debug,2025-05-15T18:47:52.400Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 829, opaque = 0x33D, type = add
[ns_server:debug,2025-05-15T18:47:52.400Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 830, opaque = 0x33E, type = add
[ns_server:debug,2025-05-15T18:47:52.400Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 831, opaque = 0x33F, type = add
[ns_server:debug,2025-05-15T18:47:52.400Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 832, opaque = 0x340, type = add
[ns_server:debug,2025-05-15T18:47:52.400Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 833, opaque = 0x341, type = add
[ns_server:debug,2025-05-15T18:47:52.400Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 834, opaque = 0x342, type = add
[ns_server:debug,2025-05-15T18:47:52.400Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 835, opaque = 0x343, type = add
[ns_server:debug,2025-05-15T18:47:52.400Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 836, opaque = 0x344, type = add
[ns_server:debug,2025-05-15T18:47:52.400Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 837, opaque = 0x345, type = add
[ns_server:debug,2025-05-15T18:47:52.400Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 838, opaque = 0x346, type = add
[ns_server:debug,2025-05-15T18:47:52.400Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 839, opaque = 0x347, type = add
[ns_server:debug,2025-05-15T18:47:52.401Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 840, opaque = 0x348, type = add
[ns_server:debug,2025-05-15T18:47:52.401Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 841, opaque = 0x349, type = add
[ns_server:debug,2025-05-15T18:47:52.401Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 842, opaque = 0x34A, type = add
[ns_server:debug,2025-05-15T18:47:52.401Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 843, opaque = 0x34B, type = add
[ns_server:debug,2025-05-15T18:47:52.401Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 844, opaque = 0x34C, type = add
[ns_server:debug,2025-05-15T18:47:52.401Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 845, opaque = 0x34D, type = add
[ns_server:debug,2025-05-15T18:47:52.401Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 846, opaque = 0x34E, type = add
[ns_server:debug,2025-05-15T18:47:52.401Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 847, opaque = 0x34F, type = add
[ns_server:debug,2025-05-15T18:47:52.401Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 848, opaque = 0x350, type = add
[ns_server:debug,2025-05-15T18:47:52.401Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 849, opaque = 0x351, type = add
[ns_server:debug,2025-05-15T18:47:52.401Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 850, opaque = 0x352, type = add
[ns_server:debug,2025-05-15T18:47:52.401Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 851, opaque = 0x353, type = add
[ns_server:debug,2025-05-15T18:47:52.401Z,ns_1@172.19.0.4:<0.4470.0>:dcp_commands:add_stream:83]Add stream for partition 852, opaque = 0x354, type = add
[ns_server:debug,2025-05-15T18:47:52.401Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_call:206]Setup DCP streams:
Current []
Streams to open [682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852]
Streams to close []

[ns_server:debug,2025-05-15T18:47:52.401Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0x6000000
80 5E 00 16
00 00 00 00
00 00 00 1E
06 00 00 00
00 00 00 00
00 00 00 00
63 6F 6E 6E
65 63 74 69
6F 6E 5F 62
75 66 66 65
72 5F 73 69
7A 65 31 33
34 32 31 37
37 33 
[ns_server:debug,2025-05-15T18:47:52.401Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0xFE (cmd_get_error_map) vbucket = 0 opaque = 0x7000000
80 FE 00 00
00 00 00 00
00 00 00 02
07 00 00 00
00 00 00 00
00 00 00 00
00 00 
[ns_server:debug,2025-05-15T18:47:52.402Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0x6000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
06 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.402Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0xFE (cmd_get_error_map) vbucket = 0 opaque = 0x7000000 status = 0x1 (key_enoent)
81 FE 00 00
00 00 00 01
00 00 00 00
07 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.403Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xAE000000
80 5E 00 0B
00 00 00 00
00 00 00 0F
AE 00 00 00
00 00 00 00
00 00 00 00
65 6E 61 62
6C 65 5F 6E
6F 6F 70 74
72 75 65 
[ns_server:debug,2025-05-15T18:47:52.403Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xAF000000
80 5E 00 11
00 00 00 00
00 00 00 19
AF 00 00 00
00 00 00 00
00 00 00 00
73 65 74 5F
6E 6F 6F 70
5F 69 6E 74
65 72 76 61
6C 30 2E 31
30 30 30 30
30 
[ns_server:debug,2025-05-15T18:47:52.403Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xAE000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
AE 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.403Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xAF000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
AF 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.403Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB0000000
80 5E 00 0C
00 00 00 00
00 00 00 10
B0 00 00 00
00 00 00 00
00 00 00 00
73 65 74 5F
70 72 69 6F
72 69 74 79
68 69 67 68

[ns_server:debug,2025-05-15T18:47:52.403Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB1000000
80 5E 00 1F
00 00 00 00
00 00 00 23
B1 00 00 00
00 00 00 00
00 00 00 00
73 75 70 70
6F 72 74 73
5F 63 75 72
73 6F 72 5F
64 72 6F 70
70 69 6E 67
5F 76 75 6C
63 61 6E 74
72 75 65 
[ns_server:debug,2025-05-15T18:47:52.403Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB2000000
80 5E 00 11
00 00 00 00
00 00 00 15
B2 00 00 00
00 00 00 00
00 00 00 00
73 75 70 70
6F 72 74 73
5F 68 69 66
69 5F 4D 46
55 74 72 75
65 
[ns_server:debug,2025-05-15T18:47:52.404Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB3000000
80 5E 00 26
00 00 00 00
00 00 00 2A
B3 00 00 00
00 00 00 00
00 00 00 00
73 65 6E 64
5F 73 74 72
65 61 6D 5F
65 6E 64 5F
6F 6E 5F 63
6C 69 65 6E
74 5F 63 6C
6F 73 65 5F
73 74 72 65
61 6D 74 72
75 65 
[ns_server:debug,2025-05-15T18:47:52.404Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB4000000
80 5E 00 14
00 00 00 00
00 00 00 18
B4 00 00 00
00 00 00 00
00 00 00 00
65 6E 61 62
6C 65 5F 65
78 70 69 72
79 5F 6F 70
63 6F 64 65
74 72 75 65

[ns_server:debug,2025-05-15T18:47:52.404Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB5000000
80 5E 00 12
00 00 00 00
00 00 00 16
B5 00 00 00
00 00 00 00
00 00 00 00
65 6E 61 62
6C 65 5F 73
79 6E 63 5F
77 72 69 74
65 73 74 72
75 65 
[ns_server:debug,2025-05-15T18:47:52.404Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB0000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B0 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.404Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB1000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B1 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.404Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB2000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B2 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.404Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB3000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B3 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.404Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB4000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B4 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.404Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB5000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B5 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.405Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB6000000
80 5E 00 0D
00 00 00 00
00 00 00 1C
B6 00 00 00
00 00 00 00
00 00 00 00
63 6F 6E 73
75 6D 65 72
5F 6E 61 6D
65 6E 73 5F
31 40 31 37
32 2E 31 39
2E 30 2E 34

[ns_server:debug,2025-05-15T18:47:52.405Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB7000000
80 5E 00 1B
00 00 00 00
00 00 00 1F
B7 00 00 00
00 00 00 00
00 00 00 00
69 6E 63 6C
75 64 65 5F
64 65 6C 65
74 65 64 5F
75 73 65 72
5F 78 61 74
74 72 73 74
72 75 65 
[ns_server:debug,2025-05-15T18:47:52.406Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB6000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B6 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.406Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB7000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B7 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.406Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB8000000
80 5E 00 13
00 00 00 00
00 00 00 17
B8 00 00 00
00 00 00 00
00 00 00 00
76 37 5F 64
63 70 5F 73
74 61 74 75
73 5F 63 6F
64 65 73 74
72 75 65 
[ns_server:debug,2025-05-15T18:47:52.407Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB8000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B8 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.407Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xB9000000
80 5E 00 19
00 00 00 00
00 00 00 1D
B9 00 00 00
00 00 00 00
00 00 00 00
66 6C 61 74
62 75 66 66
65 72 73 5F
73 79 73 74
65 6D 5F 65
76 65 6E 74
73 74 72 75
65 
[ns_server:debug,2025-05-15T18:47:52.407Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xB9000000 status = 0x0 (success)
81 5E 00 00
00 00 00 00
00 00 00 00
B9 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.408Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x5E (dcp_control) vbucket = 0 opaque = 0xBA000000
80 5E 00 0E
00 00 00 00
00 00 00 12
BA 00 00 00
00 00 00 00
00 00 00 00
63 68 61 6E
67 65 5F 73
74 72 65 61
6D 73 74 72
75 65 
[ns_server:debug,2025-05-15T18:47:52.411Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x5E (dcp_control) vbucket = 0 opaque = 0xBA000000 status = 0x83 (not_supported)
81 5E 00 00
00 00 00 83
00 00 00 00
BA 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.411Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 682 opaque = 0x1000000
80 53 00 00
30 00 02 AA
00 00 00 30
01 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.411Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 683 opaque = 0x2000000
80 53 00 00
30 00 02 AB
00 00 00 30
02 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.411Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 684 opaque = 0x3000000
80 53 00 00
30 00 02 AC
00 00 00 30
03 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.412Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 685 opaque = 0x4000000
80 53 00 00
30 00 02 AD
00 00 00 30
04 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.412Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 686 opaque = 0x5000000
80 53 00 00
30 00 02 AE
00 00 00 30
05 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.412Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 687 opaque = 0x8000000
80 53 00 00
30 00 02 AF
00 00 00 30
08 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.412Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 688 opaque = 0x9000000
80 53 00 00
30 00 02 B0
00 00 00 30
09 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.412Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 689 opaque = 0xA000000
80 53 00 00
30 00 02 B1
00 00 00 30
0A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.412Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 690 opaque = 0xB000000
80 53 00 00
30 00 02 B2
00 00 00 30
0B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.412Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 691 opaque = 0xC000000
80 53 00 00
30 00 02 B3
00 00 00 30
0C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.413Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 692 opaque = 0xD000000
80 53 00 00
30 00 02 B4
00 00 00 30
0D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.413Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 693 opaque = 0xE000000
80 53 00 00
30 00 02 B5
00 00 00 30
0E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.413Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 694 opaque = 0xF000000
80 53 00 00
30 00 02 B6
00 00 00 30
0F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.413Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 695 opaque = 0x10000000
80 53 00 00
30 00 02 B7
00 00 00 30
10 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.413Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 696 opaque = 0x11000000
80 53 00 00
30 00 02 B8
00 00 00 30
11 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.413Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 697 opaque = 0x12000000
80 53 00 00
30 00 02 B9
00 00 00 30
12 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.414Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 698 opaque = 0x13000000
80 53 00 00
30 00 02 BA
00 00 00 30
13 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.414Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 699 opaque = 0x14000000
80 53 00 00
30 00 02 BB
00 00 00 30
14 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.414Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 700 opaque = 0x15000000
80 53 00 00
30 00 02 BC
00 00 00 30
15 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.414Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 701 opaque = 0x16000000
80 53 00 00
30 00 02 BD
00 00 00 30
16 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.414Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 702 opaque = 0x17000000
80 53 00 00
30 00 02 BE
00 00 00 30
17 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.414Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 703 opaque = 0x18000000
80 53 00 00
30 00 02 BF
00 00 00 30
18 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.414Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 704 opaque = 0x19000000
80 53 00 00
30 00 02 C0
00 00 00 30
19 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.415Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 705 opaque = 0x1A000000
80 53 00 00
30 00 02 C1
00 00 00 30
1A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.415Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 706 opaque = 0x1B000000
80 53 00 00
30 00 02 C2
00 00 00 30
1B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.415Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 707 opaque = 0x1C000000
80 53 00 00
30 00 02 C3
00 00 00 30
1C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.415Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 708 opaque = 0x1D000000
80 53 00 00
30 00 02 C4
00 00 00 30
1D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.415Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 709 opaque = 0x1E000000
80 53 00 00
30 00 02 C5
00 00 00 30
1E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.415Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 710 opaque = 0x1F000000
80 53 00 00
30 00 02 C6
00 00 00 30
1F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.415Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 711 opaque = 0x20000000
80 53 00 00
30 00 02 C7
00 00 00 30
20 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.415Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 712 opaque = 0x21000000
80 53 00 00
30 00 02 C8
00 00 00 30
21 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.416Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 713 opaque = 0x22000000
80 53 00 00
30 00 02 C9
00 00 00 30
22 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.416Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 714 opaque = 0x23000000
80 53 00 00
30 00 02 CA
00 00 00 30
23 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.416Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 715 opaque = 0x24000000
80 53 00 00
30 00 02 CB
00 00 00 30
24 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.416Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 716 opaque = 0x25000000
80 53 00 00
30 00 02 CC
00 00 00 30
25 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.416Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 717 opaque = 0x26000000
80 53 00 00
30 00 02 CD
00 00 00 30
26 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.416Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 718 opaque = 0x27000000
80 53 00 00
30 00 02 CE
00 00 00 30
27 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.416Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 719 opaque = 0x28000000
80 53 00 00
30 00 02 CF
00 00 00 30
28 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.416Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 720 opaque = 0x29000000
80 53 00 00
30 00 02 D0
00 00 00 30
29 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.416Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 721 opaque = 0x2A000000
80 53 00 00
30 00 02 D1
00 00 00 30
2A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.416Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 722 opaque = 0x2B000000
80 53 00 00
30 00 02 D2
00 00 00 30
2B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.416Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 723 opaque = 0x2C000000
80 53 00 00
30 00 02 D3
00 00 00 30
2C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.417Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 724 opaque = 0x2D000000
80 53 00 00
30 00 02 D4
00 00 00 30
2D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.417Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 725 opaque = 0x2E000000
80 53 00 00
30 00 02 D5
00 00 00 30
2E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.417Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 726 opaque = 0x2F000000
80 53 00 00
30 00 02 D6
00 00 00 30
2F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.417Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 727 opaque = 0x30000000
80 53 00 00
30 00 02 D7
00 00 00 30
30 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.417Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 728 opaque = 0x31000000
80 53 00 00
30 00 02 D8
00 00 00 30
31 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.417Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 729 opaque = 0x32000000
80 53 00 00
30 00 02 D9
00 00 00 30
32 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.417Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 730 opaque = 0x33000000
80 53 00 00
30 00 02 DA
00 00 00 30
33 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.417Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 731 opaque = 0x34000000
80 53 00 00
30 00 02 DB
00 00 00 30
34 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.417Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 732 opaque = 0x35000000
80 53 00 00
30 00 02 DC
00 00 00 30
35 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.417Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 733 opaque = 0x36000000
80 53 00 00
30 00 02 DD
00 00 00 30
36 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.417Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 734 opaque = 0x37000000
80 53 00 00
30 00 02 DE
00 00 00 30
37 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.417Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 735 opaque = 0x38000000
80 53 00 00
30 00 02 DF
00 00 00 30
38 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.418Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 736 opaque = 0x39000000
80 53 00 00
30 00 02 E0
00 00 00 30
39 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.418Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 737 opaque = 0x3A000000
80 53 00 00
30 00 02 E1
00 00 00 30
3A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.418Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 738 opaque = 0x3B000000
80 53 00 00
30 00 02 E2
00 00 00 30
3B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.418Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 739 opaque = 0x3C000000
80 53 00 00
30 00 02 E3
00 00 00 30
3C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.418Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 740 opaque = 0x3D000000
80 53 00 00
30 00 02 E4
00 00 00 30
3D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.418Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 741 opaque = 0x3E000000
80 53 00 00
30 00 02 E5
00 00 00 30
3E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.418Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 742 opaque = 0x3F000000
80 53 00 00
30 00 02 E6
00 00 00 30
3F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.418Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 743 opaque = 0x40000000
80 53 00 00
30 00 02 E7
00 00 00 30
40 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.418Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 744 opaque = 0x41000000
80 53 00 00
30 00 02 E8
00 00 00 30
41 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.418Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 745 opaque = 0x42000000
80 53 00 00
30 00 02 E9
00 00 00 30
42 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.418Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 746 opaque = 0x43000000
80 53 00 00
30 00 02 EA
00 00 00 30
43 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.418Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 747 opaque = 0x44000000
80 53 00 00
30 00 02 EB
00 00 00 30
44 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.419Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 748 opaque = 0x45000000
80 53 00 00
30 00 02 EC
00 00 00 30
45 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.419Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 749 opaque = 0x46000000
80 53 00 00
30 00 02 ED
00 00 00 30
46 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.419Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 750 opaque = 0x47000000
80 53 00 00
30 00 02 EE
00 00 00 30
47 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.419Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 751 opaque = 0x48000000
80 53 00 00
30 00 02 EF
00 00 00 30
48 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.420Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 752 opaque = 0x49000000
80 53 00 00
30 00 02 F0
00 00 00 30
49 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.420Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 753 opaque = 0x4A000000
80 53 00 00
30 00 02 F1
00 00 00 30
4A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.420Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 754 opaque = 0x4B000000
80 53 00 00
30 00 02 F2
00 00 00 30
4B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.420Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 755 opaque = 0x4C000000
80 53 00 00
30 00 02 F3
00 00 00 30
4C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.420Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 756 opaque = 0x4D000000
80 53 00 00
30 00 02 F4
00 00 00 30
4D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.421Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 757 opaque = 0x4E000000
80 53 00 00
30 00 02 F5
00 00 00 30
4E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.421Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 758 opaque = 0x4F000000
80 53 00 00
30 00 02 F6
00 00 00 30
4F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.421Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 759 opaque = 0x50000000
80 53 00 00
30 00 02 F7
00 00 00 30
50 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.421Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 760 opaque = 0x51000000
80 53 00 00
30 00 02 F8
00 00 00 30
51 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.421Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 761 opaque = 0x52000000
80 53 00 00
30 00 02 F9
00 00 00 30
52 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.421Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 762 opaque = 0x53000000
80 53 00 00
30 00 02 FA
00 00 00 30
53 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.422Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 763 opaque = 0x54000000
80 53 00 00
30 00 02 FB
00 00 00 30
54 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.422Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 764 opaque = 0x55000000
80 53 00 00
30 00 02 FC
00 00 00 30
55 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.422Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 765 opaque = 0x56000000
80 53 00 00
30 00 02 FD
00 00 00 30
56 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.422Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 766 opaque = 0x57000000
80 53 00 00
30 00 02 FE
00 00 00 30
57 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.422Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 767 opaque = 0x58000000
80 53 00 00
30 00 02 FF
00 00 00 30
58 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.422Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 768 opaque = 0x59000000
80 53 00 00
30 00 03 00
00 00 00 30
59 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.422Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 769 opaque = 0x5A000000
80 53 00 00
30 00 03 01
00 00 00 30
5A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.422Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 770 opaque = 0x5B000000
80 53 00 00
30 00 03 02
00 00 00 30
5B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.422Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 771 opaque = 0x5C000000
80 53 00 00
30 00 03 03
00 00 00 30
5C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.422Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 772 opaque = 0x5D000000
80 53 00 00
30 00 03 04
00 00 00 30
5D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.422Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 773 opaque = 0x5E000000
80 53 00 00
30 00 03 05
00 00 00 30
5E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.422Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 774 opaque = 0x5F000000
80 53 00 00
30 00 03 06
00 00 00 30
5F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.422Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 775 opaque = 0x60000000
80 53 00 00
30 00 03 07
00 00 00 30
60 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.423Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 776 opaque = 0x61000000
80 53 00 00
30 00 03 08
00 00 00 30
61 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.423Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 777 opaque = 0x62000000
80 53 00 00
30 00 03 09
00 00 00 30
62 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.423Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 778 opaque = 0x63000000
80 53 00 00
30 00 03 0A
00 00 00 30
63 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.423Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 779 opaque = 0x64000000
80 53 00 00
30 00 03 0B
00 00 00 30
64 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.423Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 780 opaque = 0x65000000
80 53 00 00
30 00 03 0C
00 00 00 30
65 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.423Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 781 opaque = 0x66000000
80 53 00 00
30 00 03 0D
00 00 00 30
66 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.423Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 782 opaque = 0x67000000
80 53 00 00
30 00 03 0E
00 00 00 30
67 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.423Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 783 opaque = 0x68000000
80 53 00 00
30 00 03 0F
00 00 00 30
68 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.423Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 784 opaque = 0x69000000
80 53 00 00
30 00 03 10
00 00 00 30
69 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.423Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 785 opaque = 0x6A000000
80 53 00 00
30 00 03 11
00 00 00 30
6A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.423Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 786 opaque = 0x6B000000
80 53 00 00
30 00 03 12
00 00 00 30
6B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.423Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 787 opaque = 0x6C000000
80 53 00 00
30 00 03 13
00 00 00 30
6C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.423Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 788 opaque = 0x6D000000
80 53 00 00
30 00 03 14
00 00 00 30
6D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.423Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 789 opaque = 0x6E000000
80 53 00 00
30 00 03 15
00 00 00 30
6E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.423Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 790 opaque = 0x6F000000
80 53 00 00
30 00 03 16
00 00 00 30
6F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.424Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 791 opaque = 0x70000000
80 53 00 00
30 00 03 17
00 00 00 30
70 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.424Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 792 opaque = 0x71000000
80 53 00 00
30 00 03 18
00 00 00 30
71 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.424Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 793 opaque = 0x72000000
80 53 00 00
30 00 03 19
00 00 00 30
72 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.424Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 794 opaque = 0x73000000
80 53 00 00
30 00 03 1A
00 00 00 30
73 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.424Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 795 opaque = 0x74000000
80 53 00 00
30 00 03 1B
00 00 00 30
74 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.424Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 796 opaque = 0x75000000
80 53 00 00
30 00 03 1C
00 00 00 30
75 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.424Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 797 opaque = 0x76000000
80 53 00 00
30 00 03 1D
00 00 00 30
76 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.424Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 798 opaque = 0x77000000
80 53 00 00
30 00 03 1E
00 00 00 30
77 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.424Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 799 opaque = 0x78000000
80 53 00 00
30 00 03 1F
00 00 00 30
78 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.424Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 800 opaque = 0x79000000
80 53 00 00
30 00 03 20
00 00 00 30
79 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.425Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 801 opaque = 0x7A000000
80 53 00 00
30 00 03 21
00 00 00 30
7A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.425Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 802 opaque = 0x7B000000
80 53 00 00
30 00 03 22
00 00 00 30
7B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.425Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 803 opaque = 0x7C000000
80 53 00 00
30 00 03 23
00 00 00 30
7C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.425Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 804 opaque = 0x7D000000
80 53 00 00
30 00 03 24
00 00 00 30
7D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.425Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 805 opaque = 0x7E000000
80 53 00 00
30 00 03 25
00 00 00 30
7E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.425Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 806 opaque = 0x7F000000
80 53 00 00
30 00 03 26
00 00 00 30
7F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.425Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 807 opaque = 0x80000000
80 53 00 00
30 00 03 27
00 00 00 30
80 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.425Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 808 opaque = 0x81000000
80 53 00 00
30 00 03 28
00 00 00 30
81 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.425Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 809 opaque = 0x82000000
80 53 00 00
30 00 03 29
00 00 00 30
82 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.425Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 810 opaque = 0x83000000
80 53 00 00
30 00 03 2A
00 00 00 30
83 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.425Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 811 opaque = 0x84000000
80 53 00 00
30 00 03 2B
00 00 00 30
84 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.425Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 812 opaque = 0x85000000
80 53 00 00
30 00 03 2C
00 00 00 30
85 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.425Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 813 opaque = 0x86000000
80 53 00 00
30 00 03 2D
00 00 00 30
86 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.425Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 814 opaque = 0x87000000
80 53 00 00
30 00 03 2E
00 00 00 30
87 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.426Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 815 opaque = 0x88000000
80 53 00 00
30 00 03 2F
00 00 00 30
88 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.426Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 816 opaque = 0x89000000
80 53 00 00
30 00 03 30
00 00 00 30
89 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.426Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 817 opaque = 0x8A000000
80 53 00 00
30 00 03 31
00 00 00 30
8A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.426Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 818 opaque = 0x8B000000
80 53 00 00
30 00 03 32
00 00 00 30
8B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.426Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 819 opaque = 0x8C000000
80 53 00 00
30 00 03 33
00 00 00 30
8C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.426Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 820 opaque = 0x8D000000
80 53 00 00
30 00 03 34
00 00 00 30
8D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.426Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 821 opaque = 0x8E000000
80 53 00 00
30 00 03 35
00 00 00 30
8E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.426Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 822 opaque = 0x8F000000
80 53 00 00
30 00 03 36
00 00 00 30
8F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.426Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 823 opaque = 0x90000000
80 53 00 00
30 00 03 37
00 00 00 30
90 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.427Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 824 opaque = 0x91000000
80 53 00 00
30 00 03 38
00 00 00 30
91 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.427Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 825 opaque = 0x92000000
80 53 00 00
30 00 03 39
00 00 00 30
92 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.427Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 826 opaque = 0x93000000
80 53 00 00
30 00 03 3A
00 00 00 30
93 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.427Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 827 opaque = 0x94000000
80 53 00 00
30 00 03 3B
00 00 00 30
94 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.427Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 828 opaque = 0x95000000
80 53 00 00
30 00 03 3C
00 00 00 30
95 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.427Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 829 opaque = 0x96000000
80 53 00 00
30 00 03 3D
00 00 00 30
96 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.427Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 830 opaque = 0x97000000
80 53 00 00
30 00 03 3E
00 00 00 30
97 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.427Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 831 opaque = 0x98000000
80 53 00 00
30 00 03 3F
00 00 00 30
98 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.428Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 832 opaque = 0x99000000
80 53 00 00
30 00 03 40
00 00 00 30
99 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.428Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 833 opaque = 0x9A000000
80 53 00 00
30 00 03 41
00 00 00 30
9A 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.428Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 834 opaque = 0x9B000000
80 53 00 00
30 00 03 42
00 00 00 30
9B 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.428Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 835 opaque = 0x9C000000
80 53 00 00
30 00 03 43
00 00 00 30
9C 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.428Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 836 opaque = 0x9D000000
80 53 00 00
30 00 03 44
00 00 00 30
9D 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.428Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 837 opaque = 0x9E000000
80 53 00 00
30 00 03 45
00 00 00 30
9E 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.428Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 838 opaque = 0x9F000000
80 53 00 00
30 00 03 46
00 00 00 30
9F 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.429Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 839 opaque = 0xA0000000
80 53 00 00
30 00 03 47
00 00 00 30
A0 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.429Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 840 opaque = 0xA1000000
80 53 00 00
30 00 03 48
00 00 00 30
A1 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.429Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 841 opaque = 0xA2000000
80 53 00 00
30 00 03 49
00 00 00 30
A2 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.429Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 842 opaque = 0xA3000000
80 53 00 00
30 00 03 4A
00 00 00 30
A3 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.429Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 843 opaque = 0xA4000000
80 53 00 00
30 00 03 4B
00 00 00 30
A4 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.429Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 844 opaque = 0xA5000000
80 53 00 00
30 00 03 4C
00 00 00 30
A5 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.429Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 845 opaque = 0xA6000000
80 53 00 00
30 00 03 4D
00 00 00 30
A6 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.429Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 846 opaque = 0xA7000000
80 53 00 00
30 00 03 4E
00 00 00 30
A7 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.429Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 847 opaque = 0xA8000000
80 53 00 00
30 00 03 4F
00 00 00 30
A8 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.429Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 848 opaque = 0xA9000000
80 53 00 00
30 00 03 50
00 00 00 30
A9 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.430Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 849 opaque = 0xAA000000
80 53 00 00
30 00 03 51
00 00 00 30
AA 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.430Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 850 opaque = 0xAB000000
80 53 00 00
30 00 03 52
00 00 00 30
AB 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.430Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 851 opaque = 0xAC000000
80 53 00 00
30 00 03 53
00 00 00 30
AC 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.430Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: REQUEST: 0x53 (dcp_stream_req) vbucket = 852 opaque = 0xAD000000
80 53 00 00
30 00 03 54
00 00 00 30
AD 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
FF FF FF FF
FF FF FF FF
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.431Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x1000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
01 00 00 00
00 00 00 00
00 00 00 00
00 00 A4 BB
8F 0B CF 67
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.431Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x2000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
02 00 00 00
00 00 00 00
00 00 00 00
00 00 22 72
99 4A 12 FC
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.431Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x3000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
03 00 00 00
00 00 00 00
00 00 00 00
00 00 4E BA
BB 6C C4 0A
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.431Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x4000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
04 00 00 00
00 00 00 00
00 00 00 00
00 00 C6 61
36 01 DA DB
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.432Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x5000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
05 00 00 00
00 00 00 00
00 00 00 00
00 00 BE B3
9B 10 1F 3E
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.432Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x8000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
08 00 00 00
00 00 00 00
00 00 00 00
00 00 29 45
3C 5D 23 E7
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.432Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x9000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
09 00 00 00
00 00 00 00
00 00 00 00
00 00 D7 94
47 F4 AA 75
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.432Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
0A 00 00 00
00 00 00 00
00 00 00 00
00 00 57 F6
67 CD B9 D3
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.432Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xB000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
0B 00 00 00
00 00 00 00
00 00 00 00
00 00 58 4E
11 81 BF 10
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.432Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xC000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
0C 00 00 00
00 00 00 00
00 00 00 00
00 00 BC AC
E9 BA E6 94
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.432Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xD000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
0D 00 00 00
00 00 00 00
00 00 00 00
00 00 D5 FB
70 92 0E DB
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.432Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xE000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
0E 00 00 00
00 00 00 00
00 00 00 00
00 00 F6 92
C3 E8 6A AF
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.432Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xF000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
0F 00 00 00
00 00 00 00
00 00 00 00
00 00 45 8E
54 DF 52 52
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.432Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x10000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
10 00 00 00
00 00 00 00
00 00 00 00
00 00 AA E6
BE DF 97 5F
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.432Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x11000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
11 00 00 00
00 00 00 00
00 00 00 00
00 00 D5 2E
D4 FF 49 53
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.432Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x12000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
12 00 00 00
00 00 00 00
00 00 00 00
00 00 2B 4C
FA 97 CC 33
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.432Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x13000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
13 00 00 00
00 00 00 00
00 00 00 00
00 00 13 3B
9A 6A E9 BD
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.432Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x14000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
14 00 00 00
00 00 00 00
00 00 00 00
00 00 10 74
63 F8 C5 E8
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.432Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x15000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
15 00 00 00
00 00 00 00
00 00 00 00
00 00 57 53
6B CF 68 C9
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.433Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x16000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
16 00 00 00
00 00 00 00
00 00 00 00
00 00 C8 90
E9 D8 DC 81
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.433Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x17000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
17 00 00 00
00 00 00 00
00 00 00 00
00 00 6D 33
4E E7 87 FC
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.433Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x18000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
18 00 00 00
00 00 00 00
00 00 00 00
00 00 BD 40
8C FC 51 94
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.433Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x19000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
19 00 00 00
00 00 00 00
00 00 00 00
00 00 FB 12
84 9C 2E 62
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.433Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x1A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
1A 00 00 00
00 00 00 00
00 00 00 00
00 00 32 AF
A9 33 14 F5
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.433Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x1B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
1B 00 00 00
00 00 00 00
00 00 00 00
00 00 A6 36
62 9D 09 C1
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.433Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x1C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
1C 00 00 00
00 00 00 00
00 00 00 00
00 00 D9 92
C2 93 36 6A
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.433Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x1D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
1D 00 00 00
00 00 00 00
00 00 00 00
00 00 33 2A
8C AB 15 AD
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.433Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x1E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
1E 00 00 00
00 00 00 00
00 00 00 00
00 00 73 CB
BA FC 53 F7
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.433Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x1F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
1F 00 00 00
00 00 00 00
00 00 00 00
00 00 72 6A
11 C0 03 69
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.433Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x20000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
20 00 00 00
00 00 00 00
00 00 00 00
00 00 2D 86
76 61 6A 13
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.433Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x21000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
21 00 00 00
00 00 00 00
00 00 00 00
00 00 98 1C
CD 3B 85 54
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.433Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x22000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
22 00 00 00
00 00 00 00
00 00 00 00
00 00 A4 2A
8B 90 A4 D7
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.433Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x23000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
23 00 00 00
00 00 00 00
00 00 00 00
00 00 2D 6A
1F 09 0B 10
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.433Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x24000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
24 00 00 00
00 00 00 00
00 00 00 00
00 00 B3 5E
C4 34 D9 55
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.433Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x25000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
25 00 00 00
00 00 00 00
00 00 00 00
00 00 7D C4
0B C1 B1 0B
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.433Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x26000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
26 00 00 00
00 00 00 00
00 00 00 00
00 00 33 D5
7D 86 4F 5D
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.433Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x27000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
27 00 00 00
00 00 00 00
00 00 00 00
00 00 A8 AF
AB 2C 88 04
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.434Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x28000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
28 00 00 00
00 00 00 00
00 00 00 00
00 00 13 98
91 3C 45 F4
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.434Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x29000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
29 00 00 00
00 00 00 00
00 00 00 00
00 00 58 C4
25 07 E4 6D
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.434Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x2A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
2A 00 00 00
00 00 00 00
00 00 00 00
00 00 ED C8
B9 FA 9C F1
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.434Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x2B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
2B 00 00 00
00 00 00 00
00 00 00 00
00 00 C4 1D
14 17 B9 D2
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.434Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x2C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
2C 00 00 00
00 00 00 00
00 00 00 00
00 00 C7 25
D5 7E 2D C7
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.434Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x2D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
2D 00 00 00
00 00 00 00
00 00 00 00
00 00 23 3C
A8 0B CF A3
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.434Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x2E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
2E 00 00 00
00 00 00 00
00 00 00 00
00 00 44 B7
20 E8 CF 4E
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.434Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x2F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
2F 00 00 00
00 00 00 00
00 00 00 00
00 00 A4 83
91 83 D1 5B
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.434Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x30000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
30 00 00 00
00 00 00 00
00 00 00 00
00 00 E6 8A
E2 77 21 1B
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.434Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x31000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
31 00 00 00
00 00 00 00
00 00 00 00
00 00 83 3E
56 DF 30 B4
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.434Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x32000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
32 00 00 00
00 00 00 00
00 00 00 00
00 00 8F 34
80 A5 46 AD
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.434Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x33000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
33 00 00 00
00 00 00 00
00 00 00 00
00 00 06 53
88 27 02 20
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.434Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x34000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
34 00 00 00
00 00 00 00
00 00 00 00
00 00 88 7B
9E 84 0C 5A
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.434Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x35000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
35 00 00 00
00 00 00 00
00 00 00 00
00 00 67 6A
2A 13 75 9A
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.434Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x36000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
36 00 00 00
00 00 00 00
00 00 00 00
00 00 55 79
6F 67 FC FA
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.434Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x37000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
37 00 00 00
00 00 00 00
00 00 00 00
00 00 8A 96
96 42 6E 14
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.434Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x38000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
38 00 00 00
00 00 00 00
00 00 00 00
00 00 30 92
03 7F D2 C2
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.435Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x39000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
39 00 00 00
00 00 00 00
00 00 00 00
00 00 93 BB
5C 09 B0 5C
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.435Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x3A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
3A 00 00 00
00 00 00 00
00 00 00 00
00 00 3F 82
6A BB E0 50
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.435Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x3B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
3B 00 00 00
00 00 00 00
00 00 00 00
00 00 A1 2B
A8 8A 27 C1
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.435Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x3C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
3C 00 00 00
00 00 00 00
00 00 00 00
00 00 C7 53
9A 2D F0 B6
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.435Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x3D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
3D 00 00 00
00 00 00 00
00 00 00 00
00 00 F1 33
2E 04 E2 C6
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.435Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x3E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
3E 00 00 00
00 00 00 00
00 00 00 00
00 00 E6 77
23 7C DC D5
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.435Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x3F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
3F 00 00 00
00 00 00 00
00 00 00 00
00 00 F0 B7
E7 96 BD 0F
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.436Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x40000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
40 00 00 00
00 00 00 00
00 00 00 00
00 00 52 9E
04 59 1D A4
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.436Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x41000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
41 00 00 00
00 00 00 00
00 00 00 00
00 00 3A 5B
78 80 99 28
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.436Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x42000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
42 00 00 00
00 00 00 00
00 00 00 00
00 00 97 B0
63 39 60 32
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.436Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x43000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
43 00 00 00
00 00 00 00
00 00 00 00
00 00 08 ED
56 D6 A0 44
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.436Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x44000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
44 00 00 00
00 00 00 00
00 00 00 00
00 00 CF 81
65 05 10 2A
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.436Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2AA status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 AA
00 00 00 00
00 00 00 00
00 00 00 01

[ns_server:debug,2025-05-15T18:47:52.436Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x45000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
45 00 00 00
00 00 00 00
00 00 00 00
00 00 1F 55
41 BF 29 D4
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.436Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 682, stream opaque = 0x1000000
[ns_server:debug,2025-05-15T18:47:52.436Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2AB status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 AB
00 00 00 00
00 00 00 00
00 00 00 02

[ns_server:debug,2025-05-15T18:47:52.436Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x46000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
46 00 00 00
00 00 00 00
00 00 00 00
00 00 B7 89
62 07 5C AE
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.436Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 683, stream opaque = 0x2000000
[ns_server:debug,2025-05-15T18:47:52.436Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x47000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
47 00 00 00
00 00 00 00
00 00 00 00
00 00 C6 51
42 C0 AC FB
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.436Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2AC status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 AC
00 00 00 00
00 00 00 00
00 00 00 03

[ns_server:debug,2025-05-15T18:47:52.436Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x48000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
48 00 00 00
00 00 00 00
00 00 00 00
00 00 D9 64
F2 00 A7 64
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.437Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x49000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
49 00 00 00
00 00 00 00
00 00 00 00
00 00 60 2C
89 79 A0 50
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.437Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x4A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
4A 00 00 00
00 00 00 00
00 00 00 00
00 00 30 2A
3B 4C EC BD
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.437Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x4B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
4B 00 00 00
00 00 00 00
00 00 00 00
00 00 89 D8
D5 AF B9 9B
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.437Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x4C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
4C 00 00 00
00 00 00 00
00 00 00 00
00 00 DC 9A
0B FC 05 D3
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.437Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x4D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
4D 00 00 00
00 00 00 00
00 00 00 00
00 00 42 BC
CE 83 73 C0
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.437Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x4E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
4E 00 00 00
00 00 00 00
00 00 00 00
00 00 B3 C8
3B 3A F1 0A
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.437Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x4F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
4F 00 00 00
00 00 00 00
00 00 00 00
00 00 FA 67
D9 36 0E 43
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.437Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x50000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
50 00 00 00
00 00 00 00
00 00 00 00
00 00 B4 9F
A1 41 18 DA
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.437Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x51000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
51 00 00 00
00 00 00 00
00 00 00 00
00 00 C6 C1
B3 A3 3E 9B
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.437Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x52000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
52 00 00 00
00 00 00 00
00 00 00 00
00 00 8A 61
31 88 7D 2C
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.437Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x53000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
53 00 00 00
00 00 00 00
00 00 00 00
00 00 F2 DA
9B D2 DB 43
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.437Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 684, stream opaque = 0x3000000
[ns_server:debug,2025-05-15T18:47:52.437Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2AD status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 AD
00 00 00 00
00 00 00 00
00 00 00 04

[rebalance:debug,2025-05-15T18:47:52.437Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 685, stream opaque = 0x4000000
[ns_server:debug,2025-05-15T18:47:52.437Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2AE status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 AE
00 00 00 00
00 00 00 00
00 00 00 05

[ns_server:debug,2025-05-15T18:47:52.437Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x54000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
54 00 00 00
00 00 00 00
00 00 00 00
00 00 96 EB
99 71 E7 64
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.437Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 686, stream opaque = 0x5000000
[ns_server:debug,2025-05-15T18:47:52.438Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x55000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
55 00 00 00
00 00 00 00
00 00 00 00
00 00 B7 D0
F6 38 F0 17
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.438Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2AF status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 AF
00 00 00 00
00 00 00 00
00 00 00 08

[ns_server:debug,2025-05-15T18:47:52.438Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x56000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
56 00 00 00
00 00 00 00
00 00 00 00
00 00 BE A1
C5 26 CF C9
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.438Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 687, stream opaque = 0x8000000
[ns_server:debug,2025-05-15T18:47:52.438Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x57000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
57 00 00 00
00 00 00 00
00 00 00 00
00 00 49 18
FB 74 E1 3F
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.438Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2B0 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 B0
00 00 00 00
00 00 00 00
00 00 00 09

[ns_server:debug,2025-05-15T18:47:52.438Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x58000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
58 00 00 00
00 00 00 00
00 00 00 00
00 00 E5 5A
D1 3A E6 2D
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.438Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 688, stream opaque = 0x9000000
[ns_server:debug,2025-05-15T18:47:52.438Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2B1 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 B1
00 00 00 00
00 00 00 00
00 00 00 0A

[ns_server:debug,2025-05-15T18:47:52.438Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x59000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
59 00 00 00
00 00 00 00
00 00 00 00
00 00 1A 87
BD FF 67 6A
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.438Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 689, stream opaque = 0xA000000
[ns_server:debug,2025-05-15T18:47:52.438Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2B2 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 B2
00 00 00 00
00 00 00 00
00 00 00 0B

[rebalance:debug,2025-05-15T18:47:52.438Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 690, stream opaque = 0xB000000
[ns_server:debug,2025-05-15T18:47:52.439Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2B3 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 B3
00 00 00 00
00 00 00 00
00 00 00 0C

[rebalance:debug,2025-05-15T18:47:52.439Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 691, stream opaque = 0xC000000
[ns_server:debug,2025-05-15T18:47:52.439Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2B4 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 B4
00 00 00 00
00 00 00 00
00 00 00 0D

[rebalance:debug,2025-05-15T18:47:52.439Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 692, stream opaque = 0xD000000
[ns_server:debug,2025-05-15T18:47:52.439Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2B5 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 B5
00 00 00 00
00 00 00 00
00 00 00 0E

[rebalance:debug,2025-05-15T18:47:52.439Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 693, stream opaque = 0xE000000
[ns_server:debug,2025-05-15T18:47:52.439Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2B6 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 B6
00 00 00 00
00 00 00 00
00 00 00 0F

[rebalance:debug,2025-05-15T18:47:52.439Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 694, stream opaque = 0xF000000
[ns_server:debug,2025-05-15T18:47:52.439Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2B7 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 B7
00 00 00 00
00 00 00 00
00 00 00 10

[rebalance:debug,2025-05-15T18:47:52.439Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 695, stream opaque = 0x10000000
[ns_server:debug,2025-05-15T18:47:52.439Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x5A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
5A 00 00 00
00 00 00 00
00 00 00 00
00 00 8E 33
4A F4 93 F3
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.439Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2B8 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 B8
00 00 00 00
00 00 00 00
00 00 00 11

[rebalance:debug,2025-05-15T18:47:52.439Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 696, stream opaque = 0x11000000
[ns_server:debug,2025-05-15T18:47:52.439Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2B9 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 B9
00 00 00 00
00 00 00 00
00 00 00 12

[ns_server:debug,2025-05-15T18:47:52.439Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x5B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
5B 00 00 00
00 00 00 00
00 00 00 00
00 00 84 CD
C0 DB 99 23
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.439Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x5C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
5C 00 00 00
00 00 00 00
00 00 00 00
00 00 DB 2C
16 EE 75 45
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.439Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x5D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
5D 00 00 00
00 00 00 00
00 00 00 00
00 00 92 B7
97 D3 52 F3
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.439Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x5E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
5E 00 00 00
00 00 00 00
00 00 00 00
00 00 67 BA
E5 BC 5F 44
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.439Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 697, stream opaque = 0x12000000
[ns_server:debug,2025-05-15T18:47:52.439Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x5F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
5F 00 00 00
00 00 00 00
00 00 00 00
00 00 E5 A7
BD AB 1E 02
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.439Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2BA status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 BA
00 00 00 00
00 00 00 00
00 00 00 13

[rebalance:debug,2025-05-15T18:47:52.440Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 698, stream opaque = 0x13000000
[ns_server:debug,2025-05-15T18:47:52.440Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x60000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
60 00 00 00
00 00 00 00
00 00 00 00
00 00 1F F5
74 D5 F5 2F
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.440Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2BB status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 BB
00 00 00 00
00 00 00 00
00 00 00 14

[rebalance:debug,2025-05-15T18:47:52.440Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 699, stream opaque = 0x14000000
[ns_server:debug,2025-05-15T18:47:52.440Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2BC status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 BC
00 00 00 00
00 00 00 00
00 00 00 15

[rebalance:debug,2025-05-15T18:47:52.440Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 700, stream opaque = 0x15000000
[ns_server:debug,2025-05-15T18:47:52.440Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2BD status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 BD
00 00 00 00
00 00 00 00
00 00 00 16

[rebalance:debug,2025-05-15T18:47:52.440Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 701, stream opaque = 0x16000000
[ns_server:debug,2025-05-15T18:47:52.440Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2BE status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 BE
00 00 00 00
00 00 00 00
00 00 00 17

[rebalance:debug,2025-05-15T18:47:52.440Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 702, stream opaque = 0x17000000
[ns_server:debug,2025-05-15T18:47:52.440Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x61000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
61 00 00 00
00 00 00 00
00 00 00 00
00 00 7F FD
E6 04 47 11
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.440Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2BF status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 BF
00 00 00 00
00 00 00 00
00 00 00 18

[ns_server:debug,2025-05-15T18:47:52.440Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x62000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
62 00 00 00
00 00 00 00
00 00 00 00
00 00 A1 6F
56 E6 1F CE
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.440Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 703, stream opaque = 0x18000000
[ns_server:debug,2025-05-15T18:47:52.440Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x63000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
63 00 00 00
00 00 00 00
00 00 00 00
00 00 1A B1
19 8E FD 9B
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.440Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2C0 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 C0
00 00 00 00
00 00 00 00
00 00 00 19

[rebalance:debug,2025-05-15T18:47:52.440Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 704, stream opaque = 0x19000000
[ns_server:debug,2025-05-15T18:47:52.440Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x64000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
64 00 00 00
00 00 00 00
00 00 00 00
00 00 E5 CC
2B 40 4A 56
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.440Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x65000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
65 00 00 00
00 00 00 00
00 00 00 00
00 00 0B 06
C3 43 63 76
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.440Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2C1 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 C1
00 00 00 00
00 00 00 00
00 00 00 1A

[rebalance:debug,2025-05-15T18:47:52.440Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 705, stream opaque = 0x1A000000
[ns_server:debug,2025-05-15T18:47:52.440Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x66000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
66 00 00 00
00 00 00 00
00 00 00 00
00 00 27 91
05 16 DF 42
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.440Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2C2 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 C2
00 00 00 00
00 00 00 00
00 00 00 1B

[rebalance:debug,2025-05-15T18:47:52.440Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 706, stream opaque = 0x1B000000
[ns_server:debug,2025-05-15T18:47:52.440Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x67000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
67 00 00 00
00 00 00 00
00 00 00 00
00 00 F0 E4
9A F6 99 E1
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.441Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x68000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
68 00 00 00
00 00 00 00
00 00 00 00
00 00 54 08
53 01 5A C6
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.441Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2C3 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 C3
00 00 00 00
00 00 00 00
00 00 00 1C

[rebalance:debug,2025-05-15T18:47:52.441Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 707, stream opaque = 0x1C000000
[ns_server:debug,2025-05-15T18:47:52.441Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x69000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
69 00 00 00
00 00 00 00
00 00 00 00
00 00 F2 5E
C6 6F 47 7E
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.441Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x6A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
6A 00 00 00
00 00 00 00
00 00 00 00
00 00 EE 5D
C2 51 DC 55
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.441Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x6B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
6B 00 00 00
00 00 00 00
00 00 00 00
00 00 2B 3F
4A 7E 9B 4A
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.441Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x6C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
6C 00 00 00
00 00 00 00
00 00 00 00
00 00 BE D7
19 80 33 2D
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.441Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2C4 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 C4
00 00 00 00
00 00 00 00
00 00 00 1D

[ns_server:debug,2025-05-15T18:47:52.441Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x6D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
6D 00 00 00
00 00 00 00
00 00 00 00
00 00 65 E3
64 6D F7 90
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.441Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 708, stream opaque = 0x1D000000
[ns_server:debug,2025-05-15T18:47:52.441Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x6E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
6E 00 00 00
00 00 00 00
00 00 00 00
00 00 D4 B6
7C 03 5A A0
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.441Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2C5 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 C5
00 00 00 00
00 00 00 00
00 00 00 1E

[rebalance:debug,2025-05-15T18:47:52.441Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 709, stream opaque = 0x1E000000
[ns_server:debug,2025-05-15T18:47:52.441Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x6F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
6F 00 00 00
00 00 00 00
00 00 00 00
00 00 4E 12
8E 50 B0 30
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.441Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2C6 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 C6
00 00 00 00
00 00 00 00
00 00 00 1F

[rebalance:debug,2025-05-15T18:47:52.441Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 710, stream opaque = 0x1F000000
[ns_server:debug,2025-05-15T18:47:52.441Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x70000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
70 00 00 00
00 00 00 00
00 00 00 00
00 00 9B E2
16 5D 04 9F
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.441Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2C7 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 C7
00 00 00 00
00 00 00 00
00 00 00 20

[rebalance:debug,2025-05-15T18:47:52.441Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 711, stream opaque = 0x20000000
[ns_server:debug,2025-05-15T18:47:52.442Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x71000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
71 00 00 00
00 00 00 00
00 00 00 00
00 00 3F F2
A1 0A ED D1
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.442Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2C8 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 C8
00 00 00 00
00 00 00 00
00 00 00 21

[ns_server:debug,2025-05-15T18:47:52.442Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x72000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
72 00 00 00
00 00 00 00
00 00 00 00
00 00 0B 73
8A 6A 56 66
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.442Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 712, stream opaque = 0x21000000
[ns_server:debug,2025-05-15T18:47:52.442Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2C9 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 C9
00 00 00 00
00 00 00 00
00 00 00 22

[rebalance:debug,2025-05-15T18:47:52.442Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 713, stream opaque = 0x22000000
[ns_server:debug,2025-05-15T18:47:52.442Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x73000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
73 00 00 00
00 00 00 00
00 00 00 00
00 00 81 6F
86 5A 55 54
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.442Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2CA status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 CA
00 00 00 00
00 00 00 00
00 00 00 23

[rebalance:debug,2025-05-15T18:47:52.442Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 714, stream opaque = 0x23000000
[ns_server:debug,2025-05-15T18:47:52.442Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2CB status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 CB
00 00 00 00
00 00 00 00
00 00 00 24

[rebalance:debug,2025-05-15T18:47:52.442Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 715, stream opaque = 0x24000000
[ns_server:debug,2025-05-15T18:47:52.442Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2CC status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 CC
00 00 00 00
00 00 00 00
00 00 00 25

[rebalance:debug,2025-05-15T18:47:52.442Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 716, stream opaque = 0x25000000
[ns_server:debug,2025-05-15T18:47:52.442Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2CD status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 CD
00 00 00 00
00 00 00 00
00 00 00 26

[rebalance:debug,2025-05-15T18:47:52.442Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 717, stream opaque = 0x26000000
[ns_server:debug,2025-05-15T18:47:52.442Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2CE status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 CE
00 00 00 00
00 00 00 00
00 00 00 27

[rebalance:debug,2025-05-15T18:47:52.442Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 718, stream opaque = 0x27000000
[ns_server:debug,2025-05-15T18:47:52.442Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2CF status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 CF
00 00 00 00
00 00 00 00
00 00 00 28

[rebalance:debug,2025-05-15T18:47:52.443Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 719, stream opaque = 0x28000000
[ns_server:debug,2025-05-15T18:47:52.443Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2D0 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 D0
00 00 00 00
00 00 00 00
00 00 00 29

[rebalance:debug,2025-05-15T18:47:52.443Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 720, stream opaque = 0x29000000
[ns_server:debug,2025-05-15T18:47:52.443Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2D1 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 D1
00 00 00 00
00 00 00 00
00 00 00 2A

[rebalance:debug,2025-05-15T18:47:52.443Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 721, stream opaque = 0x2A000000
[ns_server:debug,2025-05-15T18:47:52.443Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x74000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
74 00 00 00
00 00 00 00
00 00 00 00
00 00 93 E4
8B 82 A3 F0
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.443Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2D2 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 D2
00 00 00 00
00 00 00 00
00 00 00 2B

[rebalance:debug,2025-05-15T18:47:52.443Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 722, stream opaque = 0x2B000000
[ns_server:debug,2025-05-15T18:47:52.443Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x75000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
75 00 00 00
00 00 00 00
00 00 00 00
00 00 F2 3C
AD 4A DA FE
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.445Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x76000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
76 00 00 00
00 00 00 00
00 00 00 00
00 00 A4 B4
4D 96 1E F8
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.445Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2D3 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 D3
00 00 00 00
00 00 00 00
00 00 00 2C

[ns_server:debug,2025-05-15T18:47:52.445Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x77000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
77 00 00 00
00 00 00 00
00 00 00 00
00 00 47 1D
C2 02 23 45
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.445Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x78000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
78 00 00 00
00 00 00 00
00 00 00 00
00 00 D7 35
DF 00 89 85
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.445Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x79000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
79 00 00 00
00 00 00 00
00 00 00 00
00 00 6E 88
AE AF 6C 76
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.445Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x7A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
7A 00 00 00
00 00 00 00
00 00 00 00
00 00 41 76
AD 38 0E 9E
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.445Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 723, stream opaque = 0x2C000000
[ns_server:debug,2025-05-15T18:47:52.445Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x7B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
7B 00 00 00
00 00 00 00
00 00 00 00
00 00 4B DC
10 5E D3 D2
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.445Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2D4 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 D4
00 00 00 00
00 00 00 00
00 00 00 2D

[ns_server:debug,2025-05-15T18:47:52.445Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x7C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
7C 00 00 00
00 00 00 00
00 00 00 00
00 00 D9 3C
5C 48 3A 75
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.445Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 724, stream opaque = 0x2D000000
[ns_server:debug,2025-05-15T18:47:52.445Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x7D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
7D 00 00 00
00 00 00 00
00 00 00 00
00 00 CE DD
F8 7D 6B 5A
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.445Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2D5 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 D5
00 00 00 00
00 00 00 00
00 00 00 2E

[rebalance:debug,2025-05-15T18:47:52.445Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 725, stream opaque = 0x2E000000
[ns_server:debug,2025-05-15T18:47:52.445Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2D6 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 D6
00 00 00 00
00 00 00 00
00 00 00 2F

[rebalance:debug,2025-05-15T18:47:52.445Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 726, stream opaque = 0x2F000000
[ns_server:debug,2025-05-15T18:47:52.445Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2D7 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 D7
00 00 00 00
00 00 00 00
00 00 00 30

[rebalance:debug,2025-05-15T18:47:52.445Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 727, stream opaque = 0x30000000
[ns_server:debug,2025-05-15T18:47:52.445Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x7E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
7E 00 00 00
00 00 00 00
00 00 00 00
00 00 E8 A0
60 CF 47 D7
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.445Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2D8 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 D8
00 00 00 00
00 00 00 00
00 00 00 31

[rebalance:debug,2025-05-15T18:47:52.445Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 728, stream opaque = 0x31000000
[ns_server:debug,2025-05-15T18:47:52.445Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2D9 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 D9
00 00 00 00
00 00 00 00
00 00 00 32

[rebalance:debug,2025-05-15T18:47:52.445Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 729, stream opaque = 0x32000000
[ns_server:debug,2025-05-15T18:47:52.446Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x7F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
7F 00 00 00
00 00 00 00
00 00 00 00
00 00 AE 83
38 2C 3A 6D
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.446Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2DA status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 DA
00 00 00 00
00 00 00 00
00 00 00 33

[ns_server:debug,2025-05-15T18:47:52.446Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x80000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
80 00 00 00
00 00 00 00
00 00 00 00
00 00 1B E3
64 EF BB 7A
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.446Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 730, stream opaque = 0x33000000
[ns_server:debug,2025-05-15T18:47:52.446Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2DB status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 DB
00 00 00 00
00 00 00 00
00 00 00 34

[ns_server:debug,2025-05-15T18:47:52.446Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x81000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
81 00 00 00
00 00 00 00
00 00 00 00
00 00 DE F0
F9 EF 46 AE
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.446Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 731, stream opaque = 0x34000000
[ns_server:debug,2025-05-15T18:47:52.446Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2DC status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 DC
00 00 00 00
00 00 00 00
00 00 00 35

[ns_server:debug,2025-05-15T18:47:52.446Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x82000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
82 00 00 00
00 00 00 00
00 00 00 00
00 00 61 38
27 8B E9 F5
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.446Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 732, stream opaque = 0x35000000
[ns_server:debug,2025-05-15T18:47:52.446Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x83000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
83 00 00 00
00 00 00 00
00 00 00 00
00 00 0C 3A
97 48 CF AF
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.446Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2DD status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 DD
00 00 00 00
00 00 00 00
00 00 00 36

[rebalance:debug,2025-05-15T18:47:52.446Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 733, stream opaque = 0x36000000
[ns_server:debug,2025-05-15T18:47:52.446Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x84000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
84 00 00 00
00 00 00 00
00 00 00 00
00 00 B8 F9
3F 39 7F 75
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.446Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2DE status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 DE
00 00 00 00
00 00 00 00
00 00 00 37

[rebalance:debug,2025-05-15T18:47:52.446Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 734, stream opaque = 0x37000000
[ns_server:debug,2025-05-15T18:47:52.446Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x85000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
85 00 00 00
00 00 00 00
00 00 00 00
00 00 C1 05
75 1D E4 AB
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.446Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2DF status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 DF
00 00 00 00
00 00 00 00
00 00 00 38

[ns_server:debug,2025-05-15T18:47:52.446Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x86000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
86 00 00 00
00 00 00 00
00 00 00 00
00 00 10 7A
A7 71 6B 65
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.446Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x87000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
87 00 00 00
00 00 00 00
00 00 00 00
00 00 86 4F
77 BD 62 2B
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.447Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x88000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
88 00 00 00
00 00 00 00
00 00 00 00
00 00 BE 79
7C B2 EF 6A
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.447Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x89000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
89 00 00 00
00 00 00 00
00 00 00 00
00 00 AA 55
D4 FF 8E 4F
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.447Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x8A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
8A 00 00 00
00 00 00 00
00 00 00 00
00 00 6A 91
B3 A1 F5 41
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.447Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x8B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
8B 00 00 00
00 00 00 00
00 00 00 00
00 00 6A 31
E2 2D F2 77
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.447Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x8C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
8C 00 00 00
00 00 00 00
00 00 00 00
00 00 7E B3
D6 4F F4 A9
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.447Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x8D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
8D 00 00 00
00 00 00 00
00 00 00 00
00 00 71 A9
57 F9 4A F3
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.447Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x8E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
8E 00 00 00
00 00 00 00
00 00 00 00
00 00 AE DB
87 56 73 58
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.447Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x8F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
8F 00 00 00
00 00 00 00
00 00 00 00
00 00 2F D5
1F 40 23 04
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.447Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x90000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
90 00 00 00
00 00 00 00
00 00 00 00
00 00 09 EB
EA 21 59 97
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.447Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x91000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
91 00 00 00
00 00 00 00
00 00 00 00
00 00 EF B8
D1 3A 3B 30
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.447Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x92000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
92 00 00 00
00 00 00 00
00 00 00 00
00 00 69 79
C8 44 86 01
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.447Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x93000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
93 00 00 00
00 00 00 00
00 00 00 00
00 00 E4 F4
4F 1F 18 91
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.447Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x94000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
94 00 00 00
00 00 00 00
00 00 00 00
00 00 36 45
1E D6 9A 9B
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.448Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x95000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
95 00 00 00
00 00 00 00
00 00 00 00
00 00 40 4F
E1 79 1A A1
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.448Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x96000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
96 00 00 00
00 00 00 00
00 00 00 00
00 00 E9 33
75 22 95 F9
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.448Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x97000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
97 00 00 00
00 00 00 00
00 00 00 00
00 00 96 D0
6A 6A 21 4F
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.448Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x98000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
98 00 00 00
00 00 00 00
00 00 00 00
00 00 F1 DD
98 8A 49 B9
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.448Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x99000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
99 00 00 00
00 00 00 00
00 00 00 00
00 00 6D 34
A1 8B 50 79
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.448Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x9A000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
9A 00 00 00
00 00 00 00
00 00 00 00
00 00 19 89
72 D4 EC A8
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.448Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x9B000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
9B 00 00 00
00 00 00 00
00 00 00 00
00 00 75 E7
AD FC 60 17
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.448Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x9C000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
9C 00 00 00
00 00 00 00
00 00 00 00
00 00 EC C4
3D B5 A6 84
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.448Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x9D000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
9D 00 00 00
00 00 00 00
00 00 00 00
00 00 0C CB
C1 06 F6 C3
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.448Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x9E000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
9E 00 00 00
00 00 00 00
00 00 00 00
00 00 58 A9
C7 88 02 94
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.448Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0x9F000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
9F 00 00 00
00 00 00 00
00 00 00 00
00 00 D6 6A
D6 FB AD 4D
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.448Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA0000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A0 00 00 00
00 00 00 00
00 00 00 00
00 00 B2 C0
79 2C F0 3D
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.448Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA1000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A1 00 00 00
00 00 00 00
00 00 00 00
00 00 16 68
C3 6E 32 EE
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.448Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 735, stream opaque = 0x38000000
[ns_server:debug,2025-05-15T18:47:52.448Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA2000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A2 00 00 00
00 00 00 00
00 00 00 00
00 00 DA CC
E3 94 81 56
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.449Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA3000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A3 00 00 00
00 00 00 00
00 00 00 00
00 00 FA 5A
DF AA 3C E4
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.449Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2E0 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 E0
00 00 00 00
00 00 00 00
00 00 00 39

[ns_server:debug,2025-05-15T18:47:52.449Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA4000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A4 00 00 00
00 00 00 00
00 00 00 00
00 00 1B 85
18 A2 98 98
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.449Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 736, stream opaque = 0x39000000
[ns_server:debug,2025-05-15T18:47:52.449Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2E1 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 E1
00 00 00 00
00 00 00 00
00 00 00 3A

[ns_server:debug,2025-05-15T18:47:52.449Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA5000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A5 00 00 00
00 00 00 00
00 00 00 00
00 00 33 19
23 C1 65 84
00 00 00 00
00 00 00 00

[rebalance:debug,2025-05-15T18:47:52.449Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 737, stream opaque = 0x3A000000
[ns_server:debug,2025-05-15T18:47:52.449Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2E2 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 E2
00 00 00 00
00 00 00 00
00 00 00 3B

[rebalance:debug,2025-05-15T18:47:52.449Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 738, stream opaque = 0x3B000000
[ns_server:debug,2025-05-15T18:47:52.449Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA6000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A6 00 00 00
00 00 00 00
00 00 00 00
00 00 91 B8
45 31 23 D1
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.449Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2E3 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 E3
00 00 00 00
00 00 00 00
00 00 00 3C

[rebalance:debug,2025-05-15T18:47:52.449Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 739, stream opaque = 0x3C000000
[ns_server:debug,2025-05-15T18:47:52.449Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA7000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A7 00 00 00
00 00 00 00
00 00 00 00
00 00 BD 53
7E 5D C9 B9
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.449Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2E4 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 E4
00 00 00 00
00 00 00 00
00 00 00 3D

[rebalance:debug,2025-05-15T18:47:52.449Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 740, stream opaque = 0x3D000000
[ns_server:debug,2025-05-15T18:47:52.449Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA8000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A8 00 00 00
00 00 00 00
00 00 00 00
00 00 A3 B5
63 87 90 96
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.449Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2E5 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 E5
00 00 00 00
00 00 00 00
00 00 00 3E

[rebalance:debug,2025-05-15T18:47:52.449Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 741, stream opaque = 0x3E000000
[ns_server:debug,2025-05-15T18:47:52.449Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xA9000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
A9 00 00 00
00 00 00 00
00 00 00 00
00 00 3E 3F
70 54 72 CE
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.449Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2E6 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 E6
00 00 00 00
00 00 00 00
00 00 00 3F

[rebalance:debug,2025-05-15T18:47:52.449Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 742, stream opaque = 0x3F000000
[ns_server:debug,2025-05-15T18:47:52.449Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xAA000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
AA 00 00 00
00 00 00 00
00 00 00 00
00 00 E7 C4
0B 19 91 BB
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.449Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2E7 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 E7
00 00 00 00
00 00 00 00
00 00 00 40

[rebalance:debug,2025-05-15T18:47:52.449Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 743, stream opaque = 0x40000000
[ns_server:debug,2025-05-15T18:47:52.449Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xAB000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
AB 00 00 00
00 00 00 00
00 00 00 00
00 00 E8 09
10 22 9C 96
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.449Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2E8 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 E8
00 00 00 00
00 00 00 00
00 00 00 41

[rebalance:debug,2025-05-15T18:47:52.449Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 744, stream opaque = 0x41000000
[ns_server:debug,2025-05-15T18:47:52.449Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xAC000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
AC 00 00 00
00 00 00 00
00 00 00 00
00 00 23 9F
45 CB CE 45
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.449Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2E9 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 E9
00 00 00 00
00 00 00 00
00 00 00 42

[rebalance:debug,2025-05-15T18:47:52.451Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 745, stream opaque = 0x42000000
[ns_server:debug,2025-05-15T18:47:52.451Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2EA status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 EA
00 00 00 00
00 00 00 00
00 00 00 43

[rebalance:debug,2025-05-15T18:47:52.451Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 746, stream opaque = 0x43000000
[ns_server:debug,2025-05-15T18:47:52.451Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2EB status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 EB
00 00 00 00
00 00 00 00
00 00 00 44

[rebalance:debug,2025-05-15T18:47:52.451Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 747, stream opaque = 0x44000000
[ns_server:debug,2025-05-15T18:47:52.451Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2EC status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 EC
00 00 00 00
00 00 00 00
00 00 00 45

[rebalance:debug,2025-05-15T18:47:52.451Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 748, stream opaque = 0x45000000
[ns_server:debug,2025-05-15T18:47:52.451Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2ED status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 ED
00 00 00 00
00 00 00 00
00 00 00 46

[rebalance:debug,2025-05-15T18:47:52.451Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 749, stream opaque = 0x46000000
[ns_server:debug,2025-05-15T18:47:52.451Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2EE status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 EE
00 00 00 00
00 00 00 00
00 00 00 47

[rebalance:debug,2025-05-15T18:47:52.451Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 750, stream opaque = 0x47000000
[ns_server:debug,2025-05-15T18:47:52.451Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2EF status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 EF
00 00 00 00
00 00 00 00
00 00 00 48

[rebalance:debug,2025-05-15T18:47:52.451Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 751, stream opaque = 0x48000000
[ns_server:debug,2025-05-15T18:47:52.451Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2F0 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 F0
00 00 00 00
00 00 00 00
00 00 00 49

[rebalance:debug,2025-05-15T18:47:52.451Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 752, stream opaque = 0x49000000
[ns_server:debug,2025-05-15T18:47:52.451Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2F1 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 F1
00 00 00 00
00 00 00 00
00 00 00 4A

[rebalance:debug,2025-05-15T18:47:52.451Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 753, stream opaque = 0x4A000000
[ns_server:debug,2025-05-15T18:47:52.451Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2F2 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 F2
00 00 00 00
00 00 00 00
00 00 00 4B

[rebalance:debug,2025-05-15T18:47:52.451Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 754, stream opaque = 0x4B000000
[ns_server:debug,2025-05-15T18:47:52.451Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2F3 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 F3
00 00 00 00
00 00 00 00
00 00 00 4C

[rebalance:debug,2025-05-15T18:47:52.452Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 755, stream opaque = 0x4C000000
[ns_server:debug,2025-05-15T18:47:52.452Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2F4 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 F4
00 00 00 00
00 00 00 00
00 00 00 4D

[rebalance:debug,2025-05-15T18:47:52.452Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 756, stream opaque = 0x4D000000
[ns_server:debug,2025-05-15T18:47:52.452Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2F5 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 F5
00 00 00 00
00 00 00 00
00 00 00 4E

[rebalance:debug,2025-05-15T18:47:52.452Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 757, stream opaque = 0x4E000000
[ns_server:debug,2025-05-15T18:47:52.452Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2F6 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 F6
00 00 00 00
00 00 00 00
00 00 00 4F

[rebalance:debug,2025-05-15T18:47:52.452Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 758, stream opaque = 0x4F000000
[ns_server:debug,2025-05-15T18:47:52.452Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2F7 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 F7
00 00 00 00
00 00 00 00
00 00 00 50

[rebalance:debug,2025-05-15T18:47:52.452Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 759, stream opaque = 0x50000000
[ns_server:debug,2025-05-15T18:47:52.452Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2F8 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 F8
00 00 00 00
00 00 00 00
00 00 00 51

[rebalance:debug,2025-05-15T18:47:52.452Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 760, stream opaque = 0x51000000
[ns_server:debug,2025-05-15T18:47:52.452Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2F9 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 F9
00 00 00 00
00 00 00 00
00 00 00 52

[rebalance:debug,2025-05-15T18:47:52.453Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 761, stream opaque = 0x52000000
[ns_server:debug,2025-05-15T18:47:52.453Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2FA status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 FA
00 00 00 00
00 00 00 00
00 00 00 53

[rebalance:debug,2025-05-15T18:47:52.453Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 762, stream opaque = 0x53000000
[ns_server:debug,2025-05-15T18:47:52.453Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2FB status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 FB
00 00 00 00
00 00 00 00
00 00 00 54

[rebalance:debug,2025-05-15T18:47:52.453Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 763, stream opaque = 0x54000000
[ns_server:debug,2025-05-15T18:47:52.453Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2FC status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 FC
00 00 00 00
00 00 00 00
00 00 00 55

[rebalance:debug,2025-05-15T18:47:52.453Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 764, stream opaque = 0x55000000
[ns_server:debug,2025-05-15T18:47:52.453Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2FD status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 FD
00 00 00 00
00 00 00 00
00 00 00 56

[rebalance:debug,2025-05-15T18:47:52.453Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 765, stream opaque = 0x56000000
[ns_server:debug,2025-05-15T18:47:52.453Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2FE status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 FE
00 00 00 00
00 00 00 00
00 00 00 57

[rebalance:debug,2025-05-15T18:47:52.453Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 766, stream opaque = 0x57000000
[ns_server:debug,2025-05-15T18:47:52.453Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x2FF status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 02 FF
00 00 00 00
00 00 00 00
00 00 00 58

[rebalance:debug,2025-05-15T18:47:52.453Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 767, stream opaque = 0x58000000
[ns_server:debug,2025-05-15T18:47:52.453Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x300 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 00
00 00 00 00
00 00 00 00
00 00 00 59

[rebalance:debug,2025-05-15T18:47:52.453Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 768, stream opaque = 0x59000000
[ns_server:debug,2025-05-15T18:47:52.453Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x301 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 01
00 00 00 00
00 00 00 00
00 00 00 5A

[rebalance:debug,2025-05-15T18:47:52.453Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 769, stream opaque = 0x5A000000
[ns_server:debug,2025-05-15T18:47:52.454Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x302 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 02
00 00 00 00
00 00 00 00
00 00 00 5B

[rebalance:debug,2025-05-15T18:47:52.454Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 770, stream opaque = 0x5B000000
[ns_server:debug,2025-05-15T18:47:52.454Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x303 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 03
00 00 00 00
00 00 00 00
00 00 00 5C

[rebalance:debug,2025-05-15T18:47:52.454Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 771, stream opaque = 0x5C000000
[ns_server:debug,2025-05-15T18:47:52.454Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x304 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 04
00 00 00 00
00 00 00 00
00 00 00 5D

[rebalance:debug,2025-05-15T18:47:52.454Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 772, stream opaque = 0x5D000000
[ns_server:debug,2025-05-15T18:47:52.454Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x305 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 05
00 00 00 00
00 00 00 00
00 00 00 5E

[rebalance:debug,2025-05-15T18:47:52.454Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 773, stream opaque = 0x5E000000
[ns_server:debug,2025-05-15T18:47:52.454Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x306 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 06
00 00 00 00
00 00 00 00
00 00 00 5F

[rebalance:debug,2025-05-15T18:47:52.454Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 774, stream opaque = 0x5F000000
[ns_server:debug,2025-05-15T18:47:52.454Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x307 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 07
00 00 00 00
00 00 00 00
00 00 00 60

[rebalance:debug,2025-05-15T18:47:52.454Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 775, stream opaque = 0x60000000
[ns_server:debug,2025-05-15T18:47:52.454Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x308 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 08
00 00 00 00
00 00 00 00
00 00 00 61

[rebalance:debug,2025-05-15T18:47:52.454Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 776, stream opaque = 0x61000000
[ns_server:debug,2025-05-15T18:47:52.454Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x309 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 09
00 00 00 00
00 00 00 00
00 00 00 62

[rebalance:debug,2025-05-15T18:47:52.454Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 777, stream opaque = 0x62000000
[ns_server:debug,2025-05-15T18:47:52.454Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x30A status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 0A
00 00 00 00
00 00 00 00
00 00 00 63

[rebalance:debug,2025-05-15T18:47:52.454Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 778, stream opaque = 0x63000000
[ns_server:debug,2025-05-15T18:47:52.454Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x30B status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 0B
00 00 00 00
00 00 00 00
00 00 00 64

[rebalance:debug,2025-05-15T18:47:52.456Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 779, stream opaque = 0x64000000
[ns_server:debug,2025-05-15T18:47:52.456Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x30C status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 0C
00 00 00 00
00 00 00 00
00 00 00 65

[rebalance:debug,2025-05-15T18:47:52.456Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 780, stream opaque = 0x65000000
[ns_server:debug,2025-05-15T18:47:52.456Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x30D status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 0D
00 00 00 00
00 00 00 00
00 00 00 66

[rebalance:debug,2025-05-15T18:47:52.456Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 781, stream opaque = 0x66000000
[ns_server:debug,2025-05-15T18:47:52.456Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x30E status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 0E
00 00 00 00
00 00 00 00
00 00 00 67

[rebalance:debug,2025-05-15T18:47:52.456Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 782, stream opaque = 0x67000000
[ns_server:debug,2025-05-15T18:47:52.456Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x30F status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 0F
00 00 00 00
00 00 00 00
00 00 00 68

[rebalance:debug,2025-05-15T18:47:52.456Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 783, stream opaque = 0x68000000
[ns_server:debug,2025-05-15T18:47:52.456Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x310 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 10
00 00 00 00
00 00 00 00
00 00 00 69

[rebalance:debug,2025-05-15T18:47:52.456Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 784, stream opaque = 0x69000000
[ns_server:debug,2025-05-15T18:47:52.456Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x311 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 11
00 00 00 00
00 00 00 00
00 00 00 6A

[rebalance:debug,2025-05-15T18:47:52.456Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 785, stream opaque = 0x6A000000
[ns_server:debug,2025-05-15T18:47:52.456Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x312 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 12
00 00 00 00
00 00 00 00
00 00 00 6B

[rebalance:debug,2025-05-15T18:47:52.456Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 786, stream opaque = 0x6B000000
[ns_server:debug,2025-05-15T18:47:52.456Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x313 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 13
00 00 00 00
00 00 00 00
00 00 00 6C

[rebalance:debug,2025-05-15T18:47:52.456Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 787, stream opaque = 0x6C000000
[ns_server:debug,2025-05-15T18:47:52.456Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x314 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 14
00 00 00 00
00 00 00 00
00 00 00 6D

[rebalance:debug,2025-05-15T18:47:52.456Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 788, stream opaque = 0x6D000000
[ns_server:debug,2025-05-15T18:47:52.456Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x315 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 15
00 00 00 00
00 00 00 00
00 00 00 6E

[rebalance:debug,2025-05-15T18:47:52.456Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 789, stream opaque = 0x6E000000
[ns_server:debug,2025-05-15T18:47:52.456Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x316 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 16
00 00 00 00
00 00 00 00
00 00 00 6F

[rebalance:debug,2025-05-15T18:47:52.456Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 790, stream opaque = 0x6F000000
[ns_server:debug,2025-05-15T18:47:52.456Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x317 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 17
00 00 00 00
00 00 00 00
00 00 00 70

[rebalance:debug,2025-05-15T18:47:52.456Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 791, stream opaque = 0x70000000
[ns_server:debug,2025-05-15T18:47:52.457Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x318 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 18
00 00 00 00
00 00 00 00
00 00 00 71

[rebalance:debug,2025-05-15T18:47:52.457Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 792, stream opaque = 0x71000000
[ns_server:debug,2025-05-15T18:47:52.457Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x319 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 19
00 00 00 00
00 00 00 00
00 00 00 72

[rebalance:debug,2025-05-15T18:47:52.457Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 793, stream opaque = 0x72000000
[ns_server:debug,2025-05-15T18:47:52.457Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x31A status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 1A
00 00 00 00
00 00 00 00
00 00 00 73

[rebalance:debug,2025-05-15T18:47:52.457Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 794, stream opaque = 0x73000000
[ns_server:debug,2025-05-15T18:47:52.449Z,ns_1@172.19.0.4:<0.4471.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x53 (dcp_stream_req) vbucket = 0 opaque = 0xAD000000 status = 0x0 (success)
81 53 00 00
00 00 00 00
00 00 00 10
AD 00 00 00
00 00 00 00
00 00 00 00
00 00 24 66
36 4D D8 03
00 00 00 00
00 00 00 00

[ns_server:debug,2025-05-15T18:47:52.465Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x31B status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 1B
00 00 00 00
00 00 00 00
00 00 00 74

[rebalance:debug,2025-05-15T18:47:52.465Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 795, stream opaque = 0x74000000
[ns_server:debug,2025-05-15T18:47:52.465Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x31C status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 1C
00 00 00 00
00 00 00 00
00 00 00 75

[rebalance:debug,2025-05-15T18:47:52.465Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 796, stream opaque = 0x75000000
[ns_server:debug,2025-05-15T18:47:52.465Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x31D status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 1D
00 00 00 00
00 00 00 00
00 00 00 76

[rebalance:debug,2025-05-15T18:47:52.466Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 797, stream opaque = 0x76000000
[ns_server:debug,2025-05-15T18:47:52.466Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x31E status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 1E
00 00 00 00
00 00 00 00
00 00 00 77

[rebalance:debug,2025-05-15T18:47:52.466Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 798, stream opaque = 0x77000000
[ns_server:debug,2025-05-15T18:47:52.466Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x31F status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 1F
00 00 00 00
00 00 00 00
00 00 00 78

[rebalance:debug,2025-05-15T18:47:52.466Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 799, stream opaque = 0x78000000
[ns_server:debug,2025-05-15T18:47:52.466Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x320 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 20
00 00 00 00
00 00 00 00
00 00 00 79

[rebalance:debug,2025-05-15T18:47:52.466Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 800, stream opaque = 0x79000000
[ns_server:debug,2025-05-15T18:47:52.466Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x321 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 21
00 00 00 00
00 00 00 00
00 00 00 7A

[rebalance:debug,2025-05-15T18:47:52.466Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 801, stream opaque = 0x7A000000
[ns_server:debug,2025-05-15T18:47:52.467Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x322 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 22
00 00 00 00
00 00 00 00
00 00 00 7B

[rebalance:debug,2025-05-15T18:47:52.467Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 802, stream opaque = 0x7B000000
[ns_server:debug,2025-05-15T18:47:52.467Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x323 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 23
00 00 00 00
00 00 00 00
00 00 00 7C

[rebalance:debug,2025-05-15T18:47:52.467Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 803, stream opaque = 0x7C000000
[ns_server:debug,2025-05-15T18:47:52.467Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x324 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 24
00 00 00 00
00 00 00 00
00 00 00 7D

[rebalance:debug,2025-05-15T18:47:52.467Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 804, stream opaque = 0x7D000000
[ns_server:debug,2025-05-15T18:47:52.467Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x325 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 25
00 00 00 00
00 00 00 00
00 00 00 7E

[rebalance:debug,2025-05-15T18:47:52.467Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 805, stream opaque = 0x7E000000
[ns_server:debug,2025-05-15T18:47:52.467Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x326 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 26
00 00 00 00
00 00 00 00
00 00 00 7F

[rebalance:debug,2025-05-15T18:47:52.467Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 806, stream opaque = 0x7F000000
[ns_server:debug,2025-05-15T18:47:52.467Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x327 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 27
00 00 00 00
00 00 00 00
00 00 00 80

[rebalance:debug,2025-05-15T18:47:52.467Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 807, stream opaque = 0x80000000
[ns_server:debug,2025-05-15T18:47:52.467Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x328 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 28
00 00 00 00
00 00 00 00
00 00 00 81

[rebalance:debug,2025-05-15T18:47:52.468Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 808, stream opaque = 0x81000000
[ns_server:debug,2025-05-15T18:47:52.468Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x329 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 29
00 00 00 00
00 00 00 00
00 00 00 82

[rebalance:debug,2025-05-15T18:47:52.468Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 809, stream opaque = 0x82000000
[ns_server:debug,2025-05-15T18:47:52.468Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x32A status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 2A
00 00 00 00
00 00 00 00
00 00 00 83

[rebalance:debug,2025-05-15T18:47:52.468Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 810, stream opaque = 0x83000000
[ns_server:debug,2025-05-15T18:47:52.468Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x32B status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 2B
00 00 00 00
00 00 00 00
00 00 00 84

[rebalance:debug,2025-05-15T18:47:52.468Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 811, stream opaque = 0x84000000
[ns_server:debug,2025-05-15T18:47:52.468Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x32C status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 2C
00 00 00 00
00 00 00 00
00 00 00 85

[rebalance:debug,2025-05-15T18:47:52.468Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 812, stream opaque = 0x85000000
[ns_server:debug,2025-05-15T18:47:52.468Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x32D status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 2D
00 00 00 00
00 00 00 00
00 00 00 86

[rebalance:debug,2025-05-15T18:47:52.468Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 813, stream opaque = 0x86000000
[ns_server:debug,2025-05-15T18:47:52.468Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x32E status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 2E
00 00 00 00
00 00 00 00
00 00 00 87

[rebalance:debug,2025-05-15T18:47:52.468Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 814, stream opaque = 0x87000000
[ns_server:debug,2025-05-15T18:47:52.468Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x32F status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 2F
00 00 00 00
00 00 00 00
00 00 00 88

[rebalance:debug,2025-05-15T18:47:52.468Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 815, stream opaque = 0x88000000
[ns_server:debug,2025-05-15T18:47:52.468Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x330 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 30
00 00 00 00
00 00 00 00
00 00 00 89

[rebalance:debug,2025-05-15T18:47:52.468Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 816, stream opaque = 0x89000000
[ns_server:debug,2025-05-15T18:47:52.468Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x331 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 31
00 00 00 00
00 00 00 00
00 00 00 8A

[rebalance:debug,2025-05-15T18:47:52.468Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 817, stream opaque = 0x8A000000
[ns_server:debug,2025-05-15T18:47:52.469Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x332 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 32
00 00 00 00
00 00 00 00
00 00 00 8B

[rebalance:debug,2025-05-15T18:47:52.469Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 818, stream opaque = 0x8B000000
[ns_server:debug,2025-05-15T18:47:52.469Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x333 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 33
00 00 00 00
00 00 00 00
00 00 00 8C

[rebalance:debug,2025-05-15T18:47:52.469Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 819, stream opaque = 0x8C000000
[ns_server:debug,2025-05-15T18:47:52.469Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x334 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 34
00 00 00 00
00 00 00 00
00 00 00 8D

[rebalance:debug,2025-05-15T18:47:52.469Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 820, stream opaque = 0x8D000000
[ns_server:debug,2025-05-15T18:47:52.469Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x335 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 35
00 00 00 00
00 00 00 00
00 00 00 8E

[rebalance:debug,2025-05-15T18:47:52.469Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 821, stream opaque = 0x8E000000
[ns_server:debug,2025-05-15T18:47:52.469Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x336 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 36
00 00 00 00
00 00 00 00
00 00 00 8F

[rebalance:debug,2025-05-15T18:47:52.469Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 822, stream opaque = 0x8F000000
[ns_server:debug,2025-05-15T18:47:52.470Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x337 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 37
00 00 00 00
00 00 00 00
00 00 00 90

[rebalance:debug,2025-05-15T18:47:52.470Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 823, stream opaque = 0x90000000
[ns_server:debug,2025-05-15T18:47:52.470Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x338 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 38
00 00 00 00
00 00 00 00
00 00 00 91

[rebalance:debug,2025-05-15T18:47:52.470Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 824, stream opaque = 0x91000000
[ns_server:debug,2025-05-15T18:47:52.470Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x339 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 39
00 00 00 00
00 00 00 00
00 00 00 92

[rebalance:debug,2025-05-15T18:47:52.470Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 825, stream opaque = 0x92000000
[ns_server:debug,2025-05-15T18:47:52.470Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x33A status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 3A
00 00 00 00
00 00 00 00
00 00 00 93

[rebalance:debug,2025-05-15T18:47:52.470Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 826, stream opaque = 0x93000000
[ns_server:debug,2025-05-15T18:47:52.470Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x33B status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 3B
00 00 00 00
00 00 00 00
00 00 00 94

[rebalance:debug,2025-05-15T18:47:52.470Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 827, stream opaque = 0x94000000
[ns_server:debug,2025-05-15T18:47:52.470Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x33C status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 3C
00 00 00 00
00 00 00 00
00 00 00 95

[rebalance:debug,2025-05-15T18:47:52.470Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 828, stream opaque = 0x95000000
[ns_server:debug,2025-05-15T18:47:52.470Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x33D status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 3D
00 00 00 00
00 00 00 00
00 00 00 96

[rebalance:debug,2025-05-15T18:47:52.470Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 829, stream opaque = 0x96000000
[ns_server:debug,2025-05-15T18:47:52.470Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x33E status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 3E
00 00 00 00
00 00 00 00
00 00 00 97

[rebalance:debug,2025-05-15T18:47:52.470Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 830, stream opaque = 0x97000000
[ns_server:debug,2025-05-15T18:47:52.471Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x33F status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 3F
00 00 00 00
00 00 00 00
00 00 00 98

[rebalance:debug,2025-05-15T18:47:52.471Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 831, stream opaque = 0x98000000
[ns_server:debug,2025-05-15T18:47:52.471Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x340 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 40
00 00 00 00
00 00 00 00
00 00 00 99

[rebalance:debug,2025-05-15T18:47:52.471Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 832, stream opaque = 0x99000000
[ns_server:debug,2025-05-15T18:47:52.471Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x341 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 41
00 00 00 00
00 00 00 00
00 00 00 9A

[rebalance:debug,2025-05-15T18:47:52.471Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 833, stream opaque = 0x9A000000
[ns_server:debug,2025-05-15T18:47:52.471Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x342 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 42
00 00 00 00
00 00 00 00
00 00 00 9B

[rebalance:debug,2025-05-15T18:47:52.471Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 834, stream opaque = 0x9B000000
[ns_server:debug,2025-05-15T18:47:52.471Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x343 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 43
00 00 00 00
00 00 00 00
00 00 00 9C

[rebalance:debug,2025-05-15T18:47:52.471Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 835, stream opaque = 0x9C000000
[ns_server:debug,2025-05-15T18:47:52.471Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x344 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 44
00 00 00 00
00 00 00 00
00 00 00 9D

[rebalance:debug,2025-05-15T18:47:52.471Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 836, stream opaque = 0x9D000000
[ns_server:debug,2025-05-15T18:47:52.471Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x345 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 45
00 00 00 00
00 00 00 00
00 00 00 9E

[rebalance:debug,2025-05-15T18:47:52.471Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 837, stream opaque = 0x9E000000
[ns_server:debug,2025-05-15T18:47:52.471Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x346 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 46
00 00 00 00
00 00 00 00
00 00 00 9F

[rebalance:debug,2025-05-15T18:47:52.471Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 838, stream opaque = 0x9F000000
[ns_server:debug,2025-05-15T18:47:52.471Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x347 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 47
00 00 00 00
00 00 00 00
00 00 00 A0

[rebalance:debug,2025-05-15T18:47:52.471Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 839, stream opaque = 0xA0000000
[ns_server:debug,2025-05-15T18:47:52.472Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x348 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 48
00 00 00 00
00 00 00 00
00 00 00 A1

[rebalance:debug,2025-05-15T18:47:52.472Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 840, stream opaque = 0xA1000000
[ns_server:debug,2025-05-15T18:47:52.472Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x349 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 49
00 00 00 00
00 00 00 00
00 00 00 A2

[rebalance:debug,2025-05-15T18:47:52.472Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 841, stream opaque = 0xA2000000
[ns_server:debug,2025-05-15T18:47:52.472Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x34A status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 4A
00 00 00 00
00 00 00 00
00 00 00 A3

[rebalance:debug,2025-05-15T18:47:52.472Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 842, stream opaque = 0xA3000000
[ns_server:debug,2025-05-15T18:47:52.472Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x34B status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 4B
00 00 00 00
00 00 00 00
00 00 00 A4

[rebalance:debug,2025-05-15T18:47:52.472Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 843, stream opaque = 0xA4000000
[ns_server:debug,2025-05-15T18:47:52.472Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x34C status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 4C
00 00 00 00
00 00 00 00
00 00 00 A5

[rebalance:debug,2025-05-15T18:47:52.472Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 844, stream opaque = 0xA5000000
[ns_server:debug,2025-05-15T18:47:52.472Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x34D status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 4D
00 00 00 00
00 00 00 00
00 00 00 A6

[rebalance:debug,2025-05-15T18:47:52.472Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 845, stream opaque = 0xA6000000
[ns_server:debug,2025-05-15T18:47:52.472Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x34E status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 4E
00 00 00 00
00 00 00 00
00 00 00 A7

[rebalance:debug,2025-05-15T18:47:52.472Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 846, stream opaque = 0xA7000000
[ns_server:debug,2025-05-15T18:47:52.473Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x34F status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 4F
00 00 00 00
00 00 00 00
00 00 00 A8

[rebalance:debug,2025-05-15T18:47:52.473Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 847, stream opaque = 0xA8000000
[ns_server:debug,2025-05-15T18:47:52.473Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x350 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 50
00 00 00 00
00 00 00 00
00 00 00 A9

[rebalance:debug,2025-05-15T18:47:52.473Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 848, stream opaque = 0xA9000000
[ns_server:debug,2025-05-15T18:47:52.473Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x351 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 51
00 00 00 00
00 00 00 00
00 00 00 AA

[rebalance:debug,2025-05-15T18:47:52.473Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 849, stream opaque = 0xAA000000
[ns_server:debug,2025-05-15T18:47:52.474Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x352 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 52
00 00 00 00
00 00 00 00
00 00 00 AB

[rebalance:debug,2025-05-15T18:47:52.474Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 850, stream opaque = 0xAB000000
[ns_server:debug,2025-05-15T18:47:52.474Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x353 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 53
00 00 00 00
00 00 00 00
00 00 00 AC

[rebalance:debug,2025-05-15T18:47:52.474Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 851, stream opaque = 0xAC000000
[ns_server:debug,2025-05-15T18:47:52.474Z,ns_1@172.19.0.4:<0.4470.0>:dcp_proxy:handle_packet:170]Proxy packet: RESPONSE: 0x51 (dcp_add_stream) vbucket = 0 opaque = 0x354 status = 0x0 (success)
81 51 00 00
04 00 00 00
00 00 00 04
00 00 03 54
00 00 00 00
00 00 00 00
00 00 00 AD

[rebalance:debug,2025-05-15T18:47:52.474Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:handle_packet:73]Stream has been added for partition 852, stream opaque = 0xAD000000
[ns_server:debug,2025-05-15T18:47:52.474Z,ns_1@172.19.0.4:<0.4470.0>:dcp_consumer_conn:maybe_reply_setup_streams:499]Setup stream request completed with ok. Moving to idle state
[ns_server:info,2025-05-15T18:47:52.476Z,ns_1@172.19.0.4:ns_memcached-doom-scrolling<0.4260.0>:ns_memcached:handle_call:370]Enabling traffic to bucket "doom-scrolling"
[ns_server:info,2025-05-15T18:47:52.476Z,ns_1@172.19.0.4:ns_memcached-doom-scrolling<0.4260.0>:ns_memcached:handle_call:374]Bucket "doom-scrolling" marked as warmed in 1 seconds
[ns_server:debug,2025-05-15T18:47:52.492Z,ns_1@172.19.0.4:dcp_traffic_monitor<0.2118.0>:dcp_traffic_monitor:update_bucket:148]Saw that bucket "doom-scrolling" became alive on node 'ns_1@db3.lan'
[ns_server:debug,2025-05-15T18:47:52.501Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@172.19.0.4',buckets_with_data}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                               51})
[{"doom-scrolling",<<"f70f8d64d14cf7513107bff35eb6561d">>}]
[ns_server:debug,2025-05-15T18:47:52.501Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@db3.lan',buckets_with_data}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                            52})
[{"doom-scrolling",<<"f70f8d64d14cf7513107bff35eb6561d">>}]
[ns_server:debug,2025-05-15T18:47:52.501Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@db2.lan',buckets_with_data}, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                                            53})
[{"doom-scrolling",<<"f70f8d64d14cf7513107bff35eb6561d">>}]
[ns_server:info,2025-05-15T18:47:52.700Z,ns_1@172.19.0.4:ns_doctor<0.2292.0>:ns_doctor:update_status:316]The following buckets became ready on node 'ns_1@172.19.0.4': ["doom-scrolling"]
[ns_server:info,2025-05-15T18:47:52.704Z,ns_1@172.19.0.4:ns_doctor<0.2292.0>:ns_doctor:update_status:316]The following buckets became ready on node 'ns_1@db3.lan': ["doom-scrolling"]
[ns_server:info,2025-05-15T18:47:52.710Z,ns_1@172.19.0.4:ns_doctor<0.2292.0>:ns_doctor:update_status:316]The following buckets became ready on node 'ns_1@db2.lan': ["doom-scrolling"]
[ns_server:debug,2025-05-15T18:47:54.478Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/query/migration/CBO_STATS/state">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554074}}]}|
 <<"{\"node\":\"172.19.0.4:8091\",\"state\":\"migrated\",\"when\":\"2025-05-15T18:47:54.475750678Z\"}">>]
[ns_server:debug,2025-05-15T18:47:54.478Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/query/migration/UDF/state">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554074}}]}|
 <<"{\"node\":\"172.19.0.4:8091\",\"state\":\"migrated\",\"when\":\"2025-05-15T18:47:54.475987011Z\"}">>]
[ns_server:debug,2025-05-15T18:47:54.479Z,ns_1@172.19.0.4:<0.2481.0>:menelaus_web:check_bucket_uuid:1343]Attempt to access non existent bucket "N1QL_SYSTEM_BUCKET"
[ns_server:debug,2025-05-15T18:47:54.486Z,ns_1@172.19.0.4:ns_config_rep<0.545.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([{metakv,<<"/query/migration/CBO_STATS/state">>},
                               {metakv,<<"/query/migration/UDF/state">>}]..)
[ns_server:debug,2025-05-15T18:47:54.830Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/indexing/ddl/commandToken/create/17514355370907628042/0">>} ->
[{'_vclock',[{<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554074}}]}|
 <<"{\"DefnId\":17514355370907628042,\"BucketUUID\":\"f70f8d64d14cf7513107bff35eb6561d\",\"ScopeId\":\"8\",\"CollectionId\":\"9\",\"Definitions\":{\"7a33d46fd1976e65466c5efc8b45ef2e\":[{\"defnId\":17514355370907628042,\"name\":\"#primary\",\"using\":\"GSI\",\"bucket\":\"doom-scrolling\",\"isPrimary\":true,\"exprType\":\"N1QL\",\"partitionScheme\":\"SINGLE\",\"numReplica\":1,\"NumReplica2\":{\"HasValue\":true,\"Base\":1,\"Incr\":0,\"Decr\":0},"...>>]
[ns_server:debug,2025-05-15T18:47:55.106Z,ns_1@172.19.0.4:ns_config_rep<0.545.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([{metakv,
                                   <<"/indexing/ddl/commandToken/create/17514355370907628042/0">>}]..)
[ns_server:debug,2025-05-15T18:47:55.106Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/indexing/ddl/commandToken/create/17514355370907628042/0">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554075}},
             {<<"7a33d46fd1976e65466c5efc8b45ef2e">>,{1,63914554074}}]}|
 '_deleted']
[ns_server:info,2025-05-15T18:48:00.432Z,ns_1@172.19.0.4:ns_config_rep<0.545.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@db2.lan'
[ns_server:debug,2025-05-15T18:48:01.805Z,ns_1@172.19.0.4:ldap_auth_cache<0.408.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:info,2025-05-15T18:48:10.683Z,ns_1@172.19.0.4:ns_config_rep<0.545.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@db3.lan'
[ns_server:debug,2025-05-15T18:48:19.675Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:debug,2025-05-15T18:48:19.676Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:48:19.676Z,ns_1@172.19.0.4:<0.6386.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:48:19.680Z,ns_1@172.19.0.4:<0.6388.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 780692, disk size is 6060065
[ns_server:debug,2025-05-15T18:48:19.680Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:48:19.680Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:48:19.681Z,ns_1@172.19.0.4:<0.6389.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:48:19.682Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:48:19.682Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:48:25.089Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{5,63914554105}}]}|
 <<"{\"indexer.plasma.backIndex.enableInMemoryCompression\":true,\"indexer.plasma.backIndex.enablePageBloomFilter\":true,\"indexer.plasma.mainIndex.enableInMemoryCompression\":true,\"indexer.settings.allow_large_keys\":true,\"indexer.settings.bufferPoolBlockSize\":16384,\"indexer.settings.build.batch_size\":5,\"indexer.settings.compaction.abort_exceed_interval\":false,\"indexer.settings.compaction.check_"...>>]
[ns_server:debug,2025-05-15T18:48:25.090Z,ns_1@172.19.0.4:ns_config_rep<0.545.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([{metakv,<<"/indexing/settings/config">>}]..)
[ns_server:debug,2025-05-15T18:48:25.092Z,ns_1@172.19.0.4:ns_config_log<0.284.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/indexing/settings/config/features/PlasmaInMemoryCompression">>} ->
[{'_vclock',[{<<"28569ac00b9c1d7c50e39741027d428c">>,{1,63914554105}}]}|
 <<"{}">>]
[ns_server:debug,2025-05-15T18:48:25.092Z,ns_1@172.19.0.4:ns_config_rep<0.545.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([{metakv,
                                   <<"/indexing/settings/config/features/PlasmaInMemoryCompression">>}]..)
[ns_server:debug,2025-05-15T18:48:27.157Z,ns_1@172.19.0.4:ns_audit<0.696.0>:ns_audit:handle_call:168]Audit auth_failure: [{local,{[{ip,<<"172.19.0.4">>},{port,8091}]}},
                     {remote,{[{ip,<<"172.19.0.1">>},{port,60564}]}},
                     {timestamp,<<"2025-05-15T18:48:27.156Z">>},
                     {raw_url,<<"<ud>/pools/default?waitChange=0</ud>">>}]
[ns_server:debug,2025-05-15T18:48:29.143Z,ns_1@172.19.0.4:ns_audit<0.696.0>:ns_audit:handle_call:168]Audit auth_failure: [{local,{[{ip,<<"172.19.0.4">>},{port,8091}]}},
                     {remote,{[{ip,<<"172.19.0.1">>},{port,64834}]}},
                     {timestamp,<<"2025-05-15T18:48:29.143Z">>},
                     {raw_url,<<"<ud>/pools</ud>">>}]
[ns_server:debug,2025-05-15T18:48:29.188Z,ns_1@172.19.0.4:ns_audit<0.696.0>:ns_audit:handle_call:168]Audit auth_failure: [{local,{[{ip,<<"172.19.0.4">>},{port,8091}]}},
                     {remote,{[{ip,<<"172.19.0.1">>},{port,64834}]}},
                     {timestamp,<<"2025-05-15T18:48:29.188Z">>},
                     {raw_url,<<"<ud>/pools</ud>">>}]
[ns_server:info,2025-05-15T18:48:38.100Z,ns_1@172.19.0.4:ns_config_rep<0.545.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@db3.lan'
[error_logger:info,2025-05-15T18:48:41.754Z,ns_1@172.19.0.4:alarm_handler<0.133.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
    alarm_handler: {set,{system_memory_high_watermark,[]}}
[ns_server:debug,2025-05-15T18:48:49.682Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:debug,2025-05-15T18:48:49.684Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:48:49.685Z,ns_1@172.19.0.4:<0.8249.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:48:49.687Z,ns_1@172.19.0.4:<0.8251.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 780692, disk size is 6060065
[ns_server:debug,2025-05-15T18:48:49.688Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:48:49.688Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:48:49.689Z,ns_1@172.19.0.4:<0.8252.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:48:49.690Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:48:49.690Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:49:16.807Z,ns_1@172.19.0.4:ldap_auth_cache<0.408.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2025-05-15T18:49:19.689Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:debug,2025-05-15T18:49:19.692Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:49:19.692Z,ns_1@172.19.0.4:<0.10077.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:49:19.695Z,ns_1@172.19.0.4:<0.10079.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 780692, disk size is 6060065
[ns_server:debug,2025-05-15T18:49:19.695Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:49:19.695Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:49:19.697Z,ns_1@172.19.0.4:<0.10080.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:49:19.698Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:49:19.698Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:49:30.541Z,ns_1@172.19.0.4:ns_config_rep<0.545.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@db3.lan'
[ns_server:info,2025-05-15T18:49:43.695Z,ns_1@172.19.0.4:ns_config_rep<0.545.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@db2.lan'
[ns_server:debug,2025-05-15T18:49:49.696Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:49:49.698Z,ns_1@172.19.0.4:<0.11910.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:49:49.698Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:debug,2025-05-15T18:49:49.700Z,ns_1@172.19.0.4:<0.11912.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 780692, disk size is 6060065
[ns_server:debug,2025-05-15T18:49:49.700Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:49:49.700Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:49:49.703Z,ns_1@172.19.0.4:<0.11913.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:49:49.703Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:49:49.703Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:49:57.393Z,ns_1@172.19.0.4:ns_config_rep<0.545.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@db2.lan'
[ns_server:debug,2025-05-15T18:50:19.702Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:50:19.704Z,ns_1@172.19.0.4:<0.13738.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:50:19.705Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:debug,2025-05-15T18:50:19.706Z,ns_1@172.19.0.4:<0.13740.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 780692, disk size is 6060065
[ns_server:debug,2025-05-15T18:50:19.706Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:50:19.706Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:50:19.707Z,ns_1@172.19.0.4:<0.13741.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:50:19.708Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:50:19.708Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:50:31.767Z,ns_1@172.19.0.4:ns_config_rep<0.545.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@db3.lan'
[ns_server:debug,2025-05-15T18:50:31.811Z,ns_1@172.19.0.4:ldap_auth_cache<0.408.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2025-05-15T18:50:49.711Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:debug,2025-05-15T18:50:49.714Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:50:49.715Z,ns_1@172.19.0.4:<0.15567.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:50:49.717Z,ns_1@172.19.0.4:<0.15569.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 780692, disk size is 6060065
[ns_server:debug,2025-05-15T18:50:49.717Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:50:49.717Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:50:49.719Z,ns_1@172.19.0.4:<0.15570.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:50:49.720Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:50:49.720Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:50:52.850Z,ns_1@172.19.0.4:ns_config_rep<0.545.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@db3.lan'
[ns_server:info,2025-05-15T18:51:08.555Z,ns_1@172.19.0.4:ns_config_rep<0.545.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@db2.lan'
[ns_server:debug,2025-05-15T18:51:19.720Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:51:19.721Z,ns_1@172.19.0.4:<0.17395.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:51:19.721Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:debug,2025-05-15T18:51:19.722Z,ns_1@172.19.0.4:<0.17397.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 780692, disk size is 6060065
[ns_server:debug,2025-05-15T18:51:19.722Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:51:19.722Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:51:19.724Z,ns_1@172.19.0.4:<0.17398.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:51:19.724Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:51:19.724Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:51:46.813Z,ns_1@172.19.0.4:ldap_auth_cache<0.408.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:info,2025-05-15T18:51:47.143Z,ns_1@172.19.0.4:ns_config_rep<0.545.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@db3.lan'
[ns_server:debug,2025-05-15T18:51:49.724Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:debug,2025-05-15T18:51:49.734Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:51:49.734Z,ns_1@172.19.0.4:<0.19228.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:51:49.737Z,ns_1@172.19.0.4:<0.19231.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 780692, disk size is 6060065
[ns_server:debug,2025-05-15T18:51:49.738Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:51:49.738Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:51:49.742Z,ns_1@172.19.0.4:<0.19229.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:51:49.742Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:51:49.742Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:52:19.741Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:52:19.743Z,ns_1@172.19.0.4:<0.21030.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:52:19.743Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:debug,2025-05-15T18:52:19.746Z,ns_1@172.19.0.4:<0.21032.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 780692, disk size is 6060065
[ns_server:debug,2025-05-15T18:52:19.747Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:52:19.747Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:52:19.749Z,ns_1@172.19.0.4:<0.21033.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:52:19.749Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:52:19.749Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:52:22.323Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: ns_config_vclock_ts, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                        65})
63914554042
[ns_server:debug,2025-05-15T18:52:22.324Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: ns_config_purger, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                     65})
'ns_1@172.19.0.4'
[ns_server:debug,2025-05-15T18:52:22.324Z,ns_1@172.19.0.4:tombstone_keeper<0.277.0>:tombstone_keeper:handle_call:49]Refreshed with timestamps [{ns_config_vclock_ts,63914554042}]
[ns_server:debug,2025-05-15T18:52:22.324Z,ns_1@172.19.0.4:ns_config_rep<0.545.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 34 us
[ns_server:debug,2025-05-15T18:52:22.331Z,ns_1@172.19.0.4:ns_config_rep<0.545.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 600 us
[ns_server:debug,2025-05-15T18:52:22.332Z,ns_1@172.19.0.4:ns_config_rep<0.545.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([rbac_upgrade]..)
[ns_server:debug,2025-05-15T18:52:22.333Z,ns_1@172.19.0.4:ns_config_rep<0.545.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 4 us
[ns_server:debug,2025-05-15T18:52:22.360Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: ns_config_purge_ts, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                       66})
63914554042
[ns_server:debug,2025-05-15T18:52:22.360Z,ns_1@172.19.0.4:tombstone_keeper<0.277.0>:tombstone_keeper:handle_call:49]Refreshed with timestamps [{ns_config_purge_ts,63914554042},
                           {ns_config_vclock_ts,63914554042}]
[ns_server:debug,2025-05-15T18:52:22.361Z,ns_1@172.19.0.4:tombstone_agent<0.555.0>:tombstone_agent:purge:182]Purged 1 ns_config tombstone(s) up to timestamp 63914554042. Tombstones:
[rbac_upgrade]
[ns_server:debug,2025-05-15T18:52:29.146Z,ns_1@172.19.0.4:compiled_roles_cache<0.454.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"<ud>jaba_admin</ud>",admin}
[ns_server:debug,2025-05-15T18:52:29.150Z,ns_1@172.19.0.4:ns_audit<0.696.0>:ns_audit:handle_call:168]Audit login_success: [{local,{[{ip,<<"172.19.0.4">>},{port,8091}]}},
                      {remote,{[{ip,<<"172.19.0.1">>},{port,63616}]}},
                      {sessionid,<<"dbde71207c2ff88537838bf3679e170bc5c1ba4f">>},
                      {real_userid,{[{domain,builtin},
                                     {user,<<"<ud>jaba_admin</ud>">>}]}},
                      {timestamp,<<"2025-05-15T18:52:29.147Z">>},
                      {roles,[<<"admin">>]}]
[ns_server:debug,2025-05-15T18:52:29.224Z,ns_1@172.19.0.4:users_storage<0.444.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,{auth,'_'},'_','_'},[],['$_']}],
                               100}
[ns_server:info,2025-05-15T18:52:40.321Z,ns_1@172.19.0.4:ns_config_rep<0.545.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@db3.lan'
[ns_server:debug,2025-05-15T18:52:49.751Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:debug,2025-05-15T18:52:49.754Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:52:49.754Z,ns_1@172.19.0.4:<0.22901.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:52:49.757Z,ns_1@172.19.0.4:<0.22903.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 780692, disk size is 6060065
[ns_server:debug,2025-05-15T18:52:49.758Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:52:49.758Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:52:49.760Z,ns_1@172.19.0.4:<0.22904.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:52:49.760Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:52:49.760Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:52:54.671Z,ns_1@172.19.0.4:ns_config_rep<0.545.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@db2.lan'
[ns_server:debug,2025-05-15T18:53:01.815Z,ns_1@172.19.0.4:ldap_auth_cache<0.408.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:info,2025-05-15T18:53:11.591Z,ns_1@172.19.0.4:ns_config_rep<0.545.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@db3.lan'
[ns_server:debug,2025-05-15T18:53:19.759Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:53:19.761Z,ns_1@172.19.0.4:<0.24727.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:53:19.761Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:debug,2025-05-15T18:53:19.765Z,ns_1@172.19.0.4:<0.24729.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 780692, disk size is 6060065
[ns_server:debug,2025-05-15T18:53:19.765Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:53:19.765Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:53:19.766Z,ns_1@172.19.0.4:<0.24730.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:53:19.767Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:53:19.767Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:53:22.386Z,ns_1@172.19.0.4:tombstone_keeper<0.277.0>:tombstone_keeper:handle_call:49]Refreshed with timestamps [{ns_config_purge_ts,63914554042},
                           {ns_config_vclock_ts,63914554102}]
[ns_server:debug,2025-05-15T18:53:22.386Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: ns_config_vclock_ts, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                        69})
63914554102
[ns_server:debug,2025-05-15T18:53:22.386Z,ns_1@172.19.0.4:ns_config_rep<0.545.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 8 us
[ns_server:debug,2025-05-15T18:53:22.386Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: ns_config_purger, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                     69})
'ns_1@172.19.0.4'
[ns_server:debug,2025-05-15T18:53:22.389Z,ns_1@172.19.0.4:ns_config_rep<0.545.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 284 us
[ns_server:debug,2025-05-15T18:53:22.393Z,ns_1@172.19.0.4:ns_config_rep<0.545.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([{metakv,
                                   <<"/indexing/ddl/commandToken/create/17514355370907628042/0">>},
                               {metakv,
                                   <<"/indexing/rebalance/RebalanceToken">>}]..)
[ns_server:debug,2025-05-15T18:53:22.394Z,ns_1@172.19.0.4:ns_config_rep<0.545.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 33 us
[ns_server:debug,2025-05-15T18:53:22.419Z,ns_1@172.19.0.4:chronicle_kv_log<0.502.0>:chronicle_kv_log:log:59]update (key: ns_config_purge_ts, rev: {<<"5aab03dbecad06b50ffb474da59a00c9">>,
                                       70})
63914554102
[ns_server:debug,2025-05-15T18:53:22.419Z,ns_1@172.19.0.4:tombstone_keeper<0.277.0>:tombstone_keeper:handle_call:49]Refreshed with timestamps [{ns_config_purge_ts,63914554102},
                           {ns_config_vclock_ts,63914554102}]
[ns_server:debug,2025-05-15T18:53:22.419Z,ns_1@172.19.0.4:tombstone_agent<0.555.0>:tombstone_agent:purge:182]Purged 2 ns_config tombstone(s) up to timestamp 63914554102. Tombstones:
[{metakv,<<"/indexing/rebalance/RebalanceToken">>},{metakv,<<"/indexing/ddl/commandToken/create/17514355370907628042/0">>}]
[ns_server:debug,2025-05-15T18:53:49.768Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:debug,2025-05-15T18:53:49.769Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:53:49.769Z,ns_1@172.19.0.4:<0.26587.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:53:49.770Z,ns_1@172.19.0.4:<0.26589.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 780692, disk size is 6060065
[ns_server:debug,2025-05-15T18:53:49.771Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:53:49.771Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:53:49.772Z,ns_1@172.19.0.4:<0.26590.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:53:49.772Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:53:49.773Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:54:03.083Z,ns_1@172.19.0.4:ns_config_rep<0.545.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@db2.lan'
[ns_server:debug,2025-05-15T18:54:16.831Z,ns_1@172.19.0.4:ldap_auth_cache<0.408.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2025-05-15T18:54:16.837Z,ns_1@172.19.0.4:roles_cache<0.458.0>:active_cache:renew:238]Starting roles_cache cache renewal
[ns_server:debug,2025-05-15T18:54:16.837Z,ns_1@172.19.0.4:roles_cache<0.458.0>:active_cache:renew:244]roles_cache cache renewal process originated 0 queries
[ns_server:debug,2025-05-15T18:54:19.773Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:54:19.774Z,ns_1@172.19.0.4:<0.28415.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:54:19.774Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:debug,2025-05-15T18:54:19.776Z,ns_1@172.19.0.4:<0.28417.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 780692, disk size is 6060065
[ns_server:debug,2025-05-15T18:54:19.776Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:54:19.776Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:54:19.778Z,ns_1@172.19.0.4:<0.28418.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:54:19.778Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:54:19.778Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:54:34.221Z,ns_1@172.19.0.4:ns_config_rep<0.545.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@db3.lan'
[ns_server:debug,2025-05-15T18:54:34.756Z,ns_1@172.19.0.4:ns_gc_runner<0.929.0>:ns_gc_runner:handle_info:125]GC populating new pid list of size=728, prevMaxGcDuration=114799 us
[ns_server:debug,2025-05-15T18:54:49.778Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:debug,2025-05-15T18:54:49.779Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:54:49.779Z,ns_1@172.19.0.4:<0.30261.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:54:49.781Z,ns_1@172.19.0.4:<0.30263.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 780692, disk size is 6060065
[ns_server:debug,2025-05-15T18:54:49.782Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:54:49.782Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:54:49.783Z,ns_1@172.19.0.4:<0.30264.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:54:49.783Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:54:49.783Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:55:19.784Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:55:19.785Z,ns_1@172.19.0.4:<0.32089.0>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:55:19.785Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:debug,2025-05-15T18:55:19.794Z,ns_1@172.19.0.4:<0.32091.0>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 780692, disk size is 6060065
[ns_server:debug,2025-05-15T18:55:19.795Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:55:19.795Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:55:19.808Z,ns_1@172.19.0.4:<0.32092.0>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:55:19.809Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:55:19.809Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:55:20.779Z,ns_1@172.19.0.4:ns_config_rep<0.545.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@db3.lan'
[ns_server:debug,2025-05-15T18:55:31.838Z,ns_1@172.19.0.4:ldap_auth_cache<0.408.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2025-05-15T18:55:49.796Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:55:49.798Z,ns_1@172.19.0.4:<0.1158.1>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:55:49.800Z,ns_1@172.19.0.4:<0.1160.1>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 780692, disk size is 6060065
[ns_server:debug,2025-05-15T18:55:49.801Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:55:49.801Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:55:49.810Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:55:49.815Z,ns_1@172.19.0.4:<0.1161.1>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:55:49.815Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:55:49.815Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:56:03.691Z,ns_1@172.19.0.4:ns_config_rep<0.545.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@db3.lan'
[ns_server:debug,2025-05-15T18:56:19.804Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:56:19.805Z,ns_1@172.19.0.4:<0.2986.1>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:56:19.806Z,ns_1@172.19.0.4:<0.2988.1>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 780692, disk size is 6060065
[ns_server:debug,2025-05-15T18:56:19.806Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:56:19.806Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:56:19.818Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:56:19.826Z,ns_1@172.19.0.4:<0.2989.1>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:56:19.826Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:56:19.826Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:info,2025-05-15T18:56:23.502Z,ns_1@172.19.0.4:ns_config_rep<0.545.0>:ns_config_rep:pull_one_node:422]Pulling config from: 'ns_1@db3.lan'
[ns_server:debug,2025-05-15T18:56:46.836Z,ns_1@172.19.0.4:roles_cache<0.458.0>:active_cache:cleanup:258]Cache roles_cache cleanup: 0/0 records deleted
[ns_server:debug,2025-05-15T18:56:46.838Z,ns_1@172.19.0.4:ldap_auth_cache<0.408.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2025-05-15T18:56:49.808Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_kv) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:56:49.808Z,ns_1@172.19.0.4:<0.4824.1>:compaction_daemon:spawn_scheduled_kv_compactor:482]Start compaction of vbuckets for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:56:49.809Z,ns_1@172.19.0.4:<0.4826.1>:compaction_daemon:bucket_needs_compaction:983]`doom-scrolling` data size is 780692, disk size is 6060065
[ns_server:debug,2025-05-15T18:56:49.810Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:56:49.810Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2025-05-15T18:56:49.828Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_scheduler_message:1319]Starting compaction (compact_views) for the following buckets: 
[<<"doom-scrolling">>]
[ns_server:info,2025-05-15T18:56:49.831Z,ns_1@172.19.0.4:<0.4827.1>:compaction_daemon:spawn_scheduled_views_compactor:508]Start compaction of indexes for bucket doom-scrolling with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2025-05-15T18:56:49.831Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_daemon:process_compactors_exit:1360]Finished compaction iteration.
[ns_server:debug,2025-05-15T18:56:49.831Z,ns_1@172.19.0.4:compaction_daemon<0.754.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
